New York is a state in the northeastern United States, and is the 27th-most extensive, fourth-most populous, and seventh-most densely populated U.S. state. New York is bordered by New Jersey and Pennsylvania to the south and Connecticut, Massachusetts, and Vermont to the east. The state has a maritime border in the Atlantic Ocean with Rhode Island, east of Long Island, as well as an international border with the Canadian provinces of Quebec to the north and Ontario to the west and north. The state of New York, with an estimated 19.8 million residents in 2015, is often referred to as New York State to distinguish it from New York City, the state's most populous city and its economic hub.
With an estimated population of 8.55 million in 2015, New York City is the most populous city in the United States and the premier gateway for legal immigration to the United States. The New York City Metropolitan Area is one of the most populous urban agglomerations in the world. New York City is a global city, exerting a significant impact upon commerce, finance, media, art, fashion, research, technology, education, and entertainment, its fast pace defining the term New York minute. The home of the United Nations Headquarters, New York City is an important center for international diplomacy and has been described as the cultural and financial capital of the world, as well as the world's most economically powerful city. New York City makes up over 40% of the population of New York State. Two-thirds of the state's population lives in the New York City Metropolitan Area, and nearly 40% lives on Long Island. Both the state and New York City were named for the 17th-century Duke of York, future King James II of England. The next four most populous cities in the state are Buffalo, Rochester, Yonkers, and Syracuse, while the state capital is Albany.
New York has a diverse geography. The southern part of the state consists of Long Island and several smaller associated islands, as well as New York City and the lower Hudson River Valley, most of which lie within the wider Atlantic Coastal Plain. The large region known as Upstate New York consists of several ranges of the wider Appalachian Mountains, including the Allegheny Plateau and Catskills along New York's Southern Tier, and the Adirondack Mountains, Thousand Islands archipelago, and Saint Lawrence Seaway in the Northeastern lobe of the state. These more mountainous regions are bisected by two major river valleys—the north-south Hudson River Valley and the east-west Mohawk River Valley, which forms the core of the Erie Canal. Western New York is considered part of the Great Lakes Region and straddles Lake Ontario and Lake Erie. Between the two lakes lies Niagara Falls. The central part of the state is dominated by the Finger Lakes, a popular vacation and tourist destination.
New York had been inhabited by tribes of Algonquian and Iroquoian-speaking Native Americans for several hundred years by the time the earliest Europeans came to New York. The first Europeans to arrive were French colonists and Jesuit missionaries who arrived southward from settlements at Montreal for trade and proselytizing. In 1609, the region was claimed by Henry Hudson for the Dutch, who built Fort Nassau in 1614 at the confluence of the Hudson and Mohawk rivers, where the present-day capital of Albany later developed. The Dutch soon also settled New Amsterdam and parts of the Hudson Valley, establishing the colony of New Netherland, a multicultural community from its earliest days and a center of trade and immigration. The British annexed the colony from the Dutch in 1664. The borders of the British colony, the Province of New York, were similar to those of the present-day state.
Many landmarks in New York are well known to both international and domestic visitors, with New York State hosting four of the world's ten most-visited tourist attractions in 2013: Times Square, Central Park, Niagara Falls (shared with Ontario), and Grand Central Terminal. New York is home to the Statue of Liberty, a symbol of the United States and its ideals of freedom, democracy, and opportunity. In the 21st century, New York has emerged as a global node of creativity and entrepreneurship, social tolerance, and environmental sustainability. New York's higher education network comprises approximately 200 colleges and universities, including Columbia University, Cornell University, New York University, and Rockefeller University, which have been ranked among the top 35 in the world.

In 1524, Giovanni da Verrazzano, an Italian explorer in the service of the French crown, explored the Atlantic coast of North America between the Carolinas and Newfoundland, including New York Harbor and Narragansett Bay. On April 17, 1524 Verrazanno entered New York Bay, by way of the strait now called the Narrows into the northern bay which he named Santa Margherita, in honor of the King of France's sister. Verrazzano described it as "a vast coastline with a deep delta in which every kind of ship could pass" and he adds: "that it extends inland for a league and opens up to form a beautiful lake. This vast sheet of water swarmed with native boats". He landed on the tip of Manhattan and possibly on the furthest point of Long Island. Verrazanno's stay was interrupted by a storm which pushed him north towards Martha's Vineyard.
In 1540 French traders from New France built a chateau on Castle Island, within present-day Albany; due to flooding, it was abandoned the next year. In 1614, the Dutch under the command of Hendrick Corstiaensen, rebuilt the French chateau, which they called Fort Nassau. Fort Nassau was the first Dutch settlement in North America, and was located along the Hudson River, also within present-day Albany. The small fort served as a trading post and warehouse. Located on the Hudson River flood plain, the rudimentary "fort" was washed away by flooding in 1617, and abandoned for good after Fort Orange (New Netherland) was built nearby in 1623.

Henry Hudson's 1609 voyage marked the beginning of European involvement with the area. Sailing for the Dutch East India Company and looking for a passage to Asia, he entered the Upper New York Bay on September 11 of that year. Word of his findings encouraged Dutch merchants to explore the coast in search for profitable fur trading with local Native American tribes.
During the 17th century, Dutch trading posts established for the trade of pelts from the Lenape, Iroquois, and other tribes were founded in the colony of New Netherland. The first of these trading posts were Fort Nassau (1614, near present-day Albany); Fort Orange (1624, on the Hudson River just south of the current city of Albany and created to replace Fort Nassau), developing into settlement Beverwijck (1647), and into what became Albany; Fort Amsterdam (1625, to develop into the town New Amsterdam which is present-day New York City); and Esopus, (1653, now Kingston). The success of the patroonship of Rensselaerswyck (1630), which surrounded Albany and lasted until the mid-19th century, was also a key factor in the early success of the colony. The English captured the colony during the Second Anglo-Dutch War and governed it as the Province of New York. The city of New York was recaptured by the Dutch in 1673 during the Third Anglo-Dutch War (1672–1674) and renamed New Orange. It was returned to the English under the terms of the Treaty of Westminster a year later.

The Sons of Liberty were organized in New York City during the 1760s, largely in response to the oppressive Stamp Act passed by the British Parliament in 1765. The Stamp Act Congress met in the city on October 19 of that year, composed of representatives from across the Thirteen Colonies who set the stage for the Continental Congress to follow. The Stamp Act Congress resulted in the Declaration of Rights and Grievances, which was the first written expression by representatives of the Americans of many of the rights and complaints later expressed in the United States Declaration of Independence. This included the right to representative government. At the same time, with strong trading between Britain and the United States on both business and personal levels many New York residents were Loyalists. The Capture of Fort Ticonderoga provided the cannon and gunpowder necessary to force a British withdrawal from the Siege of Boston in 1775.
New York was the only colony to not vote for independence, as the delegates were not authorized to do so. New York then endorsed the Declaration of Independence on July 9, 1776. The New York State Constitution was framed by a convention which assembled at White Plains on July 10, 1776, and after repeated adjournments and changes of location, terminated its labors at Kingston on Sunday evening, April 20, 1777, when the new constitution drafted by John Jay was adopted with but one dissenting vote. It was not submitted to the people for ratification. On July 30, 1777, George Clinton was inaugurated as the first Governor of New York at Kingston.
About one-third of the battles of the American Revolutionary War took place in New York; the first major battle after U.S. independence was declared—and the largest battle of the entire war—was fought in New York at the Battle of Long Island (a.k.a. Battle of Brooklyn) in August 1776. After their victory, the British occupied New York City, making it their military and political base of operations in North America for the duration of the conflict, and consequently the focus of General George Washington's intelligence network. On the notorious British prison ships of Wallabout Bay, more American combatants died of intentional neglect than were killed in combat in every battle of the war, combined. Both sides of combatants lost more soldiers to disease than to outright wounds.The first of two major British armies were captured by the Continental Army at the Battle of Saratoga in 1777, a success that influenced France to ally with the revolutionaries.The state constitution was enacted in 1777. New York became the 11th state to ratify the United States Constitution, on July 26, 1788.

In an attempt to retain their sovereignty and remain an independent nation positioned between the new United States and British North America, four of the Iroquois Nations fought on the side of the British; only the Oneida and their dependents, the Tuscarora, allied themselves with the Americans. In retaliation for attacks on the frontier led by Joseph Brant and Loyalist Mohawk forces, the Sullivan Expedition of 1779 destroyed nearly 50 Iroquois villages, adjacent croplands and winter stores, forcing many refugees to British-held Niagara.
As allies of the British, the Iroquois were forced out of New York, although they had not been part of treaty negotiations. They resettled in Canada after the war and were given land grants by the Crown. In the treaty settlement, the British ceded most Indian lands to the new United States. Because New York made treaty with the Iroquois without getting Congressional approval, some of the land purchases have been subject to land claim suits since the late 20th century by the federally recognized tribes. New York put up more than 5 million acres (20,000 km2) of former Iroquois territory for sale in the years after the Revolutionary War, leading to rapid development in upstate New York. As per the Treaty of Paris, the last vestige of British authority in the former Thirteen Colonies—their troops in New York City—departed in 1783, which was long afterward celebrated as Evacuation Day.

New York City was the national capital under the Articles of Confederation and Perpetual Union, the first government. That organization was found to be insufficient, and prominent New Yorker Alexander Hamilton advocated a new government that would include an executive, national courts, and the power to tax. Hamilton led the Annapolis Convention (1786) that called for the Philadelphia Convention, which drafted the United States Constitution, in which he also took part. The new government was to be a strong federal national government to replace the relatively weaker confederation of individual states. Following heated debate, which included the publication of the now quintessential constitutional interpretation—The Federalist Papers—as a series of installments in New York City newspapers, New York was the 11th state to ratify the United States Constitution, on July 26, 1788. New York remained the national capital under the new constitution until 1790, and was the site of the inauguration of President George Washington, the drafting of United States Bill of Rights, and the first session of the United States Supreme Court. Hamilton's revival of the heavily indebted United States economy after the war and the creation of a national bank significantly contributed to New York City becoming the financial center of the new nation.
Both the Dutch and the British imported African slaves as laborers to the city and colony; New York had the second-highest population of slaves after Charleston, SC. Slavery was extensive in New York City and some agricultural areas. The state passed a law for the gradual abolition of slavery soon after the Revolutionary War, but the last slave in New York was not freed until 1827.

Transportation in western New York was by expensive wagons on muddy roads before canals opened up the rich farm lands to long-distance traffic. Governor DeWitt Clinton promoted the Erie Canal that connected New York City to the Great Lakes, by the Hudson River, the new canal, and the rivers and lakes. Work commenced in 1817, and the Erie Canal opened in 1825. Packet boats pulled by horses on tow paths traveled slowly over the canal carrying passengers and freight. Farm products came in from the Midwest, and finished manufactured moved west. It was an engineering marvel which opened up vast areas of New York to commerce and settlement. It enabled Great Lakes port cities such as Buffalo and Rochester to grow and prosper. It also connected the burgeoning agricultural production of the Midwest and shipping on the Great Lakes, with the port of New York City. Improving transportation, it enabled additional population migration to territories west of New York. After 1850, railroads largely replaced the canal.
New York City was a major ocean port and had extensive traffic importing cotton from the South and exporting manufacturing goods. Nearly half of the state's exports were related to cotton. Southern cotton factors, planters and bankers visited so often that they had favorite hotels. At the same time, activism for abolitionism was strong upstate, where some communities provided stops on the Underground Railroad. Upstate, and New York City, gave strong support for the American Civil War In terms of finances, volunteer soldiers, and supplies. The state provided more than 370,000 soldiers to the Union armies. Over 53,000 New Yorkers died in service, roughly one of every seven who served. However, Irish draft riots in 1862 were a significant embarrassment.

Since the early 19th century, New York City has been the largest port of entry for legal immigration into the United States. In the United States, the federal government did not assume direct jurisdiction for immigration until 1890. Prior to this time, the matter was delegated to the individual states, then via contract between the states and the federal government. Most immigrants to New York would disembark at the bustling docks along the Hudson and East Rivers, in the eventual Lower Manhattan. On May 4, 1847 the New York State Legislature created the Board of Commissioners of Immigration to regulate immigration.
The first permanent immigration depot in New York was established in 1855 at Castle Garden, a converted War of 1812 era fort located within what is now Battery Park, at the tip of Lower Manhattan. The first immigrants to arrive at the new depot were aboard three ships that had just been released from quarantine. Castle Garden served as New York's immigrant depot until it closed on April 18, 1890 when the federal government assumed control over immigration. During that period, more than 8 million immigrants passed through its doors (two out of every three U.S. immigrants).
When the federal government assumed control, it established the Bureau of Immigration, which chose the three-acre Ellis Island in Upper New York Harbor for an entry depot. Already federally controlled, the island had served as an ammunition depot. It was chosen due its relative isolation with proximity to New York City and the rail lines of Jersey City, New Jersey, via a short ferry ride. While the island was being developed and expanded via land reclamation, the federal government operated a temporary depot at the Barge Office at the Battery.
Ellis Island opened on January 1, 1892, and operated as a central immigration center until the National Origins Act was passed in 1924, reducing immigration. After that date, the only immigrants to pass through were displaced persons or war refugees. The island ceased all immigration processing on November 12, 1954 when the last person detained on the island, Norwegian seaman Arne Peterssen, was released. He had overstayed his shore leave and left on the 10:15 a.m. Manhattan-bound ferry to return to his ship.
More than 12 million immigrants passed through Ellis Island between 1892 and 1954. More than 100 million Americans across the United States can trace their ancestry to these immigrants.
Ellis Island was the subject of a contentious and long-running border and jurisdictional dispute between New York State and the State of New Jersey, as both claimed it. The issue was settled in 1998 by the U.S. Supreme Court which ruled that the original 3.3-acre (1.3 ha) island was New York State territory and that the balance of the 27.5 acres (11 ha) added after 1834 by landfill was in New Jersey. The island was added to the National Park Service system in May 1965 by President Lyndon B. Johnson and is still owned by the Federal government as part of the Statue of Liberty National Monument. Ellis Island was opened to the public as a museum of immigration in 1990.

On September 11, 2001, two of four hijacked planes were flown into the Twin Towers of the original World Trade Center in Lower Manhattan, and the towers collapsed. 7 World Trade Center also collapsed due to damage from fires. The other buildings of the World Trade Center complex were damaged beyond repair and demolished soon thereafter. The collapse of the Twin Towers caused extensive damage and resulted in the deaths of 2,753 victims, including 147 aboard the two planes. Since September 11, most of Lower Manhattan has been restored. In the years since, many rescue workers and residents of the area have developed several life-threatening illnesses, and some have died.
A memorial at the site, the National September 11 Memorial & Museum, was opened to the public on September 11, 2011. A permanent museum later opened at the site on March 21, 2014. Upon its completion in 2014, the new One World Trade Center became the tallest skyscraper in the Western Hemisphere, at 1,776 feet (541 m). Other skyscrapers are under construction at the site.

On October 29 and 30, 2012, Hurricane Sandy caused extensive destruction of the state's shorelines, ravaging portions of New York City and Long Island with record-high storm surge, with severe flooding and high winds causing power outages for hundreds of thousands of New Yorkers, and leading to gasoline shortages and disruption of mass transit systems. The storm and its profound effects have prompted the discussion of constructing seawalls and other coastal barriers around the shorelines of New York City and Long Island to minimize the risk from another such future event. This is considered highly probable due to global warming and rise in sea levels.

New York covers 54,555 square miles (141,300 km2) and ranks as the 27th largest state by size. The Great Appalachian Valley dominates eastern New York and contains the Lake Champlain Valley as its northern half and the Hudson Valley as its southern half within the state. The rugged Adirondack Mountains, with vast tracts of wilderness, lie west of the Lake Champlain Valley. The Hudson River begins near Lake Tear of the Clouds and flows south through the eastern part of the state without draining Lakes George or Champlain. Lake George empties at its north end into Lake Champlain, whose northern end extends into Canada, where it drains into the Richelieu River and then ultimately the Saint Lawrence River. Four of New York City's five boroughs are situated on three islands at the mouth of the Hudson River: Manhattan Island; Staten Island; and Long Island, which contains Brooklyn and Queens at its western end.
Most of the southern part of the state rests on the Allegheny Plateau, which extends from the southeastern United States to the Catskill Mountains; the section in New York State is known as the Southern Tier. The Tug Hill region arises as a cuesta east of Lake Ontario. The western section of the state is drained by the Allegheny River and rivers of the Susquehanna and Delaware River systems. The Delaware River Basin Compact, signed in 1961 by New York, New Jersey, Pennsylvania, Delaware, and the federal government, regulates the utilization of water of the Delaware system. The highest elevation in New York is Mount Marcy in the Adirondacks, at 5,344 feet (1,629 meters) above sea level; while the state's lowest point is at sea level, on the Atlantic Ocean.
Much of New York State borders water, as is true for New York City as well. Of New York State's total area, 13.5% consists of water. The state's borders touch (clockwise from the west) two Great Lakes (Lake Erie and Lake Ontario, which are connected by the Niagara River); the provinces of Ontario and Quebec in Canada, with New York and Ontario sharing the Thousand Islands archipelago within the Saint Lawrence River; Lake Champlain; three New England states (Vermont, Massachusetts, and Connecticut); the Atlantic Ocean, and two Mid-Atlantic states, New Jersey and Pennsylvania. In addition, Rhode Island shares a water border with New York. New York is the second largest of the original Thirteen Colonies and is the only state that touches both the Great Lakes and the Atlantic Ocean.
In contrast with New York City's urban landscape, the vast majority of the state's geographic area is dominated by meadows, forests, rivers, farms, mountains, and lakes. New York's Adirondack Park is the largest state park in the United States and is larger than the Yellowstone, Yosemite, Grand Canyon, Glacier, and Olympic National Parks combined. New York established the first state park in the United States at Niagara Falls in 1885. Niagara Falls is shared between New York and Ontario as it flows on the Niagara River from Lake Erie to Lake Ontario.
Upstate and downstate are often used informally to distinguish New York City or its greater metropolitan area from the rest of New York State. The placement of a boundary between the two is a matter of great contention. Unofficial and loosely defined regions of Upstate New York include the Southern Tier, which often includes the counties along the border with Pennsylvania, and the North Country, which can mean anything from the strip along the Canada–US border to everything north of the Mohawk River.

In general, New York has a humid continental climate, though under the Köppen climate classification, New York City has a humid subtropical climate. Weather in New York is heavily influenced by two continental air masses: a warm, humid one from the southwest and a cold, dry one from the northwest.
Downstate New York, comprising New York City, Long Island, and lower portions of the Hudson Valley, has rather warm summers, with some periods of high humidity, and cold, damp winters which, however, are relatively mild compared to temperatures in Upstate New York, secondary to the former region's lower elevation, proximity to the Atlantic Ocean, and relatively lower latitude compared to the latter. Upstate New York experiences warm summers, marred by only occasional, brief intervals of sultry conditions, with long and cold winters. Western New York, particularly the Tug Hill region, receives heavy lake-effect snows, especially during the earlier portions of winter, before the surface of Lake Ontario itself is covered by ice. The summer climate is cool in the Adirondacks, Catskills, and at higher elevations of the Southern Tier.
Summer daytime temperatures usually range from the upper 70s to mid-80s °F (25 to 30 °C), over much of the state. In the majority of winter seasons, a temperature of −13 °F (−25 °C) or lower can be expected in the northern highlands (Northern Plateau) and 5 °F (−15 °C) or colder in the southwestern and east-central highlands of the Southern Tier.
New York ranks 46th among the 50 states in the amount of greenhouse gases generated per person. This relative efficient energy usage is primarily due to the dense, compact settlement in the New York City metropolitan area, and the state population's high rate of mass transit use in this area and between major cities.

Due to its long history, New York has several overlapping and often conflicting definitions of regions within the state. The regions are also not fully definable due to colloquial use of regional labels. The New York State Department of Economic Development provides two distinct definitions of these regions; it divides the state into ten economic regions, which approximately correspond to terminology used by residents:

The department also groups the counties into eleven regions for tourism purposes:

New York has many state parks and two major forest preserves. Adirondack Park, roughly the size of the state of Vermont and the largest state park in the United States, was established in 1892 and given state constitutional protection to remain "forever wild" in 1894. The park is larger than Yellowstone, Everglades, Glacier, and Grand Canyon national parks combined. The thinking that led to the creation of the Park first appeared in George Perkins Marsh's Man and Nature, published in 1864.
The Catskill Park was protected in legislation passed in 1885, which declared that its land was to be conserved and never put up for sale or lease. Consisting of 700,000 acres (2,800 km2) of land, the park is a habitat for deer, minks, and fishers. There are some 400 black bears living in the region. The state operates numerous campgrounds, and there are over 300 miles (480 km) of multi-use trails in the Park.
The Montauk Point State Park boasts the 1797 Montauk Lighthouse, commissioned under President George Washington, which is a major tourist attraction on the easternmost tip of Long Island. Hither Hills park offers camping and is a popular destination with surfcasting sport fishermen.

The State of New York is well represented in the National Park System with 22 national parks, which received 16,349,381 visitors in 2011. In addition, there are 4 National Heritage Areas, 27 National Natural Landmarks, 262 National Historic Landmarks, and 5,379 listings on the National Register of Historic Places.
African Burial Ground National Monument in Lower Manhattan is the only National Monument dedicated to Americans of African ancestry. It preserves a site containing the remains of more than 400 Africans buried during the late 17th and 18th centuries in a portion of what was the largest colonial-era cemetery for people of African descent, both free and enslaved, with an estimated tens of thousands of remains interred. The site's excavation and study were called "the most important historic urban archeological project in the United States."
Fire Island National Seashore is a United States National Seashore that protects a 26-mile (42 km) section of Fire Island, an approximately 30-mile (48 km) long barrier island separated from the mainland of Long Island by the Great South Bay. The island is part of Suffolk County.
Gateway National Recreation Area is more than 26,000 acres (10,522 ha) of water, salt marsh, wetlands, islands, and shoreline at the entrance to New York Harbor, the majority of which lies within New York. Including areas on Long Island and in New Jersey, it covers more area than that of two Manhattan Islands.
General Grant National Memorial is the final resting place of President Ulysses S. Grant and is the largest mausoleum in North America.
Hamilton Grange National Memorial preserves the home of Alexander Hamilton, Caribbean immigrant and orphan who rose to be a United States founding father and associate of George Washington.
Home of Franklin D. Roosevelt National Historic Site, established in 1945, preserves the Springwood estate in Hyde Park, New York. Springwood was the birthplace, lifelong home, and burial place of the 32nd President of the United States, Franklin D. Roosevelt.
Niagara Falls National Heritage Area was designated by Congress in 2008; it stretches from the western boundary of Wheatfield, New York to the mouth of the Niagara River on Lake Ontario, including the communities of Niagara Falls, Youngstown, and Lewiston. It includes Niagara Falls State Park and Colonial Niagara Historic District. It is managed in collaboration with the state.
Saratoga National Historical Park preserves the site of the Battles of Saratoga, the first significant American military victory of the American Revolutionary War. In 1777, American forces defeated a major British Army, which led France to recognize the independence of the United States, and enter the war as a decisive military ally of the struggling Americans.
Statue of Liberty National Monument includes Ellis Island and the Statue of Liberty. The statue, designed by Frédéric Bartholdi, was a gift from France to the United States to mark the Centennial of the American Declaration of Independence; it was dedicated in New York Harbor on October 28, 1886. It has since become an icon of the United States and the concepts of democracy and freedom.
Stonewall National Monument, in the Greenwich Village neighborhood of Lower Manhattan, is the first U.S. National Monument dedicated to LGBTQ rights, designated on June 24, 2016. The monument comprises the Stonewall Inn, commonly recognized to be the cradle of the gay liberation movement as the site of the 1969 Stonewall Riots; the adjacent Christopher Park; and surrounding streets and sidewalks.
Theodore Roosevelt Birthplace National Historic Site is the birthplace and childhood home of President Theodore Roosevelt, the only US President born in New York City.

New York is divided into 62 counties. Aside from the five counties of New York City, each of these counties is subdivided into towns and cities. Towns can contain incorporated villages or unincorporated hamlets. New York City is divided into five boroughs, each coterminous with a county.
Downstate New York (New York City, Long Island, and the southern portion of the Hudson Valley) can be considered to form the central core of the Northeast megalopolis, an urbanized region stretching from New Hampshire to Virginia.
The major cities of the state developed along the key transportation and trade routes of the early 19th century, including the Erie Canal and railroads paralleling it. Today, the New York Thruway acts as a modern counterpart to commercial water routes.

The distribution of change in population growth is uneven in New York State; the New York City metropolitan area is growing considerably, along with Saratoga County and the Capital District, collectively known as Tech Valley. New York City gained more residents between April 2010 and July 2014 (316,000) than any other U.S. city. Conversely, outside of the Rochester and Ithaca areas, population growth in much of Western New York is nearly stagnant. According to immigration statistics, the state is a leading recipient of migrants from around the globe. Between 2000 and 2005, immigration failed to surpass emigration, a trend that has been reversing since 2006. New York State lost two House seats in the 2011 congressional reapportionment, secondary to relatively slow growth when compared to the rest of the United States. In 2000 and 2005, more people moved from New York to Florida than from any one state to another, contributing to New York becoming the U.S.'s fourth most populous state in 2015, behind Florida, Texas, and California. However, New York State has the second-largest international immigrant population in the country among the American states, at 4.2 million as of 2008; most reside in and around New York City, due to its size, high profile, vibrant economy, and cosmopolitan culture.
The United States Census Bureau estimates that the population of New York was 19,795,791 on July 1, 2015, a 2.16% increase since the 2010 United States Census. Despite the open land in the state, New York's population is very urban, with 92% of residents living in an urban area, predominantly in the New York City metropolitan area.
Two-thirds of New York State's population resides in New York City Metropolitan Area. New York City is the most populous city in the United States, with an estimated record high population of 8,550,405 in 2015, incorporating more immigration into the city than emigration since the 2010 United States Census. More than twice as many people live in New York City as in the second-most populous U.S. city (Los Angeles), and within a smaller area. Long Island alone accounted for a Census-estimated 7,838,722 residents in 2015, representing 39.6% of New York State's population.

These are the ten counties with the largest populations as of 2010:
Kings County (Brooklyn): 2,504,700
Queens County (Queens): 2,230,722
New York County (Manhattan): 1,585,873
Suffolk County: 1,493,350
Bronx County (the Bronx): 1,385,108
Nassau County: 1,339,532
Westchester County: 949,113
Erie County: 919,040
Monroe County: 744,344
Richmond County (Staten Island): 468,730

There are 62 cities in New York. The largest city in the state and the most populous city in the United States is New York City, which comprises five counties (each coextensive with a borough): Bronx, New York County (Manhattan), Queens, Kings County (Brooklyn), and Richmond County (Staten Island). New York City is home to more than two-fifths of the state's population. Albany, the state capital, is the sixth-largest city in New York State. The smallest city is Sherrill, New York, in Oneida County. Hempstead is the most populous town in the state; if it were a city, it would be the second largest in New York State, with over 700,000 residents.

The following are the top ten metropolitan areas in the state as of the 2010 Census:
New York City and the Hudson Valley (19,567,410 in NY/NJ/PA, 13,038,826 in NY)
Buffalo-Niagara Falls (1,135,509)
Rochester (1,079,671)
Albany and the Capital District (870,716)
Syracuse (662,577)
Utica-Rome (299,397)
Binghamton (251,725)
Kingston (182,493)
Glens Falls (128,923)
Watertown-Fort Drum (116,229)

According to the U.S. Census Bureau, the 2010 racial makeup of New York State was as follows by self-identification:
White American – 65.7%
Black or African American – 15.9%
Asian American – 7.3% (3.0% Chinese, 1.6% Indian, 0.7% Korean, 0.5% Filipino, 0.3% Pakistani, 0.3% Bangladeshi, 0.2% Japanese, 0.1% Vietnamese)
Multiracial Americans – 3.0%
Native American/American Indian – 0.6%
Some other race - 7.5%
In 2004, the major ancestry groups in New York State by self-identification were Hispanic and Latino Americans (17.6%), African American (15.8%), Italian (14.4%), Irish (12.9%), German (11.1%) and English (6%). According to a 2010 estimate, 21.7% of the population is foreign-born.

The state's most populous racial group, non-Hispanic white, has declined as a proportion of the state population from 94.6% in 1940 to 58.3% in 2010. As of 2011, 55.6% of New York's population younger than age 1 were minorities. New York's robustly increasing Jewish population, the largest outside of Israel, was the highest among states both by percentage and absolute number in 2012. It is driven by the high reproductive rate of Orthodox Jewish families, particularly in Brooklyn and communities of the Hudson Valley.
New York is home to the second-largest African American population (after Georgia) and the second largest Asian-American population (after California) in the United States. New York's uniracial Black population increased by 2.0% between 2000 and 2010, to 3,073,800. The Black population is in a state of flux, as New York is the largest recipient of immigrants from Africa, while established African Americans are migrating out of New York to the southern United States. The New York City neighborhood of Harlem has historically been a major cultural capital for African-Americans of sub-Saharan descent, and Bedford-Stuyvesant in Brooklyn has the largest such population in the United States. Meanwhile, New York's uniracial Asian population increased by a notable 36% from 2000 to 2010, to 1,420,244. Queens, in New York City, is home to the state's largest Asian-American population and is the most ethnically diverse county in the United States; it is the most ethnically diverse urban area in the world.
New York's growing uniracial Hispanic-or-Latino population numbered 3,416,922 in 2010, a 19% increase from the 2,867,583 enumerated in 2000. Queens is home to the largest Andean (Colombian, Ecuadorian, Peruvian, and Bolivian) populations in the United States. In addition, New York has the largest Puerto Rican, Dominican, and Jamaican American populations in the continental United States.
The Chinese population constitutes the fastest-growing nationality in New York State; multiple satellites of the original Manhattan Chinatown (曼哈頓華埠), in Brooklyn (布鲁克林華埠), and around Flushing, Queens (法拉盛華埠), are thriving as traditionally urban enclaves, while also expanding rapidly eastward into suburban Nassau County (拿騷縣), on Long Island (長島). New York State has become the top destination for new Chinese immigrants, and large-scale Chinese immigration continues into the state. A new China City of America is also planned in Sullivan County. Long Island, including Queens and Nassau County, is also home to several Little Indias (लघु भारत) and a large Koreatown (롱 아일랜드 코리아타운), with large and growing attendant populations of Indian Americans and Korean Americans, respectively. Brooklyn has been a destination for West Indian immigrants of African descent, as well as Asian Indian immigrants.
In the 2000 Census, New York had the largest Italian American population, composing the largest self-identified ancestral group in Staten Island and Long Island, followed by Irish Americans. Albany and the Mohawk Valley also have large communities of ethnic Italians and Irish Americans, reflecting 19th and early 20th-century immigration. In Buffalo and western New York, German Americans comprise the largest ancestry. In the North Country of New York, French Canadians represent the leading ethnicity, given the area's proximity to Quebec. Americans of English ancestry are present throughout all of upstate New York, reflecting early colonial and later immigrants.
6.5% of New York's population were under five years of age, 24.7% under 18, and 12.9% were 65 or older. Females made up 51.8% of the state's population.

In 2010, the most common American English dialects spoken in New York, besides General American English, were the New York City area dialect (including New York Latino English and North Jersey English), the Western New England accent around Albany, and Inland Northern American English in Buffalo and western New York State. As many as 800 languages are spoken in New York City, making it the most linguistically diverse city in the world.
As of 2010, 70.72% (12,788,233) of New York residents aged five and older reported speaking only English at home, while 14.44% (2,611,903) spoke Spanish, 2.61% (472,955) Chinese (which includes Cantonese and Mandarin), 1.20% (216,468) Russian, 1.18% (213,785) Italian, 0.79% (142,169) French Creole, 0.75% (135,789) French, 0.67% (121,917) Yiddish, 0.63% (114,574) Korean, and Polish was spoken by 0.53% (95,413) of the population over the age of five. In total, 29.28% (5,295,016) of New York's population aged five and older reported speaking a language other than English.

In 2010, the Association of Religion Data Archives (ARDA) reported that the largest denominations were the Catholic Church with 6,286,916; Orthodox Judaism with 588,500; Islam with 392,953; and the United Methodist Church with 328,315 adherents.

Roughly 3.8 percent of the state's adult population self-identifies as lesbian, gay, bisexual, or transgender. This constitutes a total LGBT adult population of 570,388 individuals. In 2010, the number of same-sex couple households stood at roughly 48,932. New York was the fifth state to license same-sex marriages, after New Hampshire. Michael Bloomberg, the Mayor of New York City, stated that "same-sex marriages in New York City have generated an estimated $259 million in economic impact and $16 million in City revenues" in the first year after the enactment of the Marriage Equality Act". Same-sex marriages in New York were legalized on June 24, 2011 and were authorized to take place beginning 30 days thereafter. New York City is also home to the largest transgender population in the United States, estimated at 25,000 in 2016.

New York's gross state product in 2015 was $1.44 trillion. If New York State were an independent nation, it would rank as the 12th or 13th largest economy in the world, depending upon international currency fluctuations. However, in 2013, the multi-state, New York City-centered Metropolitan Statistical Area produced a gross metropolitan product (GMP) of nearly US$1.39 trillion, while in 2012, the corresponding Combined Statistical Area generated a GMP of over US$1.55 trillion, both ranking first nationally by a wide margin and behind the GDP of only twelve nations and eleven nations, respectively.

Anchored by Wall Street in the Financial District of Lower Manhattan, New York City has been called both the most economically powerful city and the leading financial center of the world. Lower Manhattan is the third-largest central business district in the United States and is home to the New York Stock Exchange, on Wall Street, and the NASDAQ, at 165 Broadway, representing the world's largest and second largest stock exchanges, respectively, as measured both by overall average daily trading volume and by total market capitalization of their listed companies in 2013. Investment banking fees on Wall Street totaled approximately $40 billion in 2012, while in 2013, senior New York City bank officers who manage risk and compliance functions earned as much as $324,000 annually. In fiscal year 2013-14, Wall Street's securities industry generated 19% of New York State's tax revenue. New York City remains the largest global center for trading in public equity and debt capital markets, driven in part by the size and financial development of the U.S. economy. New York also leads in hedge fund management; private equity; and the monetary volume of mergers and acquisitions. Several investment banks and investment managers headquartered in Manhattan are important participants in other global financial centers. New York is also the principal commercial banking center of the United States.
Many of the world's largest media conglomerates are also based in the city. Manhattan contained approximately 520 million square feet (48.1 million m2) of office space in 2013, making it the largest office market in the United States, while Midtown Manhattan is the largest central business district in the nation.

Silicon Alley, centered in New York City, has evolved into a metonym for the sphere encompassing the New York City metropolitan region's high technology and entrepreneurship ecosystem; in 2015, Silicon Alley generated over US$7.3 billion in venture capital investment. High tech industries including digital media, biotechnology, software development, game design, and other fields in information technology are growing, bolstered by New York City's position at the terminus of several transatlantic fiber optic trunk lines, its intellectual capital, as well as its growing outdoor wireless connectivity. In December 2014, New York State announced a $50 million venture-capital fund to encourage enterprises working in biotechnology and advanced materials; according to Governor Andrew Cuomo, the seed money would facilitate entrepreneurs in bringing their research into the marketplace. On December 19, 2011, then Mayor Michael R. Bloomberg announced his choice of Cornell University and Technion-Israel Institute of Technology to build a US$2 billion graduate school of applied sciences on Roosevelt Island in Manhattan, with the goal of transforming New York City into the world's premier technology capital.

Albany, Saratoga County, Rensselaer County, and the Hudson Valley, collectively recognized as eastern New York's Tech Valley, have experienced significant growth in the computer hardware side of the high-technology industry, with great strides in the nanotechnology sector, digital electronics design, and water- and electricity-dependent integrated microchip circuit manufacturing, involving companies including IBM and its Thomas J. Watson Research Center, GlobalFoundries, Samsung, and Taiwan Semiconductor, among others. The area's high technology ecosystem is supported by technologically focused academic institutions including Rensselaer Polytechnic Institute and the SUNY Polytechnic Institute. In 2015, Tech Valley, straddling both sides of the Adirondack Northway and the New York Thruway, generated over US$163 million in venture capital investment. The Rochester area is important in the field of photographic processing and imaging as well as incubating an increasingly diverse high technology sphere encompassing STEM fields, similarly in part the result of private startup enterprises collaborating with major academic institutions, including the University of Rochester and Cornell University. Westchester County has developed a burgeoning biotechnology sector in the 21st century, with over US$1 billion in planned private investment as of 2016,

Creative industries, which are concerned with generating and distributing knowledge and information, such as new media, digital media, film and television production, advertising, fashion, design, and architecture, account for a growing share of employment, with New York City possessing a strong competitive advantage in these industries. As of 2014, New York State was offering tax incentives of up to $420 million annually for filmmaking within the state, the most generous such tax rebate among the U.S. states. New York has also attracted higher-wage visual-effects employment by further augmenting its tax credit to a maximum of 35% for performing post-film production work in Upstate New York. The filmed entertainment industry has been growing in New York, contributing nearly US$9 billion to the New York City economy alone as of 2015.

I Love New York (stylized I ❤ NY) is both a logo and a song that are the basis of an advertising campaign and have been used since 1977 to promote tourism in New York City, and later to promote New York State as well. The trademarked logo, owned by New York State Empire State Development, appears in souvenir shops and brochures throughout the state, some licensed, many not. The song is the state song of New York. The Broadway League reported that Broadway shows sold approximately US$1.27 billion worth of tickets in the 2013–2014 season, an 11.4% increase from US$1.139 billion in the 2012–2013 season. Attendance in 2013–2014 stood at 12.21 million, representing a 5.5% increase from the 2012–2013 season's 11.57 million.

New York exports a wide variety of goods such as prepared foods, computers and electronics, cut diamonds, and other commodities. In 2007, the state exported a total of $71.1 billion worth of goods, with the five largest foreign export markets being Canada (US$15 billion), the United Kingdom (US$6 billion), Switzerland (US$5.9 billion), Israel (US$4.9 billion), and Hong Kong (US$3.4 billion). New York's largest imports are oil, gold, aluminum, natural gas, electricity, rough diamonds, and lumber. The state also has a large manufacturing sector that includes printing and the production of garments, mainly in New York City; and furs, railroad equipment, automobile parts, and bus line vehicles, concentrated in Upstate regions.
New York is the nation's third-largest grape producing state, and second-largest wine producer by volume, behind California. The southern Finger Lakes hillsides, the Hudson Valley, the North Fork of Long Island, and the southern shore of Lake Erie are the primary grape- and wine-growing regions in New York, with many vineyards. In 2012, New York had 320 wineries and 37,000 grape bearing acres, generating full-time employment of nearly 25,000 and annual wages of over US$1.1 billion, and yielding US$4.8 billion in direct economic impact from New York grapes, grape juice, and wine and grape products.
New York is a major agricultural producer overall, ranking among the top five states for agricultural products including maple syrup, apples, cherries, cabbage, dairy products, onions, and potatoes. The state is the largest producer of cabbage in the U.S. The state has about a quarter of its land in farms and produced $3.4 billion in agricultural products in 2001. The south shore of Lake Ontario provides the right mix of soils and microclimate for many apple, cherry, plum, pear and peach orchards. Apples are also grown in the Hudson Valley and near Lake Champlain. A moderately sized saltwater commercial fishery is located along the Atlantic side of Long Island. The principal catches by value are clams, lobsters, squid, and flounder.

The University of the State of New York accredits and sets standards for primary, middle-level, and secondary education in the state, while the New York State Education Department oversees public schools and controls their standardized tests. The New York City Department of Education manages the New York City Public Schools system. In 1894, reflecting general racial discrimination then, the state passed a law that allowed communities to set up separate schools for children of African-American descent. In 1900, the state passed another law requiring integrated schools.
At the level of post-secondary education, the statewide public university system is the State University of New York, commonly referred to as SUNY. New York City also has its own City University of New York system, which is funded by the city. The SUNY system consists of 64 community colleges, technical colleges, undergraduate colleges, and doctoral-granting institutions, including several universities. New York's largest public university is the State University of New York at Buffalo, which was founded by U.S President and Vice President Millard Fillmore. The four SUNY University Centers, offering a wide array of academic programs, are the University at Albany, Binghamton University, Stony Brook University, and the University at Buffalo.
Notable large private universities include the Columbia University in Upper Manhattan and Cornell University in Ithaca, both Ivy League institutions, as well as New York University in Lower Manhattan, and Fordham University in the Bronx, Manhattan, and Westchester County. Smaller notable private institutions of higher education include Rockefeller University, New York Institute of Technology, Yeshiva University, and Hofstra University. There are also a multitude of postgraduate-level schools in New York State, including Medical, Law, and Engineering schools. West Point, the service academy of the U.S. Army, is located just south of Newburgh, on the west bank of the Hudson River.
During the fiscal 2013 year, New York spent more on public education per pupil than any other state, according to U.S. Census Bureau statistics.

New York has one of the most extensive and one of the oldest transportation infrastructures in the country. Engineering challenges posed by the complex terrain of the state and the unique infrastructural issues of New York City brought on by urban crowding have had to be overcome perennially. Population expansion of the state has followed the path of the early waterways, first the Hudson River and Mohawk River, then the Erie Canal. In the 19th century, railroads were constructed along the river valleys, followed by the New York State Thruway in the 20th century.
The New York State Department of Transportation (NYSDOT) is the department of the government of New York responsible for the development and operation of highways, railroads, mass transit systems, ports, waterways, and aviation facilities within New York State. The NYSDOT is headquartered at 50 Wolf Road in Colonie, Albany County. The Port Authority of New York and New Jersey (PANYNJ) is a joint venture between the States of New York and New Jersey and authorized by the US Congress, established in 1921 through an interstate compact, that oversees much of the regional transportation infrastructure, including bridges, tunnels, airports, and seaports, within the geographical jurisdiction of the Port of New York and New Jersey. This 1,500 square mile (3,900 km²) port district is generally encompassed within a 25-mile (40 km) radius of the Statue of Liberty National Monument. The Port Authority is headquartered at 4 World Trade Center in Lower Manhattan.
In addition to the well known New York City Subway system – which is confined within New York City – four suburban commuter railroad systems enter and leave the city: the Long Island Rail Road, Metro-North Railroad, Port Authority Trans-Hudson, and five of New Jersey Transit's rail lines. The New York City Department of Transportation (NYCDOT) is the agency of the government of New York City responsible for the management of much of New York City's own transportation infrastructure. Other cities and towns in New York have urban and regional public transportation. In Buffalo, the Niagara Frontier Transportation Authority runs the Buffalo Metro Rail light-rail system; in Rochester, the Rochester Subway operated from 1927 until 1956, but fell into disuse as state and federal investment went to highways.
The New York State Department of Motor Vehicles (NYSDMV or DMV) is the governmental agency responsible for registering and inspecting automobiles and other motor vehicles, as well as licensing drivers in the State of New York. As of 2008, the NYSDMV has 11,284,546 drivers licenses on file and 10,697,644 vehicle registrations in force. All gasoline-powered vehicles registered in New York State are required to have an emissions inspection every 12 months, in order to ensure that environmental quality controls are working to prevent air pollution. Diesel-powered vehicles with a gross weight rating over 8,500 lb that are registered in most Downstate New York counties must get an annual emissions inspection. All vehicles registered in New York State must get an annual safety inspection.
Portions of the transportation system are intermodal, allowing travelers to switch easily from one mode of transportation to another. One of the most notable examples is AirTrain JFK which allows rail passengers to travel directly to terminals at John F. Kennedy International Airport as well as to the underground New York City Subway system.

The Government of New York embodies the governmental structure of the State of New York as established by the New York State Constitution. It is composed of three branches: executive, legislative, and judicial.
The Governor is the State's chief executive and is assisted by the Lieutenant Governor. Both are elected on the same ticket. Additional elected officers include the Attorney General, and the Comptroller. The Secretary of State, formerly an elected officer, is currently appointed by the Governor.
The New York State Legislature is bicameral and consists of the New York State Senate and the New York State Assembly. The Assembly consists of 150 members, while the Senate varies in its number of members, currently having 63. The Legislature is empowered to make laws, subject to the Governor's power to veto a bill. However, the veto may be overridden by the Legislature if there is a two-thirds majority in favor of overriding in each House. The permanent laws of a general nature are codified in the Consolidated Laws of New York.
The highest court of appeal in the Unified Court System is the Court of Appeals whereas the primary felony trial court is the County Court (or the Supreme Court in New York City). The Supreme Court also acts as the intermediate appellate court for many cases, and the local courts handle a variety of other matters including small claims, traffic ticket cases, and local zoning matters, and are the starting point for all criminal cases. The New York City courts make up the largest local court system.
The state is divided into counties, cities, towns, and villages, all of which are municipal corporations with respect to their own governments, as well as various corporate entities that serve single purposes that are also local governments, such as school districts, fire districts, and New York state public-benefit corporations, frequently known as authorities or development corporations. Each municipal corporation is granted varying home rule powers as provided by the New York Constitution. The state also has 10 Indian reservations.

Capital punishment was reintroduced in 1995 under the Pataki administration, but the statute was declared unconstitutional in 2004, when the New York Court of Appeals ruled in People v. LaValle that it violated the state constitution. The remaining death sentence was commuted by the court to life imprisonment in 2007, in People v. John Taylor, and the death row was disestablished in 2008, under executive order from Governor Paterson. No execution has taken place in New York since 1963. Legislative efforts to amend the statute have failed, and death sentences are no longer sought at the state level, though certain crimes that fall under the jurisdiction of the federal government are subject to the federal death penalty.

The State of New York sends 27 members to the House of Representatives in addition to its two United States Senators. As of the 2000 census and the redistricting for the 2002 elections, the state had 29 members in the House, but the representation was reduced to 27 in 2013 due to the state's slower overall population growth relative to the overall national population growth. From 2016, New York will have 29 electoral votes in national presidential elections (a drop from its peak of 47 votes from 1933 to 1953).
New York is represented by Chuck Schumer and Kirsten Gillibrand in the United States Senate and has the nation's third equal highest number of congressional districts, equal with Florida and behind California's 53 and Texas's 36.
The state has a strong imbalance of payments with the federal government. According to the Office of the New York State Comptroller, New York State received 91 cents in services for every $1 it sent in taxes to the U.S. federal government in the 2013 fiscal year; New York ranked in 46th place in the federal balance of payments to the state on a per capita basis.

As of April 2016, Democrats represented a plurality of voters in New York State, constituting over twice as many registered voters as any other political party affiliation or lack thereof. Since the second half of the 20th century, New York has generally supported candidates belonging to the Democratic Party in national elections. Democratic presidential candidate Barack Obama won New York State by over 25 percentage points in both 2012 and 2008. New York City, as well as the state's other major urban locales, including Albany, Buffalo, Rochester, Yonkers, and Syracuse, are significant Democratic strongholds, with liberal politics. Rural portions of upstate New York, however, are generally more conservative than the cities and tend to favor Republicans. Heavily populated suburban areas downstate, such as Westchester County and Long Island, have swung between the major parties since the 1980s, but more often than not support Democrats.
New York City is the most important source of political fundraising in the United States for both major parties. Four of the top five zip codes in the nation for political contributions are in Manhattan. The top zip code, 10021 on the Upper East Side, generated the most money for the 2000 presidential campaigns of both George W. Bush and Al Gore.
The state of New York has the distinction of being the home state for both major-party nominees in three Presidential elections. The 1904 presidential election saw former New York Governor and incumbent President Theodore Roosevelt face Alton B. Parker, chief judge of the New York Court of Appeals. The 1944 presidential election had Franklin D. Roosevelt, following in his cousin Theodore's footsteps as former New York Governor and incumbent president running for re-election against then-current New York Governor Thomas E. Dewey. In the 2016 Presidential election, former United States Senator from New York Hillary Clinton, a resident of Chappaqua, was the Democratic Party nominee. The Republican Party nominee was businessman Donald Trump, a resident of Manhattan and a native of Queens.
New York City is an important center for international diplomacy. The United Nations Headquarters has been situated on the East Side of Midtown Manhattan since 1952.

New York State is geographically home to one National Football League team, the Buffalo Bills, based in the Buffalo suburb of Orchard Park. Although the New York Giants and New York Jets represent the New York metropolitan area and were previously located in New York City, they play in MetLife Stadium, located in East Rutherford, New Jersey. New York also has two Major League Baseball teams, the New York Yankees (based in the Bronx) and the New York Mets (based in Queens). Minor league baseball teams also play in the State of New York, including the Long Island Ducks. New York is home to three National Hockey League franchises: the New York Rangers in Manhattan, the New York Islanders in Brooklyn, and the Buffalo Sabres in Buffalo. New York has two National Basketball Association teams, the New York Knicks in Manhattan, and the Brooklyn Nets in Brooklyn. New York is the home of a Major League Soccer franchise, New York City FC, currently playing in the Bronx. Although the New York Red Bulls represent the New York metropolitan area, they play in Red Bull Arena in Harrison, New Jersey.
New York hosted the 1932 and 1980 Winter Olympics at Lake Placid. The 1980 Games are known for the USA–USSR ice hockey match dubbed the "Miracle on Ice", in which a group of American college students and amateurs defeated the heavily favored Soviet national ice hockey team 4–3 and went on to win the gold medal against Finland. Along with St. Moritz, Switzerland and Innsbruck, Austria, Lake Placid is one of the three cities to have hosted the Winter Olympic Games twice. New York City bid for the 2012 Summer Olympics but lost to London.
Several U.S. national sports halls of fame are or have been situated in New York. The National Baseball Hall of Fame and Museum is located in Cooperstown, Otsego County. The National Museum of Racing and Hall of Fame in Saratoga Springs, Saratoga County, honors achievements in the sport of thoroughbred horse racing. The physical facility of the National Soccer Hall of Fame in Oneonta, also in Otsego County, closed in 2010, although the organization itself has continued inductions. The annual United States Open Tennis Championships is one of the world's four Grand Slam tennis tournaments and is held at the National Tennis Center in Flushing Meadows-Corona Park in the New York City borough of Queens.

Index of New York-related articles
Outline of New York – organized list of topics about New York

French, John Homer (1860). Historical and statistical gazetteer of New York State. Syracuse, New York: R. Pearsall Smith. OCLC 224691273. (Full text via Google Books.)
New York State Historical Association (1940). New York: A Guide to the Empire State. New York City: Oxford University Press. ISBN 978-1-60354-031-5. OCLC 504264143. (Full text via Google Books.)

New York State Guide, from the Library of Congress
New York at DMOZ
 Geographic data related to New York at OpenStreetMapSeattle (/siˈætəl/) is a seaport city on the west coast of the United States and the seat of King County, Washington. With an estimated 684,451 residents as of 2015, Seattle is the largest city in both the state of Washington and the Pacific Northwest region of North America. In July 2013, it was the fastest-growing major city in the United States and remained in the Top 5 in May 2015 with an annual growth rate of 2.1%. The city is situated on an isthmus between Puget Sound (an inlet of the Pacific Ocean) and Lake Washington, about 100 miles (160 km) south of the Canada–United States border. A major gateway for trade with Asia, Seattle is the fourth-largest port in North America in terms of container handling as of 2016.
The Seattle area was previously inhabited by Native Americans for at least 4,000 years before the first permanent European settlers. Arthur A. Denny and his group of travelers, subsequently known as the Denny Party, arrived from Illinois via Portland, Oregon, on the schooner Exact at Alki Point on November 13, 1851. The settlement was moved to the eastern shore of Elliott Bay and named "Seattle" in 1852, after Chief Si'ahl of the local Duwamish and Suquamish tribes.
Logging was Seattle's first major industry, but by the late-19th century, the city had become a commercial and shipbuilding center as a gateway to Alaska during the Klondike Gold Rush. Growth after World War II was partially due to the local Boeing company, which established Seattle as a center for aircraft manufacturing. The Seattle area developed as a technology center beginning in the 1980s, with companies like Microsoft becoming established in the region. In 1994, Internet retailer Amazon was founded in Seattle. The stream of new software, biotechnology, and Internet companies led to an economic revival, which increased the city's population by almost 50,000 between 1990 and 2000.
Seattle has a noteworthy musical history. From 1918 to 1951, nearly two dozen jazz nightclubs existed along Jackson Street, from the current Chinatown/International District, to the Central District. The jazz scene developed the early careers of Ray Charles, Quincy Jones, Ernestine Anderson, and others. Seattle is also the birthplace of rock musician Jimi Hendrix and the alternative rock subgenre grunge.

Archaeological excavations suggest that Native Americans have inhabited the Seattle area for at least 4,000 years. By the time the first European settlers arrived, the people (subsequently called the Duwamish tribe) occupied at least seventeen villages in the areas around Elliott Bay.
The first European to visit the Seattle area was George Vancouver, in May 1792 during his 1791–95 expedition to chart the Pacific Northwest. In 1851, a large party led by Luther Collins made a location on land at the mouth of the Duwamish River; they formally claimed it on September 14, 1851. Thirteen days later, members of the Collins Party on the way to their claim passed three scouts of the Denny Party. Members of the Denny Party claimed land on Alki Point on September 28, 1851. The rest of the Denny Party set sail from Portland, Oregon and landed on Alki point during a rainstorm on November 13, 1851.

After a difficult winter, most of the Denny Party relocated across Elliott Bay and claimed land a second time at the site of present-day Pioneer Square, naming this new settlement Duwamps. Charles Terry and John Low remained at the original landing location and reestablished their old land claim and called it "New York", but renamed "New York Alki" in April 1853, from a Chinook word meaning, roughly, "by and by" or "someday". For the next few years, New York Alki and Duwamps competed for dominance, but in time Alki was abandoned and its residents moved across the bay to join the rest of the settlers.
David Swinson "Doc" Maynard, one of the founders of Duwamps, was the primary advocate to name the settlement after Chief Sealth ("Seattle") of the Duwamish and Suquamish tribes.

The name "Seattle" appears on official Washington Territory papers dated May 23, 1853, when the first plats for the village were filed. In 1855, nominal land settlements were established. On January 14, 1865, the Legislature of Territorial Washington incorporated the Town of Seattle with a board of trustees managing the city. The town of Seattle was disincorporated January 18, 1867, and remained a mere precinct of King County until late 1869, when a new petition was filed and the city was re-incorporated December 2, 1869, with a Mayor-council government. The corporate seal of the City of Seattle carries the date "1869" and a likeness of Chief Sealth in left profile.

Seattle has a history of boom-and-bust cycles, like many other cities near areas of extensive natural and mineral resources. Seattle has risen several times economically, then gone into precipitous decline, but it has typically used those periods to rebuild solid infrastructure.
The first such boom, covering the early years of the city, rode on the lumber industry. (During this period the road now known as Yesler Way won the nickname "Skid Road", supposedly after the timber skidding down the hill to Henry Yesler's sawmill. The later dereliction of the area may be a possible origin for the term which later entered the wider American lexicon as Skid Row.) Like much of the American West, Seattle saw numerous conflicts between labor and management, as well as ethnic tensions that culminated in the anti-Chinese riots of 1885–1886. This violence originated with unemployed whites who were determined to drive the Chinese from Seattle (anti-Chinese riots also occurred in Tacoma). In 1900, Asians were 4.2% of the population. Authorities declared martial law and federal troops arrived to put down the disorder.
Seattle achieved sufficient economic success that when the Great Seattle Fire of 1889 destroyed the central business district, a far grander city-center rapidly emerged in its place. Finance company Washington Mutual, for example, was founded in the immediate wake of the fire. However, the Panic of 1893 hit Seattle hard.

The second and most dramatic boom and bust resulted from the Klondike Gold Rush, which ended the depression that had begun with the Panic of 1893; in a short time, Seattle became a major transportation center. On July 14, 1897, the S.S. Portland docked with its famed "ton of gold", and Seattle became the main transport and supply point for the miners in Alaska and the Yukon. Few of those working men found lasting wealth, however; it was Seattle's business of clothing the miners and feeding them salmon that panned out in the long run. Along with Seattle, other cities like Everett, Tacoma, Port Townsend, Bremerton, and Olympia, all in the Puget Sound region, became competitors for exchange, rather than mother lodes for extraction, of precious metals. The boom lasted well into the early part of the 20th century and funded many new Seattle companies and products. In 1907, 19-year-old James E. Casey borrowed $100 from a friend and founded the American Messenger Company (later UPS). Other Seattle companies founded during this period include Nordstrom and Eddie Bauer. Seattle brought in the Olmsted Brothers landscape architecture firm to design a system of parks and boulevards.

The Gold Rush era culminated in the Alaska-Yukon-Pacific Exposition of 1909, which is largely responsible for the layout of today's University of Washington campus.
A shipbuilding boom in the early part of the 20th century became massive during World War I, making Seattle somewhat of a company town; the subsequent retrenchment led to the Seattle General Strike of 1919, the first general strike in the country. A 1912 city development plan by Virgil Bogue went largely unused. Seattle was mildly prosperous in the 1920s but was particularly hard hit in the Great Depression, experiencing some of the country's harshest labor strife in that era. Violence during the Maritime Strike of 1934 cost Seattle much of its maritime traffic, which was rerouted to the Port of Los Angeles.
Seattle was also the home base of impresario Alexander Pantages who, starting in 1902, opened a number of theaters in the city exhibiting vaudeville acts and silent movies. His activities soon expanded, and the thrifty Greek went on and became one of America's greatest theater and movie tycoons. Between Pantages and his rival John Considine, Seattle was for a while the western United States' vaudeville mecca. B. Marcus Priteca, the Scottish-born and Seattle-based architect, built several theaters for Pantages, including some in Seattle. The theaters he built for Pantages in Seattle have been either demolished or converted to other uses, but many other theaters survive in other cities of the U.S., often retaining the Pantages name; Seattle's surviving Paramount Theatre, on which he collaborated, was not a Pantages theater.

War work again brought local prosperity during World War II, this time centered on Boeing aircraft. The war dispersed the city's numerous Japanese-American businessmen due to the Japanese American internment. After the war, the local economy dipped. It rose again with Boeing's growing dominance in the commercial airliner market. Seattle celebrated its restored prosperity and made a bid for world recognition with the Century 21 Exposition, the 1962 World's Fair. Another major local economic downturn was in the late 1960s and early 1970s, at a time when Boeing was heavily affected by the oil crises, loss of Government contracts, and costs and delays associated with the Boeing 747. Many people left the area to look for work elsewhere, and two local real estate agents put up a billboard reading "Will the last person leaving Seattle – Turn out the lights."
Seattle remained the corporate headquarters of Boeing until 2001, when the company separated its headquarters from its major production facilities; the headquarters were moved to Chicago. The Seattle area is still home to Boeing's Renton narrow-body plant (where the 707, 720, 727, and 757 were assembled, and the 737 is assembled today) and Everett wide-body plant (assembly plant for the 747, 767, 777, and 787). The company's credit union for employees, BECU, remains based in the Seattle area, though it is now open to all residents of Washington.
As prosperity began to return in the 1980s, the city was stunned by the Wah Mee massacre in 1983, when 13 people were killed in an illegal gambling club in the International District, Seattle's Chinatown. Beginning with Microsoft's 1979 move from Albuquerque, New Mexico, to nearby Bellevue, Washington, Seattle and its suburbs became home to a number of technology companies including Amazon.com, RealNetworks, Nintendo of America, McCaw Cellular (now part of AT&T Mobility), VoiceStream (now T-Mobile), and biomedical corporations such as HeartStream (later purchased by Philips), Heart Technologies (later purchased by Boston Scientific), Physio-Control (later purchased by Medtronic), ZymoGenetics, ICOS (later purchased by Eli Lilly and Company) and Immunex (later purchased by Amgen). This success brought an influx of new residents with a population increase within city limits of almost 50,000 between 1990 and 2000, and saw Seattle's real estate become some of the most expensive in the country. In 1993, the movie Sleepless in Seattle brought the city further national attention. Many of the Seattle area's tech companies remained relatively strong, but the frenzied dot-com boom years ended in early 2001.
Seattle in this period attracted widespread attention as home to these many companies, but also by hosting the 1990 Goodwill Games and the APEC leaders conference in 1993, as well as through the worldwide popularity of grunge, a sound that had developed in Seattle's independent music scene. Another bid for worldwide attention—hosting the World Trade Organization Ministerial Conference of 1999—garnered visibility, but not in the way its sponsors desired, as related protest activity and police reactions to those protests overshadowed the conference itself. The city was further shaken by the Mardi Gras Riots in 2001, and then literally shaken the following day by the Nisqually earthquake.
Yet another boom began as the city emerged from the Great Recession. Amazon.com moved its headquarters from North Beacon Hill to South Lake Union and began a rapid expansion. For the five years beginning in 2010, Seattle gained an average of 14,511 residents per year, with the growth strongly skewed toward the center of the city, as unemployment dropped from roughly 9 percent to 3.6 percent. The city has found itself "bursting at the seams", with over 45,000 households spending more than half their income on housing and at least 2,800 people homeless, and with the country's sixth-worst rush hour traffic.

With a land area of 83.9 square miles (217.3 km²), Seattle is the northernmost city with at least 500,000 people in the United States, farther north than Canadian cities such as Toronto, Ottawa, and Montreal, at about the same latitude as Salzburg, Austria.
The topography of Seattle is hilly. The city lies on several hills, including Capitol Hill, First Hill, West Seattle, Beacon Hill, Magnolia, Denny Hill, and Queen Anne. The Kitsap and the Olympic peninsulas along with the Olympic mountains lie to the west of Puget Sound, while the Cascade Range and Lake Sammamish lie to the east of Lake Washington. The city has over 5,540 acres (2,242 ha) of parkland.

Seattle is located between the saltwater Puget Sound (an arm of the Pacific Ocean) to the west and Lake Washington to the east. The city's chief harbor, Elliott Bay, is part of Puget Sound, which makes the city an oceanic port. To the west, beyond Puget Sound, are the Kitsap Peninsula and Olympic Mountains on the Olympic Peninsula; to the east, beyond Lake Washington and the Eastside suburbs, are Lake Sammamish and the Cascade Range. Lake Washington's waters flow to Puget Sound through the Lake Washington Ship Canal (consisting of two man-made canals, Lake Union, and the Hiram M. Chittenden Locks at Salmon Bay, ending in Shilshole Bay on Puget Sound).

The sea, rivers, forests, lakes, and fields surrounding Seattle were once rich enough to support one of the world's few sedentary hunter-gatherer societies. The surrounding area lends itself well to sailing, skiing, bicycling, camping, and hiking year-round.
The city itself is hilly, though not uniformly so. Like Rome, the city is said to lie on seven hills; the lists vary but typically include Capitol Hill, First Hill, West Seattle, Beacon Hill, Queen Anne, Magnolia, and the former Denny Hill. The Wallingford, Mount Baker, and Crown Hill neighborhoods are technically located on hills as well. Many of the hilliest areas are near the city center, with Capitol Hill, First Hill, and Beacon Hill collectively constituting something of a ridge along an isthmus between Elliott Bay and Lake Washington. The break in the ridge between First Hill and Beacon Hill is man-made, the result of two of the many regrading projects that reshaped the topography of the city center. The topography of the city center was also changed by the construction of a seawall and the artificial Harbor Island (completed 1909) at the mouth of the city's industrial Duwamish Waterway, the terminus of the Green River. The highest point within city limits is at High Point in West Seattle, which is roughly located near 35th Ave SW and SW Myrtle St. Other notable hills include Crown Hill, View Ridge/Wedgwood/Bryant, Maple Leaf, Phinney Ridge, Mt. Baker Ridge, and Highlands/Carkeek/Bitterlake.

North of the city center, Lake Washington Ship Canal connects Puget Sound to Lake Washington. It incorporates four natural bodies of water: Lake Union, Salmon Bay, Portage Bay, and Union Bay.
Due to its location in the Pacific Ring of Fire, Seattle is in a major earthquake zone. On February 28, 2001, the magnitude 6.8 Nisqually earthquake did significant architectural damage, especially in the Pioneer Square area (built on reclaimed land, as are the Industrial District and part of the city center), but caused only one fatality. Other strong quakes occurred on January 26, 1700 (estimated at 9 magnitude), December 14, 1872 (7.3 or 7.4), April 13, 1949 (7.1), and April 29, 1965 (6.5). The 1965 quake caused three deaths in Seattle directly and one more by heart failure. Although the Seattle Fault passes just south of the city center, neither it nor the Cascadia subduction zone has caused an earthquake since the city's founding. The Cascadia subduction zone poses the threat of an earthquake of magnitude 9.0 or greater, capable of seriously damaging the city and collapsing many buildings, especially in zones built on fill.
According to the United States Census Bureau, the city has a total area of 142.5 square miles (369 km2), 83.9 square miles (217 km2) of which is land and 58.7 square miles (152 km2), water (41.16% of the total area).

Seattle's climate is classified as oceanic or temperate marine, with cool, wet winters and mild, relatively dry summers.  The city and environs are part of USDA hardiness zone 8b, with isolated coastal pockets falling under 9a.
Temperature extremes are moderated by the adjacent Puget Sound, greater Pacific Ocean, and Lake Washington. Thus extreme heat waves are rare in the Seattle area, as are very cold temperatures (below about 15 F). The Seattle area is the cloudiest region of the United States, due in part to frequent storms and lows moving in from the adjacent Pacific Ocean. Despite having a reputation for frequent rain, Seattle receives less precipitation than many other US cities like Chicago or New York City. However, unlike many other US cities, Seattle has many more "rain days", when a very light drizzle falls from the sky for many days. In an average year, at least 0.01 inches (0.25 mm) of precipitation falls on 150 days, more than nearly all U.S. cities east of the Rocky Mountains. It is cloudy 201 days out of the year and partly cloudy 93 days. Official weather and climatic data is collected at Seattle–Tacoma International Airport, located about 19 km (12 mi) south of downtown in the city of SeaTac, which is at a higher elevation, and records more cloudy days and fewer partly cloudy days per year.
Hot temperature extremes are enhanced by dry, compressed wind from the west slopes of the Cascades, while cold temperatures are generated mainly from the Fraser Valley in British Columbia.
From 1981 to 2010, the average annual precipitation measured at Seattle–Tacoma International Airport was 37.49 inches (952 mm). Annual precipitation has ranged from 23.78 in (604 mm) in 1952 to 55.14 in (1,401 mm) in 1950; for water year (October 1 – September 30) precipitation, the range is 23.16 in (588 mm) in 1976–77 to 51.82 in (1,316 mm) in 1996–97. Due to local variations in microclimate, Seattle also receives significantly lower precipitation than some other locations west of the Cascades. Around 80 mi (129 km) to the west, the Hoh Rain Forest in Olympic National Park on the western flank of the Olympic Mountains receives an annual average precipitation of 142 in (3.61 m). Sixty miles (95 km) to the south of Seattle, the state capital Olympia, which is out of the Olympic Mountains' rain shadow, receives an annual average precipitation of 50 in (1,270 mm). The city of Bremerton, about 15 mi (24 km) west of downtown Seattle on the other side of the Puget Sound, receives 56.4 in (1,430 mm) of precipitation annually.
Conversely, the northeastern portion of the Olympic Peninsula, which lies east of the Olympic Mountains is located within the Olympic rain shadow and receives significantly less precipitation than its surrounding areas. Prevailing airflow from the west is forced to cool and compress when colliding with the mountain range, resulting in high levels of precipitation within the mountains and its western slopes. Once the airflow reaches the leeward side of the mountains it then lowers and expands resulting in warmer, and significantly dryer air. Sequim, Washington, nicknamed "Sunny Sequim", is located approximately 40 miles northwest of downtown Seattle and receives just 16.51" of annual precipitation, more comparable to that of Los Angeles. Oftentimes an area devoid of cloud cover can be seen extending out over the Puget Sound to the north and east of Sequim. On average Sequim observes 127 sunny days per year in addition to 127 days with partial cloud cover. Other areas influenced by the Olympic rain shadow include Port Angeles, Port Townsend, extending as far north as Victoria, British Columbia.
In November, Seattle averages more rainfall than any other U.S. city of more than 250,000 people; it also ranks highly in winter precipitation. Conversely, the city receives some of the lowest precipitation amounts of any large city from June to September. Seattle is one of the five rainiest major U.S. cities as measured by the number of days with precipitation, and it receives some of the lowest amounts of annual sunshine among major cities in the lower 48 states, along with some cities in the Northeast, Ohio and Michigan. Thunderstorms are rare, as the city reports thunder on just seven days per year. By comparison, Fort Myers, Florida, reports thunder on 93 days per year, Kansas City on 52, and New York City on 25.
Seattle experiences its heaviest rainfall during the months of November, December and January, receiving roughly half of its annual rainfall (by volume) during this period. In late fall and early winter, atmospheric rivers (also known as "Pineapple Express" systems), strong frontal systems, and Pacific low pressure systems are common. Light rain & drizzle are the predominant forms of precipitation during the remainder of the year; for instance, on average, less than 1.6 in (41 mm) of rain falls in July and August combined when rain is rare. On occasion, Seattle experiences somewhat more significant weather events. One such event occurred on December 2–4, 2007, when sustained hurricane-force winds and widespread heavy rainfall associated with a strong Pineapple Express event occurred in the greater Puget Sound area and the western parts of Washington and Oregon. Precipitation totals exceeded 13.8 in (350 mm) in some areas with winds topping out at 209 km/h (130 mph) along coastal Oregon. It became the second wettest event in Seattle history when a little over 130 mm (5.1 in) of rain fell on Seattle in a 24-hour period. Lack of adaptation to the heavy rain contributed to five deaths and widespread flooding and damage.
Autumn, winter, and early spring are frequently characterized by rain. Winters are cool and wet with December, the coolest month, averaging 40.6 °F (4.8 °C), with 28 annual days with lows that reach the freezing mark, and 2.0 days where the temperature stays at or below freezing all day; the temperature rarely lowers to 20 °F (−7 °C). Summers are sunny, dry and warm, with August, the warmest month, with high temperatures averaging 76.1 °F (24.5 °C), and reaching 90 °F (32 °C) on 3.1 days per year. In 2015 the city recorded 13 days over 90 °F. The hottest officially recorded temperature was 103 °F (39 °C) on July 29, 2009; the coldest recorded temperature was 0 °F (−18 °C) on January 31, 1950; the record cold daily maximum is 16 °F (−9 °C) on January 14, 1950, while, conversely, the record warm daily minimum is 71 °F (22 °C) the day the official record high was set. The average window for freezing temperatures is November 16 through March 10, allowing a growing season of 250 days.
Seattle typically receives some snowfall on an annual basis but heavy snow is rare. Average annual snowfall, as measured at Sea-Tac Airport, is 6.8 inches (17.3 cm). Single calendar-day snowfall of six inches (15 cm) or greater has occurred on only 15 days since 1948, and only once since February 17, 1990, when 6.8 in (17.3 cm) of snow officially fell at Sea-Tac airport on January 18, 2012. This moderate snow event was officially the 12th snowiest calendar day at the airport since 1948 and snowiest since November 1985. Much of the city of Seattle proper received somewhat lesser snowfall accumulations. Locations to the south of Seattle received more, with Olympia and Chehalis receiving 14 to 18 in (36 to 46 cm). Another moderate snow event occurred from December 12–25, 2008, when over one foot (30 cm) of snow fell and stuck on much of the roads over those two weeks, when temperatures remained below 32 °F (0 °C), causing widespread difficulties in a city not equipped for clearing snow. The largest documented snowstorm occurred from January 5–9, 1880, with snow drifting to 6 feet (1.8 m) in places at the end of the snow event. From January 31 to February 2, 1916, another heavy snow event occurred with 29 in (74 cm) of snow on the ground by the time the event was over. With official records dating to 1948, the largest single-day snowfall is 20.0 in (51 cm) on January 13, 1950. Seasonal snowfall has ranged from zero in 1991–92 to 67.5 in (171 cm) in 1968–69, with trace amounts having occurred as recently as 2009–10. The month of January 1950 was particularly severe, bringing 57.2 in (145 cm) of snow, the most of any month along with the aforementioned record cold.
The Puget Sound Convergence Zone is an important feature of Seattle's weather. In the convergence zone, air arriving from the north meets air flowing in from the south. Both streams of air originate over the Pacific Ocean; airflow is split by the Olympic Mountains to Seattle's west, then reunited to the east. When the air currents meet, they are forced upward, resulting in convection. Thunderstorms caused by this activity are usually weak and can occur north and south of town, but Seattle itself rarely receives more than occasional thunder and small hail showers. The Hanukkah Eve Wind Storm in December 2006 is an exception that brought heavy rain and winds gusting up to 69 mph (111 km/h), an event that was not caused by the Puget Sound Convergence Zone and was widespread across the Pacific Northwest.
One of many exceptions to Seattle's reputation as a damp location occurs in El Niño years, when marine weather systems track as far south as California and little precipitation falls in the Puget Sound area. Since the region's water comes from mountain snow packs during the dry summer months, El Niño winters can not only produce substandard skiing but can result in water rationing and a shortage of hydroelectric power the following summer.

According to the 2010 United States Census, Seattle had a population of 608,660 with a racial and ethnic composition as follows:
White: 69.5% (Non-Hispanic Whites: 66.3%)
Asian: 13.8% (4.1% Chinese, 2.6% Filipino, 2.2% Vietnamese, 1.3% Japanese, 1.1% Korean, 0.8% Indian, 0.3% Cambodian, 0.3% Laotian, 0.2% Pakistanis, 0.2% Indonesian, 0.2% Thai)
Black or African American: 7.9%
Hispanic or Latino (of any race): 6.6% (4.1% Mexican, 0.3% Puerto Rican, 0.2% Guatemalan, 0.2% Salvadoran, 0.2% Cuban)
American Indian and Alaska Native: 0.8%
Native Hawaiian and Other Pacific Islander: 0.4%
Other race: 2.4%
Two or more races: 5.1%
Seattle's population historically has been predominantly white. The 2010 census showed that Seattle was one of the whitest big cities in the country, although its proportion of white residents has been gradually declining. In 1960, whites comprised 91.6% of the city's population, while in 2010 they comprised 69.5%. According to the 2006–2008 American Community Survey, approximately 78.9% of residents over the age of five spoke only English at home. Those who spoke Asian languages other than Indo-European languages made up 10.2% of the population, Spanish was spoken by 4.5% of the population, speakers of other Indo-European languages made up 3.9%, and speakers of other languages made up 2.5%.
Seattle's foreign-born population grew 40% between the 1990 and 2000 censuses. The Chinese population in the Seattle area has origins in mainland China, Hong Kong, Southeast Asia, and Taiwan. The earliest Chinese-Americans that came in the late 19th and early 20th centuries were almost entirely from Guangdong Province. The Seattle area is also home to a large Vietnamese population of more than 55,000 residents, as well as over 30,000 Somali immigrants. The Seattle-Tacoma area is also home to one of the largest Cambodian communities in the United States, numbering about 19,000 Cambodian Americans, and one of the largest Samoan communities in the mainland U.S., with over 15,000 people having Samoan ancestry. Additionally, the Seattle area had the highest percentage of self-identified mixed-race people of any large metropolitan area in the United States, according to the 2000 United States Census Bureau. According to a 2012 HistoryLink study, Seattle's 98118 ZIP code (in the Columbia City neighborhood) was one of the most diverse ZIP Code Tabulation Areas in the United States.
In 1999, the median income of a city household was $45,736, and the median income for a family was $62,195. Males had a median income of $40,929 versus $35,134 for females. The per capita income for the city was $30,306. 11.8% of the population and 6.9% of families are below the poverty line. Of people living in poverty, 13.8% are under the age of 18 and 10.2% are 65 or older.
It is estimated that King County has 8,000 homeless people on any given night, and many of those live in Seattle. In September 2005, King County adopted a "Ten-Year Plan to End Homelessness", one of the near-term results of which is a shift of funding from homeless shelter beds to permanent housing.
In recent years, the city has experienced steady population growth, and has been faced with the issue of accommodating more residents. In 2006, after growing by 4,000 citizens per year for the previous 16 years, regional planners expected the population of Seattle to grow by 200,000 people by 2040. However, former mayor Greg Nickels supported plans that would increase the population by 60%, or 350,000 people, by 2040 and worked on ways to accommodate this growth while keeping Seattle's single-family housing zoning laws. The Seattle City Council later voted to relax height limits on buildings in the greater part of Downtown, partly with the aim to increase residential density in the city center. As a sign of increasing inner-city growth, the downtown population crested to over 60,000 in 2009, up 77% since 1990.
Seattle also has large lesbian, gay, bisexual, and transgender populations. According to a 2006 study by UCLA, 12.9% of city residents polled identified as gay, lesbian, or bisexual. This was the second-highest proportion of any major U.S. city, behind San Francisco Greater Seattle also ranked second among major U.S. metropolitan areas, with 6.5% of the population identifying as gay, lesbian, or bisexual. According to 2012 estimates from the United States Census Bureau, Seattle has the highest percentage of same-sex households in the United States, at 2.6 per cent, surpassing San Francisco.
In addition, Seattle has a relatively high number of people living alone. According to the 2000 U.S. Census interim measurements of 2004, Seattle has the fifth highest proportion of single-person households nationwide among cities of 100,000 or more residents, at 40.8%.

Seattle's economy is driven by a mix of older industrial companies, and "new economy" Internet and technology companies, service, design and clean technology companies. The city's gross metropolitan product was $231 billion in 2010, making it the 11th largest metropolitan economy in the United States. The Port of Seattle, which also operates Seattle–Tacoma International Airport, is a major gateway for trade with Asia and cruises to Alaska, and is the 8th largest port in the United States in terms of container capacity. Though it was affected by the Great Recession, Seattle has retained a comparatively strong economy, and remains a hotbed for start-up businesses, especially in green building and clean technologies: it was ranked as America's No. 1 "smarter city" based on its government policies and green economy. In February 2010, the city government committed Seattle to becoming North America's first "climate neutral" city, with a goal of reaching zero net per capita greenhouse gas emissions by 2030.

Still, very large companies dominate the business landscape. Four companies on the 2013 Fortune 500 list of the United States' largest companies, based on total revenue, are headquartered in Seattle: Internet retailer Amazon.com (#49), coffee chain Starbucks (#208), department store Nordstrom (#227), and freight forwarder Expeditors International of Washington (#428). Other Fortune 500 companies popularly associated with Seattle are based in nearby Puget Sound cities. Warehouse club chain Costco (#22), the largest retail company in Washington, is based in Issaquah. Microsoft (#35) is located in Redmond. Weyerhaeuser, the forest products company (#363), is based in Federal Way. Finally, Bellevue is home to truck manufacturer Paccar (#168). Other major companies in the area include Nintendo of America in Redmond, T-Mobile US in Bellevue, Expedia Inc. in Bellevue and Providence Health & Services — the state's largest health care system and fifth largest employer — in Renton. The city has a reputation for heavy coffee consumption; coffee companies founded or based in Seattle include Starbucks, Seattle's Best Coffee, and Tully's. There are also many successful independent artisanal espresso roasters and cafés.
Prior to moving its headquarters to Chicago, aerospace manufacturer Boeing (#30) was the largest company based in Seattle. Its largest division is still headquartered in nearby Renton, and the company has large aircraft manufacturing plants in Everett and Renton, so it remains the largest private employer in the Seattle metropolitan area. Former Seattle Mayor Greg Nickels announced a desire to spark a new economic boom driven by the biotechnology industry in 2006. Major redevelopment of the South Lake Union neighborhood is underway, in an effort to attract new and established biotech companies to the city, joining biotech companies Corixa (acquired by GlaxoSmithKline), Immunex (now part of Amgen), Trubion, and ZymoGenetics. Vulcan Inc., the holding company of billionaire Paul Allen, is behind most of the development projects in the region. While some see the new development as an economic boon, others have criticized Nickels and the Seattle City Council for pandering to Allen's interests at taxpayers' expense. Also in 2006, Expansion Magazine ranked Seattle among the top 10 metropolitan areas in the nation for climates favorable to business expansion. In 2005, Forbes ranked Seattle as the most expensive American city for buying a house based on the local income levels. In 2013, however, the magazine ranked Seattle No. 9 on its list of the Best Places for Business and Careers.
Alaska Airlines, operating a hub at Seattle–Tacoma International Airport, maintains its headquarters in the city of SeaTac, next to the airport.
Seattle is a hub for global health with the headquarters of the Bill & Melinda Gates Foundation, PATH, Infectious Disease Research Institute, Fred Hutchinson Cancer Research Center and the Institute for Health Metrics and Evaluation. In 2015, the Washington Global Health Alliance counted 168 global health organizations in Washington state, many are headquartered in Seattle.

From 1869 until 1982, Seattle was known as the "Queen City". Seattle's current official nickname is the "Emerald City", the result of a contest held in 1981; the reference is to the lush evergreen forests of the area. Seattle is also referred to informally as the "Gateway to Alaska" for being the nearest major city in the contiguous US to Alaska, "Rain City" for its frequent cloudy and rainy weather, and "Jet City" from the local influence of Boeing. The city has two official slogans or mottos: "The City of Flowers", meant to encourage the planting of flowers to beautify the city, and "The City of Goodwill", adopted prior to the 1990 Goodwill Games. Seattle residents are known as Seattleites.

Seattle has been a regional center for the performing arts for many years. The century-old Seattle Symphony Orchestra is among the world's most recorded and performs primarily at Benaroya Hall. The Seattle Opera and Pacific Northwest Ballet, which perform at McCaw Hall (opened 2003 on the site of the former Seattle Opera House at Seattle Center), are comparably distinguished, with the Opera being particularly known for its performances of the works of Richard Wagner and the PNB School (founded in 1974) ranking as one of the top three ballet training institutions in the United States. The Seattle Youth Symphony Orchestras (SYSO) is the largest symphonic youth organization in the United States. The city also boasts lauded summer and winter chamber music festivals organized by the Seattle Chamber Music Society.
The 5th Avenue Theatre, built in 1926, stages Broadway-style musical shows featuring both local talent and international stars. Seattle has "around 100" theatrical production companies and over two dozen live theatre venues, many of them associated with fringe theatre; Seattle is probably second only to New York for number of equity theaters (28 Seattle theater companies have some sort of Actors' Equity contract). In addition, the 900-seat Romanesque Revival Town Hall on First Hill hosts numerous cultural events, especially lectures and recitals.

Between 1918 and 1951, there were nearly two dozen jazz nightclubs along Jackson Street, running from the current Chinatown/International District to the Central District. The jazz scene developed the early careers of Ray Charles, Quincy Jones, Bumps Blackwell, Ernestine Anderson, and others.
Early popular musical acts from the Seattle/Puget Sound area include the collegiate folk group The Brothers Four, vocal group The Fleetwoods, 1960s garage rockers The Wailers and The Sonics, and instrumental surf group The Ventures, some of whom are still active.
Seattle is considered the home of grunge music, having produced artists such as Nirvana, Soundgarden, Alice in Chains, Pearl Jam, and Mudhoney, all of whom reached international audiences in the early 1990s. The city is also home to such varied artists as avant-garde jazz musicians Bill Frisell and Wayne Horvitz, hot jazz musician Glenn Crytzer, hip hop artists Sir Mix-a-Lot, Macklemore, Blue Scholars, and Shabazz Palaces, smooth jazz saxophonist Kenny G, classic rock staples Heart and Queensrÿche, and alternative rock bands such as Foo Fighters, Harvey Danger, The Presidents of the United States of America, The Posies, Modest Mouse, Band of Horses, Death Cab for Cutie, and Fleet Foxes. Rock musicians such as Jimi Hendrix, Duff McKagan, and Nikki Sixx spent their formative years in Seattle.
The Seattle-based Sub Pop record company continues to be one of the world's best-known independent/alternative music labels.
Over the years, a number of songs have been written about Seattle.
Seattle annually sends a team of spoken word slammers to the National Poetry Slam and considers itself home to such performance poets as Buddy Wakefield, two-time Individual World Poetry Slam Champ; Anis Mojgani, two-time National Poetry Slam Champ; and Danny Sherrard, 2007 National Poetry Slam Champ and 2008 Individual World Poetry Slam Champ. Seattle also hosted the 2001 national Poetry Slam Tournament. The Seattle Poetry Festival is a biennial poetry festival that (launched first as the Poetry Circus in 1997) has featured local, regional, national, and international names in poetry.
The city also has movie houses showing both Hollywood productions and works by independent filmmakers. Among these, the Seattle Cinerama stands out as one of only three movie theaters in the world still capable of showing three-panel Cinerama films.

Among Seattle's prominent annual fairs and festivals are the 24-day Seattle International Film Festival, Northwest Folklife over the Memorial Day weekend, numerous Seafair events throughout July and August (ranging from a Bon Odori celebration to the Seafair Cup hydroplane races), the Bite of Seattle, one of the largest Gay Pride festivals in the United States, and the art and music festival Bumbershoot, which programs music as well as other art and entertainment over the Labor Day weekend. All are typically attended by 100,000 people annually, as are the Seattle Hempfest and two separate Independence Day celebrations.
Other significant events include numerous Native American pow-wows, a Greek Festival hosted by St. Demetrios Greek Orthodox Church in Montlake, and numerous ethnic festivals (many associated with Festál at Seattle Center).
There are other annual events, ranging from the Seattle Antiquarian Book Fair & Book Arts Show; an anime convention, Sakura-Con; Penny Arcade Expo, a gaming convention; a two-day, 9,000-rider Seattle to Portland Bicycle Classic; and specialized film festivals, such as the Maelstrom International Fantastic Film Festival, the Seattle Asian American Film Festival (formerly known as the Northwest Asian American Film Festival), Children's Film Festival Seattle, Translation: the Seattle Transgender Film Festival, the Seattle Gay and Lesbian Film Festival, Seattle Latino Film Festival, and the Seattle Polish Film Festival.
The Henry Art Gallery opened in 1927, the first public art museum in Washington. The Seattle Art Museum (SAM) opened in 1933; SAM opened a museum downtown in 1991 (expanded and reopened 2007); since 1991, the 1933 building has been SAM's Seattle Asian Art Museum (SAAM). SAM also operates the Olympic Sculpture Park (opened 2007) on the waterfront north of the downtown piers. The Frye Art Museum is a free museum on First Hill.
Regional history collections are at the Loghouse Museum in Alki, Klondike Gold Rush National Historical Park, the Museum of History and Industry, and the Burke Museum of Natural History and Culture. Industry collections are at the Center for Wooden Boats and the adjacent Northwest Seaport, the Seattle Metropolitan Police Museum, and the Museum of Flight. Regional ethnic collections include the Nordic Heritage Museum, the Wing Luke Asian Museum, and the Northwest African American Museum. Seattle has artist-run galleries, including ten-year veteran Soil Art Gallery, and the newer Crawl Space Gallery.

The Seattle Great Wheel, one of the largest Ferris wheels in the US, opened in June 2012 as a new, permanent attraction on the city's waterfront, at Pier 57, next to Downtown Seattle. The city also has many community centers for recreation, including Rainier Beach, Van Asselt, Rainier, and Jefferson south of the Ship Canal and Green Lake, Laurelhurst, Loyal Heights north of the Canal, and Meadowbrook.
Woodland Park Zoo opened as a private menagerie in 1889 but was sold to the city in 1899. The Seattle Aquarium has been open on the downtown waterfront since 1977 (undergoing a renovation 2006). The Seattle Underground Tour is an exhibit of places that existed before the Great Fire.
Since the middle 1990s, Seattle has experienced significant growth in the cruise industry, especially as a departure point for Alaska cruises. In 2008, a record total of 886,039 cruise passengers passed through the city, surpassing the number for Vancouver, BC, the other major departure point for Alaska cruises.

Seattle has three major men's professional sports teams: the National Football League (NFL)'s Seattle Seahawks, Major League Baseball (MLB)'s Seattle Mariners, and Major League Soccer (MLS)'s Seattle Sounders FC. Other professional sports teams include the Women's National Basketball Association (WNBA)'s Seattle Storm, who won the WNBA championship in 2004 and 2010, and the Seattle Reign of the National Women's Soccer League.
The Seahawks' CenturyLink Field has hosted NFL playoff games in 2006, 2008, 2011, 2014, 2015, and 2017. The Seahawks have advanced to the Super Bowl three times: 2005, 2013 and 2014. They defeated the Denver Broncos 43–8 to win their first Super Bowl championship in Super Bowl XLVIII, but lost 24–28 against the New England Patriots in Super Bowl XLIX. The Seahawks also held the NFL playoffs at the Kingdome in 1983, 1984 and 2000. The 2000 playoff game was the last game of football of any type and of any sport at The Kingdome.
Seattle Sounders FC has played in Major League Soccer since 2009, sharing CenturyLink Field with the Seahawks, as a continuation of earlier teams in the lower divisions of American soccer. The Sounders have won the MLS Supporters' Shield in 2014 and the Lamar Hunt U.S. Open Cup on four occasions: 2009, 2010, 2011, and 2014. The Sounders won their first MLS Cup after defeating Toronto FC, 5-4 in penalty kicks, in MLS Cup 2016. With the Sounders' first MLS Cup championship in franchise history, the Mariners are currently the only men's professional sports team in the city without a championship, let alone a championship series appearance.

Seattle's professional sports history began at the start of the 20th century with the PCHA's Seattle Metropolitans, which in 1917 became the first American hockey team to win the Stanley Cup. Seattle was also home to a previous Major League Baseball franchise in 1969: the Seattle Pilots. The Pilots relocated to Milwaukee, Wisconsin and became the Milwaukee Brewers for the 1970 season. From 1967 to 2008 Seattle was also home to an National Basketball Association (NBA) franchise: the Seattle SuperSonics, who were the 1978–79 NBA champions. The SuperSonics relocated to Oklahoma City and became the Oklahoma City Thunder for the 2008–09 season.
The Major League Baseball All-Star Game was held in Seattle twice, first at the Kingdome in 1979 and again at Safeco Field in 2001. That same year, the Seattle Mariners tied the all-time single regular season wins record with 116 wins. The NBA All-Star Game was also held in Seattle twice: the first in 1974 at the Seattle Center Coliseum and the second in 1987 at the Kingdome.
The Seattle Thunderbirds hockey team plays in the Canadian major-junior Western Hockey League and are based in the Seattle suburb of Kent. Seattle also boasts a strong history in collegiate sports. The University of Washington and Seattle University are NCAA Division I schools. The University of Washington's athletic program, nicknamed the Huskies, competes in the Pac-12 Conference, and Seattle University's athletic program, nicknamed the Redhawks, competes in the Western Athletic Conference.

Seattle's mild, temperate, marine climate allows year-round outdoor recreation, including walking, cycling, hiking, skiing, snowboarding, kayaking, rock climbing, motor boating, sailing, team sports, and swimming.
In town, many people walk around Green Lake, through the forests and along the bluffs and beaches of 535-acre (2.2 km2) Discovery Park (the largest park in the city) in Magnolia, along the shores of Myrtle Edwards Park on the Downtown waterfront, along the shoreline of Lake Washington at Seward Park, along Alki Beach in West Seattle, or along the Burke-Gilman Trail.

Gas Works Park features the majestic preserved superstructure of a coal gasification plant closed in 1956. Located across Lake Union from downtown, the park provides panoramic views of the Seattle skyline.
Also popular are hikes and skiing in the nearby Cascade or Olympic Mountains and kayaking and sailing in the waters of Puget Sound, the Strait of Juan de Fuca, and the Strait of Georgia. In 2005, Men's Fitness magazine named Seattle the fittest city in the United States.
In its 2013 ParkScore ranking, the Trust for Public Land reported that Seattle had the tenth best park system among the 50 most populous US cities. ParkScore ranks city park systems by a formula that analyzes acreage, access, and service and investment.

Seattle is a charter city, with a mayor–council form of government. From 1911 to 2013, Seattle's nine city councillors were elected at large, rather than by geographic subdivisions. For the 2015 election, this changed to a hybrid system of seven district members and two at-large members as a result of a ballot measure passed on November 5, 2013. The only other elected offices are the city attorney and Municipal Court judges. All city offices are officially non-partisan.
Like some other parts of the United States, government and laws are also run by a series of ballot initiatives (allowing citizens to pass or reject laws), referenda (allowing citizens to approve or reject legislation already passed), and propositions (allowing specific government agencies to propose new laws or tax increases directly to the people). Federally, Seattle is part of Washington's 7th congressional district, represented by Democrat Jim McDermott, elected in 1988 and one of Congress's liberal members. Ed Murray is currently serving as mayor.
Seattle's political culture is very liberal and progressive for the United States, with over 80% of the population voting for the Democratic Party. All precincts in Seattle voted for Democratic Party candidate Barack Obama in the 2012 presidential election. In partisan elections for the Washington State Legislature and United States Congress, nearly all elections are won by Democrats. Seattle is considered the first major American city to elect a female mayor, Bertha Knight Landes. It has also elected an openly gay mayor, Ed Murray, and a socialist councillor, Kshama Sawant. For the first time in United States history, an openly gay black woman was elected to public office when Sherry Harris was elected as a Seattle city councillor in 1991. The majority of the current city council is female, while white men comprise a minority.
Seattle is widely considered one of the most liberal cities in the United States, even surpassing its neighbor, Portland, Oregon. Support for issues such as same-sex marriage and reproductive rights are largely taken for granted in local politics. In the 2012 U.S. general election, an overwhelming majority of Seattleites voted to approve Referendum 74 and legalize gay marriage in Washington state. In the same election, an overwhelming majority of Seattleites also voted to approve the legalization of the recreational use of cannabis in the state. Like much of the Pacific Northwest (which has the lowest rate of church attendance in the United States and consistently reports the highest percentage of atheism), church attendance, religious belief, and political influence of religious leaders are much lower than in other parts of America.
Seattle also has a thriving alternative press, with the Web-based daily Seattle Post-Intelligencer, several other online dailies (including Publicola and Crosscut), The Stranger (an alternative, left-leaning weekly), Seattle Weekly, and a number of issue-focused publications, including the nation's two largest online environmental magazines, Worldchanging and Grist.org.
In July 2012, Seattle banned plastic shopping bags. In June 2014 the city passed a local ordinance to increase the minimum wage to $15 an hour on a staged basis from 2015 to 2021. When fully implemented the $15 hourly rate will be the highest minimum wage in the nation.
On October 6, 2014, Seattle officially replaced Columbus Day with Indigenous Peoples' Day, honoring Seattle's Native American community and controversies surrounding the legacy of Christopher Columbus.

Of the city's population over the age of 25, 53.8% (vs. a national average of 27.4%) hold a bachelor's degree or higher, and 91.9% (vs. 84.5% nationally) have a high school diploma or equivalent. A 2008 United States Census Bureau survey showed that Seattle had the highest percentage of college and university graduates of any major U.S. city. The city was listed as the most literate of the country's 69 largest cities in 2005 and 2006, the second most literate in 2007 and the most literate in 2008 in studies conducted by Central Connecticut State University.

Seattle Public Schools desegregated without a court order but continue to struggle to achieve racial balance in a somewhat ethnically divided city (the south part of town having more ethnic minorities than the north). In 2007, Seattle's racial tie-breaking system was struck down by the United States Supreme Court, but the ruling left the door open for desegregation formulae based on other indicators (e.g., income or socioeconomic class).
The public school system is supplemented by a moderate number of private schools: five of the private high schools are Catholic, one is Lutheran, and six are secular.
Seattle is home to the University of Washington, as well as the institution's professional and continuing education unit, the University of Washington Educational Outreach. A study by Newsweek International in 2006 cited the University of Washington as the twenty-second best university in the world. Seattle also has a number of smaller private universities including Seattle University and Seattle Pacific University, the former a Jesuit Catholic institution, the latter Free Methodist; universities aimed at the working adult, like City University and Antioch University; colleges within the Seattle Colleges District system, comprising North, Central, and South; seminaries, including Western Seminary and a number of arts colleges, such as Cornish College of the Arts, Pratt Fine Arts Center, and The Art Institute of Seattle. In 2001, Time magazine selected Seattle Central Community College as community college of the year, stating the school "pushes diverse students to work together in small teams".

As of 2010, Seattle has one major daily newspaper, The Seattle Times. The Seattle Post-Intelligencer, known as the P-I, published a daily newspaper from 1863 to March 17, 2009, before switching to a strictly on-line publication. There is also the Seattle Daily Journal of Commerce, and the University of Washington publishes The Daily, a student-run publication, when school is in session. The most prominent weeklies are the Seattle Weekly and The Stranger; both consider themselves "alternative" papers. The weekly LGBT newspaper is the Seattle Gay News. Real Change is a weekly street newspaper that is sold mainly by homeless persons as an alternative to panhandling. There are also several ethnic newspapers, including The Facts, Northwest Asian Weekly and the International Examiner, and numerous neighborhood newspapers.
Seattle is also well served by television and radio, with all major U.S. networks represented, along with at least five other English-language stations and two Spanish-language stations. Seattle cable viewers also receive CBUT 2 (CBC) from Vancouver, British Columbia.
Non-commercial radio stations include NPR affiliates KUOW-FM 94.9 and KNKX 88.5 (Tacoma), as well as classical music station KING-FM 98.1. Other non-commercial stations include KEXP-FM 90.3 (affiliated with the UW), community radio KBCS-FM 91.3 (affiliated with Bellevue College), and high school radio KNHC-FM 89.5, which broadcasts an electronic dance music radio format and is owned by the public school system and operated by students of Nathan Hale High School. Many Seattle radio stations are also available through Internet radio, with KEXP in particular being a pioneer of Internet radio. Seattle also has numerous commercial radio stations. In a March 2012 report by the consumer research firm Arbitron, the top FM stations were KRWM (adult contemporary format), KIRO-FM (news/talk), and KISW (active rock) while the top AM stations were KOMO (AM) (all news), KJR (AM) (all sports), KIRO (AM) (all sports).
Seattle-based online magazines Worldchanging and Grist.org were two of the "Top Green Websites" in 2007 according to TIME.
Seattle also has many online news media websites. The two largest are The Seattle Times and the Seattle Post-Intelligencer.

The University of Washington is consistently ranked among the country's top leading institutions in medical research, earning special merits for programs in neurology and neurosurgery. Seattle has seen local developments of modern paramedic services with the establishment of Medic One in 1970. In 1974, a 60 Minutes story on the success of the then four-year-old Medic One paramedic system called Seattle "the best place in the world to have a heart attack".
Three of Seattle's largest medical centers are located on First Hill. Harborview Medical Center, the public county hospital, is the only Level I trauma hospital in a region that includes Washington, Alaska, Montana, and Idaho. Virginia Mason Medical Center and Swedish Medical Center's two largest campuses are also located in this part of Seattle, including the Virginia Mason Hospital. This concentration of hospitals resulted in the neighborhood's nickname "Pill Hill".
Located in the Laurelhurst neighborhood, Seattle Children's, formerly Children's Hospital and Regional Medical Center, is the pediatric referral center for Washington, Alaska, Montana, and Idaho. The Fred Hutchinson Cancer Research Center has a campus in the Eastlake neighborhood. The University District is home to the University of Washington Medical Center which, along with Harborview, is operated by the University of Washington. Seattle is also served by a Veterans Affairs hospital on Beacon Hill, a third campus of Swedish in Ballard, and Northwest Hospital and Medical Center near Northgate Mall.

The first streetcars appeared in 1889 and were instrumental in the creation of a relatively well-defined downtown and strong neighborhoods at the end of their lines. The advent of the automobile sounded the death knell for rail in Seattle. Tacoma–Seattle railway service ended in 1929 and the Everett–Seattle service came to an end in 1939, replaced by inexpensive automobiles running on the recently developed highway system. Rails on city streets were paved over or removed, and the opening of the Seattle trolleybus system brought the end of streetcars in Seattle in 1941. This left an extensive network of privately owned buses (later public) as the only mass transit within the city and throughout the region.

King County Metro provides frequent stop bus service within the city and surrounding county, as well as a South Lake Union Streetcar line between the South Lake Union neighborhood and Westlake Center in downtown. Seattle is one of the few cities in North America whose bus fleet includes electric trolleybuses. Sound Transit currently provides an express bus service within the metropolitan area, two Sounder commuter rail lines between the suburbs and downtown, and its Central Link light rail line between the University of Washington and Sea-Tac Airport. Washington State Ferries, which manages the largest network of ferries in the United States and third largest in the world, connects Seattle to Bainbridge and Vashon Islands in Puget Sound and to Bremerton and Southworth on the Kitsap Peninsula.

According to the 2007 American Community Survey, 18.6% of Seattle residents used one of the three public transit systems that serve the city, giving it the highest transit ridership of all major cities without heavy or light rail prior to the completion of Sound Transit's Central Link line. The city has also been described by Bert Sperling as the fourth most walkable U.S. city and by Walk Score as the sixth most walkable of the fifty largest U.S. cities.
Seattle–Tacoma International Airport, locally known as Sea-Tac Airport and located just south in the neighboring city of SeaTac, is operated by the Port of Seattle and provides commercial air service to destinations throughout the world. Closer to downtown, Boeing Field is used for general aviation, cargo flights, and testing/delivery of Boeing airliners.

The main mode of transportation, however, relies on Seattle's streets, which are laid out in a cardinal directions grid pattern, except in the central business district where early city leaders Arthur Denny and Carson Boren insisted on orienting their plats relative to the shoreline rather than to true North. Only two roads, Interstate 5 and State Route 99 (both limited-access highways), run uninterrupted through the city from north to south. State Route 99 runs through downtown Seattle on the Alaskan Way Viaduct, which was built in 1953. However, due to damage sustained during the 2001 Nisqually earthquake the viaduct will be replaced by a tunnel. The 2-mile (3.2 km) Alaskan Way Viaduct replacement tunnel was originally scheduled to be completed in December 2015 at a cost of US$4.25 billion. Unfortunately, due to issues with the worlds largest tunnel boring machine (TBM), which is nicknamed "Bertha" and is 57 feet (17 m) in diameter, the projected date of completion has been pushed back to 2017. Seattle has the 8th worst traffic congestion of all American cities, and is 10th among all North American cities.
The city has started moving away from the automobile and towards mass transit. From 2004 to 2009, the annual number of unlinked public transportation trips increased by approximately 21%. In 2006, voters in King County passed proposition 2 (Transit Now) which increased bus service hours on high ridership routes and paid for five bus rapid transit lines called RapidRide. After rejecting a roads and transit measure in 2007, Seattle-area voters passed a transit only measure in 2008 to increase ST Express bus service, extend the Link light rail system, and expand and improve Sounder commuter rail service. A light rail line from downtown heading south to Sea-Tac Airport began service on December 19, 2009, giving the city its first rapid transit line with intermediate stations within the city limits. An extension north to the University of Washington opened on March 19, 2016; and further extensions are planned to reach Lynnwood to the north, Des Moines to the south, and Bellevue and Redmond to the east by 2023. Voters in the Puget Sound region approved an additional tax increase in November, 2016 to expand light rail to West Seattle and Ballard as well as Tacoma, Everett, and Issaquah.

Water and electric power are municipal services, provided by Seattle Public Utilities and Seattle City Light respectively. Other utility companies serving Seattle include Puget Sound Energy (natural gas, electricity); Seattle Steam Company (steam); Waste Management, Inc and CleanScapes, Inc. (curbside recycling and solid waste removal); CenturyLink, Frontier Communications, Wave Broadband, and Comcast (telecommunications and television).
About 90% of Seattle's electricity is produced using hydropower. Less than 2% of electricity is produced using fossil fuels.

Seattle is partnered with:

National Register of Historic Places listings in Seattle, Washington
Seattle Freeze
Seattle process
Seattle tugboats
Tillicum Village

Jones, Nard (1972). Seattle. New York: Doubleday. ISBN 978-0-385-01875-3.
Morgan, Murray (1982) [1951]. Skid Road: an Informal Portrait of Seattle (revised and updated, first illustrated ed.). Seattle and London: University of Washington Press. ISBN 978-0-295-95846-0.
Ochsner, Jeffrey Karl, ed. (1998) [1994]. Shaping Seattle Architecture: A Historical Guide to the Architects. Seattle and London: University of Washington Press. ISBN 978-0-295-97366-1.
Sale, Roger (1976). Seattle: Past to Present. Seattle and London: University of Washington Press. ISBN 978-0-295-95615-2.
Speidel, William C. (1978). Doc Maynard: The Man Who Invented Seattle. Seattle: Nettle Creek Publishing Company. pp. 196–197, 200. ISBN 978-0-914890-02-7.
Speidel, William C. (1967). Sons of the profits; or, There's no business like grow business: the Seattle story, 1851–1901. Seattle: Nettle Creek Publishing Company. pp. 196–197, 200. ISBN 0-914890-00-X.

Klingle, Matthew (2007). Emerald City: An Environmental History of Seattle. New Haven: Yale University Press. ISBN 978-0-300-11641-0.
MacGibbon, Elma (1904). "Seattle, the city of destiny". Leaves of knowledge (DJVU). Washington State Library's Classics in Washington History collection. Shaw & Borden. OCLC 61326250.
Pierce, J. Kingston (2003). Eccentric Seattle: Pillars and Pariahs Who Made the City Not Such a Boring Place After All. Pullman, Washington: Washington State University Press. ISBN 978-0-87422-269-2.
Sanders, Jeffrey Craig. Seattle and the Roots of Urban Sustainability: Inventing Ecotopia (University of Pittsburgh Press; 2010) 288 pages; the rise of environmental activism

Official website of the City of Seattle
Historylink.org, history of Seattle and Washington
Seattle Photographs from the University of Washington Digital Collections
Seattle Historic Photograph Collection from the Seattle Public Library
Seattle Civil Rights and Labor History Project
Seattle, a National Park Service Discover Our Shared Heritage Travel ItineraryChicago (/ʃᵻˈkɑːɡoʊ/ or /ʃᵻˈkɔːɡoʊ/), officially the City of Chicago, is the third-most populous city in the United States, and the fifth-most populous city in North America. With over 2.7 million residents, it is the most populous city in the state of Illinois and the Midwestern United States, and the county seat of Cook County. The Chicago metropolitan area, often referred to as Chicagoland, has nearly 10 million people and is the third-largest in the U.S.
Chicago was incorporated as a city in 1837, near a portage between the Great Lakes and the Mississippi River watershed, and grew rapidly in the mid-nineteenth century. The city is an international hub for finance, commerce, industry, technology, telecommunications, and transportation: O'Hare International Airport is the second-busiest airport in the world when measured by aircraft traffic; the region also has the largest number of U.S. highways and rail road freight. In 2012, Chicago was listed as an alpha global city by the Globalization and World Cities Research Network, and ranked seventh in the world in the 2014 Global Cities Index. Chicago has the third-largest gross metropolitan product in the United States—about $640 billion according to 2015 estimates. The city has one of the world's largest and most diversified economies with no single industry employing more than 14% of the workforce.
In 2016, Chicago hosted over 54 million international and domestic visitors, a new record for the city making it one of the top visited cities in the nation. Chicago's culture includes the visual arts, novels, film, theater, especially improvisational comedy, and music, particularly jazz, blues, soul, gospel and house music. There are many colleges and universities in the Chicago area; among these, Northwestern University, the University of Chicago, and the University of Illinois at Chicago are classified as "highest research" doctoral universities. Chicago also has professional sports teams in each of the major professional leagues. The city has many nicknames, the best-known being the Windy City.

The name "Chicago" is derived from a French rendering of the Native American word shikaakwa, known to botanists as Allium tricoccum, from the Miami-Illinois language. The first known reference to the site of the current city of Chicago as "Checagou" was by Robert de LaSalle around 1679 in a memoir. Henri Joutel, in his journal of 1688, noted that the wild garlic, called "chicagoua", grew abundantly in the area. According to his diary of late September 1687:

when we arrived at the said place called Chicagou which, according to what we were able to learn of it, has taken this name because of the quantity of garlic which grows in the forests in this region.

In the mid-18th century, the area was inhabited by a Native American tribe known as the Potawatomi, who had taken the place of the Miami and Sauk and Fox peoples. The first known non-indigenous permanent settler in Chicago was Jean Baptiste Point du Sable. Du Sable was of African and French descent and arrived in the 1780s. He is commonly known as the "Founder of Chicago".
In 1795, following the Northwest Indian War, an area that was to be part of Chicago was turned over to the United States for a military post by native tribes in accordance with the Treaty of Greenville. In 1803, the United States Army built Fort Dearborn, which was destroyed in 1812 in the Battle of Fort Dearborn and later rebuilt. The Ottawa, Ojibwe, and Potawatomi tribes had ceded additional land to the United States in the 1816 Treaty of St. Louis. The Potawatomi were forcibly removed from their land after the Treaty of Chicago in 1833.

On August 12, 1833, the Town of Chicago was organized with a population of about 200. Within seven years it grew to more than 4,000 people. On June 15, 1835, the first public land sales began with Edmund Dick Taylor as U.S. receiver of public moneys. The City of Chicago was incorporated on Saturday, March 4, 1837 and for several decades was the world's fastest growing city.
As the site of the Chicago Portage, the city became an important transportation hub between the eastern and western United States. Chicago's first railway, Galena and Chicago Union Railroad, and the Illinois and Michigan Canal opened in 1848. The canal allowed steamboats and sailing ships on the Great Lakes to connect to the Mississippi River.
A flourishing economy brought residents from rural communities and immigrants from abroad. Manufacturing and retail and finance sectors became dominant, influencing the American economy. The Chicago Board of Trade (established 1848) listed the first ever standardized 'exchange traded' forward contracts, which were called futures contracts.

In the 1850s, Chicago gained national political prominence as the home of Senator Stephen Douglas, the champion of the Kansas–Nebraska Act and the "popular sovereignty" approach to the issue of the spread of slavery. These issues also helped propel another Illinoisan, Abraham Lincoln, to the national stage. Lincoln was nominated in Chicago for US President at the 1860 Republican National Convention. He defeated Douglas in the general election, and this set the stage for the American Civil War.
To accommodate rapid population growth and demand for better sanitation, the city improved its infrastructure. In February 1856, Chicago's Common Council approved Chesbrough's plan to build the United States' first comprehensive sewerage system. The project raised much of central Chicago to a new grade. While elevating Chicago, and at first improving the city's health, the untreated sewage and industrial waste now flowed into the Chicago River, then into Lake Michigan, polluting the city's primary freshwater source.
The city responded by tunneling two miles (3 km) out into Lake Michigan to newly-built water cribs. In 1900, the problem of sewage contamination was largely resolved when the city completed a major engineering feat. It reversed the flow of the Chicago River so the water flowed away from Lake Michigan rather than into it. This project began with the construction and improvement of the Illinois and Michigan Canal, and was completed with the Chicago Sanitary and Ship Canal that connects to the Illinois River, which flows into the Mississippi River.
In 1871, the Great Chicago Fire destroyed an area of about 4 miles long and 1 mile wide, a large section of the city at the time. Much of the city, including railroads and stockyards, survived intact, and from the ruins of the previous wooden structures arose more modern constructions of steel and stone. These set a precedent for worldwide construction. During its rebuilding period, Chicago constructed the world's first skyscraper in 1885, using steel-skeleton construction.
The city grew significantly in size and population by incorporating many neighboring townships between 1851 and 1920, with the largest annexation happening in 1889, with five townships joining the city, including the Hyde Park Township, which now comprises most of the South Side of Chicago and the far southeast of Chicago, and the Jefferson Township, which now makes up most of Chicago's Northwest Side. The desire to join the city was driven by municipal services the city could provide its residents.

Chicago's flourishing economy attracted huge numbers of new immigrants from Europe and migrants from the Eastern United States. Of the total population in 1900, more than 77% were either foreign-born or born in the United States of foreign parentage. Germans, Irish, Poles, Swedes and Czechs made up nearly two-thirds of the foreign-born population (by 1900, whites were 98.1% of the city's population).
Labor conflicts followed the industrial boom and the rapid expansion of the labor pool, including the Haymarket affair on May 4, 1886. Concern for social problems among Chicago's immigrant poor led Jane Addams and Ellen Gates Starr to found Hull House in 1889. Programs developed there became a model for the new field of social work.
During the 1870s and 1880s, Chicago attained national stature as the leader in the movement to improve public health. City, and later state laws, that upgraded standards for the medical profession and fought urban epidemics of cholera, smallpox, and yellow fever were both passed and enforced. These laws became templates for public health reform in other cities and states.
The city established many large, well-landscaped municipal parks, which also included public sanitation facilities. The chief advocate for improving public health in Chicago was Dr. John H. Rauch, M.D.. Rauch established a plan for Chicago's park system in 1866. He created Lincoln Park by closing a cemetery filled with shallow graves, and in 1867, in response to an outbreak of cholera he helped establish a new Chicago Board of Health. Ten years later, he became the secretary and then the president of the first Illinois State Board of Health, which carried out most of its activities in Chicago.
In the 19th century, Chicago became the nation's railroad center, and by 1910 over 20 railroads operated passenger service out of six different downtown terminals. In 1883, Chicago's railway managers needed a general time convention, so they developed the standardized system of North American time zones. This system for telling time spread throughout the continent.
In 1893, Chicago hosted the World's Columbian Exposition on former marshland at the present location of Jackson Park. The Exposition drew 27.5 million visitors, and is considered the most influential world's fair in history. The University of Chicago, formerly at another location, moved to the same South Side location in 1892. The term "midway" for a fair or carnival referred originally to the Midway Plaisance, a strip of park land that still runs through the University of Chicago campus and connects the Washington and Jackson Parks.

During World War I and the 1920s there was a major expansion in industry. The availability of jobs attracted African-Americans from the Southern United States. Between 1910 and 1930, the African-American population of Chicago increased dramatically, from 44,103 to 233,903. This Great Migration had an immense cultural impact, called the Chicago Black Renaissance, part of the New Negro Movement, in art, literature, and music. Continuing racial tensions and violence, such as the Chicago Race Riot of 1919, also occurred.
The ratification of the 18th amendment to the Constitution in 1919 made the production and sale (including exportation) of alcoholic beverages illegal in the United States. This ushered in the beginning of what is known as the Gangster Era, a time that roughly spans from 1919 until 1933 when Prohibition was repealed. The 1920s saw gangsters, including Al Capone, Dion O'Banion, Bugs Moran and Tony Accardo battle law enforcement and each other on the streets of Chicago during the Prohibition era. Chicago was the location of the infamous St. Valentine's Day Massacre in 1929, where Al Capone sent men to gun down members of his rival gang, North Side, led by Bugs Moran.
In 1924, Chicago was the first American city to have a homosexual-rights organization, the Society for Human Rights. This organization produced the first American publication for homosexuals, Friendship and Freedom. Police and political pressure caused the organization to disband.
In 1933, Chicago Mayor Anton Cermak was fatally wounded in Miami, Florida during a failed assassination attempt on President-elect Franklin D. Roosevelt. In 1933 and 1934, the city celebrated its centennial by hosting the Century of Progress International Exposition Worlds Fair. The theme of the fair was technological innovation over the century since Chicago's founding.
In March 1937, there was a violent strike by about 3,500 drivers for Checker and Yellow Cab Companies which included rioting that went on for weeks. The cab companies hired "strike breakers", and the cab drivers union hired "sluggers" who ragged through the downtown Chicago area looking for cabs and drivers not participating in the strike.

On December 2, 1942, physicist Enrico Fermi conducted the world's first controlled nuclear reaction at the University of Chicago as part of the top-secret Manhattan Project. This led to the creation of the atomic bomb by the United States, which it used in World War II in 1945.
Mayor Richard J. Daley, a Democrat, was elected in 1955, in the era of machine politics.
By the early 1960s, white residents in several neighborhoods left the city for the suburban areas – in many Northern American cities, a process known as White flight – as African Americans continued to move beyond the Black Belt. While home loan discriminatory redlining against blacks continued, the real estate industry, to promote turnover practiced what became known as blockbusting. Whole neighborhoods were completely changed based on race. Structural changes in industry, such as globalization and job outsourcing, caused heavy losses of jobs for lower skilled workers. In 1966, Martin Luther King, Jr. and Albert Raby led the Chicago Freedom Movement, which culminated in agreements between Mayor Richard J. Daley and the movement leaders.

Two years later, the city hosted the tumultuous 1968 Democratic National Convention, which featured physical confrontations both inside and outside the convention hall, with anti-war protesters, journalists and bystanders being beaten by police. Major construction projects, including the Sears Tower (now known as the Willis Tower, which in 1974 became the world's tallest building), University of Illinois at Chicago, McCormick Place, and O'Hare International Airport, were undertaken during Richard J. Daley's tenure. In 1979, Jane Byrne, the city's first female mayor, was elected. She helped reduce crime in the Cabrini-Green housing project and led Chicago's school system out of a financial crisis.

In 1983, Harold Washington became the first black mayor of the city of Chicago. Washington's first term in office directed attention to poor and previously neglected minority neighborhoods. He was re‑elected in 1987 but died of a heart attack soon after. Washington was succeeded by 6th ward Alderman Eugene Sawyer who was elected by the Chicago City Council and served until a special election.
Richard M. Daley, son of Richard J. Daley, was elected in 1989. His accomplishments included improvements to parks and creating incentives for sustainable development. After successfully standing for re-election five times, and becoming Chicago's longest serving mayor, Richard M. Daley declined to run for a seventh term.
On February 23, 2011, former Illinois Congressman and White House Chief of Staff, Rahm Emanuel, won the mayoral election, beating five rivals with 55 percent of the vote, and was sworn in as Mayor on May 16, 2011.

Chicago is located in northeastern Illinois on the southwestern shores of Lake Michigan. It is the principal city in the Chicago Metropolitan Area, situated in the Midwestern United States and the Great Lakes region. Chicago rests on a continental divide at the site of the Chicago Portage, connecting the Mississippi River and the Great Lakes watersheds. The city lies beside huge freshwater Lake Michigan, and two rivers—the Chicago River in downtown and the Calumet River in the industrial far South Side—flow entirely or partially through Chicago. Chicago's history and economy are closely tied to its proximity to Lake Michigan. While the Chicago River historically handled much of the region's waterborne cargo, today's huge lake freighters use the city's Lake Calumet Harbor on the South Side. The lake also provides another positive effect, moderating Chicago's climate; making waterfront neighborhoods slightly warmer in winter and cooler in summer.
When Chicago was founded in 1833, most of the early building was around the mouth of the Chicago River, as can be seen on a map of the city's original 58 blocks. The overall grade of the city's central, built-up areas, is relatively consistent with the natural flatness of its overall natural geography, generally exhibiting only slight differentiation otherwise. The average land elevation is 579 ft (176.5 m) above sea level. The lowest points are along the lake shore at 578 ft (176.2 m), while the highest point, at 672 ft (205 m), is the morainal ridge of Blue Island in the city's far south side.
The Chicago Loop is the central business district, but Chicago is also a city of neighborhoods. Lake Shore Drive runs adjacent to a large portion of Chicago's lakefront. Some of the parks along the waterfront include Lincoln Park, Grant Park, Burnham Park and Jackson Park. There are twenty-four public beaches across 26 miles (42 km) of the waterfront. Landfill extends into portions of the lake providing space for Navy Pier, Northerly Island, the Museum Campus, and large portions of the McCormick Place Convention Center. Most of the city's high-rise commercial and residential buildings are close to the waterfront.
An informal name for the entire Chicago metropolitan area is "Chicagoland". There is no precise definition for the term "Chicagoland", but it generally means the entire conurbation. The Chicago Tribune, which coined the term, includes the city of Chicago, the rest of Cook County, eight nearby Illinois counties: Lake, McHenry, DuPage, Kane, Kendall, Grundy, Will and Kankakee, and three counties in Indiana: Lake, Porter and LaPorte. The Illinois Department of Tourism defines Chicagoland as Cook County without the city of Chicago, and only Lake, DuPage, Kane and Will counties. The Chicagoland Chamber of Commerce defines it as all of Cook and DuPage, Kane, Lake, McHenry and Will counties.

Major sections of the city include the central business district, called The Loop, and the North, the South, and West Sides. The three sides of the city are represented on the Flag of Chicago by three horizontal white stripes. The North Side is the most densely populated residential section of the city, and many high-rises are located on this side of the city along the lakefront. The South Side is the largest section of the city, encompassing roughly 60% of the city's land area. The South Side contains the University of Chicago and most of the facilities of the Port of Chicago.
In the late 1920s, sociologists at the University of Chicago subdivided the city into 77 distinct community areas, which can further be subdivided into over 200 informally defined neighborhoods.

Chicago's streets were laid out in a street grid that grew from the city's original townsite plat, which was bounded by Lake Michigan on the east, North Avenue on the north, Wood Street on the west, and 22nd Street on the south. Streets following the Public Land Survey System section lines later became arterial streets in outlying sections. As new additions to the city were platted, city ordinance required them to be laid out with eight streets to the mile in one direction and sixteen in the other direction (about one street per 201 meters by two in the other direction). The grid's regularity provided an efficient means of developing new real estate property. A scattering of diagonal streets, many of them originally Native American trails, also cross the city (Elston, Milwaukee, Ogden, Lincoln, etc.). Many additional diagonal streets were recommended in the Plan of Chicago, but only the extension of Ogden Avenue was ever constructed.
In 2016, Chicago was ranked the sixth-most walkable large city in the United States. Many of the city's residential streets have a wide patch of grass and/or trees between the street and the sidewalk itself. This helps to keep pedestrians on the sidewalk further away from the street traffic. Chicago's Western Avenue is the longest continuous urban street in the world. Other famous streets include Michigan Avenue, State Street, Clark Street, and Belmont Avenue. The City Beautiful movement inspired Chicago's boulevards and parkways.

The destruction caused by the Great Chicago Fire led to the largest building boom in the history of the nation. In 1885, the first steel-framed high-rise building, the Home Insurance Building, rose in the city as Chicago ushered in the skyscraper era, which would then be followed by many other cities around the world. Today, Chicago's skyline is among the world's tallest and most dense.
Some of the United States' tallest towers are located in Chicago; Willis Tower (formerly Sears Tower) is the second tallest building in the Western Hemisphere after One World Trade Center, and Trump International Hotel and Tower is the third tallest in the country. The Loop's historic buildings include the Chicago Board of Trade Building, the Fine Arts Building, 35 East Wacker, and the Chicago Building, 860-880 Lake Shore Drive Apartments by Mies van der Rohe. Many other architects have left their impression on the Chicago skyline such as Daniel Burnham, Louis Sullivan, Charles B. Atwood, John Root, and Helmut Jahn.
The Merchandise Mart, once first on the list of largest buildings in the world, currently listed as 44th-largest (as of September 9, 2013), had its own zip code until 2008, and stands near the junction of the North and South branches of the Chicago River. Presently, the four tallest buildings in the city are Willis Tower (formerly the Sears Tower, also a building with its own zip code), Trump International Hotel and Tower, the Aon Center (previously the Standard Oil Building), and the John Hancock Center. Industrial districts, such as some areas on the South Side, the areas along the Chicago Sanitary and Ship Canal, and the Northwest Indiana area are clustered.
Chicago gave its name to the Chicago School and was home to the Prairie School, two movements in architecture. Multiple kinds and scales of houses, townhouses, condominiums, and apartment buildings can be found throughout Chicago. Large swaths of the city's residential areas away from the lake are characterized by brick bungalows built from the early 20th century through the end of World War II. Chicago is also a prominent center of the Polish Cathedral style of church architecture. The Chicago suburb of Oak Park was home to famous architect Frank Lloyd Wright, who had designed The Robie House located near the University of Chicago.

Chicago is famous for its outdoor public art with donors establishing funding for such art as far back as Benjamin Ferguson's 1905 trust. A number of Chicago's public art works are by modern figurative artists. Among these are Chagall's Four Seasons; the Chicago Picasso; Miro's Chicago; Calder's Flamingo; Oldenburg's Batcolumn; Moore's Large Interior Form, 1953-54, Man Enters the Cosmos and Nuclear Energy; Dubuffet's Monument with Standing Beast, Abakanowicz's Agora; and, Anish Kapoor's Cloud Gate which has become an icon of the city. Some events which shaped the city's history have also been memorialized by art works, including the Great Northern Migration (Saar) and the centennial of statehood for Illinois. Finally, two fountains near the Loop also function as monumental works of art: Plensa's Crown Fountain as well as Burnham and Bennett's Buckingham Fountain.
More representational and portrait statuary includes a number of works by Lorado Taft (Fountain of Time, The Crusader, Eternal Silence, and the Heald Square Monument completed by Crunelle), French's Statue of the Republic, Edward Kemys's Lions, Saint-Gaudens's Abraham Lincoln: The Man (a.k.a. Standing Lincoln) and Abraham Lincoln: The Head of State (a.k.a. Seated Lincoln), Brioschi's Christopher Columbus, Meštrović's The Bowman and The Spearman, Dallin's Signal of Peace, Fairbanks's The Chicago Lincoln, Boyle's The Alarm, Polasek's memorial to Masaryk, memorials along Solidarity Promenade to Kościuszko, Havliček and Copernicus by Chodzinski, Strachovský, and Thorvaldsen, a memorial to General Logan by Saint-Gaudens, and Kearney's Moose (W-02-03). A number of statues also honor recent local heroes such as Michael Jordan (by Amrany and Rotblatt-Amrany), Stan Mikita, and Bobby Hull outside of the United Center; Harry Caray (by Amrany and Cella) outside Wrigley field, Jack Brickhouse (by McKenna) next to the WGN studios, and Irv Kupcinet at the Wabash Avenue Bridge.
There are preliminary plans to erect a 1:1‑scale replica of Wacław Szymanowski's Art Nouveau statue of Frédéric Chopin found in Warsaw's Royal Baths along Chicago's lakefront in addition to a different sculpture commemorating the artist in Chopin Park for the 200th anniversary of Frédéric Chopin's birth.

The city lies within the humid continental climate zone (Köppen: Dfa), and experiences four distinct seasons. Summers are warm to hot and often humid, with a July daily average of 75.8 °F (24.3 °C). In a normal summer, temperatures can exceed 90 °F (32 °C) as many as 21 days. Winters are cold and snowy with few sunny days, and the normal January high is just below freezing. Spring and autumn are mild seasons with low humidity. Dewpoint temperatures in the summer range from 55.7 °F (13.2 °C) in June to 61.7 °F (16.5 °C) in July. The city is part of the USDA Plant Hardiness zone 6a, transitioning to 5b in the suburbs.
According to the National Weather Service, Chicago's highest official temperature reading of 105 °F (41 °C) was recorded on July 24, 1934, although Midway Airport reached 109 °F (43 °C) one day prior and recorded a heat index of 125 °F (52 °C) during the 1995 heatwave. The lowest official temperature of −27 °F (−33 °C) was recorded on January 20, 1985, at O'Hare Airport. The city can experience extreme winter cold waves and summer heat waves that may last for several consecutive days. Thunderstorms are common during the spring and summer months which may sometimes produce hail, high winds, and tornadoes. Like other major cities, Chicago also experiences urban heat island, making the city and its suburbs milder than surrounding rural areas, especially at night and in winter. Also, the proximity to Lake Michigan keeps lakefront Chicago cooler in early summer and milder in winter than areas to the west.

During its first hundred years, Chicago was one of the fastest-growing cities in the world. When founded in 1833, fewer than 200 people had settled on what was then the American frontier. By the time of its first census, seven years later, the population had reached over 4,000. In the forty years from 1850 to 1890, the city's population grew from slightly under 30,000 to over 1 million. At the end of the 19th century, Chicago was the fifth-largest city in the world, and the largest of the cities that did not exist at the dawn of the century. Within sixty years of the Great Chicago Fire of 1871, the population went from about 300,000 to over 3 million, and reached its highest ever recorded population of 3.6 million for the 1950 census.
From the last two decades of the 19th century, Chicago was the destination of waves of immigrants from Ireland, Southern, Central and Eastern Europe, including Italians, Jews, Poles, Lithuanians, Serbs and Czechs. To these ethnic groups, the basis of the city's industrial working class, were added an additional influx of African-Americans from the American South — with Chicago's black population doubling between 1910 and 1920 and doubling again between 1920 and 1930.
In the 1920s and 1930s, the great majority of African Americans moving to Chicago were clustered in a so‑called "Black Belt" on the city's South Side. By 1930, two-thirds of Chicago's African-American population lived in sections of the city which were 90% black in racial composition. Chicago's South Side emerged as America's second-largest urban black concentration, following New York's Harlem.
Since 2010, Chicago's population has rebounded adding nearly 25,000 people in the most recent 2015 population estimates.
As of the 2010 census, there were 2,695,598 people with 1,045,560 households living in Chicago. More than half the population of the state of Illinois lives in the Chicago metropolitan area. Chicago is one of the United States' most densely populated major cities, and the largest city in the Great Lakes Megalopolis. The racial composition of the city was:
45.0% White (31.7% non-Hispanic whites);
32.9% Black or African American;
28.9% Hispanic or Latino (of any race);
13.4% from some other race;
5.5% Asian (1.6% Chinese, 1.1% Indian, 1.1% Filipino, 0.4% Korean, 0.3% Pakistani, 0.3% Vietnamese, 0.2% Japanese, 0.1% Thai);
2.7% from two or more races;
0.5% American Indian.
Chicago has a Hispanic or Latino population of 28.9%. (Its members may belong to any race; 21.4% Mexican, 3.8% Puerto Rican, 0.7% Guatemalan, 0.6% Ecuadorian, 0.3% Cuban, 0.3% Colombian, 0.2% Honduran, 0.2% Salvadoran, 0.2% Peruvian)
The city's previous largest ethnic group, non-Hispanic white, declined from 59% in 1970 to 31.7% in 2010.
Chicago has the third-largest LGBT population in the United States. In 2015, roughly 4% of the population identified as LGBT. Since the legalization of same-sex marriage in the State of Illinois in 2013, over 10,000 same-sex couples have wed in Cook County, a majority in Chicago.
According to the U.S. Census Bureau's American Community Survey data estimates for 2008–2012, the median income for a household in the city was $47,408, and the median income for a family was $54,188. Male full-time workers had a median income of $47,074 versus $42,063 for females. About 18.3% of families and 22.1% of the population lived below the poverty line.
According to the 2008–2012 American Community Survey, the ancestral groups having 10,000 or more persons in Chicago were:

Persons identifying themselves as "Other groups" were classified at 1.72 million, and unclassified or not reported were approximately 153,000.

71% of Chicagoans identify as Christians, 7% identity with other faiths, and 22% have no religious affiliation. Chicago also has many Jews, Muslims, Buddhists, Hindus, and others. Chicago is the headquarters of several religious denominations, including the Evangelical Covenant Church and the Evangelical Lutheran Church in America. The Fourth Presbyterian Church is one of the largest Presbyterian congregations in the United States based on memberships.
The first two Parliament of the World's Religions in 1893 and 1993 were held in Chicago. Many international religious leaders have visited Chicago, including Mother Teresa, the Dalai Lama, and Pope John Paul II in 1979.

Chicago has the third-largest gross metropolitan product in the United States—about $630.3 billion according to 2014–2016 estimates. The city has also been rated as having the most balanced economy in the United States, due to its high level of diversification. In 2007, Chicago was named the fourth-most important business center in the world in the MasterCard Worldwide Centers of Commerce Index. Additionally, the Chicago metropolitan area recorded the greatest number of new or expanded corporate facilities in the United States for calendar year 2014. The Chicago metropolitan area has the third-largest science and engineering work force of any metropolitan area in the nation. In 2009 Chicago placed 9th on the UBS list of the world's richest cities. Chicago was the base of commercial operations for industrialists John Crerar, John Whitfield Bunn, Richard Teller Crane, Marshall Field, John Farwell, Julius Rosenwald and many other commercial visionaries who laid the foundation for Midwestern and global industry.
Chicago is a major world financial center, with the second-largest central business district in the United States. The city is the headquarters of the Federal Reserve Bank of Chicago (the Seventh District of the Federal Reserve). The city has major financial and futures exchanges, including the Chicago Stock Exchange, the Chicago Board Options Exchange (CBOE), and the Chicago Mercantile Exchange (the "Merc"), which is owned, along with the Chicago Board of Trade (CBOT) by Chicago's CME Group. The CME Group, in addition, owns the New York Mercantile Exchange (NYMEX), the Commodities Exchange Inc. (COMEX) and the Dow Jones Indexes. Perhaps due to the influence of the Chicago school of economics, the city also has markets trading unusual contracts such as emissions (on the Chicago Climate Exchange) and equity style indices (on the U.S. Futures Exchange). Chase Bank has its commercial and retail banking headquarters in Chicago's Chase Tower.
The city and its surrounding metropolitan area contain the third-largest labor pool in the United States with about 4.48 million workers, as of 2014. In addition, the state of Illinois is home to 66 Fortune 1000 companies, including those in Chicago. The city of Chicago also hosts 12 Fortune Global 500 companies and 17 Financial Times 500 companies. The city claims one Dow 30 company: aerospace giant Boeing, which moved its headquarters from Seattle to the Chicago Loop in 2001. Two more Dow 30 companies, Kraft Foods and McDonald's are in the Chicago suburbs, as are Sears Holdings Corporation and the technology spin-offs of Motorola. The headquarters of United Continental Holdings, are in the United Building and its operations center and its United Airlines subsidiary are in the Willis Tower in Chicago. In June 2016, McDonald's confirmed plans to move its global headquarters to Chicago's West Loop neighborhood by early 2018.

Manufacturing, printing, publishing and food processing also play major roles in the city's economy. Several medical products and services companies are headquartered in the Chicago area, including Baxter International, Boeing, Abbott Laboratories, and the Healthcare division of General Electric. In addition to Boeing, which located its headquarters in Chicago in 2001, and United Airlines in 2011, GE Transportation moved its offices to the city in 2013 and GE Healthcare moved its HQ to the city in 2016, as did ThyssenKrupp North America, and agriculture giant Archer Daniels Midland. Moreover, the construction of the Illinois and Michigan Canal, which helped move goods from the Great Lakes south on the Mississippi River, and of the railroads in the 19th century made the city a major transportation center in the United States. In the 1840s, Chicago became a major grain port, and in the 1850s and 1860s Chicago's pork and beef industry expanded. As the major meat companies grew in Chicago many, such as Armour and Company, created global enterprises. Though the meatpacking industry currently plays a lesser role in the city's economy, Chicago continues to be a major transportation and distribution center. Lured by a combination of large business customers, federal research dollars, and a large hiring pool fed by the area's universities, Chicago is also the site of a growing number of web startup companies like CareerBuilder, Orbitz, 37signals, Groupon, Feedburner, and NowSecure.
Chicago has been a hub of the Retail sector since its early development, with Montgomery Ward, Sears, and Marshall Field's. Today the Chicago metropolitan area is the headquarters of several retailers, including Walgreens, Sears, Ace Hardware, Claire's, ULTA Beauty and Crate & Barrel.
Late in the 19th century, Chicago was part of the bicycle craze, with the Western Wheel Company, which introduced stamping to the production process and significantly reduced costs, while early in the 20th century, the city was part of the automobile revolution, hosting the Brass Era car builder Bugmobile, which was founded there in 1907. Chicago was also the site of the Schwinn Bicycle Company.
Chicago is a major world convention destination. The city's main convention center is McCormick Place. With its four interconnected buildings, it is the largest convention center in the nation and third-largest in the world. Chicago also ranks third in the U.S. (behind Las Vegas and Orlando) in number of conventions hosted annually.
Chicago's minimum wage for non-tipped employees is one of the highest in the nation and will incrementally reach $13 per hour by 2019.

The city's waterfront location and nightlife has attracted residents and tourists alike. Over a third of the city population is concentrated in the lakefront neighborhoods from Rogers Park in the north to South Shore in the south. The city has many upscale dining establishments as well as many ethnic restaurant districts. These districts include the Mexican American neighborhoods, such as Pilsen along 18th street, and La Villita along 26th Street; the Puerto Rican enclave of Paseo Boricua in the Humboldt Park neighborhood; Greektown, along South Halsted Street, immediately west of downtown; Little Italy, along Taylor Street; Chinatown in Armour Square; Polish Patches in West Town; Little Seoul in Albany Park around Lawrence Avenue; Little Vietnam near Broadway in Uptown; and the Desi area, along Devon Avenue in West Ridge.
Downtown is the center of Chicago's financial, cultural, governmental and commercial institutions and the site of Grant Park and many of the city's skyscrapers. Many of the city's financial institutions, such as the CBOT and the Federal Reserve Bank of Chicago, are located within a section of downtown called "The Loop", which is an eight-block by five-block area of city streets that is encircled by elevated rail tracks. The term "The Loop" is largely used by locals to refer to the entire downtown area as well. The central area includes the Near North Side, the Near South Side, and the Near West Side, as well as the Loop. These areas contribute famous skyscrapers, abundant restaurants, shopping, museums, a stadium for the Chicago Bears, convention facilities, parkland, and beaches.
Lincoln Park contains the Lincoln Park Zoo and the Lincoln Park Conservatory. The River North Gallery District features the nation's largest concentration of contemporary art galleries outside of New York City.
Lakeview is home to Boystown (pronounced boys town), which, along with Andersonville, are some of the best-known LGBT neighborhoods in the nation. Each year in June, Boystown hosts the Chicago Pride Parade, one of the world's largest with over 1,000,000 people in attendance.

The South Side neighborhood of Hyde Park is the home of former US President Barack Obama. It also contains the University of Chicago (U of C), ranked one of the world's top ten universities; and the Museum of Science and Industry. The 6-mile (9.7 km) long Burnham Park stretches along the waterfront of the South Side. Two of the city's largest parks are also located on this side of the city: Jackson Park, bordering the waterfront, hosted the World's Columbian Exposition in 1893, and is the site of the aforementioned museum; and slightly west sits Washington Park. The two parks themselves are connected by a wide strip of parkland called the Midway Plaisance, running adjacent to the University of Chicago. The South Side hosts one of the city's largest parades, the annual African American Bud Billiken Parade and Picnic, which travels through Bronzeville to Washington Park. Ford Motor Company has an automobile assembly plant on the South Side in Hegewisch, and most of the facilities of the Port of Chicago are also on the South Side.
The West Side holds the Garfield Park Conservatory, one of the largest collections of tropical plants in any U.S. city. Prominent Latino cultural attractions found here include Humboldt Park's Institute of Puerto Rican Arts and Culture and the annual Puerto Rican People's Parade, as well as the National Museum of Mexican Art and St. Adalbert's Church in Pilsen. The Near West Side holds the University of Illinois at Chicago and was once home to Oprah Winfrey's Harpo Studios.
The city's distinctive accent, made famous by its use in classic films like The Blues Brothers and television programs like the Saturday Night Live skit "Bill Swerski's Superfans", is an advanced form of Inland Northern American English. This dialect can also be found in other cities bordering the Great Lakes such as Cleveland, Milwaukee, Detroit, and Rochester, New York, and most prominently features a rearrangement of certain vowel sounds, such as the short 'a' sound as in "cat", which can sound more like "kyet" to outsiders. The accent remains well associated with the city.

Renowned Chicago theater companies include the Goodman Theatre in the Loop; the Steppenwolf Theatre Company and Victory Gardens Theater in Lincoln Park; and the Chicago Shakespeare Theater at Navy Pier. Broadway In Chicago offers Broadway-style entertainment at five theaters: the Ford Center for the Performing Arts Oriental Theatre, Bank of America Theatre, Cadillac Palace Theatre, Auditorium Building of Roosevelt University, and Broadway Playhouse at Water Tower Place. Polish language productions for Chicago's large Polish speaking population can be seen at the historic Gateway Theatre in Jefferson Park. Since 1968, the Joseph Jefferson Awards are given annually to acknowledge excellence in theater in the Chicago area. Chicago's theater community spawned modern improvisational theater, and includes the prominent groups The Second City and I.O. (formerly ImprovOlympic).
The Chicago Symphony Orchestra (CSO) performs at Symphony Center, and is recognized as one of the best orchestras in the world. Also performing regularly at Symphony Center is the Chicago Sinfonietta, a more diverse and multicultural counterpart to the CSO. In the summer, many outdoor concerts are given in Grant Park and Millennium Park. Ravinia Festival, located 25 miles (40 km) north of Chicago, is the summer home of the CSO, and is a favorite destination for many Chicagoans. The Civic Opera House is home to the Lyric Opera of Chicago. The Lithuanian Opera Company of Chicago was founded by Lithuanian Chicagoans in 1956, and presents operas in Lithuanian.
The Joffrey Ballet and Chicago Festival Ballet perform in various venues, including the Harris Theater in Millennium Park. Chicago has several other contemporary and jazz dance troupes, such as the Hubbard Street Dance Chicago and Chicago Dance Crash.
Other live-music genre which are part of the city's cultural heritage include Chicago blues, Chicago soul, jazz, and gospel. The city is the birthplace of house music, a very popular form of Electronic Dance Music, and industrial music and is the site of an influential hip-hop scene. In the 1980s and 90s, the city was the global center for house and industrial music, two forms of music created in Chicago, as well as being popular for alternative rock, punk, and new wave. The city has been an epicenter for rave culture, since the 1980s. A flourishing independent rock music culture brought forth Chicago indie. Annual festivals feature various acts, such as Lollapalooza and the Pitchfork Music Festival. A 2007 report on the Chicago music industry by the University of Chicago Cultural Policy Center ranked Chicago third among metropolitan U.S. areas in "size of music industry" and fourth among all U.S. cities in "number of concerts and performances".
Chicago has a distinctive fine art tradition. For much of the twentieth century, it nurtured a strong style of figurative surrealism, as in the works of Ivan Albright and Ed Paschke. In 1968 and 1969, members of the Chicago Imagists, such as Roger Brown, Leon Golub, Robert Lostutter, Jim Nutt, and Barbara Rossi produced bizarre representational paintings.
Chicago contains a number of large, outdoor works by well-known artists. These include the Chicago Picasso, Miró's Chicago, Flamingo and Flying Dragon by Alexander Calder, Agora by Magdalena Abakanowicz, Monument with Standing Beast by Jean Dubuffet, Batcolumn by Claes Oldenburg, Cloud Gate by Anish Kapoor, Crown Fountain by Jaume Plensa, and the Four Seasons mosaic by Marc Chagall.
Chicago also has a nationally televised Thanksgiving parade that occurs annually. The McDonald's Thanksgiving Parade is seen across the nation on WGN-TV and WGN America, featuring a variety of diverse acts from the community, marching bands from across the country, and is the only parade in the city to feature inflatable balloons every year.

In 2014, Chicago attracted 50.17 million domestic leisure travelers, 11.09 million domestic business travelers and 1.308 million overseas visitors. These visitors contributed more than US$13.7 billion to Chicago's economy. Upscale shopping along the Magnificent Mile and State Street, thousands of restaurants, as well as Chicago's eminent architecture, continue to draw tourists. The city is the United States' third-largest convention destination. A 2011 study by Walk Score ranked Chicago the fourth-most walkable of fifty largest cities in the United States. Most conventions are held at McCormick Place, just south of Soldier Field. The historic Chicago Cultural Center (1897), originally serving as the Chicago Public Library, now houses the city's Visitor Information Center, galleries and exhibit halls. The ceiling of its Preston Bradley Hall includes a 38-foot (12 m) Tiffany glass dome. Grant Park holds Millennium Park, Buckingham Fountain (1927), and the Art Institute of Chicago. The park also hosts the annual Taste of Chicago festival. In Millennium Park, there is the reflective Cloud Gate sculpture. Cloud Gate, a public sculpture by Indian-born British artist Anish Kapoor, is the centerpiece of the AT&T Plaza in Millennium Park. Also, an outdoor restaurant transforms into an ice rink in the winter season. Two tall glass sculptures make up the Crown Fountain. The fountain's two towers display visual effects from LED images of Chicagoans' faces, along with water spouting from their lips. Frank Gehry's detailed, stainless steel band shell, the Jay Pritzker Pavilion, hosts the classical Grant Park Music Festival concert series. Behind the pavilion's stage is the Harris Theater for Music and Dance, an indoor venue for mid-sized performing arts companies, including the Chicago Opera Theater and Music of the Baroque.
Navy Pier, located just east of Streeterville, is 3,000 ft (910 m) long and houses retail stores, restaurants, museums, exhibition halls and auditoriums. In the summer of 2016, Navy Pier will have constructed their new DW60 Ferris wheel. Dutch Wheels a world renowned company that manufactures ferris wheels was selected to design the new wheel. It will feature 42 navy blue gondolas that can hold up to eight adults and two kids. It will also have entertainment systems inside the gondolas as well as a climate controlled environment. The DW60 will stand at approximately 196 ft (60 m), which is 46 ft taller than the previous wheel. The new DW60 will be the first in the United States and will be the sixth tallest in the U.S. Chicago was the first city in the world to ever erect a ferris wheel.
On June 4, 1998, the city officially opened the Museum Campus, a 10-acre (4.0 ha) lakefront park, surrounding three of the city's main museums, each of which is of national importance: the Adler Planetarium & Astronomy Museum, the Field Museum of Natural History, and the Shedd Aquarium. The Museum Campus joins the southern section of Grant Park, which includes the renowned Art Institute of Chicago. Buckingham Fountain anchors the downtown park along the lakefront. The University of Chicago Oriental Institute has an extensive collection of ancient Egyptian and Near Eastern archaeological artifacts. Other museums and galleries in Chicago include the Chicago History Museum, the Driehaus Museum, the DuSable Museum of African American History, the Museum of Contemporary Art, the Peggy Notebaert Nature Museum, the Polish Museum of America, the Museum of Broadcast Communications, the Pritzker Military Library, the Chicago Architecture Foundation, and the Museum of Science and Industry.
With an estimated completion date of 2020, the Barack Obama Presidential Center will be housed at the University of Chicago in Hyde Park and include both the Obama presidential library and offices of the Obama Foundation.
The Willis Tower (formerly named Sears Tower) is a popular destination for tourists. The Willis Tower has an observation deck open to tourists year round with high up views overlooking Chicago and Lake Michigan. The observation deck includes an enclosed glass balcony that extends 10 feet out on the side of the building. Tourists are able to look straight down.
In 2013, Chicago was chosen as one of the "Top Ten Cities in the United States" to visit for its restaurants, skyscrapers, museums, and waterfront, by the readers of Condé Nast Traveler.

Chicago lays claim to a large number of regional specialties that reflect the city's ethnic and working-class roots. Included among these are its nationally renowned deep-dish pizza; this style is said to have originated at Pizzeria Uno. The Chicago-style thin crust is also popular in the city.
The Chicago-style hot dog, typically an all-beef hot dog, is loaded with an array of toppings that often includes pickle relish, yellow mustard, pickled sport peppers, tomato wedges, dill pickle spear and topped off with celery salt on a poppy seed bun. Enthusiasts of the Chicago-style dog frown upon the use of ketchup as a garnish, but may prefer to add giardiniera.
There are several distinctly Chicago sandwiches, among them the Italian beef sandwich, which is thinly sliced beef simmered in au jus and served on an Italian roll with sweet peppers or spicy giardiniera. A popular modification is the Combo—an Italian beef sandwich with the addition of an Italian sausage. Another is the Maxwell Street Polish, a grilled or deep-fried kielbasa — on a hot dog roll, topped with grilled onions, yellow mustard, and hot sport peppers.
Ethnically originated creations include chicken Vesuvio, with roasted bone-in chicken cooked in oil and garlic next to garlicky oven-roasted potato wedges and a sprinkling of green peas. Another is the Puerto Rican-influenced jibarito, a sandwich made with flattened, fried green plantains instead of bread. There is also the mother-in-law, a tamale topped with chili and served on a hot dog bun. The tradition of serving the Greek dish, saganaki while aflame, has its origins in Chicago's Greek community. The appetizer, which consists of a square of fried cheese, is doused with Metaxa and flambéed table-side.
Two of the world's most decorated restaurants and also receiving the Michelin Guide 3 Star Award, Alinea and Grace are both located in Chicago. In addition, a number of well-known chefs have had restaurants in Chicago, including Charlie Trotter, Rick Tramonto, Grant Achatz, and Rick Bayless. In 2003, Robb Report named Chicago the country's "most exceptional dining destination".

Chicago literature finds its roots in the city's tradition of lucid, direct journalism, lending to a strong tradition of social realism. In the Encyclopedia of Chicago, Northwestern University Professor Bill Savage describes Chicago fiction as prose which tries to "capture the essence of the city, its spaces and its people". The challenge for early writers was that Chicago was a frontier outpost that transformed into a global metropolis in the span of two generations. Narrative fiction of that time, much of it in the style of "high-flown romance" and "genteel realism", needed a new approach to describe the urban social, political, and economic conditions of Chicago. Nonetheless, Chicagoans worked hard to create a literary tradition that would stand the test of time, and create a "city of feeling" out of concrete, steel, vast lake, and open prairie. Much notable Chicago fiction focuses on the city itself, with social criticism keeping exultation in check.
At least, three short periods in the history of Chicago have had a lasting influence on American Literature. These include from the time of the Great Chicago Fire to about 1900, what became known as the Chicago Literary Renaissance in the 1910s and early 1920s, and the period of the Great Depression through the 1940s.
What would become the influential Poetry magazine was founded in 1912 by Harriet Monroe, who was working as an art critic for the Chicago Tribune. The magazine discovered such poets as Gwendolyn Brooks, James Merrill, and John Ashbery. T. S. Eliot's first professionally published poem, "The Love Song of J. Alfred Prufrock", was first published by Poetry. Contributors have included Ezra Pound, William Butler Yeats, William Carlos Williams, Langston Hughes, and Carl Sandburg, among others. The magazine was instrumental in launching the Imagist and Objectivist poetic movements.

Sporting News named Chicago the "Best Sports City" in the United States in 1993, 2006, and 2010. Along with Boston, Chicago is the only city to continuously host major professional sports since 1871, having only taken 1872 and 1873 off due to the Great Chicago Fire. Additionally, Chicago is one of the six cities in the United States to have won championships in the four major professional leagues and, along with New York and Los Angeles, is one of three cities to have won soccer championships as well. Several major franchises have won championships within recent years – the Bears (1985), the Bulls (91, '92, '93, '96, '97, and '98), the White Sox (2005), the Cubs (2016), the Blackhawks (2010, 2013, 2015), and the Fire (1998).
The city has two Major League Baseball (MLB) teams: the Chicago Cubs of the National League play in Wrigley Field on the North Side; and the Chicago White Sox of the American League play in Guaranteed Rate Field on the South Side. Chicago is the only city that has had more than one MLB franchise every year since the AL began in 1901 (New York hosted only one between 1958 and early 1962). The Cubs are the oldest Major League Baseball team to have never changed their city; they have played in Chicago since 1871, and continuously so since 1874 due to the Great Chicago Fire. They have played more games and have more wins than any other team in Major League baseball since 1876. They have won three World Series titles, but had the dubious honor of having the two longest droughts in American professional sports: They had not won their sport's title since 1908, and had not participated in a World Series since 1945, both records, until they beat the Cleveland Indians in the 2016 World Series.
The White Sox have played on the South Side continuously since 1901, with all three of their home fields throughout the years being within blocks of one another. They have won three World Series titles (1906, 1917, 2005) and six American League pennants, including the first in 1901. The Sox are fifth in the American League in all-time wins, and sixth in pennants.
The Chicago Bears, one of the last two remaining charter members of the National Football League (NFL), have won nine NFL Championships, including the 1985 Super Bowl XX. The other remaining charter franchise, the Chicago Cardinals, also started out in the city, but is now known as the Arizona Cardinals. The Bears have won more games in the history of the NFL than any other team, and only the Green Bay Packers, their longtime rivals, have won more championships. The Bears play their home games at Soldier Field. Soldier Field re-opened in 2003 after an extensive renovation.

The Chicago Bulls of the National Basketball Association (NBA) is one of the most recognized basketball teams in the world. During the 1990s, with Michael Jordan leading them, the Bulls won six NBA championships in eight seasons. They also boast the youngest player to win the NBA Most Valuable Player Award, Derrick Rose, who won it for the 2010–11 season.
The Chicago Blackhawks of the National Hockey League (NHL) began play in 1926, and are one of the "Original Six" teams of the NHL. The Blackhawks have won six Stanley Cups, including in 2010, 2013, and 2015. Both the Bulls and the Blackhawks play at the United Center.
The Chicago Fire Soccer Club is a member of Major League Soccer (MLS) and plays at Toyota Park in suburban Bridgeview, after playing its first eight seasons at Soldier Field. The Fire have won one league title and four U.S. Open Cups, since their founding in 1997. In 1994, the United States hosted a successful FIFA World Cup with games played at Soldier Field. The Chicago Sky is a professional basketball team based in Rosemont, Illinois, playing in the Women's National Basketball Association (WNBA). They play home games at the Allstate Arena. The team was founded before the 2006 WNBA season began.
The Chicago Marathon has been held each year since 1977 except for 1987, when a half marathon was run in its place. The Chicago Marathon is one of six World Marathon Majors.
Five area colleges play in Division I conferences: two from major conferences — the DePaul Blue Demons (Big East Conference) and the Northwestern Wildcats (Big Ten Conference) — and three from other D1 conferences — the Chicago State Cougars (Western Athletic Conference); the Loyola Ramblers (Missouri Valley Conference); and the UIC Flames (Horizon League).

When Chicago was incorporated in 1837, it chose the motto Urbs in Horto, a Latin phrase which means "City in a Garden". Today, the Chicago Park District consists of more than 570 parks with over 8,000 acres (3,200 ha) of municipal parkland. There are 31 sand beaches, a plethora of museums, two world-class conservatories, and 50 nature areas. Lincoln Park, the largest of the city's parks, covers 1,200 acres (490 ha) and has over 20 million visitors each year, making it third in the number of visitors after Central Park in New York City, and the National Mall and Memorial Parks in Washington, D.C.
There is an historic boulevard system, a network of wide, tree-lined boulevards which connect a number of Chicago parks. The boulevards and the parks were authorized by the Illinois legislature in 1869. A number of Chicago neighborhoods emerged along these roadways in the 19th century. The building of the boulevard system continued intermittently until 1942. It includes nineteen boulevards, eight parks, and six squares, along twenty-six miles of interconnected streets. Part of the system in the Logan Square Boulevards Historic District was listed in the National Register of Historic Places in 1985.
With berths for more than 6,000 boats, the Chicago Park District operates the nation's largest municipal harbor system. In addition to ongoing beautification and renewal projects for the existing parks, a number of new parks have been added in recent years, such as the Ping Tom Memorial Park in Chinatown, DuSable Park on the Near North Side, and most notably, Millennium Park, which is in the northwestern corner of one of Chicago's oldest parks, Grant Park in the Chicago Loop.
The wealth of greenspace afforded by Chicago's parks is further augmented by the Cook County Forest Preserves, a network of open spaces containing forest, prairie, wetland, streams, and lakes that are set aside as natural areas which lie along the city's outskirts, including both the Chicago Botanic Garden in Glencoe and the Brookfield Zoo in Brookfield. Washington Park is also one of the city's biggest parks; covering nearly 400 acres (160 ha). The park is listed on the National Register of Historic Places listings in South Side Chicago.

The government of the City of Chicago is divided into executive and legislative branches. The Mayor of Chicago is the chief executive, elected by general election for a term of four years, with no term limits. The current mayor is Rahm Emanuel. The mayor appoints commissioners and other officials who oversee the various departments. As well as the mayor, Chicago's clerk and treasurer are also elected citywide. The City Council is the legislative branch and is made up of 50 aldermen, one elected from each ward in the city. The council takes official action through the passage of ordinances and resolutions and approves the city budget.
The Chicago Police Department provides law enforcement and the Chicago Fire Department provides fire suppression and emergency medical services for the city and its residents. Civil and criminal law cases are heard in the Cook County Circuit Court of the State of Illinois court system, or in the Northern District of Illinois, in the federal system. In the state court, the public prosecutor is the Illinois State's Attorney; in the Federal court it is the United States Attorney.

During much of the last half of the 19th century, Chicago's politics were dominated by a growing Democratic Party organization. During the 1880s and 1890s, Chicago had a powerful radical tradition with large and highly organized socialist, anarchist and labor organizations. For much of the 20th century, Chicago has been among the largest and most reliable Democratic strongholds in the United States; with Chicago's Democratic vote the state of Illinois has been "solid blue" in presidential elections since 1992. Even before then, it was not unheard of for Republican presidential candidates to win handily in downstate Illinois, only to lose statewide due to large Democratic margins in Chicago. The citizens of Chicago have not elected a Republican mayor since 1927, when William Thompson was voted into office. The strength of the party in the city is partly a consequence of Illinois state politics, where the Republicans have come to represent rural and farm concerns while the Democrats support urban issues such as Chicago's public school funding. Chicago contains less than 25% of the state's population, but 8 of Illinois' 19 U.S. Representatives have part of Chicago in their districts.
Machine politics persisted in Chicago after the decline of similar machines in other large U.S. cities. During much of that time, the city administration found opposition mainly from a liberal "independent" faction of the Democratic Party. The independents finally gained control of city government in 1983 with the election of Harold Washington (in office 1983–1987). From 1989 until May 16, 2011, Chicago was under the leadership of its longest serving mayor, Richard M. Daley, the son of Richard J. Daley. On May 16, 2011, Rahm Emanuel was sworn in as the 55th mayor of Chicago. Because of the dominance of the Democratic Party in Chicago, the Democratic primary vote held in the spring is generally more significant than the general elections in November for U.S. House and Illinois State seats. The aldermanic, mayoral, and other city offices are filled through nonpartisan elections with runoffs as needed.
Formerly a state legislator representing Chicago and later a US Senator, the city is home of United States President Barack Obama and First Lady Michelle Obama. The Obama's residence is located near the University of Chicago in Kenwood on the city's south side.

Chicago had a murder rate of 18.5 per 100,000 residents in 2012, ranking 16th among cities with 100,000 people or more. This was higher than in New York City and Los Angeles, the two largest cities in the United States, which have lower murder rates and lower total homicides. However, it was less than in many smaller American cities, including New Orleans, Newark, and Detroit, which had 53 murders per 100,000 residents in 2012. The 2015 year-end crime statistics showed there were 468 murders in Chicago in 2015 compared with 416 the year before, a 12.5% increase, as well as 2,900 shootings—13% more than the year prior, and up 29% since 2013. Chicago had more homicides than any other city in 2015, according to the Chicago Tribune. In its annual crime statistics for 2016, the Chicago Police Department reported that the city experienced a dramatic rise in gun violence, with 4,331 shooting victims. The department also reported 762 murders in Chicago for the year 2016, a total that marked a 62.8% increase in homicides from 2015.
According to reports in 2013, "most of Chicago's violent crime comes from gangs trying to maintain control of drug-selling territories", and is specifically related to the activities of the Sinaloa Cartel, which by 2006 had decided to seek to control illicit drug distribution, against local street gangs. Violent crime rates vary significantly by area of the city, with more economically developed areas having low rates, but other sections have much higher rates of crime. In 2013, the violent crime rate was 910 per 100,000 people; the murder rate was 10.4 – while high crime districts saw 38.9, low crime districts saw 2.5 murders per 100,000.
The number of murders in Chicago peaked at 970 in 1974, when the city's population was over 3 million people (a murder rate of about 29 per 100,000), and it reached 943 murders in 1992, (a murder rate of 34 per 100,000). However, Chicago and other major U.S. cities, experienced a significant reduction in violent crime rates through the 1990s, falling to 448 homicides in 2004, its lowest total since 1965 and only 15.65 murders per 100,000). Chicago's homicide tally remained low during 2005 (449), 2006 (452), and 2007 (435) but rose to 510 in 2008, breaking 500 for the first time since 2003. In 2009, the murder count fell to 458 (10% down). and in 2010 Chicago's murder rate fell to 435 (16.14 per 100,000), a 5% decrease from 2009 and lowest levels since 1965. In 2011, Chicago's murders fell another 1.2% to 431 (a rate of 15.94 per 100,000). but shot up to 506 in 2012.
In 2012, Chicago ranked 21st in the United States in numbers of homicides per person, but in the first half of 2013 there was a significant drop per-person, in all categories of violent crime, including homicide (down 26%). Chicago ended 2013 with 415 murders, the lowest number of murders since 1965, and overall crime rates dropped by 16 percent. (In 1965, there were 397 murders.)
Jens Ludwig, director of the University of Chicago Crime Lab, estimated that shootings cost the city of Chicago $2.5 billion in 2012.
In 2014, the Chicago police department reported a total murder count of 390 through December 20, 2014, according to the Chicago Sun-Times. That means that Chicago was able to record their lowest number of murder totals in close to five years for the second continuous calendar year, despite an overall increase in shootings. The Cook County medical examiner's office had reported a total of 410 homicides with 16 of those including fatal police shootings, all within the same time period.

Chicago Public Schools (CPS) is the governing body of the school district that contains over 600 public elementary and high schools citywide, including several selective-admission magnet schools. There are eleven selective enrollment high schools in the Chicago Public Schools, designed to meet the needs of Chicago's most academically advanced students. These schools offer a rigorous curriculum with mainly honors and Advanced Placement (AP) courses. Northside College Preparatory High School is ranked number one in the city of Chicago and the state of Illinois. Walter Payton College Prep High School is ranked second, Jones College Prep is third, and the oldest magnet school in the city, Whitney M. Young Magnet High School, which was opened in 1975, is ranked fourth. The magnet school with the largest enrollment is Lane Technical College Prep High School. Lane is one of the oldest schools in Chicago and in 2012 was designated a National Blue Ribbon School by the U.S. Department of Education.
Chicago high school rankings are determined by the average test scores on state achievement tests. The district, with an enrollment exceeding 400,545 students (2013–2014 20th Day Enrollment), is the third-largest in the U.S. On September 10, 2012, teachers for the Chicago Teachers Union went on strike for the first time since 1987 over pay, resources and other issues. According to data complied in 2014, Chicago's "choice system", where students who test or apply and may attend one of a number of public high schools (there are about 130), sorts students of different achievement levels into different schools (high performing, middle performing, and low performing schools).
Chicago has a network of Lutheran schools, and several private schools are run by other denominations and faiths, such as the Ida Crown Jewish Academy in West Ridge. Several private schools are completely secular, such as the Latin School of Chicago in the Near North Side neighborhood, the University of Chicago Laboratory Schools in Hyde Park, the British School of Chicago and the Francis W. Parker School in Lincoln Park, the Lycée Français de Chicago in Uptown, the Feltre School in River North and the Morgan Park Academy. There are also the private Chicago Academy for the Arts, a high school focused on six different categories of the arts and the public Chicago High School for the Arts, a high school focused on five categories (visual arts, theatre, musical theatre, dance, and music) of the arts.
The Roman Catholic Archdiocese of Chicago operates Catholic schools, that include Jesuit preparatory schools and others including St. Rita of Cascia High School, De La Salle Institute, Josephinum Academy, DePaul College Prep, Cristo Rey Jesuit High School, Brother Rice High School, St. Ignatius College Preparatory School, Mount Carmel High School, Queen of Peace High School, Mother McAuley Liberal Arts High School, Marist High School, St. Patrick High School and Resurrection High School.
The Chicago Public Library system operates 79 public libraries, including the central library, two regional libraries, and numerous branches distributed throughout the city.

Since the 1850s, Chicago has been a world center of higher education and research with several universities that are in the city proper or in the immediate environs. These institutions consistently rank among the top "National Universities" in the United States, as determined by U.S. News & World Report. Top universities in Chicago are: the University of Chicago; Illinois Institute of Technology; Northwestern University; Loyola University Chicago; DePaul University and University of Illinois at Chicago. Other notable schools include: Chicago State University; the School of the Art Institute of Chicago, the Illinois Institute of Art – Chicago; East–West University; National Louis University; North Park University; Northeastern Illinois University; Columbia College Chicago; Robert Morris University Illinois; Roosevelt University; Saint Xavier University; Rush University; and Shimer College.
William Rainey Harper, the first president of the University of Chicago, was instrumental in the creation of the junior college concept, establishing nearby Joliet Junior College as the first in the nation in 1901. His legacy continues with the multiple community colleges in the Chicago proper, including the seven City Colleges of Chicago: Richard J. Daley College, Kennedy–King College, Malcolm X College, Olive–Harvey College, Truman College, Harold Washington College and Wilbur Wright College, in addition to the privately held MacCormac College.
Chicago also has a high concentration of post-baccalaureate institutions, graduate schools, seminaries, and theological schools, such as the Adler School of Professional Psychology, The Chicago School of Professional Psychology, the Erikson Institute, The Institute for Clinical Social Work, the Lutheran School of Theology at Chicago, the Catholic Theological Union, the Moody Bible Institute, the John Marshall Law School and the University of Chicago Divinity School.

The Chicago metropolitan area is the third-largest media market in North America, after New York City and Los Angeles. Each of the big four U.S. television networks, CBS, ABC, NBC and Fox, directly owns and operates a high-definition television station in Chicago (WBBM 2, WLS 7, WMAQ 5 and WFLD 32, respectively). Former CW affiliate WGN-TV 9, which is owned by the Tribune Media, is carried with some programming differences, as "WGN America" on cable and satellite TV nationwide and in parts of the Caribbean. The city has also been the base of several talk shows, including, formerly, The Oprah Winfrey Show. Chicago Public Radio produces programs such as PRI's This American Life and NPR's Wait Wait...Don't Tell Me! The city also has two PBS member stations: WTTW 11, producer of shows such as Sneak Previews, The Frugal Gourmet, Lamb Chop's Play-Along and The McLaughlin Group, just to name a few, and WYCC 20.
Two major daily newspapers are published in Chicago: the Chicago Tribune and the Chicago Sun-Times, with the Tribune having the larger circulation. There are also several regional and special-interest newspapers and magazines, such as Chicago, the Dziennik Związkowy (Polish Daily News), Draugas (the Lithuanian daily newspaper), the Chicago Reader, the SouthtownStar, the Chicago Defender, the Daily Herald, Newcity, StreetWise and the Windy City Times. The entertainment and cultural magazine Time Out Chicago and GRAB magazine are also published in the city, as well as local music magazine Chicago Innerview. In addition, Chicago is the recent home of satirical national news outlet, The Onion, as well as its sister pop-culture publication, The A.V. Club.
Since the 1980s, many motion pictures have been filmed and/or set in the city such as The Blues Brothers, Brewster's Millions, Ferris Bueller's Day Off, Sixteen Candles, Home Alone, The Fugitive, I, Robot, Mean Girls, Wanted, Batman Begins, The Dark Knight, Transformers: Dark of the Moon, Transformers: Age of Extinction, Divergent, Insurgent, Batman v Superman: Dawn of Justice, Sinister 2 and Suicide Squad.
Chicago has also been the setting for many popular television shows, including the situation comedies Perfect Strangers and its spinoff Family Matters, Punky Brewster, Married... with Children, Kenan & Kel, Still Standing, The League, The Bob Newhart Show, and Shake It Up. The city served as the venue for the medical dramas ER and Chicago Hope, as well as the fantasy drama series Early Edition and the 2005–2009 drama Prison Break. Discovery Channel films two shows in Chicago: Cook County Jail and the Chicago version of Cash Cab. Chicago is currently the setting for CBS's The Good Wife and Mike and Molly, Showtime's Shameless, and NBC's Chicago Fire, Chicago P.D. and Chicago Med.
Chicago has five 50,000 watt AM radio stations: the CBS Radio-owned WBBM and WSCR; the Tribune Broadcasting-owned WGN; the Cumulus Media-owned WLS; and the ESPN Radio-owned WMVP. Chicago is also home to a number of national radio shows, including Beyond the Beltway with Bruce DuMont on Sunday evenings.
Chicago is also featured in a few video games, including Watch Dogs and Midtown Madness, a real-life, car-driving simulation game. In 2005, indie rock artist Sufjan Stevens created a concept album about Illinois titled Illinois; many of its songs were about Chicago and its history.

Chicago is a major transportation hub in the United States. It is an important component in global distribution, as it is the third-largest inter-modal port in the world after Hong Kong and Singapore.

Seven mainline and four auxiliary interstate highways (55, 57, 65 (only in Indiana), 80 (also in Indiana), 88, 90 (also in Indiana), 94 (also in Indiana), 190, 290, 294, and 355) run through Chicago and its suburbs. Segments that link to the city center are named after influential politicians, with three of them named after former U.S. Presidents (Eisenhower, Kennedy, and Reagan) and one named after two-time Democratic candidate Adlai Stevenson.
The Kennedy and Dan Ryan Expressways are the busiest state maintained routes in the entire state of Illinois.

The Regional Transportation Authority (RTA) coordinates the operation of the three service boards: CTA, Metra, and Pace.
The Chicago Transit Authority (CTA) handles public transportation in the City of Chicago and a few adjacent suburbs outside of the Chicago city limits. The CTA operates an extensive network of buses and a rapid transit elevated and subway system known as the 'L' (for "elevated"), with lines designated by colors. These rapid transit lines also serve both Midway and O'Hare Airports. The CTA's rail lines consist of the Red, Blue, Green, Orange, Brown, Purple, Pink, and Yellow lines. Both the Red and Blue lines offer 24‑hour service which makes Chicago one of a handful of cities around the world (and one of two in the United States, the other being New York City) to offer rail service 24 hours a day, every day of the year, within the city's limits.
Metra, the nation's second-most used passenger regional rail network, operates an 11-line commuter rail service in Chicago and throughout the Chicago suburbs. The Metra Electric Line shares its trackage with Northern Indiana Commuter Transportation District's South Shore Line, which provides commuter service between South Bend and Chicago.
Pace provides bus and paratransit service in over 200 surrounding suburbs with some extensions into the city as well. A 2005 study found that one quarter of commuters used public transit.
Greyhound Lines provides inter-city bus service to and from the city, and Chicago is also the hub for the Midwest network of Megabus (North America).

Amtrak long distance and commuter rail services originate from Union Station. Chicago is one of the largest hubs of passenger rail service in the nation. The services terminate in San Francisco, Washington, D.C., New York City, Indianapolis, New Orleans, Portland, Seattle, Milwaukee, Quincy, St. Louis, Carbondale, Boston, Grand Rapids, Port Huron, Pontiac, Los Angeles, and San Antonio. An attempt was made in the early 20th century to link Chicago with New York City via the Chicago – New York Electric Air Line Railroad. Parts of this were built, but it was never completed.

Chicago's bike share program, Divvy bikes, was launched in 2013. In 2016, there are 5,837 bikes and 576 rental stations across the city. PBSC Urban Solutions Inc., provides bikes and docking stations.

Chicago is the largest hub in the railroad industry. Six of the seven Class I railroads meet in Chicago, with the exception being the Kansas City Southern Railway. As of 2002, severe freight train congestion caused trains to take as long to get through the Chicago region as it took to get there from the West Coast of the country (about 2 days). According to U.S. Department of Transportation, the volume of imported and exported goods transported via rail to, from, or through Chicago is forecast to increase nearly 150 percent between 2010 and 2040. CREATE, the Chicago Region Environmental and Transport Efficiency program, comprises about 70 programs, including crossovers, overpasses and underpasses, that intend to significantly improve the speed of freight movements in the Chicago area.

Chicago is served by O'Hare International Airport, the world's second-busiest airport measured by airline operations, on the far Northwest Side, and Midway International Airport on the Southwest Side. In 2005, O'Hare was the world's busiest airport by aircraft movements and the second-busiest by total passenger traffic (due to government enforced flight caps). Both O'Hare and Midway are owned and operated by the City of Chicago. Gary/Chicago International Airport and Chicago Rockford International Airport, located in Gary, Indiana and Rockford, Illinois, respectively, can serve as alternate Chicago area airports, however they do not offer as many commercial flights as O'Hare and Midway. In recent years the state of Illinois has been leaning towards building an entirely new airport in the Illinois suburbs of Chicago. The City of Chicago is the world headquarters for United Airlines, the world's third-largest airline.

The Port of Chicago consists of several major port facilities within the city of Chicago operated by the Illinois International Port District (formerly known as the Chicago Regional Port District). The central element of the Port District, Calumet Harbor, is maintained by the U.S. Army Corps of Engineers.
Iroquois Landing Lakefront Terminal: at the mouth of the Calumet River, it includes 100 acres (0.40 km2) of warehouses and facilities on Lake Michigan with over 780,000 square meters (8,390,000 square feet) of storage.
Lake Calumet terminal: located at the union of the Grand Calumet River and Little Calumet River 6 miles (9.7 km) inland from Lake Michigan. Includes three transit sheds totaling over 29,000 square meters (315,000 square feet) adjacent to over 900 linear meters (3,000 linear feet) of ship and barge berthing.
Grain (14 million bushels) and bulk liquid (800,000 barrels) storage facilities along Lake Calumet.
The Illinois International Port district also operates Foreign trade zone No. 22, which extends 60 miles (97 km) from Chicago's city limits.

Electricity for most of northern Illinois is provided by Commonwealth Edison, also known as ComEd. Their service territory borders Iroquois County to the south, the Wisconsin border to the north, the Iowa border to the west and the Indiana border to the east. In northern Illinois, ComEd (a division of Exelon) operates the greatest number of nuclear generating plants in any US state. Because of this, ComEd reports indicate that Chicago receives about 75% of its electricity from nuclear power. Recently, the city began installing wind turbines on government buildings to promote renewable energy.
Natural gas is provided by Peoples Gas, a subsidiary of Integrys Energy Group, which is headquartered in Chicago.
Domestic and industrial waste was once incinerated but it is now landfilled, mainly in the Calumet area. From 1995 to 2008, the city had a blue bag program to divert recyclable refuse from landfills. Because of low participation in the blue bag programs, the city began a pilot program for blue bin recycling like other cities. This proved successful and blue bins were rolled out across the city.

The Illinois Medical District is on the Near West Side. It includes Rush University Medical Center, ranked as the second best hospital in the Chicago metropolitan area by U.S. News & World Report for 2014–15, the University of Illinois Medical Center at Chicago, Jesse Brown VA Hospital, and John H. Stroger, Jr. Hospital of Cook County, one of the busiest trauma centers in the nation.
Two of the country's premier academic medical centers reside in Chicago, including Northwestern Memorial Hospital and the University of Chicago Medical Center. The Chicago campus of Northwestern University includes the Feinberg School of Medicine; Northwestern Memorial Hospital, which is ranked as the best hospital in the Chicago metropolitan area by U.S. News & World Report for 2010–11; the Rehabilitation Institute of Chicago, which is ranked the best U.S. rehabilitation hospital by U.S. News & World Report; the new Prentice Women's Hospital; and Ann & Robert H. Lurie Children's Hospital of Chicago.
The University of Illinois College of Medicine at UIC is the second largest medical school in the United States (2,600 students including those at campuses in Peoria, Rockford and Urbana–Champaign).
In addition, the Chicago Medical School and Loyola University Chicago's Stritch School of Medicine are located in the suburbs of North Chicago and Maywood, respectively. The Midwestern University Chicago College of Osteopathic Medicine is in Downers Grove.
The American Medical Association, Accreditation Council for Graduate Medical Education, Accreditation Council for Continuing Medical Education, American Osteopathic Association, American Dental Association, Academy of General Dentistry, Academy of Nutrition and Dietetics, American Association of Nurse Anesthetists, American College of Surgeons, American Society for Clinical Pathology, American College of Healthcare Executives, the American Hospital Association and Blue Cross and Blue Shield Association are all based in Chicago.

Chicago has 28 sister cities around the world. Like Chicago, many of them are or were the second-most populous city or second-most influential city of their country, or they are the main city of a country that has had large amounts of immigrants settle in Chicago. These relationships have sought to promote economic, cultural, educational, and other ties.
To celebrate the sister cities, Chicago hosts a yearly festival in Daley Plaza, which features cultural acts and food tastings from the other cities. In addition, the Chicago Sister Cities program hosts a number of delegation and formal exchanges. In some cases, these exchanges have led to further informal collaborations, such as the academic relationship between the Buehler Center on Aging, Health & Society at the Feinberg School of Medicine of Northwestern University and the Institute of Gerontology of Ukraine (originally of the Soviet Union), that was originally established as part of the Chicago-Kiev sister cities program.
Sister cities

Chicago Wilderness
List of cities with the most skyscrapers
List of fiction set in Chicago
National Register of Historic Places listings in Central Chicago
National Register of Historic Places listings in North Side Chicago
National Register of Historic Places listings in South Side Chicago
National Register of Historic Places listings in West Side Chicago

Notes

References

Official website
Choose Chicago official tourism website
Chicago at DMOZ
Maps of Chicago from the American Geographical Society Library
Historic American Landscapes Survey (HALS) No. IL-10, "Chicago Cityscape, Chicago, Cook County, IL", 45 photos, 4 photo caption pages
Chicago – LocalWiki Local Chicago WikiTampa (/ˈtæmpə/) is a major city in, and the county seat of, Hillsborough County, Florida. It is located on the west coast of Florida on Tampa Bay, near the Gulf of Mexico, and is part of the Tampa Bay Metropolitan Area. The city had a population of 346,037 in 2011.
The current location of Tampa was once inhabited by indigenous peoples of the Safety Harbor culture (most notably the Tocobaga and the Pohoy, who lived along the shores of Tampa Bay). The area was explored by Spanish explorers in the 16th century, resulting in violent conflicts and the introduction of European diseases, which wiped out the original native cultures. Although Spain claimed Florida as part of New Spain, it did not found a colony in the Tampa area, and there were no permanent American or European settlements within today's city limits until after the United States acquired Florida from Spain in 1819.
In 1824, the United States Army established a frontier outpost called Fort Brooke at the mouth of the Hillsborough River, near the site of today's Tampa Convention Center. The first civilian residents were pioneers who settled near the fort for protection from the nearby Seminole population, and the small village was first incorporated as "Tampa" in 1849. The town grew slowly until the 1880s, when railroad links, the discovery of phosphate, and the arrival of the cigar industry jump-started its development, helping it to grow from a quiet village of less than 800 residents in 1880 to a bustling city of over 30,000 by the early 1900s.
Today, Tampa is part of the metropolitan area most commonly referred to as the "Tampa Bay Area". For U.S. Census purposes, Tampa is part of the Tampa-St. Petersburg-Clearwater, Florida Metropolitan Statistical Area. The four-county area is composed of roughly 2.9 million residents, making it the second largest metropolitan statistical area (MSA) in the state, and the fourth largest in the Southeastern United States, behind Miami, Washington, D.C. and Atlanta. The Greater Tampa Bay area has over 4 million residents and generally includes the Tampa and Sarasota metro areas. The Tampa Bay Partnership and U.S. Census data showed an average annual growth of 2.47 percent, or a gain of approximately 97,000 residents per year. Between 2000 and 2006, the Greater Tampa Bay Market experienced a combined growth rate of 14.8 percent, growing from 3.4 million to 3.9 million and hitting the 4 million population mark on April 1, 2007. A 2012 estimate shows the Tampa Bay area population to have 4,310,524 people and a 2017 projection of 4,536,854 people.
Tampa was ranked as the 5th best outdoor city by Forbes in 2008. Tampa also ranks as the fifth most popular American city, based on where people want to live, according to a 2009 Pew Research Center study. A 2004 survey by the NYU newspaper Washington Square News ranked Tampa as a top city for "twenty-somethings." Tampa is ranked as a "Gamma+" world city by Loughborough University, ranked alongside other world cities such as Phoenix, Charlotte, Rotterdam, and Santo Domingo.

The word "Tampa" may mean "sticks of fire" in the language of the Calusa, a Native American tribe that once lived south of today's Tampa Bay. This might be a reference to the many lightning strikes that the area receives during the summer months. Other historians claim the name means "the place to gather sticks". Toponymist George R. Stewart writes that the name was the result of a miscommunication between the Spanish and the Indians, the Indian word being "itimpi", meaning simply "near it". The name first appears in the "Memoir" of Hernando de Escalante Fontaneda (1575), who had spent 17 years as a Calusa captive. He calls it "Tanpa" and describes it as an important Calusa town. While "Tanpa" may be the basis for the modern name "Tampa", archaeologist Jerald Milanich places the Calusa village of Tanpa at the mouth of Charlotte Harbor, the original "Bay of Tanpa". A later Spanish expedition did not notice Charlotte Harbor while sailing north along the west coast of Florida and assumed that the current Tampa Bay was the bay they sought. The name was accidentally transferred north. Map makers were using the term Bay or Bahia Tampa as early as 1695.
People from Tampa are known as "Tampans" or "Tampanians". Local authorities consulted by Michael Kruse of the Tampa Bay Times suggest that "Tampan" was historically more common, while "Tampanian" became popular when the former term came to be seen as a potential insult. Latin Americans from Tampa are known as "tampeños", or "tampeñas" for females. These terms of Spanish origin emerged after 1900 for the immigrant communities in West Tampa and Ybor City. The tampeño, or "Tampa Latin", community is a mix of Cuban, Italian, Spanish, and American influences, with Cuban influence being dominant.

Not much is known about the cultures who called the Tampa Bay area home before European contact. When Spanish explorers arrived in the 1520s, they found Tocobaga villages around the northern half of Tampa Bay and Calusa villages along the southern portion of the bay.

Expeditions led by Pánfilo de Narváez and Hernando de Soto landed near Tampa, but neither conquistador stayed long. The native inhabitants repulsed any Spanish attempt to establish a permanent settlement or convert them to Catholicism. The newcomers brought with them infectious disease, resulting in a total collapse of the native cultures of Florida. The Tampa area was depopulated and ignored for more than 200 years.
In the mid-18th century, events in American colonies drove the Seminole Indians into northern Florida. During this period, the Tampa area had only a handful of residents: Cuban and Native American fishermen. They lived in a small village at the mouth of Spanishtown Creek on Tampa Bay, in today's Hyde Park neighborhood along Bayshore Boulevard.

After purchasing Florida from Spain in 1821, the United States built forts and trading posts in the new territory. Fort Brooke was established in January 1824 at the mouth of the Hillsborough River on Tampa Bay, in Downtown Tampa. Tampa was initially an isolated frontier outpost. The sparse civilian population practically abandoned the area during the Second Seminole War from 1835 to 1842, after which the Seminoles were forced out and many settlers returned.

Florida became the 27th state in 1845. On January 18, 1849, Tampa was officially incorporated as the "Village of Tampa". Tampa was home to 185 civilians, or 974 total residents including military personnel, in 1850. Tampa was reincorporated as a town on December 15, 1855.

During the Civil War, Florida seceded along with most of the southern states to form the Confederate States of America, and Fort Brooke was manned by Confederate troops. Martial law was declared in Tampa in January 1862, and Tampa's city government ceased to operate for the duration of the war.
In 1861, the Union Navy set up a blockade around many southern ports to cut off the Confederacy, and several ships were stationed near the mouth of Tampa Bay. The Battle of Fort Brooke on October 16 and the Battle of Ballast Point on October 18, 1863 damaged the Confederates, with Union troops destroying Confederate blockade runners. The Civil War ended in April 1865 with a Confederate defeat.
In May 1865, federal troops arrived in Tampa to occupy the fort and the town as part of Reconstruction. They remained until August 1869.
Tampa was a fishing village with very few people and little industry, and limited prospects for development. Tampa's chronic yellow fever epidemics, borne by mosquitoes from the swampland, were widespread during the late 1860s and 1870s, and many residents left.
In 1869, residents voted to abolish the city of Tampa government. The population of "Tampa Town" was below 800 by 1870, and had fallen further by 1880. Fort Brooke was decommissioned in 1883, and except for two cannons displayed on the University of Tampa campus, all traces of the fort are gone.

In the mid-1880s, Tampa's fortunes took several sudden turns for the better. First, phosphate was discovered in the Bone Valley region southeast of Tampa in 1883. The mineral, vital for the production of fertilizers and other products, was soon being shipped out from the Port of Tampa in great volume. Tampa is still a major phosphate exporter.
The discovery of phosphate, the arrival of Plant's railroad, and the founding of Ybor City and West Tampa—all in the mid-1880s—were crucial to Tampa's development. The once-struggling village of Tampa became a bustling boomtown almost overnight, and had grown into one of the largest cities in Florida by 1900.

Henry B. Plant's narrow-gauge South Florida Railroad reached Tampa and its port in late 1883, finally connecting the small town to the nation's railroad system after years of efforts by local leaders. Previously, Tampa's overland transportation links had consisted of sandy roads stretching across the Florida countryside. Plant's railroad made it much easier to get goods in and out of the Tampa Bay area. Phosphate and commercial fishing exports could be sent north by rail and many new products were brought into the Tampa market, along with the first tourists.

The new railroad link enabled another important industry to come to Tampa. In 1885, the Tampa Board of Trade enticed Vicente Martinez Ybor to move his cigar manufacturing operations to Tampa from Key West. Proximity to Cuba made importation of "clear Havana tobacco" easy by sea, and Plant's railroad made shipment of finished cigars to the rest of the US market easy by land.
Since Tampa was still a small town at the time (population less than 5000), Ybor built hundreds of small houses around his factory to accommodate the immediate influx of mainly Cuban and Spanish cigar workers. Ybor City's factories rolled their first cigars in 1886, and many different cigar manufacturers moved their operations to town in ensuing years. Many Italian and a few eastern European Jewish immigrants arrived starting in the late 1880s, opening businesses and shops that catered to cigar workers. By 1900, over 10,000 immigrants had moved to the neighborhood. Several thousand more Cuban immigrants built West Tampa, another cigar-centric suburb founded a few years later by Hugh MacFarlane. Between them, two "Latin" communities combined to exponentially expand Tampa's population, economic base, and tax revenues, as Tampa became the "Cigar Capital of the World".

During the first few decades of the 20th century, the cigar-making industry was the backbone of Tampa's economy. The factories in Ybor City and West Tampa made an enormous number of cigars—in the peak year of 1929, over 500,000,000 cigars were hand rolled in the city.
In 1904, a local civic association of local businessmen dubbed themselves Ye Mystic Krewe of Gasparilla (named after local mythical pirate Jose Gaspar), and staged an "invasion" of the city followed by a parade. With a few exceptions, the Gasparilla Pirate Festival has been held every year since.

Beginning in the late 19th century, illegal bolita lotteries were very popular among the Tampa working classes, especially in Ybor City. In the early 1920s, this small-time operation was taken over by Charlie Wall, the rebellious son of a prominent Tampa family, and went big-time. Bolita was able to openly thrive only because of kick-backs and bribes to key local politicians and law enforcement officials, and many were on the take.
Profits from the bolita lotteries and Prohibition-era bootlegging led to the development of several organized crime factions in the city. Charlie Wall was the first major boss, but various power struggles culminated in consolidation of control by Sicilian mafioso Santo Trafficante, Sr., and his faction in the 1950s. After his death in 1954 from cancer, control passed to his son Santo Trafficante, Jr., who established alliances with families in New York City and extended his power throughout Florida and into Batista-era Cuba.
The era of rampant and open corruption ended in the 1950s, when the Estes Kefauver's traveling organized crime hearings came to town and were followed by the sensational misconduct trials of several local officials. Although many of the worst offenders in government and the mob were not charged, the trials helped to end the sense of lawlessness which had prevailed in Tampa for decades.

Tampa grew considerably as a result of World War II. Prior to the United States' involvement in the conflict, construction began on MacDill Field, the predecessor of present-day MacDill Air Force Base. MacDill Field served as a main base for Army Air Corps and later Army Air Forces operations just before and during World War II, with multiple auxiliary airfields around the Tampa Bay area and surrounding counties. At the end of the war, MacDill remained as an active military installation while the auxiliary fields reverted to civilian control. Two of these auxiliary fields would later become the present day Tampa International Airport and St. Petersburg-Clearwater International Airport. With the establishment of an independent U.S. Air Force in 1947, MacDill Field became MacDill AFB.
During the 1950s and 1960s Tampa saw record-setting population growth that has not been seen since. This amazing growth spurred major expansion of the city's highways and bridges bringing thousands into the city and creating endless possibilities for Tampa business owners who welcomed tourists and new citizens alike into their neighborhoods. It was during this time period in the city's history that two of the most popular tourist attractions in the area were developed – Busch Gardens and Lowry Park. Many of the well-known institutions that play an important role in the economic development of the city were established during this time period.
The University of South Florida was established in North Tampa in 1956 and opened for students in September 1960. The school spurred the construction of several residential and commercial development in the previously agriculture-dominated area around the new campus. Overall, Tampa continued to expand away from the city center during the 1960s as new hospitals, schools, churches and subdivisions all began appearing to accommodate the growth. Many business offices began moving away from the traditional downtown office building into more convenient neighborhood office plazas.
In 1970, the Census Bureau reported city's population as 80.0% white and 19.7% black.
Four attempts have been made to consolidate the municipal government of the city of Tampa with the county government of Hillsborough County (1967, 1970, 1971, and 1972), all of which failed at the ballot box; the greatest loss was also the most recent attempt in 1972, with the final tally being 33,160 (31%) in favor and 73,568 (69%) against the proposed charter.
The biggest recent growth in the city was the development of New Tampa, which started in 1988 when the city annexed a mostly rural area of 24 square miles (62 km2) between I-275 and I-75.
East Tampa, historically a mostly black community, was the scene of several race riots during and for some time after the period of racial segregation, mainly due to problems between residents and the Tampa Police Department.

According to the United States Census Bureau, the city has a total area of 170.6 square miles (442 km2) including 112.1 square miles (290 km2) of land and 58.5 square miles (151.5 km2) (34.31%) of water. The highest point in the city is only 48 feet (15 m). Tampa is bordered by two bodies of water, Old Tampa Bay and Hillsborough Bay, both of which flow together to form Tampa Bay, which in turn flows into the Gulf of Mexico. The Hillsborough River flows out into Hillsborough Bay, passing directly in front of Downtown Tampa and supplying Tampa's main source of fresh water. Palm River is a smaller river flowing from just east of the city into McKay Bay, which is a smaller inlet, sited at the northeast end of Hillsborough Bay Tampa's geography is marked by the Interbay Peninsula which divides Hillsborough Bay (the eastern) from Old Tampa Bay (the western).

Tampa's climate displays characteristics of a tropical climate, but is situated on the southern fringe of the humid subtropical climate (Köppen Cfa) zone. Tampa's climate generally features hot summer days with frequent thunderstorms in the summer (rain is less frequent in the fall and winter), and a threat of a light winter freeze from November 15 through March 5 caused by occasional cold fronts from the north. Average highs range from 70 to 90 °F (21 to 32 °C) year round, and lows 52 to 76 °F (11 to 24 °C). Tampa's official recorded high has never hit 100 °F (37.8 °C) – the all-time record high temperature is 99 °F (37 °C), recorded on June 5, 1985.

Because of Tampa Bay, Tampa is split between two USDA climate zones. According to the 2012 USDA Plant Hardiness Zone Map, Tampa is listed as USDA zone 9b north of Kennedy Boulevard away from the bay and 10a south of Kennedy Boulevard and along the bay, Zone 10a is about the northern limit of where coconut palms and royal palms can be grown, although some specimens do grow in northern Tampa. Southern Tampa has much more tropical foliage than the northern parts of the city.

Temperatures are warm to hot from around mid-May through mid-October, which roughly coincides with the rainy season. Summertime weather is very consistent from June through September, with daytime highs near 90 °F (32 °C), lows in the mid-70s °F (23–24 °C), and high humidity.
Afternoon thunderstorms, usually generated by the interaction of the Gulf and Atlantic sea breezes, are such a regular occurrence during the summer that the Tampa Bay area is recognized as the "Lightning Capital of North America". Every year, Florida averages 10 deaths and 30 injuries from lightning strikes, with several of these usually occurring in or around Tampa. Because of the frequent summer thunderstorms, Tampa has a pronounced wet season, receiving an average of 26.1 inches (663 mm) of rain from June to September but only about 18.6 inches (472 mm) during the remaining eight months of the year. The historical averages during the late summer, especially September, are augmented by passing tropical systems, which can easily dump many inches of rain in one day. Tropical Storm Debby in 2012 dropped 8.57 inches (218 mm) of rain at Tampa International Airport on June 24, 2012 and amounts up to 10.36 inches (263 mm) was reported by a CoCoRaHS observer in NW Tampa. Outside of the summer rainy season, most of the area's precipitation is delivered by the occasional passage of a weather front.
The regular summertime afternoon thundershowers occasionally intensify into a severe thunderstorm, bringing heavy downpours, frequent lightning, strong straight-line winds, and sometimes hail.

Though it is affected by tropical storms every few years and threatened by tropical systems almost annually, Tampa has not taken a direct hit from a hurricane since 1921. That seemed about to change in 2004, when Hurricane Charley was forecast to make landfall near downtown Tampa, with potentially devastating effects for the entire region. The danger prompted the largest evacuation order for Pinellas County history and the largest evacuation request in Florida since Hurricane Floyd five years before. But Charley never reached Tampa Bay. After paralleling Florida's southwest coastline, the storm swerved to the east and slammed into Punta Gorda instead.

In the winter, average temperatures range from the low to mid 70s °F (21–24 °C) during the day to the low to mid 50s °F (10–13 °C) at night. However, sustained colder air pushes into the area on several occasions every winter, dropping the highs and lows to 15 °F (8 °C) or more below the seasonal averages for several days at a time. The temperature can fall below freezing an average of 2 to 3 times per year, though this does not occur every season.
Since the Tampa area is home to a diverse range of freeze-sensitive agriculture and aquaculture, hard freezes, although very infrequent, are a major concern. Since Tampa has some characteristics of a tropical climate, hard freezes (defined as below 28 °F (−2.2 °C)) happen rarely (every 5 to 20 years depending on location). The last officially recorded freeze at Tampa International Airport took place on the morning of January 13, 2011, when the temperature dropped to 31 °F (−1 °C).
The lowest temperature ever recorded in Tampa was 18 °F (−8 °C) on December 13, 1962. The last measurable snow in Tampa fell on January 19, 1977, with a total accumulation of 0.2 inches (0.5 cm). Three major freezes occurred in the 1980s: in January 1982, January 1985, and December 1989. The losses suffered by farmers forced many to sell off their citrus groves, which helped fuel a boom in subdivision development in the 1990s and 2000s.
In January 2010, a prolonged cold snap was the longest stretch of cold weather in the history of Tampa. Temperatures did not get above 49 °F (9.4 °C) for 5 days and there were freezes every night in northern Tampa for a week straight, causing significant damage to tropical foliage.

The city is divided into many neighborhoods, many of which were towns and unincorporated communities annexed by the growing city. Generally, the city is divided into the following areas: Downtown Tampa, New Tampa, West Tampa, East Tampa, North Tampa, and South Tampa. Well-known neighborhoods include Ybor City, Forest Hills, Ballast Point, Sulphur Springs, Seminole Heights, Tampa Heights, Palma Ceia, Hyde Park, Davis Islands, Tampa Palms, College Hill, and non-residential areas of Gary and the Westshore Business District.

Tampa displays a wide variety of architectural designs and styles. Most of Tampa's high rises demonstrate Post-modern architecture. The design for the renovated Tampa Museum of Art, displays Post-modern architecture, while the city hall and the Tampa Theatre belong to Art Deco architecture. The Tampa mayor Pam Iorio made the redevelopment of Tampa's downtown, especially residential development, a priority. Several residential and mixed-development high-rises have been constructed. Another of Mayor Iorio's initiatives was the Tampa Riverwalk, a mixed use path along the Hillsborough River in downtown and Channelside (Channelside was recently approved to undergo major renovations by Tampa Bay Lightning owner Jeff Vinik along with other investors). Several museums are part of the plan, including new homes for the Tampa Bay History Center, the Tampa Children's Museum, and the Tampa Museum of Art. Mayor Bob Buckhorn has continued these developments.
Tampa is the site of several skyscrapers. Overall, there are 18 completed buildings that rise over 250 feet (76 m) high. The city also has 69 high-rises, second only to Miami in the state of Florida. The tallest building in the city is 100 North Tampa, formerly the AmSouth Building, which rises 42 floors and 579 feet (176 m) in Downtown Tampa. The structure was completed in 1992, and is the tallest building in Florida outside of Miami and Jacksonville.

The Sulphur Springs Water Tower, a landmark in Sulphur Springs section of the city, dates back to the late 1920s. This boom period for Florida also saw the construction of an ornate movie palace, the Tampa Theatre, a Mediterranean revival on Davis Islands, and Bayshore Boulevard, which borders Hillsborough Bay from downtown Tampa to areas in South Tampa. The road has a 6-mile (10 km) continuous sidewalk on the eastern end, the longest in the world.
The Ybor City District is home to several buildings on the National Register of Historic Places and has been declared a National Historic Landmark. Notable structures include El Centro Español de Tampa, Centro Asturiano de Tampa and other social clubs built in the early 1900s.

Babe Zaharias Golf Course in the Forest Hills area of Tampa has been designated a Historical Landmark by the National Register of Historic Places. It was bought in 1949 by the famous 'Babe', who had a residence nearby, and closed upon her death. In 1974, the city of Tampa opened the golf course to the public. The Story of Tampa, a public painting by Lynn Ash, is a 4-by-8-foot (1.2 m × 2.4 m) oil on masonite mural that weaves together many of the notable aspects of Tampa's unique character and identity. It was commissioned in 2003 by the city's Public Art Program and can be found in the lobby of the Tampa Municipal Office Building. Park Tower (originally the First Financial Bank of Florida) is the first substantial skyscraper in downtown Tampa. Completed in 1973, it was the tallest skyscraper in Tampa until the completion of One Tampa City Center in 1981. The Rivergate building, a cylindrical building known as the "Beer Can building", was featured in the movie "The Punisher".
Spanning the southern part of Tampa Bay, is the massive steel-span Sunshine Skyway Bridge.
Tampa is home to the Bro Bowl, one of the last remaining skateparks built during skateboarding's "Golden Era" in the 1970s. It opened in 1979 and was constructed by Tampa Parks and Recreation. It was the first public skatepark to be constructed in Florida and the third on the East Coast.

As of 2000, the largest European ancestries in the city were German (9.2%), Irish (8.4%), English (7.7%), Italian (5.6%), and French (2.4%).
As of 2010, there were 157,130 households out of which 13.5% were vacant. In 2000, 27.6% households had children under the age of 18 living with them, 36.4% were married couples living together, 16.1% had a female householder with no husband present, and 42.9% were non-families. 33.7% of all households were made up of individuals and 10.2% had someone living alone who was 65 years of age or older. The average household size was 2.36 and the average family size was 3.07.
In 2000, the city's population was spread out with 24.6% under the age of 18, 10.0% from 18 to 24, 32.3% from 25 to 44, 20.5% from 45 to 64, and 12.5% who were 65 years of age or older. The median age was 34.7 years old. For every 100 females there were 95.3 males. For every 100 females age 18 and over, there were 92.1 males.
In 2006, the median income for a household in the city was $39,602, and the median income for a family was $45,823. Males had a median income of $40,461 versus $29,868 for females. The per capita income for the city was $26,522. 20.1% of the population and 16.4% of families were below the poverty line. 31.0% of those under the age of 18 and 13.6% of those 65 and older are living below the poverty level.
As of 2000, those who spoke only English at home accounted for 77.4% of all residents, while 22.6% spoke other languages in their homes. The most significant was Spanish speakers who made up 17.8% of the population, while French came up as the third most spoken language, which made up 0.6%, and Italian was at fourth, with 0.6% of the population.
There is a large gay population and a gay cultural center known as the GaYbor District.

Communities of faith have organized in Tampa from 1846, when a Methodist congregation established the city's first church, to 1939, when a 21-year-old Billy Graham began his career as a spiritual evangelist and preacher on downtown's Franklin Street, and through to today. Among Tampa's noteworthy religious structures are Sacred Heart Catholic Church, a 1905 downtown landmark noted for its soaring, Romanesque revival construction in granite and marble with German-crafted stained glass windows, the distinctive rock and mortar St. James Episcopal House of Prayer, listed with the U.S. historic registry, and the St. Paul AME church, which has seen the likes of Dr. Martin Luther King, Jr., and President Bill Clinton speak from its pulpit. The later two have been designated by the city government as Local Landmark Structures.
Tampa's religious community includes a broad representation of Christian denominations, including those above, and Presbyterian, Lutheran, Christian Science, Church of God, United Church of Christ, Philippine Independent Church, Unitarian Universalist, Metropolitan Community Church, Seventh-day Adventist, Eastern Orthodox (Greek, Coptic, Syrian, and OCA), various Pentecostal movements, Anglicans, the Quakers, Jehovah's Witnesses, and The Church of Jesus Christ of Latter-day Saints. There is also at least one congregation of Messianic Jews in Tampa. In addition there is a Korean Baptist church., as well as a Mennonite Church, several Haitian Churches, and a Vietnamese Baptist Church. Tampa also has several Jewish synagogues practicing Orthodox, Conservative, and Reform. In addition, there is a small Zoroastrian community present in Tampa.
Around the city are located a handful of mosques for followers of Islam, as well as a Tibetan-style Buddhist temple, a Thai Buddhist Wat, and local worship centers for the Sikh, Hindu and Bahá'í faiths. The Church of Scientology, based in nearby Clearwater, maintains a location for its members in Tampa.
Overall, Tampa is 50th out of the largest 51 metropolitan area in the percentage of the populace that attends religious services of any kind.

Finance, retail, healthcare, insurance, shipping by air and sea, national defense, professional sports, tourism, and real estate all play a vital role in the area's economy. Hillsborough County alone has an estimated 740,000 employees, a figure which is projected to increase to 922,000 by 2015. Several large corporations, such as banks and telecommunications companies, maintain regional offices in Tampa.

Several Fortune 1000 companies are headquartered in the metropolitan area, including OSI Restaurant Partners, WellCare, TECO Energy, and Raymond James Financial.
Downtown Tampa is undergoing significant development and redevelopment in line with a general national trend toward urban residential development. As of April 2007, the Tampa Downtown Partnership noted development proceeding on 20 residential, hotel, and mixed-use projects. Many of the new downtown developments are nearing completion in the midst of a housing market slump, which has caused numerous projects to be delayed or revamped, and some of the 20 projects TDP lists have not broken ground and are being refinanced. Nonetheless several developments are nearing completion, which city leaders hope will make downtown into a 24-hour neighborhood instead of 9 to 5 business district. As of 2010, Tampa residents faced a decline in rent of 2%. Nationally rent had decreased 4%. The Tampa Business Journal found Tampa to be the number two city for real estate investment in 2014.
Tampa's port is now the seventh largest in the nation and Florida's largest tonnage port, handling nearly half of all seaborne commerce that passes through the state. Tampa currently ranks second in the state behind Miami in terms of cruise ship travel. Besides smaller regional cruise ships such as Yacht Starship and SunCruz Casino, Tampa also serves as a port of call for three cruise lines: Holland America's MS Ryndam, Royal Caribbean's Grandeur of the Seas and Radiance of the Seas, and Carnival's Inspiration and Legend.
The main server farm for Wikipedia and other Wikimedia Foundation projects is located in Tampa.

MacDill Air Force Base remains a major employer as the parent installation for over 15,000 active uniformed military, Department of Defense (DoD) civil service and DoD contractor personnel in the Tampa Bay area. A significant majority of the civil service and contractor personnel are, in fact, themselves retired career military personnel. In addition to the 6th Air Mobility Wing, which is "host wing" for the base, MacDill is also home to Headquarters, United States Central Command (USCENTCOM), Headquarters, United States Special Operations Command (USSOCOM), the 927th Air Refueling Wing, Headquarters, United States Marine Forces Central Command (USMARCENT), Headquarters, United States Special Operations Command Central (USSOCCENT), and numerous other military activities of the active and reserve components of the armed forces.
Since the year 2000, Tampa has seen a notable upsurge in high-market demand from consumers, signaling more wealth concentrated in the area.

Tampa is home to a variety of stage and performing arts venues and theaters, including the David A. Straz Jr. Center for the Performing Arts, Tampa Theatre, Gorilla Theatre, and the MidFlorida Credit Union Amphitheatre next to the Florida State Fairgrounds.

Performing arts companies and organizations which call Tampa home include the Florida Orchestra, Opera Tampa, Jobsite Theater, the Master Chorale of Tampa Bay, Stageworks Theatre, Spanish Lyric Theater, Tampa Bay Opera, and the Tampa Bay Symphony.
Current popular nightlife districts include Channelside, Ybor City, SoHo, International Plaza and Bay Street, and Seminole Hard Rock. Downtown Tampa also contains some nightlife, and there are more clubs/bars to be found in other areas of the city. Tampa is rated sixth on Maxim magazine's list of top party cities.
The area has become a "de facto" headquarters of professional wrestling, with many pros living in the area. WWE's developmental territory, Florida Championship Wrestling, is also based in Tampa.
Tampa is home to several death metal bands, an extreme form of heavy metal music that evolved from thrash metal. Many of the genre's pioneers and foremost figures are based in and around the city. Chief among these are Deicide, Six Feet Under, Obituary, Cannibal Corpse, Death and Morbid Angel. The Tampa scene grew with the birth of Morrisound Recording, which established itself as an international recording destination for metal bands.
The underground rock band, the Baskervils, got their start in Tampa. They played the Tampa Bay area between 1994 and 1997 and then moved to New York City. Underground hip-hop group Equilibrium is based out of Tampa, as well as the Christian metalcore band, Underoath.
In 2009, the new Frank Wildhorn musical Wonderland: Alice's New Musical Adventure hosted its world premiere at the Straz Center.

The Tampa area is home to a number of museums that cover a wide array of subjects and studies. These include the Museum of Science & Industry (MOSI), which has several floors of science-related exhibits plus the only domed IMAX theater in Florida and a planetarium; the Tampa Museum of Art; the USF Contemporary Art Museum; the Tampa Bay History Center; the Tampa Firefighters Museum; the Henry B. Plant Museum; and Ybor City Museum State Park. Permanently docked in downtown's Channel District is the SS American Victory, a former World War II Victory Ship which is now used as a museum ship.

Tampa has a diverse culinary scene from small cafes and bakeries to bistros and farm-to-table restaurants. The food of Tampa has a history of Cuban, Spanish, Floribbean and Italian cuisines. There are also many Colombian cuisine, Puerto Rican cuisine, Vietnamese cuisine and Barbecue restaurants. Seafood is also very popular in Tampa, and Greek cuisine is prominent in the area, including around Tarpon Springs. Food trucks in Tampa, Florida are popular and the area holds the record for the world's largest food truck rally. In addition to Ybor, the areas of Seminole Heights and South Tampa are known for their restaurants.
Tampa is the birthplace of the Florida version of the deviled crab and the Cuban sandwich, which has been officially designated as the "signature sandwich of the city of Tampa" by the city council. A Tampa Cuban sandwich is distinct from other regional versions, as Genoa salami is layered in with the other ingredients. likely due to the influence of Italian immigrants living next to Cubans and Spaniards in Ybor City.
Tampa is also where several restaurant chains were founded or headquartered, including Outback Steakhouse, Melting Pot, Front Burner Brands, Carrabba's, Fleming's Prime Steakhouse & Wine Bar, Bonefish Grill, Columbia Restaurant, Checkers and Rally's, Taco Bus, and PDQ.

The city of Tampa operates over 165 parks and beaches covering 2,286 acres (9.25 km2) within city limits; 42 more in surrounding suburbs covering 70,000 acres (280 km2), are maintained by Hillsborough County. These areas include the Hillsborough River State Park, just northeast of the city. Tampa is also home to a number of attractions and theme parks, including Busch Gardens Tampa Bay, Adventure Island, Lowry Park Zoo, and Florida Aquarium.
Lowry Park Zoo features over 2,000 animals, interactive exhibits, rides, educational shows and more. The zoo serves as an economic, cultural, environmental and educational anchor in Tampa.
Big Cat Rescue is one of the largest accredited sanctuaries in the world dedicated entirely to abused and abandoned big cats. It is home to about 80 lions, tigers, bobcats, cougars and other species, most of whom have been abandoned, abused, orphaned, saved from being turned into fur coats, or retired from performing acts. They have a variety of different tours available.
Busch Gardens Tampa Bay is a 335-acre (1.36 km2) Africa-themed park located near the University of South Florida. It features many thrilling roller coasters, for which it is known, including Sheikra, Montu, Gwazi and Kumba. Visitors can also view and interact with a number of African wildlife. Adventure Island is a 30-acre (12 ha) water park adjacent to Busch Gardens.
The Florida Aquarium is a 250,000 sq ft (23,000 m2) aquarium located in the Channel District. It hosts over 20,000 species of aquatic plants and animals. It is known for its unique glass architecture. Adjacent to the Aquarium is the SS American Victory, a World War II Victory ship preserved as a museum ship.
The Tampa Bay History Center is a museum located in the Channel District. It boasts over 60,000 sq ft (5,600 m2) of exhibits through 12,000 years. Theaters, map gallery, research center and museum store.
Well-known shopping areas include International Plaza and Bay Street, WestShore Plaza, SoHo district, and Hyde Park Village. Palma Ceia is also home to a shopping district, called Palma Ceia Design District. Previously, Tampa had also been home to the Floriland Mall (now an office park), Tampa Bay Center (demolished and replaced with the new Tampa Bay Buccaneers training facility, known as "One Buc Place"), and East Lake Square Mall (now an office park).
The Tampa Port Authority currently operates three cruise ship terminals in Tampa's Channel District. The Port of Tampa is the year-round home port for Carnival Cruise Lines' MS Carnival Inspiration and MS Carnival Legend. In 2010 Tampa will also be a seasonal port for Holland America Line's MS Ryndam, as well as Royal Caribbean International's MS Grandeur of the Seas and MS Radiance of the Seas. A fourth company, Norwegian Cruise Line, has announced plans to sail out of Tampa for the first time. The 2,240 passenger MS Norwegian Star will be Tampa's largest cruise ship when it debuts a seasonal schedule in 2011. Cruise itineraries from Tampa include stops in the Eastern and Western Caribbean islands, Honduras, Belize, and Mexico.

Perhaps the most well known and anticipated events are those from Tampa's annual celebration of "Gasparilla", particularly the Gasparilla Pirate Festival, a mock pirate invasion held since 1904 in late January or early February. Often referred to as Tampa's "Mardi Gras", the invasion flotilla led by the pirate ship, Jose Gasparilla, and subsequent parade draw over 400,000 attendees, contributing tens of millions of dollars to the city's economy. Beyond the initial invasion, numerous Gasparilla festivities take place each year between January and March, including the Gasparilla Children's Parade, the more adult-oriented Sant'Yago Knight Parade, the Gasparilla Distance Classic, Gasparilla Festival of the Arts, and the Gasparilla International Film Festival, among other pirate themed events.
Other notable events include the Outback Bowl, which is held New Year's Day at Raymond James Stadium. Each February, The Florida State Fair brings crowds from across the state, while "Fiesta Day" celebrates Tampa's Cuban, Spanish, German, Italian, English, Irish, Jewish, and African-Cuban immigrant heritage. The India International Film Festival (IIFF) of Tampa Bay also takes place in February. In April the MacDill Air Fest entertains as one of the largest military air shows in the U.S. Guavaween, a nighttime street celebration infuses Halloween with the Latin flavor of Ybor City. Downtown Tampa hosts the largest anime convention in Florida, Metrocon, a three-day event held in either June or July at the Tampa Convention Center. Ybor also hosts "GaYbor Days", an annual street party in the GLBT-friendly GaYbor district. The Tampa International Gay and Lesbian Film Festival, held annually since 1989, is the city's largest film festival event, and one of the largest independent gay film festivals in the country.
Tampa hosted the 2012 Republican National Convention and the 15th International Indian Film Academy Awards in April 2014.

Tampa is represented by teams in three major professional sports leagues: the National Football League, the National Hockey League, and Major League Baseball. The NFL's Tampa Bay Buccaneers and the NHL's Tampa Bay Lightning call Tampa home, while the Tampa Bay Rays of the MLB play across the bay in St. Petersburg. As indicated by their names, these teams, plus several other sports teams, represent the entire Tampa metropolitan area.
The Tampa Bay area has long been a site for Major League Baseball spring training facilities and minor league baseball teams. The New York Yankees conduct spring training in Tampa, and the Tampa Yankees play there in the summer. On the collegiate level, the University of South Florida Bulls and the University of Tampa Spartans participate in many different sports.

The Tampa Bay Buccaneers began in 1976 as an expansion team of the NFL. They struggled at first, losing their first 26 games in a row to set a league record for futility. After a brief taste of success in the late 1970s, the Bucs again returned to their losing ways, and at one point lost 10+ games for 12 seasons in a row. The hiring of Tony Dungy in 1996 started an improving trend that eventually led to the team's victory in Super Bowl XXXVII in 2003 under coach Jon Gruden.
Tampa has hosted four Super Bowls: Super Bowl XVIII (1984), Super Bowl XXV (1991), Super Bowl XXXV (2001), and Super Bowl XLIII (2009). The first two events were held at Tampa Stadium, and the other two at Raymond James Stadium.
Originally the Pittsburgh Gladiators and a charter member of the Arena Football League, the Tampa Bay Storm relocated from Pittsburgh in 1991 and won ArenaBowl V that year. They have won 4 more ArenaBowls since then: ArenaBowl VII, IX, X, and XVII, and also appeared in ArenaBowl I, III, XII and XXIII. They have the most Arena Bowl titles.
Tampa was also home to the Tampa Bay Bandits of the United States Football League. The Bandits made the playoffs twice in their three seasons under head coach Steve Spurrier and drew league-leading crowds to Tampa Stadium, but the team folded along with the rest of the USFL after the 1985 season. They played at Tampa Stadium, which hosted the 1984 USFL Championship Game.

The Tampa Bay area has long been home to spring training, minor league, and excellent amateur baseball. The Tampa Bay Rays (originally "Devil Rays") began playing in 1998 at Tropicana Field in St. Petersburg. After a decade of futility, the Rays won the 2008 American League Pennant and made it to the World Series. They also won American League East titles in 2008 and 2010.
In 2007, the Rays began the process of searching for a stadium site closer to the center of the area's population, possibly in Tampa.
Several Major League baseball teams conduct spring training in the area, and most also operate minor league teams in the Class-A Florida State League. The New York Yankees and the affiliated Tampa Yankees use George M. Steinbrenner Field across Dale Mabry Highway from Raymond James Stadium.

The NHL's Tampa Bay Lightning was established in 1992, and currently play their home games in the Amalie Arena, located in downtown Tampa. In 2004, the team won their first Stanley Cup. The Lightning made the Eastern Conference Final in 2011 and were Eastern Conference Champions in 2015. They returned to the Eastern Conference Final in 2016.

The Tampa Bay Rowdies compete in the United Soccer League (2nd Division) after spending their first 6 seasons in the North American Soccer League. The team began play at Tampa's George M. Steinbrenner Field in 2010, then moved to St. Petersburg's Al Lang Field in 2011. The Rowdies won their first league championship in Soccer Bowl 2012.
Previously, Tampa had hosted two top-level soccer teams. The Tampa Bay Rowdies of the original North American Soccer League was the area's first major sports franchise, beginning play in 1975 at Tampa Stadium. The Rowdies were an immediate success, drawing good crowds and winning Soccer Bowl '75 in their first season to bring Tampa its first professional sports championship. Though the NASL ceased operations in 1984, the Rowdies continued to compete in various soccer leagues until finally folding in 1993.
The success of the Rowdies prompted Major League Soccer (MLS) to award Tampa a charter member of the new league in 1996. The Tampa Bay Mutiny were the first MLS Supporters' Shield winner and had much early success beginning in 1996. However, the club folded in 2001 when local ownership could not be secured mainly due to a financially poor lease agreement for Raymond James Stadium. The city has no current representation in MLS, however the Rowdies are currently seeking to join the league.

The University of South Florida is the only NCAA Division I sports program in Tampa. USF began playing intercollegiate sports in 1965. The South Florida Bulls established a basketball team in 1971 and a football team in 1997. The Bulls joined the Big East in 2005, and the football team rose to as high as #2 in the BCS rankings in 2007. They are now part of the American Athletic Conference.
The University of Tampa Spartans compete at the NCAA Division II level in the Sunshine State Conference (SSC).

Tampa is governed under the strong mayor form of government. The Mayor of Tampa is the chief executive officer of city government and is elected in four-year terms, with a limit of two consecutive terms. The current mayor is Bob Buckhorn, who took office on April 1, 2011. The City Council is a legislative body served by seven members. Four members are elected from specific numbered areas designated City Districts, and the other three are "at-large" members (serving citywide).

The City of Tampa is served by Tampa Fire Rescue. With 22 fire stations, the department provides fire and medical protection for Tampa and New Tampa, and provides support to other departments such as Tampa International Airport and Hillsborough County Fire Rescue.

The city of Tampa has a large police department that provides law enforcement services. The Tampa Police Department has over 1000 sworn officers and many civilian service support personnel.

Public primary and secondary education is operated by Hillsborough County Public Schools, officially known as the School District of Hillsborough County (SDHC). It is ranked the eighth largest school district in the United States, with around 189,469 enrolled students. SDHC runs 208 schools, 133 being elementary, 42 middle, 27 high schools, two K-8s, and four career centers. There are 73 additional schools in the district that are charter, ESE, alternative, etc. Twelve out of 27 high schools in the SDHC are included in Newsweek's list of America's Best High Schools.

Tampa's library system is operated by the Tampa-Hillsborough County Public Library System. THPLS operates 25 libraries throughout Tampa and Hillsborough County, including the John F. Germany Public Library in Downtown Tampa. The Tampa library system first started in the early 20th century, with the West Tampa Library, which was made possible with funds donated by Andrew Carnegie. Tampa's libraries are also a part of a larger library network, The Hillsborough County Public Library Cooperative, which includes the libraries of the neighboring municipalities of Temple Terrace and Plant City.

There are a number of institutions of higher education in Tampa.
The city is home to the main campus of the University of South Florida (USF), a member of the State University System of Florida founded in 1956. In 2010, it was the eleventh highest individual campus enrollment in the U.S. with over 46,000 students. The University of Tampa (UT) is a private, four-year liberal arts institution. It was founded in 1931, and in 1933, it moved into the former Tampa Bay Hotel across the Hillsborough River from downtown Tampa. "UT" has undergone several expansions in recent years, and had an enrollment of over 8000 students in 2015.
Hillsborough Community College is a two-year community college in the Florida College System with campuses in Tampa and Hillsborough County. Southern Technical College is a private two-year college that operates a campus in Tampa. Hillsborough Technical Education Centers (HiTEC) is the postsecondary extension of the local areas Public Schools district. The schools provide for a variety of technical training certification courses as well as job placement skills.
The Stetson University College of Law is located in Gulfport and has a second campus, the Tampa Law Center, in downtown Tampa. The Law Center houses the Tampa branch of Florida's Second District Court of Appeal.
Other colleges and universities in the wider Tampa Bay Area include Jersey College, Eckerd College, and St. Petersburg College in St. Petersburg.

The major daily newspaper serving the city is the Tampa Bay Times, which purchased its longtime competition, the The Tampa Tribune, in 2016. Print news coverage is also provided by a variety of smaller regional newspapers, alternative weeklies, and magazines, including the Florida Sentinel Bulletin, Creative Loafing, Reax Music Magazine, The Oracle, Tampa Bay Business Journal, MacDill Thunderbolt, and La Gaceta, which notable for being the nation's only trilingual newspaper - English, Spanish, and Italian, owing to its roots in the cigar-making immigrant neighborhood of Ybor City.
Major television stations include WFTS 28 (ABC), WTSP 10 (CBS), WFLA 8 (NBC), WTVT 13 (Fox), WTOG 44 (The CW), WTTA 38 (MyNetworkTV), WEDU 3 (PBS), WUSF-TV 16 (PBS), WMOR 32 (Independent), WXPX 66 (ION), WCLF 22 (CTN), WFTT 50 (UniMás) and WVEA 62 (Univision).
The area is served by dozens of FM and AM radio stations including WDAE, which was the first radio station in Florida when it went on the air in 1922.

Three motor vehicle bridges cross Tampa Bay to Pinellas County from Tampa city limits: the Howard Frankland Bridge (I-275), the Courtney Campbell Causeway (SR 60), and the Gandy Bridge (U.S. 92). The old Gandy Bridge was completely replaced by new spans during the 1990s, but a span of the old bridge was saved and converted into a pedestrian and biking bridge renamed The Friendship Trail. It is the longest overwater recreation trail in the world. However, the bridge was closed in 2008 due to structural problems.

There are two major expressways (toll) bringing traffic in and out of Tampa. The Lee Roy Selmon Expressway (SR 618) (formerly known as the Crosstown Expressway), runs from suburban Brandon at its eastern terminus, through Downtown Tampa, to the neighborhoods in South Tampa (near MacDill Air Force Base) at its western terminus. The Veterans Expressway (SR 589), meanwhile connects Tampa International Airport and the bay bridges to the northwestern suburbs of Carrollwood, Northdale, Westchase, Citrus Park, Cheval, and Lutz, before continuing north as the Suncoast Parkway into Pasco and Hernando counties.
Three interstate highways run through the city. Interstate 4 and Interstate 275 cut across the city and intersect near downtown. Interstate 75 runs along the east side of town for much of its route through Hillsborough County until veering to the west to bisect New Tampa.
Along with highways, major surface roads serve as main arteries of the city. These roads are Hillsborough Avenue (U.S. 92 and U.S. 41), Dale Mabry Highway (U.S. 92), Nebraska Avenue (U.S. 41/SR 45), Florida Avenue (U.S. 41 Business), Bruce B. Downs Boulevard, Fowler Avenue, Busch Boulevard, Kennedy Boulevard (SR 60), Adamo Drive, and Dr. Martin Luther King Jr. Boulevard.

Tampa is served by three airports (one in Tampa, two in the metro area) that provide significant scheduled passenger air service:
Tampa International Airport (IATA: TPA) is Tampa's main airport and the primary location for commercial passenger airline service into the Tampa Bay area. It is also a consistent favorite in surveys of the industry and the traveling public. The readers of Condé Nast Traveler have frequently placed Tampa International in their list of Best Airports, ranking it #1 in 2003, and #2 in 2008 A survey by Zagat in 2007 ranked Tampa International first among U.S. airports in overall quality. During 2008, it was the 26th-busiest airport in North America.
St. Petersburg-Clearwater International Airport (IATA: PIE) lies just across the bay from Tampa International Airport in neighboring Pinellas County. The airport has become a popular destination for discount carriers, with over 90% of its flights are on low-cost carrier Allegiant Air. A joint civil-military aviation facility, it is also home to Coast Guard Air Station Clearwater, the largest air station in the U.S. Coast Guard.
Sarasota–Bradenton International Airport (IATA: SRQ) is located in nearby Sarasota. Sarasota airport has more flights to Delta's Atlanta hub than any other city, but also serves several other large U.S. cities.

Tampa's intercity passenger rail service is based at Tampa Union Station, a historic facility, adjacent to downtown between the Channel District and Ybor City. The station is served by Amtrak's Silver Star, which calls on Tampa twice daily: southbound to Miami and northbound for New York City. Union Station also serves as the transfer hub for Amtrak Thruway Motorcoach service, offering bus connections to several cities in Southwest Florida and to Orlando.
Uceta Rail Yard on Tampa's east side services CSX as a storage and intermodal freight transport facility. Freight and container cargo operations at the city's seaports also depend upon dockside rail facilities.

The Port of Tampa is the largest port in Florida in throughput tonnage, making it one of the busiest commercial ports in North America. Petroleum and phosphate are the lead commodities, accounting for two-thirds of the 37 million tons of total bulk and general cargo handled by the port in 2009. The port is also home to Foreign Trade Zone #79, which assists companies in Tampa Bay and along the I-4 Corridor in importing, exporting, manufacturing, and distribution activities as part of the United States foreign trade zone program.
Weekly containerized cargo service is available in the Port of Tampa. Cargo service is offered by Ports America, Zim American Integrated Shipping Company, and MSC which has recently partnered with Zim. Currently 3,000 to 4,250 TEU containerships regularly call the Port of Tampa.
The bay bottom is very sandy, with the U.S. Army Corps of Engineers constantly dredging the ship channels to keep them navigable to large cargo ships.

Public mass transit in Tampa is operated by the Hillsborough Area Regional Transit Authority (HART), and includes public bus as well as a streetcar line. The HART bus system's main hub is the Marion Transit Center in Downtown Tampa, serving nearly 30 local and express routes. HART is also currently making a bus rapid transit system called MetroRapid that will run between Downtown and the University of South Florida.
The TECO Line Streetcar System runs electric streetcar service along eleven stations on a 2.7-mile (4.3 km) route, connecting Ybor City, the Channel District, the Tampa Convention Center, and downtown Tampa. The TECO Line fleet features varnished wood interiors reminiscent of late 19th and mid-20th century streetcars.
Limited transportation by privately operated "Neighborhood Electric Vehicles" (NEV) is available, primarily in Downtown Tampa and Ybor City. Water taxis are available on a charter basis for tours along the downtown waterfront and the Hillsborough River.
The Tampa Bay Area Regional Transportation Authority (TBARTA) develops bus, light rail, and other transportation options for the seven-county Tampa Bay area.

Tampa and its surrounding suburbs are host to over 20 hospitals, four trauma centers, and multiple Cancer treatment centers. Three of the area's hospitals were ranked among "America's best hospitals" by US News and World Report. Tampa is also home to many health research institutions. The major hospitals in Tampa include Tampa General Hospital, St. Joseph's Children's & Women's Hospital, James A. Haley Veterans Hospital, H. Lee Moffitt Cancer Center & Research Institute, and The Pepin Heart Institute. Shriners Hospitals for Children is based in Tampa. USF's Byrd Alzheimer's Institute is both a prominent research facility and Alzheimer's patient care center in Tampa. Along with human health care, there are hundreds of animal medical centers including a Humane Society of America.

Water in the area is managed by the Southwest Florida Water Management District. The water is mainly supplied by the Hillsborough River, which in turn arises from the Green Swamp, but several other rivers and desalination plants in the area contribute to the supply. Power is mainly generated by TECO Energy.

Tampa has formalized sister city agreements with the following cities:

Baldomero Lopez
Largest metropolitan areas in the Americas
List of public art in Tampa, Florida
United States cities by population
Seal of Tampa

Official website
Tampa Bay Convention and Visitors Bureau
Tampa Chamber of Commerce
Tampa website dedicated to historic Tampa photographs
Tampa Bay at DMOZ
Tampa Changing – Historical and modern photographs of Tampa
National Park Service Battle Description of the Battle of Fort Brooke and Ballast PointAtlanta is the capital of and the most populous city in the U.S. state of Georgia, with an estimated 2015 population of 463,878. Atlanta is the cultural and economic center of the Atlanta metropolitan area, home to 5,710,795 people and the ninth largest metropolitan area in the United States. Atlanta is the county seat of Fulton County, and a small portion of the city extends eastward into DeKalb County.
Atlanta was established in 1837 at the intersection of two railroad lines, and the city rose from the ashes of the American Civil War to become a national center of commerce. In the decades following the Civil Rights Movement, during which the city earned a reputation as "too busy to hate" for the relatively progressive views of some of its citizens and leaders compared to other cities in the Deep South Atlanta attained international prominence. Atlanta is the primary transportation hub of the Southeastern United States, via highway, railroad, and air, with Hartsfield–Jackson Atlanta International Airport being the world's busiest airport since 1998.
Atlanta is an "alpha-" or "world city", exerting a significant impact upon commerce, finance, research, technology, education, media, art, and entertainment. It ranks 36th among world cities and 8th in the nation with a gross domestic product of $270 billion. Atlanta's economy is considered diverse, with dominant sectors including logistics, professional and business services, media operations, and information technology. Topographically, Atlanta is marked by rolling hills and dense tree coverage. Revitalization of Atlanta's neighborhoods, initially spurred by the 1996 Olympics in Atlanta, has intensified in the 21st century, altering the city's demographics, politics, and culture.

Prior to the arrival of European settlers in north Georgia, Creek Indians inhabited the area. Standing Peachtree, a Creek village located where Peachtree Creek flows into the Chattahoochee River, was the closest Indian settlement to what is now Atlanta. As part of the systematic removal of Native Americans from northern Georgia from 1802 to 1825, the Creek ceded the area in 1821, and white settlers arrived the following year.

In 1836, the Georgia General Assembly voted to build the Western and Atlantic Railroad in order to provide a link between the port of Savannah and the Midwest. The initial route was to run southward from Chattanooga to a terminus east of the Chattahoochee River, which would then be linked to Savannah. After engineers surveyed various possible locations for the terminus, the "zero milepost" was driven into the ground in what is now Five Points. A year later, the area around the milepost had developed into a settlement, first known as "Terminus", and later as "Thrasherville" after a local merchant who built homes and a general store in the area. By 1842, the town had six buildings and 30 residents, and was renamed "Marthasville" to honor the Governor's daughter. J. Edgar Thomson, Chief Engineer of the Georgia Railroad, suggested the town be renamed "Atlantica-Pacifica," which was shortened to "Atlanta". The residents approved, and the town was incorporated as Atlanta on December 29, 1847.
By 1860, Atlanta's population had grown to 9,554. During the Civil War, the nexus of multiple railroads in Atlanta made the city a hub for the distribution of military supplies. In 1864, following the capture of Chattanooga, the Union Army moved southward and began its invasion of north Georgia. The region surrounding Atlanta was the location of several major army battles, culminating with the Battle of Atlanta and a four-month-long siege of the city by the Union Army under the command of General William Tecumseh Sherman. On September 1, 1864, Confederate General John Bell Hood made the decision to retreat from Atlanta, ordering all public buildings and possible assets to the Union Army destroyed. On the next day, Mayor James Calhoun surrendered Atlanta to the Union Army, and on September 7, General Sherman ordered the city's civilian population to evacuate. On November 11, 1864, in preparation of the Union Army's march to Savannah, Sherman ordered Atlanta to be burned to the ground, sparing only the city's churches and hospitals.

After the Civil War ended in 1865, Atlanta was gradually rebuilt. Due to the city's superior rail transportation network, the state capital was moved to Atlanta from Milledgeville in 1868. In the 1880 Census, Atlanta surpassed Savannah as Georgia's largest city. Beginning in the 1880s, Henry W. Grady, the editor of the Atlanta Constitution newspaper, promoted Atlanta to potential investors as a city of the "New South" that would be based upon a modern economy and less reliant on agriculture. By 1885, the founding of the Georgia School of Technology (now Georgia Tech) and the city's black colleges had established Atlanta as a center for higher education. In 1895, Atlanta hosted the Cotton States and International Exposition, which attracted nearly 800,000 attendees and successfully promoted the New South's development to the world.
During the first decades of the 20th century, Atlanta experienced a period of unprecedented growth. In three decades' time, Atlanta's population tripled as the city limits expanded to include nearby streetcar suburbs; the city's skyline emerged with the construction of the Equitable, Flatiron, Empire, and Candler buildings; and Sweet Auburn emerged as a center of black commerce. However, the period was also marked by strife and tragedy. Increased racial tensions led to the Atlanta Race Riot of 1906, which left at least 27 people dead and over 70 injured. In 1915, Leo Frank, a Jewish-American factory superintendent, convicted of murder, was hanged in Marietta by a lynch mob, drawing attention to antisemitism in the United States. On May 21, 1917, the Great Atlanta Fire destroyed 1,938 buildings in what is now the Old Fourth Ward, resulting in one fatality and the displacement of 10,000 people.

On December 15, 1939, Atlanta hosted the film premiere of Gone with the Wind, the epic film based on the best-selling novel by Atlanta's Margaret Mitchell. The film's legendary producer, David O. Selznick, as well as the film's stars Clark Gable, Vivien Leigh, and Olivia de Havilland attended the gala event at Loew's Grand Theatre, but Oscar winner Hattie McDaniel, an African American, was barred from the event due to racial segregation laws and policies.
Atlanta played a vital role in the Allied effort during World War II due the city's war-related manufacturing companies, railroad network, and military bases, leading to rapid growth in the city's population and economy. In the 1950s, the city's newly constructed freeway system allowed middle class Atlantans the ability to relocate to the suburbs. As a result, the city began to make up an ever-smaller proportion of the metropolitan area's population.
During the 1960s, Atlanta was a major organizing center of the Civil Rights Movement, with Dr. Martin Luther King, Jr., Ralph David Abernathy, and students from Atlanta's historically black colleges and universities playing major roles in the movement's leadership. While minimal compared to other cities, Atlanta was not completely free of racial strife. In 1961, the city attempted to thwart blockbusting by erecting road barriers in Cascade Heights, countering the efforts of civic and business leaders to foster Atlanta as the "city too busy to hate". Desegregation of the public sphere came in stages, with public transportation desegregated by 1959, the restaurant at Rich's department store by 1961, movie theaters by 1963, and public schools by 1973.

In 1960, whites comprised 61.7% of the city's population. By 1970, African Americans were a majority of the city's population and exercised new-found political influence by electing Atlanta's first black mayor, Maynard Jackson, in 1973. Under Mayor Jackson's tenure, Atlanta's airport was modernized, solidifying the city's role as a transportation center. The opening of the Georgia World Congress Center in 1976 heralded Atlanta's rise as a convention city. Construction of the city's subway system began in 1975, with rail service commencing in 1979. However, despite these improvements, Atlanta lost over 100,000 residents between 1970 and 1990, over 20% of its population.
In 1990, Atlanta was selected as the site for the 1996 Summer Olympic Games. Following the announcement, the city government undertook several major construction projects to improve Atlanta's parks, sporting venues, and transportation infrastructure. While the games themselves were marred by numerous organizational inefficiencies, as well as the Centennial Olympic Park bombing, they were a watershed event in Atlanta's history, initiating a fundamental transformation of the city in the decade that followed.
During the 2000s, Atlanta underwent a profound transformation demographically, physically, and culturally. Suburbanization, a booming economy, and new migrants decreased the city's black percentage from a high of 67% in 1990 to 54% in 2010. From 2000 to 2010, Atlanta gained 22,763 white residents, 5,142 Asian residents, and 3,095 Hispanic residents, while the city's black population decreased by 31,678. Much of the city's demographic change during the decade was driven by young, college-educated professionals: from 2000 to 2009, the three-mile radius surrounding Downtown Atlanta gained 9,722 residents aged 25 to 34 holding at least a four-year degree, an increase of 61%. Between the mid-1990s and 2010, stimulated by funding from the HOPE VI program, Atlanta demolished nearly all of its public housing, a total of 17,000 units and about 10% of all housing units in the city. In 2005, the $2.8 billion BeltLine project was adopted, with the stated goals of converting a disused 22-mile freight railroad loop that surrounds the central city into an art-filled multi-use trail and increasing the city's park space by 40%. Lastly, Atlanta's cultural offerings expanded during the 2000s: the High Museum of Art doubled in size; the Alliance Theatre won a Tony Award; and numerous art galleries were established on the once-industrial Westside.

Atlanta encompasses 134.0 square miles (347.1 km2), of which 133.2 square miles (344.9 km2) is land and 0.85 square miles (2.2 km2) is water. The city is situated among the foothills of the Appalachian Mountains, and at 1,050 feet (320 m) above mean sea level, Atlanta has the highest elevation of major cities east of the Mississippi River. Atlanta straddles the Eastern Continental Divide, such that rainwater that falls on the south and east side of the divide flows into the Atlantic Ocean, while rainwater on the north and west side of the divide flows into the Gulf of Mexico. Atlanta sits atop a ridge south of the Chattahoochee River, which is part of the ACF River Basin. Located at the far northwestern edge of the city, much of the river's natural habitat is preserved, in part by the Chattahoochee River National Recreation Area.

Most of Atlanta was burned during the Civil War, depleting the city of a large stock of its historic architecture. Yet architecturally, the city had never been particularly "southern"—because Atlanta originated as a railroad town, rather than a patrician southern seaport like Savannah or Charleston, many of the city's landmarks could have easily been erected in the Northeast or Midwest.

During the Cold War era, Atlanta embraced global modernist trends, especially regarding commercial and institutional architecture. Examples of modernist architecture include the 1,196,240sq.ft Westin Peachtree Plaza (1976), Georgia-Pacific Tower (1982), the State of Georgia Building (1966), and the Atlanta Marriott Marquis (1985). In the latter half of the 1980s, Atlanta became one of the early adopters of postmodern designs that reintroduced classical elements to the cityscape. Many of Atlanta's tallest skyscrapers were built in the late 1980s and early 1990s, with most displaying tapering spires or otherwise ornamented crowns, such as the 1,187,676 sq.ftOne Atlantic Center (1987), 191 Peachtree Tower (1991), and the Four Seasons Hotel Atlanta (1992). Also completed during the era is Atlanta's tallest skyscraper, the Bank of America Plaza (1992), which, at 1,023 feet (312 m), is the 61st-tallest building in the world and the 9th-tallest building in the United States. The Bank of America Plaza is the tallest building outside of New York City and Chicago, and was the last building built in the United States to be in the top 10 tallest buildings in the world until One World Trade Center was completed externally in May 2013. The city's embrace of modern architecture, however, translated into an ambivalent approach toward historic preservation, leading to the destruction of notable architectural landmarks, including the Equitable Building (1892–1971), Terminal Station (1905–1972), and the Carnegie Library (1902–1977). The Fox Theatre (1929)—Atlanta's cultural icon—would have met the same fate had it not been for a grassroots effort to save it in the mid-1970s.
Atlanta is divided into 242 officially defined neighborhoods. The city contains three major high-rise districts, which form a north-south axis along Peachtree: Downtown, Midtown, and Buckhead. Surrounding these high-density districts are leafy, low-density neighborhoods, most of which are dominated by single-family homes.
Downtown Atlanta contains the most office space in the metro area, much of it occupied by government entities. Downtown is also home to the city's sporting venues and many of its tourist attractions. Midtown Atlanta is the city's second-largest business district, containing the offices of many of the region's law firms. Midtown is also known for its art institutions, cultural attractions, institutions of higher education, and dense form. Buckhead, the city's uptown district, is eight miles (13 km) north of Downtown and the city's third-largest business district. The district is marked by an urbanized core along Peachtree Road, surrounded by suburban single-family neighborhoods situated among dense forests and rolling hills.

Surrounding Atlanta's three high-rise districts are the city's low- and medium-density neighborhoods, where the craftsman bungalow single-family home is dominant. The eastside is marked by historic streetcar suburbs built from the 1890s-1930s as havens for the upper middle class. These neighborhoods, many of which contain their own villages encircled by shaded, architecturally-distinct residential streets, include the Victorian Inman Park, Bohemian East Atlanta, and eclectic Old Fourth Ward. On the westside, former warehouses and factories have been converted into housing, retail space, and art galleries, transforming the once-industrial West Midtown into a model neighborhood for smart growth, historic rehabilitation, and infill construction. In southwest Atlanta, neighborhoods closer to downtown originated as streetcar suburbs, including the historic West End, while those farther from downtown retain a postwar suburban layout, including Collier Heights and Cascade Heights, home to much of the city's affluent African American population. Northwest Atlanta contains the areas of the city to west of Marietta Boulevard and to the north of Martin Luther King, Jr. Drive, including those neighborhoods remote to downtown, such as Riverside, Bolton and Whittier Mill, which is one of Atlanta's designated Landmark Historical Neighborhoods. Vine City, though technically Northwest, adjoins the city's Downtown area and has recently been the target of community outreach programs and economic development initiatives.
Gentrification of the city's neighborhoods is one of the more controversial and transformative forces shaping contemporary Atlanta. The gentrification of Atlanta has its origins in the 1970s, after many of Atlanta's neighborhoods had undergone the urban decay that affected other major American cities in the mid-20th century. When neighborhood opposition successfully prevented two freeways from being built through city's the east side in 1975, the area became the starting point for Atlanta's gentrification. After Atlanta was awarded the Olympic games in 1990, gentrification expanded into other parts of the city, stimulated by infrastructure improvements undertaken in preparation for the games. Gentrification was also aided by the Atlanta Housing Authority's eradication of the city's public housing.

Under the Köppen classification, Atlanta has a humid subtropical climate (Cfa) with four distinct seasons and generous precipitation year-round, typical for the inland South. Summers are hot and humid, with temperatures somewhat moderated by the city's elevation. Winters are cool but variable, with an average of 48 freezing days per year and temperatures dropping to 0 °F (−17.8 °C) on rare occasions. Warm air from the Gulf of Mexico can bring spring-like highs while strong Arctic air masses can push lows into the teens (≤ −7 °C).
July averages 80.2 °F (26.8 °C), with high temperatures reaching 90 °F (32 °C) on an average 44 days per year, though 100 °F (38 °C) readings are not seen most years. January averages 43.5 °F (6.4 °C), with temperatures in the suburbs slightly cooler due largely to the urban heat island effect. Lows at or below freezing can be expected 40 nights annually, but extended stretches with daily high temperatures below 40 °F (4 °C) are very rare, with a recent exception in January 2014. Extremes range from −9 °F (−23 °C) on February 13, 1899 to 106 °F (41 °C) on June 30, 2012. Dewpoints in the summer range from 63.6 °F (18 °C) in June to 67.8 °F (20 °C) in July.
Typical of the southeastern U.S., Atlanta receives abundant rainfall that is relatively evenly distributed throughout the year, though spring and early fall are markedly drier. The average annual rainfall is 50.2 inches (1,280 mm), while snowfall is typically light at around 2.1 inches (5.3 cm) per year. The heaviest single snowfall occurred on January 23, 1940, with around 10 inches (25 cm) of snow. However, ice storms usually cause more problems than snowfall does, the most severe occurring on January 7, 1973. Tornadoes are rare in the city itself, but the March 15, 2008 EF2 tornado damaged prominent structures in downtown Atlanta.

The 2010 United States Census reported that Atlanta had a population of 420,003. The population density was 3,154 per square mile (1232/km2). The racial makeup and population of Atlanta was 54.0% Black or African American, 38.4% White, 3.1% Asian and 0.2% Native American. Those from some other race made up 2.2% of the city's population, while those from two or more races made up 2.0%. Hispanics of any race made up 5.2% of the city's population. The median income for a household in the city was $45,171. The per capita income for the city was $35,453. 22.6% percent of the population was living below the poverty line. However, compared to the rest of the country, Atlanta's cost of living is 6.00% lower than the U.S. average. Atlanta has one of the highest LGBT populations per capita, ranking third among major American cities, behind San Francisco and slightly behind Seattle, with 12.8% of the city's total population identifying as gay, lesbian, or bisexual. 7.3% of Atlantans were born abroad.
In the 2010 Census, Atlanta was recorded as the nation's fourth-largest majority-black city. It has long been known as a center of African-American political power, education, and culture, often called a black mecca. African-American residents of Atlanta have followed whites to newer housing in the suburbs in the early 21st century. From 2000 to 2010, the city's black population decreased by 31,678 people, shrinking from 61.4% of the city's population in 2000 to 54.0% in 2010.
At the same time, the white population of Atlanta has increased. Between 2000 and 2010, the proportion of whites in the city's population grew faster than that of any other U.S. city. In that decade, Atlanta's white population grew from 31% to 38% of the city's population, an absolute increase of 22,753 people, more than triple the increase that occurred between 1990 and 2000.
Out of the total population five years and older, 83.3% spoke only English at home, while 8.8% spoke Spanish, 3.9% another Indo-European language, and 2.8% an Asian language. Atlanta's dialect has traditionally been a variation of Southern American English. The Chattahoochee River long formed a border between the Coastal Southern and Southern Appalachian dialects. Because of the development of corporate headquarters in the region, attracting migrants from other areas of the country, by 2003, Atlanta magazine concluded that Atlanta had become significantly "de-Southernized." A Southern accent was considered a handicap in some circumstances. In general, Southern accents are less prevalent among residents of the city and inner suburbs and among younger people; they are more common in the outer suburbs and among older people. At the same time, residents of the city express Southern variations of African American Vernacular English.
Religion in Atlanta, while historically centered on Protestant Christianity, now involves many faiths as a result of the city and metro area's increasingly international population. Protestant Christianity still maintains a strong presence in the city (63%), but in recent decades the Catholic Church has increased in numbers and influence because of new migrants in the region. Metro Atlanta also has numerous ethnic or national Christian congregations, including Korean and Indian churches. The larger non-Christian faiths are Judaism, Islam and Hinduism. Overall, there are over 1,000 places of worship within Atlanta.

Encompassing $304 billion, the Atlanta metropolitan area is the eighth-largest economy in the country and 17th-largest in the world. Corporate operations comprise a large portion of the Atlanta's economy, with the city serving as the regional, national, or global headquarters for many corporations. Atlanta contains the country's third largest concentration of Fortune 500 companies, and the city is the global headquarters of corporations such as The Coca-Cola Company, The Home Depot, Delta Air Lines, AT&T Mobility, Chick-fil-A, UPS, and Newell-Rubbermaid. Over 75 percent of Fortune 1000 companies conduct business operations in the Atlanta metropolitan area, and the region hosts offices of about 1,250 multinational corporations. Many corporations are drawn to Atlanta on account of the city's educated workforce; as of 2014, 45% of adults 25 or older in the city of Atlanta have at least 4-year college degrees, compared to 28% in the nation as a whole.

Atlanta began as a railroad town and logistics has remained a major component of the city's economy to this day. Atlanta is an important rail junction and contains major classification yards for Norfolk Southern and CSX. Since its construction in the 1950s, Hartsfield-Jackson Atlanta International Airport has served as a key engine of Atlanta's economic growth. Delta Air Lines, the city's largest employer and the metro area's third largest, operates the world's largest airline hub at Hartsfield-Jackson and has helped make it the world's busiest airport, both in terms of passenger traffic and aircraft operations. Partly due to the airport, Atlanta has become a hub for diplomatic missions; as of 2012, the city contains 25 general consulates, the seventh-highest concentration of diplomatic missions in the United States.
Media is also an important aspect of Atlanta's economy. The city is a major cable television programming center. Ted Turner established the headquarters of both the Cable News Network (CNN) and the Turner Broadcasting System (TBS) in Atlanta. Cox Enterprises, the country's third-largest cable television service and the publisher of over a dozen major American newspapers, is headquartered in the city. The Weather Channel, owned by NBCUniversal, Bain Capital, and The Blackstone Group, is headquartered just outside Atlanta in Cobb County.
Information technology, an economic sector that includes publishing, software development, entertainment and data processing has garnered a larger percentage of Atlanta's economic output. Indeed, Atlanta has been nicknamed the Silicon peach due to its burgeoning technology sector. As of 2013, Atlanta contains the fourth-largest concentration of information technology jobs in the United States, numbering 85,000. Atlanta also ranks as the sixth fastest-growing city for information technology jobs, with an employment growth of 4.8% in 2012 and a three-year growth near 9%, or 16,000 jobs. Information technology companies are drawn to Atlanta's lower costs and educated workforce.
Largely due to a statewide tax incentive enacted in 2005, the Georgia Entertainment Industry Investment Act, which awards qualified productions a transferable income tax credit of 20% of all in-state costs for film and television investments of $500,000 or more, Atlanta has become a center for film and television production. Film and television production facilities in Atlanta include Turner Studios, Pinewood Studios (Pinewood Atlanta), Tyler Perry Studios, Williams Street Productions, and the EUE/Screen Gems soundstages. Film and television production injected $6 billion into Georgia's economy in 2015, with Atlanta garnering most of the projects. Atlanta has gained recognition as a center of production of horror and zombie-related productions, with Atlanta magazine dubbing the city the "Zombie Capital of the World".

Compared to other American cities, Atlanta's economy has been disproportionately affected by the 2008 financial crisis and subsequent recession, with the city's economy earning a ranking of 68 among 100 American cities in a September 2014 report due to an elevated unemployment rate, declining real income levels, and a depressed housing market. From 2010 to 2011, Atlanta saw a 0.9% contraction in employment and only a 0.4% rise in income. Though unemployment had dropped to 7% by late 2014, this was still higher than the national unemployment rate of 5.8% Atlanta's housing market has also struggled, with home prices falling by 2.1% in January 2012, reaching levels not seen since 1996. Compared with a year earlier, the average home price in Atlanta fell 17.3% in February 2012, the largest annual drop in the history of the index for any city. The collapse in home prices has led some economists to deem Atlanta the worst housing market in the country. Nevertheless, in August 2013, Atlanta appeared on Forbes magazine's list of the Best Places for Business and Careers.

Atlanta, while located in the South, has a culture that is no longer strictly Southern. This is due to a large population of migrants from other parts of the U.S., in addition to many recent immigrants to the U.S. who have made the metropolitan area their home, establishing Atlanta as the cultural and economic hub of an increasingly multi-cultural metropolitan area. Thus, although traditional Southern culture is part of Atlanta's cultural fabric, it is mostly the backdrop to one of the nation's most cosmopolitan cities. This unique cultural combination reveals itself in the arts district of Midtown, the quirky neighborhoods on the city's eastside, and the multi-ethnic enclaves found along Buford Highway.

Atlanta is one of few United States cities with permanent, professional, resident companies in all major performing arts disciplines: opera (Atlanta Opera), ballet (Atlanta Ballet), orchestral music (Atlanta Symphony Orchestra), and theater (the Alliance Theatre). Atlanta also attracts many touring Broadway acts, concerts, shows, and exhibitions catering to a variety of interests. Atlanta's performing arts district is concentrated in Midtown Atlanta at the Woodruff Arts Center, which is home to the Atlanta Symphony Orchestra and the Alliance Theatre. The city also frequently hosts touring Broadway acts, especially at The Fox Theatre, a historic landmark that is among the highest grossing theatres of its size.
As a national center for the arts, Atlanta is home to significant art museums and institutions. The renowned High Museum of Art is arguably the South's leading art museum and among the most-visited art museums in the world. The Museum of Design Atlanta (MODA), a design museum, is the only such museum in the Southeast. Contemporary art museums include the Atlanta Contemporary Art Center and the Museum of Contemporary Art of Georgia. Institutions of higher education also contribute to Atlanta's art scene, with the Savannah College of Art and Design's Atlanta campus providing the city's arts community with a steady stream of curators, and Emory University's Michael C. Carlos Museum containing the largest collection of ancient art in the Southeast.

Atlanta has played a major or contributing role in the development of various genres of American music at different points in the city's history. Beginning as early as the 1920s, Atlanta emerged as a center for country music, which was brought to the city by migrants from Appalachia. During the countercultural 1960s, Atlanta hosted the Atlanta International Pop Festival, with the 1969 festival taking place more than a month before Woodstock and featuring many of the same bands. The city was also a center for Southern rock during its 1970s heyday: the Allman Brothers Band's hit instrumental "Hot 'Lanta" is an ode to the city, while Lynyrd Skynyrd's famous live rendition of "Free Bird" was recorded at the Fox Theatre in 1976, with lead singer Ronnie Van Zant directing the band to "play it pretty for Atlanta". During the 1980s, Atlanta had an active Punk rock scene that was centered on two of the city's music venues, 688 Club and the Metroplex, and Atlanta famously played host to the Sex Pistols first U.S. show, which was performed at the Great Southeastern Music Hall. The 1990s saw the birth of Atlanta hip hop, a subgenre that gained relevance following the success of home-grown duo OutKast; however, it was not until the 2000s that Atlanta moved "from the margins to becoming hip-hop's center of gravity, part of a larger shift in hip-hop innovation to the South". Also in the 2000s, Atlanta was recognized by the Brooklyn-based Vice magazine for its impressive yet under-appreciated Indie rock scene, which revolves around the various live music venues found on the city's alternative eastside.

As of 2010, Atlanta is the seventh-most visited city in the United States, with over 35 million visitors per year. Although the most popular attraction among visitors to Atlanta is the Georgia Aquarium, the world's largest indoor aquarium, Atlanta's tourism industry is mostly driven by the city's history museums and outdoor attractions. Atlanta contains a notable amount of historical museums and sites, including the Martin Luther King, Jr. National Historic Site, which includes the preserved childhood home of Dr. Martin Luther King, Jr., as well as his final resting place; the Atlanta Cyclorama & Civil War Museum, which houses a massive painting and diorama in-the-round, with a rotating central audience platform, depicting the Battle of Atlanta in the Civil War; the World of Coca-Cola, featuring the history of the world-famous soft drink brand and its well-known advertising; the College Football Hall of Fame which honors college football and its athletes; the National Center for Civil and Human Rights, which explores the Civil Rights Movement and its connection to contemporary human rights movements throughout the world; the Carter Center and Presidential Library, housing U.S. President Jimmy Carter's papers and other material relating to the Carter administration and the Carter family's life; and the Margaret Mitchell House and Museum, where Mitchell wrote the best-selling novel Gone with the Wind.
Atlanta also contains various outdoor attractions. The Atlanta Botanical Garden, adjacent to Piedmont Park, is home to the 600-foot-long (180 m) Kendeda Canopy Walk, a skywalk that allows visitors to tour one of the city's last remaining urban forests from 40-foot-high (12 m). The Canopy Walk is considered the only canopy-level pathway of its kind in the United States. Zoo Atlanta, located in Grant Park, accommodates over 1,300 animals representing more than 220 species. Home to the nation's largest collections of gorillas and orangutans, the Zoo is also one of only four zoos in the U.S. to house giant pandas. Festivals showcasing arts and crafts, film, and music, including the Atlanta Dogwood Festival, the Atlanta Film Festival, and Music Midtown, respectively, are also popular with tourists.

Tourists are also drawn to the city's culinary scene, which comprises a mix of urban establishments garnering national attention, ethnic restaurants serving cuisine from every corner of the world, and traditional eateries specializing in Southern dining. Since the turn of the 21st century, Atlanta has emerged as a sophisticated restaurant town. Many restaurants opened in the city's gentrifying neighborhoods have received praise at the national level, including Bocado, Bacchanalia, and Miller Union in West Midtown, Empire State South in Midtown, and Two Urban Licks and Rathbun's on the east side. In 2011, the New York Times characterized Empire State South and Miller Union as reflecting "a new kind of sophisticated Southern sensibility centered on the farm but experienced in the city." Visitors seeking to sample international Atlanta are directed to Buford Highway, the city's international corridor. There, the million-plus immigrants that make Atlanta home have established various authentic ethnic restaurants representing virtually every nationality on the globe. For traditional Southern fare, one of the city's most famous establishments is The Varsity, a long-lived fast food chain and the world's largest drive-in restaurant. Mary Mac's Tea Room and Paschal's are more formal destinations for Southern food.

Atlanta is home to professional franchises for three major team sports: the Atlanta Braves of Major League Baseball, the Atlanta Hawks of the National Basketball Association, and the Atlanta Falcons of the National Football League. The Braves, who moved to Atlanta in 1966, were established as the Boston Red Stockings in 1871 and are the oldest continually operating professional sports franchise in the United States. The Braves won the World Series in 1995, and had an unprecedented run of 14 straight divisional championships from 1991 to 2005.
The Atlanta Falcons have played in Atlanta since their inception in 1966. The Falcons have won the division title five times (1980, 1998, 2004, 2010, 2012) and the conference championship once, when they finished as the runner-up to the Denver Broncos in Super Bowl XXXIII in 1999. The Atlanta Hawks began in 1946 as the Tri-Cities Blackhawks, playing in Moline, Illinois. The team moved to Atlanta in 1968, and they currently play their games in Philips Arena. The Atlanta Dream is the city's Women's National Basketball Association franchise.
Atlanta has also had its own professional ice hockey and soccer franchises. The National Hockey League (NHL) has had two Atlanta franchises: the Atlanta Flames began play in 1972 before moving to Calgary in 1980, while the Atlanta Thrashers began play in 1999 before moving to Winnipeg in 2011. The Atlanta Chiefs was the city's professional soccer team from 1967 to 1972, and the team won a national championship in 1968. In 1998 another professional soccer team was formed, the Atlanta Silverbacks of the North American Soccer League. In April 2014, a Major League Soccer team, Atlanta United FC, was formed as an expansion team to begin play in 2017.
Atlanta has been the host city for various international, professional and collegiate sporting events. Most famously, Atlanta hosted the Centennial 1996 Summer Olympics. Atlanta has also hosted Super Bowl XXVIII in 1994 and Super Bowl XXXIV in 2000. In professional golf, The Tour Championship, the final PGA Tour event of the season, is played annually at East Lake Golf Club. In 2001 and 2011, Atlanta hosted the PGA Championship, one of the four major championships in men's professional golf, at the Atlanta Athletic Club. In professional ice hockey, the city hosted the 56th NHL All-Star Game in 2008, three years before the Thrashers moved. In 2011, Atlanta hosted professional wrestling's annual WrestleMania. The city has hosted the NCAA Final Four Men's Basketball Championship four times, most recently in 2013. In college football, Atlanta hosts the Chick-fil-A Kickoff Game, the SEC Championship Game, and the Chick-fil-A Peach Bowl.

Atlanta's 343 parks, nature preserves, and gardens cover 3,622 acres (14.66 km2), which amounts to only 5.6% of the city's total acreage, compared to the national average of just over 10%. However, 64% of Atlantans live within a 10-minute walk of a park, a percentage equal to the national average. Furthermore, in its 2013 ParkScore ranking, The Trust for Public Land, a national land conservation organization, reported that among the park systems of the 50 most populous U.S. cities, Atlanta's park system received a ranking of 31. Piedmont Park, located in Midtown is Atlanta's most iconic green space. The park, which has undergone a major renovation and expansion in recent years, attracts visitors from across the region and hosts cultural events throughout the year. Other notable city parks include Centennial Olympic Park, a legacy of the 1996 Summer Olympics that forms the centerpiece of the city's tourist district; Woodruff Park, which anchors the campus of Georgia State University; Grant Park, home to Zoo Atlanta; and Chastain Park, which houses an amphitheater used for live music concerts. The Chattahoochee River National Recreation Area, located in the northwestern corner of the city, preserves a 48 mi (77 km) stretch of the river for public recreation opportunities. The Atlanta Botanical Garden, adjacent to Piedmont Park, contains formal gardens, including a Japanese garden and a rose garden, woodland areas, and a conservatory that includes indoor exhibits of plants from tropical rainforests and deserts. The BeltLine, a former rail corridor that forms a 22 mi (35 km) loop around Atlanta's core, will eventually be transformed into a series of parks, connected by a multi-use trail, increasing Atlanta's park space by 40%.
Atlanta offers resources and opportunities for amateur and participatory sports and recreation. Jogging is a particularly popular local sport. The Peachtree Road Race, the world's largest 10 km race, is held annually on Independence Day. The Georgia Marathon, which begins and ends at Centennial Olympic Park, routes through the city's historic east side neighborhoods. Golf and tennis are also popular in Atlanta, and the city contains six public golf courses and 182 tennis courts. Facilities located along the Chattahoochee River cater to watersports enthusiasts, providing the opportunity for kayaking, canoeing, fishing, boating, or tubing. The city's only skate park, a 15,000 square feet (1,400 m2) facility that offers bowls, curbs, and smooth-rolling concrete mounds, is located at Historic Fourth Ward Park.

Atlanta is governed by a mayor and the Atlanta City Council. The city council consists of 15 representatives—one from each of the city's 12 districts and three at-large positions. The mayor may veto a bill passed by the council, but the council can override the veto with a two-thirds majority. The mayor of Atlanta is Kasim Reed, a Democrat elected on a nonpartisan ballot whose first term in office expired at the end of 2013. Reed was elected to a second term on November 5, 2013. Every mayor elected since 1973 has been black. In 2001, Shirley Franklin became the first woman to be elected Mayor of Atlanta, and the first African-American woman to serve as mayor of a major southern city. Atlanta city politics suffered from a notorious reputation for corruption during the 1990s administration of Bill Campbell, who was convicted by a federal jury in 2006 on three counts of tax evasion in connection with gambling income he received while Mayor during trips he took with city contractors.
As the state capital, Atlanta is the site of most of Georgia's state government. The Georgia State Capitol building, located downtown, houses the offices of the governor, lieutenant governor and secretary of state, as well as the General Assembly. The Governor's Mansion is located in a residential section of Buckhead. Atlanta serves as the regional hub for many arms of the federal bureaucracy, including the Federal Reserve Bank of Atlanta and the Centers for Disease Control and Prevention. Atlanta also plays an important role in federal judiciary system, containing the United States Court of Appeals for the Eleventh Circuit and of the United States District Court for the Northern District of Georgia.
Historically, Atlanta has been a stronghold for the Democratic Party. Although municipal elections are officially nonpartisan, nearly all of the city's elected officials are registered Democrats. The city is split among 14 state house districts and four state senate districts, all held by Democrats. At the federal level, Atlanta is split between two congressional districts. The northern three-fourths of the city is located in the 5th district, represented by Democrat John Lewis. The southern fourth is in the 13th district, represented by Democrat David Scott.
The city is served by the Atlanta Police Department, which numbers 2,000 officers and oversaw a 40% decrease in the city's crime rate between 2001 and 2009. Specifically, homicide decreased by 57%, rape by 72%, and violent crime overall by 55%. Crime is down across the country, but Atlanta's improvement has occurred at more than twice the national rate. Nevertheless, Forbes ranked Atlanta as the sixth most dangerous city in the United States in 2012.

Due to the more than 30 colleges and universities located in the city, Atlanta is considered a center for higher education. Among the most prominent public universities in Atlanta is the Georgia Institute of Technology, a research university located in Midtown that has been consistently ranked among the nation's top ten public universities for its degree programs in engineering, computing, management, the sciences, architecture, and liberal arts. Georgia State University, a public research university located in Downtown Atlanta, is the largest of the 29 public colleges and universities in the University System of Georgia and a major contributor to the revitalization of the city's central business district. Atlanta is also home to nationally renowned private colleges and universities, most notably Emory University, a leading liberal arts and research institution that ranks among the top 20 schools in the United States and operates Emory Healthcare, the largest health care system in Georgia.  Also located in the city is the Atlanta University Center, the largest contiguous consortium of historically black colleges, comprising Spelman College, Clark Atlanta University, Morehouse College, Morehouse School of Medicine, and Interdenominational Theological Center. Atlanta also contains a campus of the Savannah College of Art and Design, a private art and design university that has proven to be a major factor in the recent growth of Atlanta's visual art community.
Atlanta Public Schools enrolls 55,000 students in 106 schools, some of which are operated as charter schools. The district has been plagued by a widely publicized cheating scandal exposed in 2009. Atlanta is also served by many private schools, including parochial Roman Catholic schools operated by the Archdiocese of Atlanta.

The primary network-affiliated television stations in Atlanta are WXIA-TV (NBC), WGCL-TV (CBS), WSB-TV (ABC), and WAGA-TV (FOX). The Atlanta metropolitan area is served by two public television stations and one public radio station. WGTV is the flagship station of the statewide Georgia Public Television network and is a PBS member station, while WPBA is owned by Atlanta Public Schools. Georgia Public Radio is listener-funded and comprises one NPR member station, WABE, a classical music station operated by Atlanta Public Schools.
Atlanta is served by the Atlanta Journal-Constitution, its only major daily newspaper with wide distribution. The Atlanta Journal-Constitution is the result of a 1950 merger between The Atlanta Journal and The Atlanta Constitution, with staff consolidation occurring in 1982 and separate publication of the morning Constitution and afternoon Journal ceasing in 2001. Alternative weekly newspapers include Creative Loafing, which has a weekly print circulation of 80,000. Atlanta magazine is an award-winning, monthly general-interest magazine based in and covering Atlanta.

Atlanta's transportation infrastructure comprises a complex network that includes a heavy rail rapid transit system, a light rail streetcar loop, a multi-county bus system, Amtrak service via the Crescent, multiple freight train lines, an Interstate Highway System, several airports, including the world's busiest, and over 45 miles (72 kilometres) of bike paths.
With a network of freeways that radiate out from the city, automobiles are the dominant mode of transportation in the region. Three major interstate highways converge in Atlanta: I-20 (east-west), I-75 (northwest-southeast), and I-85 (northeast-southwest). The latter two combine in the middle of the city to form the Downtown Connector (I-75/85), which carries more than 340,000 vehicles per day and is one of the most congested segments of interstate highway in the United States. Atlanta is mostly encircled by Interstate 285, a beltway locally known as "the Perimeter" that has come to mark the boundary between "Inside the Perimeter" (ITP), the city and close-in suburbs, and "Outside the Perimeter" (OTP), the outer suburbs and exurbs. The heavy reliance on automobiles for transportation in Atlanta has resulted in traffic, commute, and air pollution rates that rank among the worst in the country.
The Metropolitan Atlanta Rapid Transit Authority (MARTA) provides public transportation in the form of buses and heavy rail. Notwithstanding heavy automotive usage in Atlanta, the city's subway system is the eighth busiest in the country. MARTA rail lines connect many key destinations, such as the airport, Downtown, Midtown, Buckhead, and Perimeter Center. However, significant destinations, such as Emory University and Cumberland, remain unserved. As a result, a 2012 Brookings Institution study placed Atlanta 87th of 100 metro areas for transit accessibility. Emory University operates its Cliff shuttle buses with 200,000 boardings per month, while private minibuses supply Buford Highway. Amtrak, the national rail passenger system, provides service to Atlanta via the Crescent train (New York–New Orleans), which stops at Peachtree Station. In 2014, the Atlanta Streetcar opened to the public. The streetcar's line, which is also known as the Downtown Loop, runs 2.7 miles around the downtown tourist areas of Peachtree Center, Centennial Olympic Park, the Martin Luther King, Jr. National Historic Site, and Sweet Auburn. The Atlanta Streetcar line is also being expanded on in the coming years to include a wider range of Atlanta's neighborhoods and important places of interest, with a total of over 50 miles of track in the plan.
Hartsfield-Jackson Atlanta International Airport, the world's busiest airport as measured by passenger traffic and aircraft traffic, offers air service to over 150 U.S. destinations and more than 80 international destinations in 52 countries, with over 2,700 arrivals and departures daily. Delta Air Lines maintains its largest hub at the airport. Situated 10 miles (16 km) south of downtown, the airport covers most of the land inside a wedge formed by Interstate 75, Interstate 85, and Interstate 285.
Cycling is a growing mode of transportation in Atlanta, more than doubling since 2009, when it comprised 1.1% of all commutes (up from 0.3% in 2000). Although Atlanta's lack of bike lanes and hilly topography may deter many residents from cycling, the city's transportation plan calls for the construction of 226 miles (364 kilometres) of bike lanes by 2020, with the BeltLine helping to achieve this goal. In 2012, Atlanta's first "bike track" was constructed on 10th Street in Midtown. The two lane bike track runs from Monroe Drive west to Charles Allen Drive, with connections to the Beltline and Piedmont Park. Starting in June 2016, Atlanta received a bike sharing program with 100 bikes in Downtown, with 500 more being expected by the end of the year.

Atlanta has a reputation as a "city in a forest" due to an abundance of trees that is rare among major cities. The city's main street is named after a tree, and beyond the Downtown, Midtown, and Buckhead business districts, the skyline gives way to a dense canopy of woods that spreads into the suburbs. The city is home to the Atlanta Dogwood Festival, an annual arts and crafts festival held one weekend during early April, when the native dogwoods are in bloom. However, the nickname is also factually accurate, as the city's tree coverage percentage is at 36%, the highest out of all major American cities, and above the national average of 27%. Atlanta's tree coverage does not go unnoticed—it was the main reason cited by National Geographic in naming Atlanta a "Place of a Lifetime".
The city's lush tree canopy, which filters out pollutants and cools sidewalks and buildings, has increasingly been under assault from man and nature due to heavy rains, drought, aged forests, new pests, and urban construction. A 2001 study found that Atlanta's heavy tree cover declined from 48% in 1974 to 38% in 1996. However, the problem is being addressed by community organizations and city government: Trees Atlanta, a non-profit organization founded in 1985, has planted and distributed over 75,000 shade trees in the city, while Atlanta's government has awarded $130,000 in grants to neighborhood groups to plant trees.

Atlanta has 19 sister cities, as designated by Sister Cities International, Inc. (SCI):

List of people from Atlanta
Urban forest

Official website
Atlanta Department of Watershed Management
Atlanta Police Department
Atlanta Convention and Visitors Bureau
Entry in the New Georgia Encyclopedia
Atlanta Historic Newspapers Archive Digital Library of Georgia
Atlanta History Photograph Collection from the Atlanta History Center
Atlanta Time Machine
Atlanta, Georgia, a National Park Service Discover Our Shared Heritage Travel Itinerary
Atlanta City Online Travel GuideCalifornia (/ˌkælᵻˈfɔːrnjə, -ni.ə/ KAL-ə-FORN-yə, KAL-ə-FORN-ee-ə) is the most populous state in the United States and the third most extensive by area. Located on the western (Pacific Ocean) coast of the U.S., California is bordered by the other U.S. states of Oregon, Nevada, and Arizona and shares an international border with the Mexican state of Baja California. The state capital is Sacramento. Los Angeles is California's most populous city, and the country's second largest after New York City. The state also has the nation's most populous county, Los Angeles County, and its largest county by area, San Bernardino County.
California's diverse geography ranges from the Pacific Coast in the west to the Sierra Nevada mountain range in the east; and from the redwood–Douglas fir forests in the northwest to the Mojave Desert in the southeast. The Central Valley, a major agricultural area, dominates the state's center. Though California is well-known for its warm Mediterranean climate, the large size of the state means it can vary from moist temperate rainforest in the north, to arid desert in the interior, as well as snowy alpine in the mountains.
What is now California was first settled by various Native American tribes before being explored by a number of European expeditions during the 16th and 17th centuries. The Spanish Empire then claimed it as part of Alta California in their New Spain colony. The area became a part of Mexico in 1821 following its successful war for independence, but was ceded to the United States in 1848 after the Mexican–American War. The western portion of Alta California then was organized as the State of California, and admitted as the 31st state on September 9, 1850. The California Gold Rush starting in 1848 led to dramatic social and demographic changes, with large-scale emigration from the east and abroad with an accompanying economic boom.
If it were a country, California would be the 6th largest economy in the world and the 35th most populous. It is also regarded as a global trendsetter in both popular culture and politics, and is the birthplace of the film industry, the hippie counterculture, the Internet, and the personal computer, among others. Fifty-eight percent of the state's economy is centered on finance, government, real estate services, technology, and professional, scientific and technical business services. The San Francisco Bay Area has the nation's highest median household income by metropolitan area, and is the headquarters of three of the world's largest 20 firms by revenue, Chevron, Apple, and McKesson. Although it accounts for only 1.5 percent of the state's economy, California's agriculture industry has the highest output of any U.S. state.

The word California originally referred to the Baja California Peninsula of Mexico; it was later extended to the entire region composed of the current United States states of California, Nevada, and Utah, and parts of Arizona, New Mexico, Texas and Wyoming.
The name California is surmised by some writers to have derived from a fictional paradise peopled by Black Amazons and ruled by Queen Calafia, who fought alongside Muslims and whose name was chosen to echo the title of a Muslim leader, the Caliph, fictionally implying that California was the Caliphate. The story of Calafia is recorded in a 1510 work The Adventures of Esplandián, written as a sequel to Amadis de Gaula by Spanish adventure writer Garci Rodríguez de Montalvo. The kingdom of Queen Calafia, according to Montalvo, was said to be a remote land inhabited by griffins and other strange beasts, and rich in gold.

Know ye that at the right hand of the Indies there is an island called California, very close to that part of the Terrestrial Paradise, which was inhabited by black women without a single man among them, and they lived in the manner of Amazons. They were robust of body with strong passionate hearts and great virtue. The island itself is one of the wildest in the world on account of the bold and craggy rocks.

When Spanish explorer Francisco de Ulloa was exploring the western coast of North America, his initial surveys of the Baja California Peninsula led him to believe that it was an island rather than part of the larger continent, so he dubbed the "island" after the mythical island in Montalvo's writing. This conventional wisdom that California was an island, with maps drawn to reflect this belief, lasted as late as the 1700's.
Shortened forms of the state's name include CA, Cal., Calif. and US-CA.

Settled by successive waves of arrivals during the last 10,000 years, California was one of the most culturally and linguistically diverse areas in pre-Columbian North America. Various estimates of the native population range from 100,000 to 300,000. The Indigenous peoples of California included more than 70 distinct groups of Native Americans, ranging from large, settled populations living on the coast to groups in the interior. California groups also were diverse in their political organization with bands, tribes, villages, and on the resource-rich coasts, large chiefdoms, such as the Chumash, Pomo and Salinan. Trade, intermarriage and military alliances fostered many social and economic relationships among the diverse groups.

The first European effort to explore the coast as far north as the Russian River was a Spanish sailing expedition, led by Portuguese captain Juan Rodríguez Cabrillo, in 1542. Some 37 years later English explorer Francis Drake also explored and claimed an undefined portion of the California coast in 1579. Spanish traders made unintended visits with the Manila galleons on their return trips from the Philippines beginning in 1565. The first Asians to set foot on what would be the United States occurred in 1587, when Filipino sailors arrived in Spanish ships at Morro Bay. Sebastián Vizcaíno explored and mapped the coast of California in 1602 for New Spain.
Despite the on-the-ground explorations of California in the 16th century, Rodríguez's idea of California as an island persisted. That depiction appeared on many European maps well into the 18th century.
After the Portolà expedition of 1769–70, Spanish missionaries began setting up 21 California Missions on or near the coast of Alta (Upper) California, beginning in San Diego. During the same period, Spanish military forces built several forts (presidios) and three small towns (pueblos). Two of the pueblos grew into the cities of Los Angeles and San Jose. The Spanish colonization brought the genocide of the indigenous Californian peoples.

Imperial Russia explored the California coast and established a trading post at Fort Ross. Its early 19th-century coastal settlements north of San Francisco Bay constituted the southernmost Russian colony in North America and were spread over an area stretching from Point Arena to Tomales Bay.
In 1821, the Mexican War of Independence gave Mexico (including California) independence from Spain; for the next 25 years, Alta California remained a remote northern province of the nation of Mexico.
Cattle ranches, or ranchos, emerged as the dominant institutions of Mexican California. After Mexican independence from Spain, the chain of missions became the property of the Mexican government and were secularized by 1834. The ranchos developed under ownership by Californios (Spanish-speaking Californians) who had received land grants, and traded cowhides and tallow with Boston merchants.
From the 1820s, trappers and settlers from the United States and Canada arrived in Northern California. These new arrivals used the Siskiyou Trail, California Trail, Oregon Trail and Old Spanish Trail to cross the rugged mountains and harsh deserts in and surrounding California.
Between 1831 and 1836, California experienced a series of revolts against Mexico; this culminated in the 1836 California revolt led by Juan Bautista Alvarado, which ended after Mexico appointed him governor of the department. The revolt, which had momentarily declared California an independent state, was successful with the assistance of American and British residents of California, including Isaac Graham; after 1840, 100 of those residents who did not have passports were arrested, leading to the Graham affair in 1840.
One of the largest ranchers in California was John Marsh. After failing to obtain justice against squatters on his land from the Mexican courts, he determined that California should become part of the United States. Marsh conducted a letter-writing campaign espousing the California climate, soil and other reasons to settle there, as well as the best route to follow, which became known as "Marsh's route." His letters were read, reread, passed around, and printed in newspapers throughout the country, and started the first wagon trains rolling to California. He invited immigrants to stay on his ranch until they could get settled, and assisted in their obtaining passports.
After ushering in the period of organized emigration to California, Marsh helped end the rule of the last Mexican governor of California, thereby paving the way to California's ultimate acquisition by the United States.
In 1846, settlers rebelled against Mexican rule during the Bear Flag Revolt. Afterwards, rebels raised the Bear Flag (featuring a bear, a star, a red stripe and the words "California Republic") at Sonoma. The Republic's only president was William B. Ide, who played a pivotal role during the Bear Flag Revolt.
The California Republic was short lived; the same year marked the outbreak of the Mexican–American War (1846–48). When Commodore John D. Sloat of the United States Navy sailed into Monterey Bay and began the military occupation of California by the United States, Northern California capitulated in less than a month to the United States forces. After a series of defensive battles in Southern California, the Treaty of Cahuenga was signed by the Californios on January 13, 1847, securing American control in California.
Following the Treaty of Guadalupe Hidalgo that ended the war, the western territory of Alta California, became the United States state of California, and Arizona, Nevada, Colorado and Utah became United States Territories. The lightly populated lower region of California, the Baja Peninsula, remained in the possession of Mexico.
In 1846, the non-native population of California was estimated to be no more than 8,000, plus about 100,000 Native Americans down from about 300,000 before Hispanic settlement in 1769. After gold was discovered in 1848, the population burgeoned with United States citizens, Europeans, Chinese and other immigrants during the great California Gold Rush. By 1854 over 300,000 settlers had come. Between 1847 and 1870, the population of San Francisco increased from 500 to 150,000. On September 9, 1850, as part of the Compromise of 1850, California was admitted to the United States undivided as a free state, denying the expansion of slavery to the Pacific Coast.
California's native population precipitously declined, above all, from Eurasian diseases to which they had no natural immunity. As in other states, the native inhabitants were forcibly removed from their lands by incoming miners, ranchers, and farmers. And although California entered the union as a free state, the "loitering or orphaned Indians" were de facto enslaved by Mexican and Anglo-American masters under the 1853 Act for the Government and Protection of Indians. There were massacres in which hundreds of indigenous people were killed. Between 1850 and 1860, California paid around 1.5 million dollars (some 250,000 of which was reimbursed by the federal government) to hire militias whose purpose was to protect settlers from the indigenous populations. In later decades, the native population was placed in reservations and rancherias, which were often small and isolated and without enough natural resources or funding from the government to sustain the populations living on them. As a result, the rise of California was a calamity for the native inhabitants. Several scholars and Native American activists, including Benjamin Madley and Ed Castillo, have described the actions of the California government as a genocide.
The seat of government for California under Spanish and later Mexican rule was located at Monterey from 1777 until 1845. Pio Pico, last Mexican governor of Alta California, moved the capital to Los Angeles in 1845. The United States consulate was also located in Monterey, under consul Thomas O. Larkin.
In 1849, the Constitutional Convention was first held in Monterey. Among the tasks was a decision on a location for the new state capital. The first legislative sessions were held in San Jose (1850–1851). Subsequent locations included Vallejo (1852–1853), and nearby Benicia (1853–1854); these locations eventually proved to be inadequate as well. The capital has been located in Sacramento since 1854 with only a short break in 1862 when legislative sessions were held in San Francisco due to flooding in Sacramento.
Initially, travel between California and the rest of the continental United States was time consuming and dangerous. A more direct connection came in 1869 with the completion of the First Transcontinental Railroad through Donner Pass in the Sierra Nevada mountains. Once completed, hundreds of thousands of United States citizens came west, where new Californians were discovering that land in the state, if irrigated during the dry summer months, was extremely well suited to fruit cultivation and agriculture in general. Vast expanses of wheat, other cereal crops, vegetable crops, cotton, and nut and fruit trees were grown (including oranges in Southern California), and the foundation was laid for the state's prodigious agricultural production in the Central Valley and elsewhere.

Migration to California accelerated during the early 20th century with the completion of major transcontinental highways like the Lincoln Highway and Route 66. In the period from 1900 to 1965, the population grew from fewer than one million to become the most populous state in the Union. In 1940, the Census Bureau reported California's population as 6.0% Hispanic, 2.4% Asian, and 89.5% non-Hispanic white.
To meet the population's needs, major engineering feats like the California and Los Angeles Aqueducts; the Oroville and Shasta Dams; and the Bay and Golden Gate Bridges were built across the state. The state government also adopted the California Master Plan for Higher Education in 1960 to develop a highly efficient system of public education.
Meanwhile, attracted to the mild Mediterranean climate, cheap land, and the state's wide variety of geography, filmmakers established the studio system in Hollywood in the 1920s. California manufactured 8.7 percent of total United States military armaments produced during World War II, ranking third (behind New York and Michigan) among the 48 states. After World War II, California's economy greatly expanded due to strong aerospace and defense industries, whose size decreased following the end of the Cold War. Stanford University and its Dean of Engineering Frederick Terman began encouraging faculty and graduates to stay in California instead of leaving the state, and develop a high-tech region in the area now known as Silicon Valley. As a result of these efforts, California is regarded as a world center of the entertainment and music industries, of technology, engineering, and the aerospace industry, and as the United States center of agricultural production. Just before the "Dot Com Bust" California had the 5th largest economy in the world among nations. Yet since 1991, and starting in the late 1980s in Southern California, California has seen a net loss of domestic migrants most years. This is often referred to by the media as the California exodus.
However, during the 20th century, two great disasters happened in California. The 1906 San Francisco earthquake and 1928 St. Francis Dam flood remain the deadliest in U.S history.

California is the 3rd largest state in the United States in area, after Alaska and Texas. California is often geographically bisected into two regions, Southern California, comprising the 10 southernmost counties, and Northern California, comprising the 48 northernmost counties.
In the middle of the state lies the California Central Valley, bounded by the Sierra Nevada in the east, the coastal mountain ranges in the west, the Cascade Range to the north and by the Tehachapi Mountains in the south. The Central Valley is California's productive agricultural heartland.
Divided in two by the Sacramento-San Joaquin River Delta, the northern portion, the Sacramento Valley serves as the watershed of the Sacramento River, while the southern portion, the San Joaquin Valley is the watershed for the San Joaquin River. Both valleys derive their names from the rivers that flow through them. With dredging, the Sacramento and the San Joaquin Rivers have remained deep enough for several inland cities to be seaports.
The Sacramento-San Joaquin River Delta is a critical water supply hub for the state. Water is diverted from the delta and through an extensive network of pumps and canals that traverse nearly the length of the state, to the Central Valley and the State Water Projects and other needs. Water from the Delta provides drinking water for nearly 23 million people, almost two-thirds of the state's population as well as water for farmers on the west side of the San Joaquin Valley.
The Channel Islands are located off the Southern coast.
The Sierra Nevada (Spanish for "snowy range") includes the highest peak in the contiguous 48 states, Mount Whitney, at 14,505 feet (4,421 m). The range embraces Yosemite Valley, famous for its glacially carved domes, and Sequoia National Park, home to the giant sequoia trees, the largest living organisms on Earth, and the deep freshwater lake, Lake Tahoe, the largest lake in the state by volume.
To the east of the Sierra Nevada are Owens Valley and Mono Lake, an essential migratory bird habitat. In the western part of the state is Clear Lake, the largest freshwater lake by area entirely in California. Though Lake Tahoe is larger, it is divided by the California/Nevada border. The Sierra Nevada falls to Arctic temperatures in winter and has several dozen small glaciers, including Palisade Glacier, the southernmost glacier in the United States.
About 45 percent of the state's total surface area is covered by forests, and California's diversity of pine species is unmatched by any other state. California contains more forestland than any other state except Alaska. Many of the trees in the California White Mountains are the oldest in the world; an individual bristlecone pine is over 5,000 years old.
In the south is a large inland salt lake, the Salton Sea. The south-central desert is called the Mojave; to the northeast of the Mojave lies Death Valley, which contains the lowest and hottest place in North America, the Badwater Basin at −279 feet (−85 m). The horizontal distance from the bottom of Death Valley to the top of Mount Whitney is less than 90 miles (140 km). Indeed, almost all of southeastern California is arid, hot desert, with routine extreme high temperatures during the summer. The southeastern border of California with Arizona is entirely formed by the Colorado River, from which the southern part of the state gets about half of its water.
California contains both the highest point (Mount Whitney) and the lowest point (Death Valley) in the contiguous United States.
A majority of California's cities are located in either the San Francisco Bay Area or the Sacramento metropolitan area in Northern California; or the Los Angeles area, the Riverside-San Bernardino-Inland Empire, or the San Diego metropolitan area in Southern California. The Los Angeles Area, the Bay Area, and the San Diego metropolitan area are among several major metropolitan areas along the California coast.
As part of the Ring of Fire, California is subject to tsunamis, floods, droughts, Santa Ana winds, wildfires, landslides on steep terrain, and has several volcanoes. It has many earthquakes due to several faults running through the state, in particular the San Andreas Fault. About 37,000 earthquakes are recorded each year, but most are too small to be felt.

Although most of the state has a Mediterranean climate, due to the state's large size, the climate ranges from subarctic to subtropical. The cool California Current offshore often creates summer fog near the coast. Farther inland, there are colder winters and hotter summers. The maritime moderation results in the shoreline summertime temperatures of Los Angeles and San Francisco being the coolest of all major metropolitan areas of the United States and uniquely cool compared to areas on the same latitude in the interior and on the east coast of the North American continent. Even the San Diego shoreline bordering Mexico is cooler in summer than most areas in the contiguous United States. Just a few miles inland, summer temperature extremes are significantly higher, with downtown Los Angeles being several degrees warmer than at the coast. The same microclimate phenomenon is seen in the climate of the Bay Area, where areas sheltered from the sea experience significantly hotter summers than nearby areas that are close to the ocean.
Northern parts of the state have more rain than the south. California's mountain ranges also influence the climate: some of the rainiest parts of the state are west-facing mountain slopes. Northwestern California has a temperate climate, and the Central Valley has a Mediterranean climate but with greater temperature extremes than the coast. The high mountains, including the Sierra Nevada, have an alpine climate with snow in winter and mild to moderate heat in summer.
California's mountains produce rain shadows on the eastern side, creating extensive deserts. The higher elevation deserts of eastern California have hot summers and cold winters, while the low deserts east of the Southern California mountains have hot summers and nearly frostless mild winters. Death Valley, a desert with large expanses below sea level, is considered the hottest location in the world; the highest temperature in the world, 134 °F (56.7 °C), was recorded there on July 10, 1913. The lowest temperature in California was −45 °F (−43 °C) in 1937 in Boca.
The table below lists average temperatures for August and December in a selection of places throughout the state; some highly populated and some not. This includes the relatively cool summers of the Humboldt Bay region around Eureka, the extreme heat of Death Valley, and the mountain climate of Mammoth in the Sierra Nevadas.

California is one of the richest and most diverse parts of the world, and includes some of the most endangered ecological communities. California is part of the Nearctic ecozone and spans a number of terrestrial ecoregions.
California's large number of endemic species includes relict species, which have died out elsewhere, such as the Catalina ironwood (Lyonothamnus floribundus). Many other endemics originated through differentiation or adaptive radiation, whereby multiple species develop from a common ancestor to take advantage of diverse ecological conditions such as the California lilac (Ceanothus). Many California endemics have become endangered, as urbanization, logging, overgrazing, and the introduction of exotic species have encroached on their habitat.

California boasts several superlatives in its collection of flora: the largest trees, the tallest trees, and the oldest trees. California's native grasses are perennial plants. After European contact, these were generally replaced by invasive species of European annual grasses; and, in modern times, California's hills turn a characteristic golden-brown in summer.
Because California has the greatest diversity of climate and terrain, the state has six life zones which are the lower Sonoran (desert); upper Sonoran (foothill regions and some coastal lands), transition (coastal areas and moist northeastern counties); and the Canadian, Hudsonian, and Arctic Zones, comprising the state's highest elevations.
Plant life in the dry climate of the lower Sonoran zone contains a diversity of native cactus, mesquite, and paloverde. The Joshua tree is found in the Mojave Desert. Flowering plants include the dwarf desert poppy and a variety of asters. Fremont cottonwood and valley oak thrive in the Central Valley. The upper Sonoran zone includes the chaparral belt, characterized by forests of small shrubs, stunted trees, and herbaceous plants. Nemophila, mint, Phacelia, Viola, and the California poppy (Eschscholzia californica) – the state flower – also flourish in this zone, along with the lupine, more species of which occur here than anywhere else in the world.
The transition zone includes most of California's forests with the redwood (Sequoia sempervirens) and the "big tree" or giant sequoia (Sequoiadendron giganteum), among the oldest living things on earth (some are said to have lived at least 4,000 years). Tanbark oak, California laurel, sugar pine, madrona, broad-leaved maple, and Douglas-fir also grow here. Forest floors are covered with swordfern, alumnroot, barrenwort, and trillium, and there are thickets of huckleberry, azalea, elder, and wild currant. Characteristic wild flowers include varieties of mariposa, tulip, and tiger and leopard lilies.

The high elevations of the Canadian zone allow the Jeffrey pine, red fir, and lodgepole pine to thrive. Brushy areas are abundant with dwarf manzanita and ceanothus; the unique Sierra puffball is also found here. Right below the timberline, in the Hudsonian zone, the whitebark, foxtail, and silver pines grow. At about 10,500 feet (3,200 m), begins the Arctic zone, a treeless region whose flora include a number of wildflowers, including Sierra primrose, yellow columbine, alpine buttercup, and alpine shooting star.
Common plants that have been introduced to the state include the eucalyptus, acacia, pepper tree, geranium, and Scotch broom. The species that are federally classified as endangered are the Contra Costa wallflower, Antioch Dunes evening primrose, Solano grass, San Clemente Island larkspur, salt marsh bird's beak, McDonald's rock-cress, and Santa Barbara Island liveforever. As of December 1997, 85 plant species were listed as threatened or endangered.
In the deserts of the lower Sonoran zone, the mammals include the jackrabbit, kangaroo rat, squirrel, and opossum. Common birds include the owl, roadrunner, cactus wren, and various species of hawk. The area's reptilian life include the sidewinder viper, desert tortoise, and horned toad. The upper Sonoran zone boasts mammals such as the antelope, brown-footed woodrat, and ring-tailed cat. Birds unique to this zone are the California thrasher, bushtit, and California condor.
In the transition zone, there are Colombian black-tailed deer, black bears, gray foxes, cougars, bobcats, and Roosevelt elk. Reptiles such as the garter snakes and rattlesnakes inhabit the zone. In addition, amphibians such as the water puppy and redwood salamander are common too. Birds such as the kingfisher, chickadee, towhee, and hummingbird thrive here as well.
The Canadian zone mammals include the mountain weasel, snowshoe hare, and several species of chipmunks. Conspicuous birds include the blue-fronted jay, Sierra chickadee. Sierra hermit thrush, water ouzel, and Townsend's solitaire. As one ascends into the Hudsonian zone, birds become scarcer. While the Sierra rosy finch is the only bird native to the high Arctic region, other bird species such as the hummingbird and Clark's nutcracker. Principal mammals found in this region include the Sierra coney, white-tailed jackrabbit, and the bighorn sheep. As of April 2003, the bighorn sheep was listed as endangered by the US Fish and Wildlife Service. The fauna found throughout several zones are the mule deer, coyote, mountain lion, northern flicker, and several species of hawk and sparrow.
Aquatic life in California thrives, from the state's mountain lakes and streams to the rocky Pacific coastline. Numerous trout species are found, among them rainbow, golden, and cutthroat. Migratory species of salmon are common as well. Deep-sea life forms include sea bass, yellowfin tuna, barracuda, and several types of whale. Native to the cliffs of northern California are seals, sea lions, and many types of shorebirds, including migratory species.
As of April 2003, 118 California animals were on the federal endangered list; 181 plants were listed as endangered or threatened. Endangered animals include the San Joaquin kitfox, Point Arena mountain beaver, Pacific pocket mouse, salt marsh harvest mouse, Morro Bay kangaroo rat (and five other species of kangaroo rat), Amargosa vole, California least tern, California condor, loggerhead shrike, San Clemente sage sparrow, San Francisco garter snake, five species of salamander, three species of chub, and two species of pupfish. Eleven butterflies are also endangered and two that are threatened are on the federal list. Among threatened animals are the coastal California gnatcatcher, Paiute cutthroat trout, southern sea otter, and northern spotted owl. California has a total of 290,821 acres (1,176.91 km2) of National Wildlife Refuges. As of September 2010, 123 California animals were listed as either endangered or threatened on the federal list provided by the US Fish & Wildlife Service. Also, as of the same year, 178 species of California plants were listed either as endangered or threatened on this federal list.

The vast majority of rivers in California are dammed as part of two massive water projects: the Central Valley Project, providing water to the agricultural central valley, the California State Water Project diverting water from northern to southern California. The state's coasts, rivers, and other bodies of water are regulated by the California Coastal Commission.
The two most prominent rivers within California are the Sacramento River and the San Joaquin River, which drain the Central Valley and the west slope of the Sierra Nevada and flow to the Pacific Ocean through San Francisco Bay. Several major tributaries feed into the Sacramento and the San Joaquin, including the Pit River, the Tuolumne River, and the Feather River.
The Eel River and Salinas River each drain portions of the California coast, north and south of San Francisco Bay, respectively, and the Eel River is the largest river in the state to remain in its natural un-dammed state. The Mojave River is the primary watercourse in the Mojave Desert, and the Santa Ana River drains much of the Transverse Ranges as it bisects Southern California. Some other important rivers are the Klamath River and the Trinity River in the far north coast, and the Colorado River on the southeast border with Arizona.

The United States Census Bureau estimates that the population of California was 39,144,818 on July 1, 2015, a 5.08% increase since the 2010 United States Census. Between 2000 and 2009, there was a natural increase of 3,090,016 (5,058,440 births minus 2,179,958 deaths). During this time period, international migration produced a net increase of 1,816,633 people while domestic migration produced a net decrease of 1,509,708, resulting in a net in-migration of 306,925 people. The state of California's own statistics show a population of 38,292,687 for January 1, 2009. However, according to the Manhattan Institute for Policy Research, since 1990 almost 3.4 million Californians have moved to other states, with most leaving to Texas, Nevada, and Arizona.
California is the 2nd-most populous subnational entity in the Western Hemisphere and the Americas, with a population second to that of the state of São Paulo in Brazil. California's population is greater than that of all but 34 countries of the world. The Greater Los Angeles Area is the 2nd-largest metropolitan area in the United States, after the New York metropolitan area, while Los Angeles, with nearly half the population of New York, is the 2nd-largest city in the United States. Also, Los Angeles County has held the title of most populous United States county for decades, and it alone is more populous than 42 United States states. Including Los Angeles, four of the top 15 most populous cities in the U.S. are in California: Los Angeles (2nd), San Diego (8th), San Jose (10th), and San Francisco (13th). The center of population of California is located in the town of Buttonwillow, Kern County.

The state has 482 incorporated cities and towns; of which 460 are cities and 22 are towns. Under California law, the terms "city" and "town" are explicitly interchangeable; the name of an incorporated municipality in the state can either be "City of (Name)" or "Town of (Name)".
Sacramento became California's first incorporated city on February 27, 1850. San Jose, San Diego and Benicia tied for California's second incorporated city, each receiving incorporation on March 27, 1850. Jurupa Valley became the state's most recent and 482nd incorporated municipality on July 1, 2011.
The majority of these cities and towns are within one of five metropolitan areas: the Los Angeles Metropolitan Area, the San Francisco Bay Area, the Riverside-San Bernardino Area, the San Diego metropolitan area and the Sacramento metropolitan area.

Starting in the year 2010, for the first time since the California Gold Rush, California-born residents make up the majority of the state's population. Along with the rest of the United States, California's immigration pattern has also shifted over the course of the late 2000s-early 2010s. Immigration from Latin American countries has dropped significantly with most immigrants now coming from Asia. In total for 2011, there were 277,304 immigrants. 57% came from Asian countries vs. 22% from Latin American countries. Net immigration from Mexico, previously the most common country of origin for new immigrants has dropped to zero/less than zero, since more Mexican nationals are departing for their home country than immigrating. As a result it is estimated that Hispanic citizens will constitute 49% of the population by 2060, instead of the previously projected 2050, due primarily to domestic births.
The state's population of undocumented immigrants has been shrinking in recent years, due to increased enforcement and decreased job opportunities for lower-skilled workers. The number of migrants arrested attempting to cross the Mexican border in the Southwest plunged from a high of 1.1 million in 2005 to just 367,000 in 2011. Despite these recent trends, illegal aliens constituted an estimated 7.3 percent of the state's population, the third highest percentage of any state in the country, totaling nearly 2.6 million. In particular, illegal immigrants tended to be concentrated in Los Angeles, Monterey, San Benito, Imperial, and Napa Counties – the latter four of which have significant agricultural industries that depend on manual labor. More than half of illegal immigrants originate from Mexico.

California is considered generally liberal in its policies regarding the LGBT community, and the rights of lesbian, gay, bisexual, and transgender people have received greater recognition since 1960 at both the state and municipal level. California is home to a number of gay villages such as the Castro District in San Francisco, Hillcrest in San Diego, and West Hollywood. Through the Domestic Partnership Act of 1999, California became the first state in the United States to recognize same-sex relationships in any legal capacity. In 2000, voters passed Proposition 22, which restricted state recognition of marriage to opposite-sex couples. This was struck down by the California Supreme Court in May 2008, effectively legalizing same-sex marriage; however, this was overruled later that same year when California voters passed Proposition 8. After further judicial cases, in 2013 the U.S. Supreme Court rendered the law void, allowing same-sex marriages in California to resume.

According to the United States Census Bureau in 2015 the population self-identifies as (alone or in combination):
72.9% White
14.7% Asian
6.5% Black or African American
3.8% Two or More Races
1.7% Native American and Alaska Native
0.5% Native Hawaiian or Pacific Islander
By ethnicity, in 2015 the population was 61.2% non-Hispanic (of any race) and 38.8% Hispanic or Latino (of any race).
As of 2011, 75.1% of California's population younger than age 1 were minorities, meaning that they had at least one parent who was not non-Hispanic white (white Hispanics are counted as minorities).
In terms of total numbers, California has the largest population of White Americans in the United States, an estimated 22,200,000 residents. The state has the 5th largest population of African Americans in the United States, an estimated 2,250,000 residents. California's Asian American population is estimated at 4.4 million, constituting a third of the nation's total. California's Native American population of 285,000 is the most of any state.
According to estimates from 2011, California has the largest minority population in the United States by numbers, making up 60% of the state population. Over the past 25 years, the population of non-Hispanic whites has declined, while Hispanic and Asian populations have grown. Between 1970 and 2011, non-Hispanic whites declined from 80% of the State's population to 40%, while Hispanics grew from 32% in 2000 to 38% in 2011. It is currently projected that Hispanics will rise to 49% of the population by 2060, primarily due to domestic births rather than immigration. With the decline of immigration from Latin America, Asian Americans now constitute the fastest growing racial/ethnic group in California; this growth primarily driven by immigration from China, India and the Philippines, respectively.

English serves as California's de jure and de facto official language. In 2010, the Modern Language Association of America estimated that 57.02% (19,429,309) of California residents age 5 and older spoke only English at home, while 42.98% spoke another primary language at home. According to the 2007 American Community Survey, 73% of people who speak a language other than English at home are able to speak English well or very well, with 9.8% not speaking English at all. Unlike most United States States, California law enshrines English as its official language (rather than it being simply the most commonly used), since the passage of Proposition 63 by California voters. Various government agencies do, and are often required to, furnish documents in the various languages needed to reach their intended audiences.
In total, 16 languages other than English were spoken as primary languages at home by more than 100,000 persons, more than any other state in the nation. New York State, in second place, had 9 languages other than English spoken by more than 100,000 persons. The most common language spoken besides English was Spanish, spoken by 28.46% (9,696,638) of the population. With Asia contributing most of California's new immigrants, California had the highest concentration nationwide of Vietnamese and Chinese speakers, the second highest concentration of Korean, and the third highest concentration of Tagalog speakers.
California has historically been one of the most linguistically diverse areas in the world, with more than 70 indigenous languages derived from 64 root languages in 6 language families. A survey conducted between 2007 and 2009 identified 23 different indigenous languages of Mexico that are spoken among California farmworkers. All of California's indigenous languages are endangered, although there are now efforts toward language revitalization.
As a result of the state's increasing diversity and migration from other areas across the country and around the globe, linguists began noticing a noteworthy set of emerging characteristics of spoken English in California since the late 20th century. This dialect, known as California English, has a vowel shift and several other phonological processes that are different from the dialects used in other regions of the country.

The culture of California is a Western culture and most clearly has its modern roots in the culture of the United States, but also, historically, many Hispanic influences. As a border and coastal state, Californian culture has been greatly influenced by several large immigrant populations, especially those from Latin America and Asia.
California has long been a subject of interest in the public mind and has often been promoted by its boosters as a kind of paradise. In the early 20th century, fueled by the efforts of state and local boosters, many Americans saw the Golden State as an ideal resort destination, sunny and dry all year round with easy access to the ocean and mountains. In the 1960s, popular music groups such as The Beach Boys promoted the image of Californians as laid-back, tanned beach-goers.
The California Gold Rush of the 1850s is still seen as a symbol of California's economic style, which tends to generate technology, social, entertainment, and economic fads and booms and related busts.

The largest religious denominations by number of adherents as a percentage of California's population in 2014 were the Catholic Church with 28 percent, Evangelical Protestants with 20 percent, and Mainline Protestants with 10 percent. Together, all kinds of Protestants accounted for 32 percent. Those unaffiliated with any religion represented 27 percent of the population. The breakdown of other religions is 1% Muslim, 2% Hindu and 2% Buddhist. This is a change from 2008, when the population identified their religion with the Catholic Church with 31 percent; Evangelical Protestants with 18 percent; and Mainline Protestants with 14 percent. In 2008, those unaffiliated with any religion represented 21 percent of the population. The breakdown of other religions in 2008 was 0.5% Muslim, 1% Hindu and 2% Buddhist. The American Jewish Year Book placed the total Jewish population of California at about 1,194,190 in 2006. According to the Association of Religion Data Archives (ARDA) the largest denominations by adherents in 2010 were the Roman Catholic Church with 10,233,334; The Church of Jesus Christ of Latter-day Saints with 763,818; and the Southern Baptist Convention with 489,953.
The first priests to come to California were Roman Catholic missionaries from Spain. Roman Catholics founded 21 missions along the California coast, as well as the cities of Los Angeles and San Francisco. California continues to have a large Roman Catholic population due to the large numbers of Mexicans and Central Americans living within its borders. California has twelve dioceses and two archdioceses, the Archdiocese of Los Angeles and the Archdiocese of San Francisco, the former being the largest archdiocese in the United States.
A Pew Research Center survey revealed that California is somewhat less religious than the rest of the US: 62 percent of Californians say they are "absolutely certain" of their belief in God, while in the nation 71 percent say so. The survey also revealed 48 percent of Californians say religion is "very important", compared to 56 percent nationally.

California has twenty major professional sports league franchises, far more than any other state. The San Francisco Bay Area has seven major league teams spread in its three major cities: San Francisco, San Jose, and Oakland. While the Greater Los Angeles Area is home to ten major league franchises. San Diego has two major league teams, and Sacramento has one. The NFL Super Bowl has been hosted in California 11 times at four different stadiums: Los Angeles Memorial Coliseum, the Rose Bowl, Stanford Stadium, and San Diego's Qualcomm Stadium. A twelfth, Super Bowl 50, was held at Levi's Stadium in Santa Clara on February 7, 2016.
California has long had many respected collegiate sports programs. California is home to the oldest college bowl game, the annual Rose Bowl, among others.
California is the only US state to have hosted both the Summer and Winter Olympics. The 1932 and 1984 Summer Olympics were held in Los Angeles. Squaw Valley Ski Resort in the Lake Tahoe region hosted the 1960 Winter Olympics. Multiple games during the 1994 FIFA World Cup took place in California, with the Rose Bowl hosting eight matches including the final, while Stanford Stadium hosted six matches.

Below is a list of major league sports teams in California:

Public secondary education consists of high schools that teach elective courses in trades, languages, and liberal arts with tracks for gifted, college-bound and industrial arts students. California's public educational system is supported by a unique constitutional amendment that requires a minimum annual funding level for grades K–12 and community colleges that grows with the economy and student enrollment figures.
California had over 6.2 million school students in the 2005–06 school year. Funding and staffing levels in California schools lag behind other states. In expenditure per pupil, California ranked 29th (of the 50 states and the District of Columbia) in 2005–06. In teaching staff expenditure per pupil, California ranked 49th of 51. In overall teacher-pupil ratio, California was also 49th, with 21 students per teacher. Only Arizona and Utah were lower.
A 2007 study concluded that California's public school system was "broken" in that it suffered from over-regulation.
California's public postsecondary education offers three separate systems:
The research university system in the state is the University of California (UC), a public university system. As of fall 2011, the University of California had a combined student body of 234,464 students. There are ten general UC campuses, and a number of specialized campuses in the UC system. The system was originally intended to accept the top one-eighth of California high school students, but several of the schools have become even more selective. The UC system was originally given exclusive authority in awarding Ph.Ds, but this has since changed and the CSU is also able to award several Doctoral degrees.
The California State University (CSU) system has almost 430,000 students, making it the largest university system in the United States. The CSU was originally intended to accept the top one-third of California high school students, but several of the schools have become much more selective. The CSU was originally set up to award only bachelor's and master's degrees, but has since been granted the authority to award several Doctoral degrees.
The California Community Colleges System provides lower division coursework as well as basic skills and workforce training. It is the largest network of higher education in the US, composed of 112 colleges serving a student population of over 2.6 million.
California is also home to such notable private universities as Stanford University, the University of Southern California, the California Institute of Technology, and the Claremont Colleges. California has hundreds of other private colleges and universities, including many religious and special-purpose institutions.

The economy of California is large enough to be comparable to that of the largest of countries. As of 2016, the gross state product (GSP) is about $2.514 trillion, the largest in the United States. California is responsible for 13.9 percent of the United States' approximate $18.1 trillion gross domestic product (GDP). California's GSP is larger than the GDP of all but 5 countries in dollar terms (the United States, China, Japan, Germany, and the United Kingdom), larger than Brazil, France, Russia, Italy, India, Canada, Australia, Spain and Turkey. In Purchasing Power Parity, it is larger than all but 10 countries (the United States, China, India, Japan, Germany, Russia, Brazil, France, the United Kingdom, and Indonesia), larger than Italy, Mexico, Spain, South Korea, Saudi Arabia, Canada and Turkey.
The five largest sectors of employment in California are trade, transportation, and utilities; government; professional and business services; education and health services; and leisure and hospitality. In output, the five largest sectors are financial services, followed by trade, transportation, and utilities; education and health services; government; and manufacturing. As of September 2016, California has an unemployment rate of 5.5%.
California's economy is dependent on trade and international related commerce accounts for about one-quarter of the state's economy. In 2008, California exported $144 billion worth of goods, up from $134 billion in 2007 and $127 billion in 2006. Computers and electronic products are California's top export, accounting for 42 percent of all the state's exports in 2008.
Agriculture is an important sector in California's economy. Farming-related sales more than quadrupled over the past three decades, from $7.3 billion in 1974 to nearly $31 billion in 2004. This increase has occurred despite a 15 percent decline in acreage devoted to farming during the period, and water supply suffering from chronic instability. Factors contributing to the growth in sales-per-acre include more intensive use of active farmlands and technological improvements in crop production. In 2008, California's 81,500 farms and ranches generated $36.2 billion products revenue. In 2011, that number grew to $43.5 billion products revenue. The Agriculture sector accounts for two percent of the state's GDP and employs around three percent of its total workforce. According to the USDA in 2011, the three largest California agricultural products by value were milk and cream, shelled almonds, and grapes.
Per capita GDP in 2007 was $38,956, ranking eleventh in the nation. Per capita income varies widely by geographic region and profession. The Central Valley is the most impoverished, with migrant farm workers making less than minimum wage. According to a 2005 report by the Congressional Research Service, the San Joaquin Valley was characterized as one of the most economically depressed regions in the United States, on par with the region of Appalachia. California has a poverty rate of 23.5%, the highest of any state in the country. Many coastal cities include some of the wealthiest per-capita areas in the United States The high-technology sectors in Northern California, specifically Silicon Valley, in Santa Clara and San Mateo counties, have emerged from the economic downturn caused by the dot-com bust.

In 2010, there were more than 663,000 millionaires in the state, more than any other state in the nation. In 2010, California residents were ranked first among the states with the best average credit score of 754.

State spending increased from $56 billion in 1998 to $127 billion in 2011. California, with 12% of the United States population, has one-third of the nation's welfare recipients. California has the third highest per capita spending on welfare among the states, as well as the highest spending on welfare at $6.67 billion. In January 2011 the California's total debt was at least $265 billion. On June 27, 2013, Governor Jerry Brown signed a balanced budget (no deficit) for the state, its first in decades; however the state's debt remains at $132 billion.
With the passage of Proposition 30 in 2012, California now levies a 13.3% maximum marginal income tax rate with ten tax brackets, ranging from 1% at the bottom tax bracket of $0 annual individual income to 13.3% for annual individual income over $1,000,000. California has a state sales tax of 7.5%, though local governments can and do levy additional sales taxes. Many of these taxes are temporary for a seven-year period (as stipulated in Proposition 30) and afterwards will revert to a previous maximum marginal income tax bracket of 10.3% and state sales tax rate of 7.25%.
All real property is taxable annually; the tax is based on the property's fair market value at the time of purchase or new construction. Property tax increases are capped at 2% per year (see Proposition 13).

Because it is the most populous United States state, California is one of the country's largest users of energy. However because of its high energy rates, conservation mandates, mild weather in the largest population centers and strong environmental movement, its per capita energy use is one of the smallest of any United States state. Due to the high electricity demand, California imports more electricity than any other state, primarily hydroelectric power from states in the Pacific Northwest (via Path 15 and Path 66) and coal- and natural gas-fired production from the desert Southwest via Path 46.
As a result of the state's strong environmental movement, California has some of the most aggressive renewable energy goals in the United States, with a target for California to obtain a third of its electricity from renewables by 2020. Currently, several solar power plants such as the Solar Energy Generating Systems facility are located in the Mojave Desert. California's wind farms include Altamont Pass, San Gorgonio Pass, and Tehachapi Pass. Several dams across the state provide hydro-electric power. It would be possible to convert the total supply to 100% renewable energy, including heating, cooling and mobility, by 2050.
The state's crude oil and natural gas deposits are located in the Central Valley and along the coast, including the large Midway-Sunset Oil Field. Natural gas-fired power plants typically account for more than one-half of state electricity generation.
California is also home to two major nuclear power plants: Diablo Canyon and San Onofre, the latter having been shut down in 2013. Also voters banned the approval of new nuclear power plants since the late 1970s because of concerns over radioactive waste disposal. In addition, several cities such as Oakland, Berkeley and Davis have declared themselves as nuclear-free zones.

California's vast terrain is connected by an extensive system of controlled-access highways ('freeways'), limited-access roads ('expressways'), and highways. California is known for its car culture, giving California's cities a reputation for severe traffic congestion. Construction and maintenance of state roads and statewide transportation planning are primarily the responsibility of the California Department of Transportation, nicknamed "Caltrans". The rapidly growing population of the state is straining all of its transportation networks, and California has some of the worst roads in the United States. The Reason Foundation's 19th Annual Report on the Performance of State Highway Systems ranked California's highways the third-worst of any state, with Alaska second, and Rhode Island first.
The state has been a pioneer in road construction. One of the state's more visible landmarks, the Golden Gate Bridge, was once the longest suspension bridge main span in the world at 4,200 feet (1,300 m) when it opened in 1937. With its orange paint and panoramic views of the bay, this highway bridge is a popular tourist attraction and also accommodates pedestrians and bicyclists. The San Francisco–Oakland Bay Bridge (often abbreviated the "Bay Bridge"), completed in 1936, transports about 280,000 vehicles per day on two-decks. Its two sections meet at Yerba Buena Island through the world's largest diameter transportation bore tunnel, at 76 feet (23 m) wide by 58 feet (18 m) high. The Arroyo Seco Parkway, connecting Los Angeles and Pasadena, opened in 1940 as the first freeway in the Western United States. It was later extended south to the Four Level Interchange in downtown Los Angeles, regarded as the first stack interchange ever built.
Los Angeles International Airport (LAX), the 6th busiest airport in the world, and San Francisco International Airport (SFO), the 21st busiest airport in the world, are major hubs for trans-Pacific and transcontinental traffic. There are about a dozen important commercial airports and many more general aviation airports throughout the state.
California also has several important seaports. The giant seaport complex formed by the Port of Los Angeles and the Port of Long Beach in Southern California is the largest in the country and responsible for handling about a fourth of all container cargo traffic in the United States. The Port of Oakland, fourth largest in the nation, also handles trade entering from the Pacific Rim to the rest of the country. The Port of Stockton is the easternmost port on the west coast of the United States.
The California Highway Patrol is the largest statewide police agency in the United States in employment with over 10,000 employees. They are responsible for providing any police-sanctioned service to anyone on California's state maintained highways and on state property.
The California Department of Motor Vehicles is by far the largest in North America. By the end of 2009, the California DMV had 26,555,006 driver's licenses and ID cards on file. In 2010, there were 1.17 million new vehicle registrations in force.
Intercity rail travel is provided by Amtrak California, which manages the three busiest intercity rail lines in the United States outside the Northeast Corridor, all of which are funded by Caltrans. This service is becoming increasingly popular over flying and ridership is continuing to set records, especially on the LAX-SFO route. Integrated subway and light rail networks are found in Los Angeles (Metro Rail) and San Francisco (MUNI Metro). Light rail systems are also found in San Jose (VTA), San Diego (San Diego Trolley), Sacramento (RT Light Rail), and Northern San Diego County (Sprinter). Furthermore, commuter rail networks serve the San Francisco Bay Area (ACE, BART, Caltrain), Greater Los Angeles (Metrolink), and San Diego County (Coaster).
The California High-Speed Rail Authority was created in 1996 by the state to implement an extensive 700 miles (1,100 km) rail system. Construction was approved by the voters during the November 2008 general election, a $9.95 billion state bond will go toward its construction. Nearly all counties operate bus lines, and many cities operate their own city bus lines as well. Intercity bus travel is provided by Greyhound and Amtrak Thruway Coach.

California's interconnected water system is the world's largest, managing over 40,000,000 acre feet (49 km3) of water per year, centered on six main systems of aqueducts and infrastructure projects. Water use and conservation in California is a politically divisive issue, as the state experiences periodic droughts and has to balance the demands of its large agricultural and urban sectors, especially in the arid southern portion of the state. The state's widespread redistribution of water also invites the frequent scorn of environmentalists.
The California Water Wars, a conflict between Los Angeles and the Owens Valley over water rights, is one of the most well-known examples of the struggle to secure adequate water supplies. Former California Governor Arnold Schwarzenegger said: "We've been in crisis for quite some time because we're now 38 million people and not anymore 18 million people like we were in the late 60s. So it developed into a battle between environmentalists and farmers and between the south and the north and between rural and urban. And everyone has been fighting for the last four decades about water."

The state's capital is Sacramento.
California is organized into three branches of government – the executive branch consisting of the Governor and the other independently elected constitutional officers; the legislative branch consisting of the Assembly and Senate; and the judicial branch consisting of the Supreme Court of California and lower courts. The state also allows ballot propositions: direct participation of the electorate by initiative, referendum, recall, and ratification. Before the passage of California Proposition 14 (2010), California allowed each political party to choose whether to have a closed primary or a primary where only party members and independents vote. After June 8, 2010 when Proposition 14 was approved, excepting only the United States President and county central committee offices, all candidates in the primary elections are listed on the ballot with their preferred party affiliation, but they are not the official nominee of that party. At the primary election, the two candidates with the top votes will advance to the general election regardless of party affiliation. If at a special primary election, one candidate receives more than 50% of all the votes cast, they are elected to fill the vacancy and no special general election will be held.
Executive branch
The California executive branch consists of the Governor of California and seven other elected constitutional officers: Lieutenant Governor, Attorney General, Secretary of State, State Controller, State Treasurer, Insurance Commissioner, and State Superintendent of Public Instruction. They serve four-year terms and may be re-elected only once.
Legislative branch
The California State Legislature consists of a 40-member Senate and 80-member Assembly. Senators serve four-year terms and Assembly members two. Members of the Assembly are subject to term limits of three terms, and members of the Senate are subject to term limits of two terms.
Judicial branch
California's legal system is explicitly based upon English common law (as is the case with all other states except Louisiana) but carries a few features from Spanish civil law, such as community property. California's prison population grew from 25,000 in 1980 to over 170,000 in 2007. Capital punishment is a legal form of punishment and the state has the largest "Death Row" population in the country (though Texas is far more active in carrying out executions).
California's judiciary system is the largest in the United States (with a total of 1,600 judges, while the federal system has only about 840). At the apex is the seven Justices of the Supreme Court of California, while the California Courts of Appeal serve as the primary appellate courts and the California Superior Courts serve as the primary trial courts. Justices of the Supreme Court and Courts of Appeal are appointed by the Governor, but are subject to retention by the electorate every 12 years. The administration of the state's court system is controlled by the Judicial Council, composed of the Chief Justice of the California Supreme Court, 14 judicial officers, four representatives from the State Bar of California, and one member from each house of the state legislature.

California is divided into 58 counties. Per Article 11, Section 1, of the Constitution of California, they are the legal subdivisions of the state. The county government provides countywide services such as law enforcement, jails, elections and voter registration, vital records, property assessment and records, tax collection, public health, health care, social services, libraries, flood control, fire protection, animal control, agricultural regulations, building inspections, ambulance services, and education departments in charge of maintaining statewide standards. In addition, the county serves as the local government for all unincorporated areas. Each county is governed by an elected board of supervisors.

Incorporated cities and towns in California are either charter or general-law municipalities. General-law municipalities owe their existence to state law and are consequently governed by it; charter municipalities are governed by their own city or town charters. Municipalities incorporated in the 19th century tend to be charter municipalities. All ten of the state's most populous cities are charter cities. Most small cities have a council-manager form of government, where the elected city council appoints a city manager to supervise the operations of the city. Some larger cities have a directly-elected mayor who oversees the city government. In many council-manager cities, the city council selects one of its members as a mayor, sometimes rotating through the council membership—but this type of mayoral position is primarily ceremonial.
The Government of San Francisco is the only consolidated city-county in California, where both the city and county governments have been merged into one unified jurisdiction. The San Francisco Board of Supervisors also acts as the city council and the Mayor of San Francisco also serves as the county administrative officer.

About 1,102 school districts, independent of cities and counties, handle California's public education. California school districts may be organized as elementary districts, high school districts, unified school districts combining elementary and high school grades, or community college districts.
There are about 3,400 special districts in California. A special district, defined by California Government Code § 16271(d) as "any agency of the state for the local performance of governmental or proprietary functions within limited boundaries", provides a limited range of services within a defined geographic area. The geographic area of a special district can spread across multiple cities or counties, or could consist of only a portion of one. Most of California's special districts are single-purpose districts, and provide one service.

The state of California sends 53 members to the House of Representatives, the nation's largest congressional state delegation. Consequently California also has the largest number of electoral votes in national presidential elections, with 55. California's U.S. Senators are Dianne Feinstein, a native and former mayor of San Francisco, and Kamala Harris, a native, former District Attorney from San Francisco and former Attorney General of California. In 1992, California became the first state to have a Senate delegation entirely composed of women.

California has an idiosyncratic political culture compared to the rest of the country, and is sometimes regarded as a trendsetter. In socio-cultural mores and national politics, Californians are perceived as more liberal than other Americans, especially those who live in the inland states.
Among the political idiosyncrasies and trendsetting, California was the second state to recall their state governor, the second state to legalize abortion, and the only state to ban marriage for gay couples twice by voters (including Proposition 8 in 2008). Voters also passed Proposition 71 in 2004 to fund stem cell research, and Proposition 14 in 2010 to completely change the state's primary election process. California has also experienced disputes over water rights; and a tax revolt, culminating with the passage of Proposition 13 in 1978, limiting state property taxes.
The state's trend towards the Democratic Party and away from the Republican Party can be seen in state elections. From 1899 to 1939, California had Republican governors. Since 1990, California has generally elected Democratic candidates to federal, state and local offices, including current Governor Jerry Brown; however, the state has elected Republican Governors, though many of its Republican Governors, such as Arnold Schwarzenegger, tend to be considered moderate Republicans and more centrist than the national party.

The Democrats also now hold a majority in both houses of the state legislature. There are 56 Democrats and 24 Republicans in the Assembly; and 26 Democrats and 12 Republicans in the Senate.
The trend towards the Democratic Party is most obvious in presidential elections; Republicans have not won California's electoral votes since 1988.
In the United States House, the Democrats held a 34–19 edge in the CA delegation of the 110th United States Congress in 2007. As the result of gerrymandering, the districts in California were usually dominated by one or the other party, and few districts were considered competitive. In 2008, Californians passed Proposition 20 to empower a 14-member independent citizen commission to redraw districts for both local politicians and Congress. After the 2012 elections, when the new system took effect, Democrats gained 4 seats and held a 38–15 majority in the delegation.
In general, Democratic strength is centered in the populous coastal regions of the Los Angeles metropolitan area and the San Francisco Bay Area. Republican strength is still greatest in eastern parts of the state. Orange County also remains mostly Republican. One study ranked Berkeley, Oakland, Inglewood and San Francisco in the top 20 most liberal American cities; and Bakersfield, Orange, Escondido, Garden Grove, and Simi Valley in the top 20 most conservative cities.
In October 2012, out of the 23,802,577 people eligible to vote, 18,245,970 people were registered to vote. Of the people registered, the three largest registered groups were Democrats (7,966,422), Republicans (5,356,608), and Decline to State (3,820,545). Los Angeles County had the largest number of registered Democrats (2,430,612) and Republicans (1,037,031) of any county in the state.

In California, as of 2009, the U.S. Department of Defense had a total of 117,806 active duty servicemembers of which 88,370 were Sailors or Marines, 18,339 were Airmen, and 11,097 were Soldiers, with 61,365 Department of Defense civilian employees. Additionally, there were a total of 57,792 Reservists and Guardsman in California.
In 2010, Los Angeles County was the largest origin of military recruits in the United States by county, with 1,437 individuals enlisting in the military. However, as of 2002, Californians were relatively under-represented in the military as a proportion to its population.
In 2000, California, had 2,569,340 veterans of United States military service: 504,010 served in World War II, 301,034 in the Korean War, 754,682 during the Vietnam War, and 278,003 during 1990–2000 (including the Persian Gulf War). As of 2010, there were 1,942,775 veterans living in California, of which 1,457,875 served during a period of armed conflict, and just over four thousand served before World War II (the largest population of this group of any state).
California's military forces consist of the Army and Air National Guard, the naval and state military reserve (militia), and the California Cadet Corps.

California has a twinning arrangement with  Catalonia, Spain

Index of California-related articles
Outline of California – organized list of topics about California
Timeline of the far future

Cohen, Saul Bernard (2003). Geopolitics of the World System. Rowman & Littlefield. ISBN 978-0-8476-9907-0.
Starr, Kevin (2007). California: A History. Modern Library Chronicles. 23. Random House Digital, Inc. ISBN 978-0-8129-7753-0.

Chartkoff, Joseph L.; Chartkoff, Kerry Kona (1984). The archaeology of California. Stanford: Stanford University Press. ISBN 0-8047-1157-7. OCLC 11351549.
Fagan, Brian (2003). Before California: An archaeologist looks at our earliest inhabitants. Lanham, MD: Rowman & Littlefield Publishers. ISBN 0-7425-2794-8. OCLC 226025645.
Hart, James D. (1978). A Companion to California. New York, NY: Oxford University Press. ISBN 0-19-502400-1.
Matthews, Glenna. The Golden State in the Civil War: Thomas Starr King, the Republican Party, and the Birth of Modern California. New York: Cambridge University Press, 2012.
Moratto, Michael J.; Fredrickson, David A. (1984). California archaeology. Orlando: Academic Press. ISBN 0-12-506182-X. OCLC 228668979.

State of California
California State Guide, from the Library of Congress
 Geographic data related to California at OpenStreetMap
data.ca.gov: open data portal from California state agencies
California State Facts from USDA
California Drought: Farm and Food Impacts from USDA, Economic Research Service
California at DMOZ
1973 documentary featuring aerial views of the California coastline from Mt. Shasta to Los Angeles
Time-Lapse Tilt-Shift Portrait of California by Ryan and Sheri KillackeyThe Pacific Ocean is the largest of the Earth's oceanic divisions. It extends from the Arctic Ocean in the north to the Southern Ocean (or, depending on definition, to Antarctica) in the south and is bounded by Asia and Australia in the west and the Americas in the east.
At 165.25 million square kilometers (63.8 million square miles) in area, this largest division of the World Ocean—and, in turn, the hydrosphere—covers about 46% of the Earth's water surface and about one-third of its total surface area, making it larger than all of the Earth's land area combined.
The equator subdivides it into the North Pacific Ocean and South Pacific Ocean, with two exceptions: the Galápagos and Gilbert Islands, while straddling the equator, are deemed wholly within the South Pacific. The Mariana Trench in the western North Pacific is the deepest point in the world, reaching a depth of 10,911 metres (35,797 ft).
Both the center of the Water Hemisphere and the Western Hemisphere are in the Pacific Ocean.
Though the peoples of Asia and Oceania have travelled the Pacific Ocean since prehistoric times, the eastern Pacific was first sighted by Europeans in the early 16th century when Spanish explorer Vasco Núñez de Balboa crossed the Isthmus of Panama in 1513 and discovered the great "southern sea" which he named Mar del Sur. The ocean's current name was coined by Portuguese explorer Ferdinand Magellan during the Spanish circumnavigation of the world in 1521, as he encountered favourable winds on reaching the ocean. He called it Mar Pacífico, which in both Portuguese and Spanish means "peaceful sea".

Important human migrations occurred in the Pacific in prehistoric times. About 3000 BC, the Austronesian peoples on the island of Taiwan mastered the art of long-distance canoe travel and spread themselves and their languages south to the Philippines, Indonesia, and maritime Southeast Asia; west towards Madagascar; southeast towards New Guinea and Melanesia (intermarrying with native Papuans); and east to the islands of Micronesia, Oceania and Polynesia.
Long-distance trade developed all along the coast from Mozambique to Japan. Trade, and therefore knowledge, extended to the Indonesian islands but apparently not Australia. By at least 878 when there was a significant Islamic settlement in Canton much of this trade was controlled by Arabs or Muslims. In 219 BC Xu Fu sailed out into the Pacific searching for the elixir of immortality. From 1404 to 1433 Zheng He led expeditions into the Indian Ocean.

The first contact of European navigators with the western edge of the Pacific Ocean was made by the Portuguese expeditions of António de Abreu and Francisco Serrão to the Maluku Islands in 1512, and with Jorge Álvares's expedition to southern China in 1513, both ordered by Afonso de Albuquerque.
The east side of the ocean was discovered by Spanish explorer Vasco Núñez de Balboa in 1513 after his expedition crossed the Isthmus of Panama and reached a new ocean. He named it Mar del Sur (literally, "Sea of the South" or "South Sea") because the ocean was to the south of the coast of the isthmus where he first observed the Pacific.
Later, Portuguese explorer Ferdinand Magellan sailed the Pacific on a Castilian (Spanish) expedition of world circumnavigation starting in 1519. Magellan called the ocean Pacífico (or "Pacific" meaning, "peaceful") because, after sailing through the stormy seas off Cape Horn, the expedition found calm waters. The ocean was often called the Sea of Magellan in his honor until the eighteenth century. Although Magellan himself died in the Philippines in 1521, Spanish Basque navigator Juan Sebastián Elcano led the expedition back to Spain across the Indian Ocean and round the Cape of Good Hope, completing the first world circumnavigation in 1522. Sailing around and east of the Moluccas, between 1525 and 1527, Portuguese expeditions discovered the Caroline Islands and Papua New Guinea. In 1542–43 the Portuguese also reached Japan.
In 1564, five Spanish ships consisting of 379 explorers crossed the ocean from Mexico led by Miguel López de Legazpi and sailed to the Philippines and Mariana Islands. For the remainder of the 16th century, Spanish influence was paramount, with ships sailing from Mexico and Peru across the Pacific Ocean to the Philippines, via Guam, and establishing the Spanish East Indies. The Manila galleons operated for two and a half centuries linking Manila and Acapulco, in one of the longest trade routes in history. Spanish expeditions also discovered Tuvalu, the Marquesas, the Cook Islands, the Solomon Islands, and the Admiralty Islands in the South Pacific.
Later, in the quest for Terra Australis (i.e., "the [great] Southern Land"), Spanish explorers in the 17th century discovered the Pitcairn and Vanuatu archipelagos, and sailed the Torres Strait between Australia and New Guinea, named after navigator Luís Vaz de Torres. Dutch explorers, sailing around southern Africa, also engaged in discovery and trade; Abel Janszoon Tasman discovered Tasmania and New Zealand in 1642.
In the 16th and 17th century Spain considered the Pacific Ocean a Mare clausum—a sea closed to other naval powers. As the only known entrance from the Atlantic the Strait of Magellan was at times patrolled by fleets sent to prevent entrance of non-Spanish ships. On the western end of the Pacific Ocean the Dutch threatened the Spanish Philippines.
The 18th century marked the beginning of major exploration by the Russians in Alaska and the Aleutian Islands. Spain also sent expeditions to the Pacific Northwest reaching Vancouver Island in southern Canada, and Alaska. The French explored and settled Polynesia, and the British made three voyages with James Cook to the South Pacific and Australia, Hawaii, and the North American Pacific Northwest. In 1768, Pierre-Antoine Véron, a young astronomer accompanying Louis Antoine de Bougainville on his voyage of exploration, established the width of the Pacific with precision for the first time in history. One of the earliest voyages of scientific exploration was organized by Spain in the Malaspina Expedition of 1789–1794. It sailed vast areas of the Pacific, from Cape Horn to Alaska, Guam and the Philippines, New Zealand, Australia, and the South Pacific.

Growing imperialism during the 19th century resulted in the occupation of much of Oceania by other European powers, and later, Japan and the United States. Significant contributions to oceanographic knowledge were made by the voyages of HMS Beagle in the 1830s, with Charles Darwin aboard; HMS Challenger during the 1870s; the USS Tuscarora (1873–76); and the German Gazelle (1874–76).

In Oceania, France got a leading position as imperial power after making Tahiti and New Caledonia protectorates in 1842 and 1853 respectively. After navy visits to Easter Island in 1875 and 1887, Chilean navy officer Policarpo Toro managed to negotiate an incorporation of the island into Chile with native Rapanui in 1888. By occupying Easter Island, Chile joined the imperial nations. By 1900 nearly all Pacific islands were in control of Britain, France, United States, Germany, Japan, and Chile.
Although the United States gained control of Guam and the Philippines from Spain in 1898, Japan controlled most of the western Pacific by 1914 and occupied many other islands during World War II. However, by the end of that war, Japan was defeated and the U.S. Pacific Fleet was the virtual master of the ocean. Since the end of World War II, many former colonies in the Pacific have become independent states.

The Pacific separates Asia and Australia from the Americas. It may be further subdivided by the equator into northern (North Pacific) and southern (South Pacific) portions. It extends from the Antarctic region in the South to the Arctic in the north. The Pacific Ocean encompasses approximately one-third of the Earth's surface, having an area of 165.2 million square kilometers (63.8 million square miles)—significantly larger than Earth's entire landmass of some 150 million square kilometers (58 million square miles).
Extending approximately 15,500 km (9,600 mi) from the Bering Sea in the Arctic to the northern extent of the circumpolar Southern Ocean at 60°S (older definitions extend it to Antarctica's Ross Sea), the Pacific reaches its greatest east-west width at about 5°N latitude, where it stretches approximately 19,800 km (12,300 mi) from Indonesia to the coast of Colombia—halfway around the world, and more than five times the diameter of the Moon. The lowest known point on Earth—the Mariana Trench—lies 10,911 m (35,797 ft; 5,966 fathoms) below sea level. Its average depth is 4,280 m (14,040 ft; 2,340 fathoms), putting the total water volume at 710,000,000 cubic kilometers.
Due to the effects of plate tectonics, the Pacific Ocean is currently shrinking by roughly 2.5 centimetres (0.98 in) per year on three sides, roughly averaging 0.52 square kilometres (0.20 sq mi) a year. By contrast, the Atlantic Ocean is increasing in size.
Along the Pacific Ocean's irregular western margins lie many seas, the largest of which are the Celebes Sea, Coral Sea, East China Sea, Philippine Sea, Sea of Japan, South China Sea, Sulu Sea, Tasman Sea, and Yellow Sea. The Indonesian Seaway (including the Strait of Malacca and Torres Strait) joins the Pacific and the Indian Ocean to the west, and Drake Passage and the Strait of Magellan link the Pacific with the Atlantic Ocean on the east. To the north, the Bering Strait connects the Pacific with the Arctic Ocean.

As the Pacific straddles the 180th meridian, the West Pacific (or western Pacific, near Asia) is in the Eastern Hemisphere, while the East Pacific (or eastern Pacific, near the Americas) is in the Western Hemisphere.
The Southern Pacific Ocean harbors the Southeast Indian Ridge crossing from south of Australia turning into the Pacific-Antarctic Ridge (north of the South Pole) and merges with another ridge (south of South American) to form the East Pacific Rise which also connects with another ridge (south of North America) which overlooks the Juan de Fuca Ridge.
For most of Magellan's voyage from the Strait of Magellan to the Philippines, the explorer indeed found the ocean peaceful. However, the Pacific is not always peaceful. Many tropical storms batter the islands of the Pacific. The lands around the Pacific Rim are full of volcanoes and often affected by earthquakes. Tsunamis, caused by underwater earthquakes, have devastated many islands and in some cases destroyed entire towns.
The Martin Waldseemüller map of 1507 was the first to show the Americas separating two distinct oceans. Later, the Diogo Ribeiro map of 1529 was the first to show the Pacific at about its proper size.

1 The status of Taiwan and China is disputed. For more information, see political status of Taiwan.

The islands entirely within the Pacific Ocean can be divided into three main groups known as Micronesia, Melanesia and Polynesia. Micronesia, which lies north of the equator and west of the International Date Line, includes the Mariana Islands in the northwest, the Caroline Islands in the center, the Marshall Islands to the west and the islands of Kiribati in the southwest.
Melanesia, to the southwest, includes New Guinea, the world's second largest island after Greenland and by far the largest of the Pacific islands. The other main Melanesian groups from north to south are the Bismarck Archipelago, the Solomon Islands, Santa Cruz, Vanuatu, Fiji and New Caledonia.
The largest area, Polynesia, stretching from Hawaii in the north to New Zealand in the south, also encompasses Tuvalu, Tokelau, Samoa, Tonga and the Kermadec Islands to the west, the Cook Islands, Society Islands and Austral Islands in the center, and the Marquesas Islands, Tuamotu, Mangareva Islands, and Easter Island to the east.
Islands in the Pacific Ocean are of four basic types: continental islands, high islands, coral reefs and uplifted coral platforms. Continental islands lie outside the andesite line and include New Guinea, the islands of New Zealand, and the Philippines. Some of these islands are structurally associated with nearby continents. High islands are of volcanic origin, and many contain active volcanoes. Among these are Bougainville, Hawaii, and the Solomon Islands.
The coral reefs of the South Pacific are low-lying structures that have built up on basaltic lava flows under the ocean's surface. One of the most dramatic is the Great Barrier Reef off northeastern Australia with chains of reef patches. A second island type formed of coral is the uplifted coral platform, which is usually slightly larger than the low coral islands. Examples include Banaba (formerly Ocean Island) and Makatea in the Tuamotu group of French Polynesia.

The volume of the Pacific Ocean, representing about 50.1 percent of the world's oceanic water, has been estimated at some 714 million cubic kilometres (171 million cubic miles). Surface water temperatures in the Pacific can vary from −1.4 °C (29.5 °F), the freezing point of sea water, in the poleward areas to about 30 °C (86 °F) near the equator. Salinity also varies latitudinally, reaching a maximum of 37 parts per thousand in the southeastern area. The water near the equator, which can have a salinity as low as 34 parts per thousand, is less salty than that found in the mid-latitudes because of abundant equatorial precipitation throughout the year. The lowest counts of less than 32 parts per thousand are found in the far north as less evaporation of seawater takes place in these frigid areas. The motion of Pacific waters is generally clockwise in the Northern Hemisphere (the North Pacific gyre) and counter-clockwise in the Southern Hemisphere. The North Equatorial Current, driven westward along latitude 15°N by the trade winds, turns north near the Philippines to become the warm Japan or Kuroshio Current.
Turning eastward at about 45°N, the Kuroshio forks and some water moves northward as the Aleutian Current, while the rest turns southward to rejoin the North Equatorial Current. The Aleutian Current branches as it approaches North America and forms the base of a counter-clockwise circulation in the Bering Sea. Its southern arm becomes the chilled slow, south-flowing California Current. The South Equatorial Current, flowing west along the equator, swings southward east of New Guinea, turns east at about 50°S, and joins the main westerly circulation of the South Pacific, which includes the Earth-circling Antarctic Circumpolar Current. As it approaches the Chilean coast, the South Equatorial Current divides; one branch flows around Cape Horn and the other turns north to form the Peru or Humboldt Current.

The climate patterns of the Northern and Southern Hemispheres generally mirror each other. The trade winds in the southern and eastern Pacific are remarkably steady while conditions in the North Pacific are far more varied with, for example, cold winter temperatures on the east coast of Russia contrasting with the milder weather off British Columbia during the winter months due to the preferred flow of ocean currents.
In the tropical and subtropical Pacific, the El Niño Southern Oscillation (ENSO) affects weather conditions. To determine the phase of ENSO, the most recent three-month sea surface temperature average for the area approximately 3,000 kilometres (1,900 mi) to the southeast of Hawaii is computed, and if the region is more than 0.5 °C (0.9 °F) above or below normal for that period, then an El Niño or La Niña is considered in progress.
In the tropical western Pacific, the monsoon and the related wet season during the summer months contrast with dry winds in the winter which blow over the ocean from the Asian landmass. Worldwide, tropical cyclone activity peaks in late summer, when the difference between temperatures aloft and sea surface temperatures is the greatest. However, each particular basin has its own seasonal patterns. On a worldwide scale, May is the least active month, while September is the most active month. November is the only month in which all the tropical cyclone basins are active. The Pacific hosts the two most active tropical cyclone basins, which are the northwestern Pacific and the eastern Pacific. Pacific hurricanes form south of Mexico, sometimes striking the western Mexican coast and occasionally the southwestern United States between June and October, while typhoons forming in the northwestern Pacific moving into southeast and east Asia from May to December. Tropical cyclones also form in the South Pacific basin, where they occasionally impact island nations.
In the arctic, icing from October to May can present a hazard for shipping while persistent fog occurs from June to December. A climatological low in the Gulf of Alaska keeps the southern coast wet and mild during the winter months. The Westerlies and associated jet stream within the Mid-Latitudes can be particularly strong, especially in the Southern Hemisphere, due to the temperature difference between the tropics and Antarctica, which records the coldest temperature readings on the planet. In the Southern hemisphere, because of the stormy and cloudy conditions associated with extratropical cyclones riding the jet stream, it is usual to refer to the Westerlies as the Roaring Forties, Furious Fifties and Shrieking Sixties according to the varying degrees of latitude.

The ocean was first mapped by Abraham Ortelius; he called it Maris Pacifici following Ferdinand Magellan's description of it as "a pacific sea" during his circumnavigation from 1519 to 1522. To Magellan, it seemed much more calm (pacific) than the Atlantic.
The andesite line is the most significant regional distinction in the Pacific. A petrologic boundary, it separates the deeper, mafic igneous rock of the Central Pacific Basin from the partially submerged continental areas of felsic igneous rock on its margins. The andesite line follows the western edge of the islands off California and passes south of the Aleutian arc, along the eastern edge of the Kamchatka Peninsula, the Kuril Islands, Japan, the Mariana Islands, the Solomon Islands, and New Zealand's North Island.
The dissimilarity continues northeastward along the western edge of the Andes Cordillera along South America to Mexico, returning then to the islands off California. Indonesia, the Philippines, Japan, New Guinea, and New Zealand lie outside the andesite line.
Within the closed loop of the andesite line are most of the deep troughs, submerged volcanic mountains, and oceanic volcanic islands that characterize the Pacific basin. Here basaltic lavas gently flow out of rifts to build huge dome-shaped volcanic mountains whose eroded summits form island arcs, chains, and clusters. Outside the andesite line, volcanism is of the explosive type, and the Pacific Ring of Fire is the world's foremost belt of explosive volcanism. The Ring of Fire is named after the several hundred active volcanoes that sit above the various subduction zones.
The Pacific Ocean is the only ocean which is almost totally bounded by subduction zones. Only the Antarctic and Australian coasts have no nearby subduction zones.

The Pacific Ocean was born 750 million years ago at the breakup of Rodinia, although it is generally called the Panthalassic Ocean until the breakup of Pangea, about 200 million years ago. The oldest Pacific Ocean floor is only around 180 Ma old, with older crust subducted by now.

The Pacific Ocean contains several long seamount chains, formed by hotspot volcanism. These include the Hawaiian–Emperor seamount chain and the Louisville seamount chain.

The exploitation of the Pacific's mineral wealth is hampered by the ocean's great depths. In shallow waters of the continental shelves off the coasts of Australia and New Zealand, petroleum and natural gas are extracted, and pearls are harvested along the coasts of Australia, Japan, Papua New Guinea, Nicaragua, Panama, and the Philippines, although in sharply declining volume in some cases.

Fish are an important economic asset in the Pacific. The shallower shoreline waters of the continents and the more temperate islands yield herring, salmon, sardines, snapper, swordfish, and tuna, as well as shellfish. Overfishing has become a serious problem in some areas. For example, catches in the rich fishing grounds of the Okhotsk Sea off the Russian coast have been reduced by at least half since the 1990s as a result of overfishing.

The quantity of small plastic fragments floating in the north-east Pacific Ocean increased a hundredfold between 1972 and 2012.
Marine pollution is a generic term for the harmful entry into the ocean of chemicals or particles. The main culprits are those using the rivers for disposing of their waste. The rivers then empty into the ocean, often also bringing chemicals used as fertilizers in agriculture. The excess of oxygen-depleting chemicals in the water leads to hypoxia and the creation of a dead zone.
Marine debris, also known as marine litter, is human-created waste that has ended up floating in a lake, sea, ocean, or waterway. Oceanic debris tends to accumulate at the center of gyres and coastlines, frequently washing aground where it is known as beach litter.
In addition, the Pacific Ocean has served as the crash site of satellites, including Mars 96, Fobos-Grunt, and Upper Atmosphere Research Satellite.

EPIC Pacific Ocean Data Collection Viewable on-line collection of observational data
NOAA In-situ Ocean Data Viewer plot and download ocean observations
NOAA PMEL Argo profiling floats Realtime Pacific Ocean data
NOAA TAO El Niño data Realtime Pacific Ocean El Niño buoy data
NOAA Ocean Surface Current Analyses—Realtime (OSCAR) Near-realtime Pacific Ocean Surface Currents derived from satellite altimeter and scatterometer dataThe Atlantic Ocean is the second largest of the world's oceans with a total area of about 106,460,000 square kilometres (41,100,000 sq mi). It covers approximately 20 percent of the Earth's surface and about 29 percent of its water surface area. It separates the "Old World" from the "New World".
The Atlantic Ocean occupies an elongated, S-shaped basin extending longitudinally between Eurasia and Africa to the east, and the Americas to the west. As one component of the interconnected global ocean, it is connected in the north to the Arctic Ocean, to the Pacific Ocean in the southwest, the Indian Ocean in the southeast, and the Southern Ocean in the south (other definitions describe the Atlantic as extending southward to Antarctica). The Equatorial Counter Current subdivides it into the North Atlantic Ocean and South Atlantic Ocean at about 8°N.
Scientific explorations of the Atlantic include the Challenger expedition, the German Meteor expedition, Columbia University's Lamont-Doherty Earth Observatory and the United States Navy Hydrographic Office.

The oldest known mention of "Atlantic" is in The Histories of Herodotus around 450 BC (Hdt. 1.202.4): Atlantis thalassa (Greek: Ἀτλαντὶς θάλασσα; English: Sea of Atlas) where the name refers to "the sea beyond the pillars of Heracles" which is said to be part of the ocean that surrounds all land. Thus, on one hand, the name refers to Atlas, the Titan of Greek mythology, who supported the heavens and who later appeared as a frontispiece in Medieval maps and also lend his name to modern atlases. On the other hand, to early Greek sailors and in Ancient Greek mythological literature such as the Iliad and the Odyssey, this all-encompassing ocean was instead known as Oceanus, the gigantic river that encircled the world; in contrast to the enclosed seas well-known to the Greeks: the Mediterranean and the Black Sea. In contrast, the term "Atlantic" originally referred specifically to the Atlas Mountains in Morocco and the sea off the Strait of Gibraltar and the North African coast. The Greek word thalassa has been reused by scientists for the huge Panthalassa ocean that surrounded the supercontinent Pangaea hundreds of million years ago.
The term "Aethiopian Ocean", derived from Ancient Ethiopia, was applied to the Southern Atlantic as late as the mid-19th century.

In modern times, some idioms refer to the ocean in a humorously diminutive way as "the Pond", describing both the geographical and cultural divide between North America and Europe, in particular between the English-speaking nations of both continents. Many Irish or British people refer to the United States and Canada as "across the pond", and vice versa.
The "Black Atlantic" refers to the role of this ocean in shaping black people's history, especially through the Atlantic slave trade. Irish migration to the US is meant when the term "The Green Atlantic" is used. The term "Red Atlantic" has been used in reference to the Marxian concept of an Atlantic working class, as well as to the Atlantic experience of indigenous Americans.

The International Hydrographic Organization (IHO) defined the limits of the oceans and seas in 1953, but some of these definitions have been revised since then and some are not used by various authorities, institutions, and countries, see for example the CIA World Factbook. Correspondingly, the extent and number of oceans and seas varies.
The Atlantic Ocean is bounded on the west by North and South America. It connects to the Arctic Ocean through the Denmark Strait, Greenland Sea, Norwegian Sea and Barents Sea. To the east, the boundaries of the ocean proper are Europe: the Strait of Gibraltar (where it connects with the Mediterranean Sea–one of its marginal seas–and, in turn, the Black Sea, both of which also touch upon Asia) and Africa.
In the southeast, the Atlantic merges into the Indian Ocean. The 20° East meridian, running south from Cape Agulhas to Antarctica defines its border. In the 1953 definition it extends south to Antarctica, while in later maps it is bounded at the 60° parallel by the Southern Ocean.
The Atlantic has irregular coasts indented by numerous bays, gulfs, and seas. These include the Baltic Sea, Black Sea, Caribbean Sea, Davis Strait, Denmark Strait, part of the Drake Passage, Gulf of Mexico, Labrador Sea, Mediterranean Sea, North Sea, Norwegian Sea, almost all of the Scotia Sea, and other tributary water bodies. Including these marginal seas the coast line of the Atlantic measures 111,866 km (69,510 mi) compared to 135,663 km (84,297 mi) for the Pacific.
Including its marginal seas, the Atlantic covers an area of 106,460,000 km2 (41,100,000 sq mi) or 23.5% of the global ocean and has a volume of 310,410,900 km3 (74,471,500 cu mi) or 23.3%. Excluding its marginal seas, the Atlantic covers 81,760,000 km2 (31,570,000 sq mi) and has a volume of 305,811,900 km3 (73,368,200 cu mi). The North Atlantic covers 41,490,000 km2 (16,020,000 sq mi) (11.5%) and the South Atlantic 40,270,000 km2 (15,550,000 sq mi) (11.1%). The average depth is 3,646 m (11,962 ft) and the maximum depth, the Milwaukee Deep in the Puerto Rico Trench, is 8,486 m (27,841 ft).

The bathymetry of the Atlantic is dominated by a submarine mountain range called the Mid-Atlantic Ridge (MAR). It runs from 87°N or 300 km (190 mi) south of the North Pole to the subantarctic Bouvet Island at 42°S.

The MAR divides the Atlantic longitudinally into two halves, in each of which a series of basins are delimited by secondary, transverse ridges. The MAR reaches above 2000 m along most of its length, but is interrupted by larger transform faults at two places: the Romanche Trench near the Equator and the Gibbs Fracture Zone at 53°N. The MAR is a barrier for bottom water, but at these two transform faults deep water currents can pass from one side to the other.
The MAR rises 2–3 km (1.2–1.9 mi) above the surrounding ocean floor and its rift valley is the divergent boundary between the North American and Eurasian plates in the North Atlantic and the South American and African plates in the South Atlantic. The MAR produces basaltic volcanoes in Eyjafjallajökull, Iceland, and pillow lava on the ocean floor. The depth of water at the apex of the ridge is less than 2,700 metres (1,500 fathoms; 8,900 ft) in most places, while the bottom of the ridge is three times as deep.
The MAR is intersected by two perpendicular ridges: the Azores–Gibraltar Transform Fault, the boundary between the Nubian and Eurasian plates, intersects the MAR at the Azores Triple Junction, on either side of the Azores microplate, near the 40°N. A much vaguer, nameless boundary, between the North American and South American plates, intersects the MAR near or just north of the Fifteen-Twenty Fracture Zone, approximately at 16°N.
In the 1870s, the Challenger expedition discovered parts of what is now known as the Mid-Atlantic Ridge, or:

An elevated ridge rising to an average height of about 1,900 fathoms below the surface traverses the basins of the North and South Atlantic in a meridianal direction from Cape Farewell, probably its far south at least as Gough Island, following roughly the outlines of the coasts of the Old and the New Worlds.

The remainder of the ridge was discovered in the 1920s by the German Meteor expedition using echo-sounding equipment. The exploration of the MAR in the 1950s lead to the general acceptance of seafloor spreading and plate tectonics.
Most of the MAR runs under water but where it reaches the surfaces it has produced volcanic islands. While nine of these have collectively been nominated a World Heritage Site for their geological value, four of them are considered of "Outstanding Universal Value" based on their cultural and natural criteria: Þingvellir, Iceland; Landscape of the Pico Island Vineyard Culture, Portugal; Gough and Inaccessible Islands, United Kingdom; and Brazilian Atlantic Islands: Fernando de Noronha and Atol das Rocas Reserves, Brazil.

Continental shelves in the Atlantic are wide off Newfoundland, southern-most South America, and north-eastern Europe. In the western Atlantic carbonate platforms dominate large areas, for example the Blake Plateau and Bermuda Rise. The Atlantic is surrounded by passive margins except at a few locations where active margins form deep trenches: the Puerto Rico Trench (8,414 m (27,605 ft) maximum depth) in the western Pacific and South Sandwich Trench (8,264 m (27,113 ft)) in the South Atlantic. There are numerous submarine canyons off north-eastern North America, western Europe, and north-western Africa. Some of these canyons extend along the continental rises and farther into the abyssal plains as deep-sea channels.
The deep ocean floor is thought to be fairly flat with occasional deeps, abyssal plains, trenches, seamounts, basins, plateaus, canyons, and some guyots. Various shelves along the margins of the continents constitute about 11% of the bottom topography with few deep channels cut across the continental rise.
The mean depth between 60°N and 60°S is 3,730 m (12,240 ft), or close to the average for the global ocean, with a modal depth between 4,000 and 5,000 m (13,000 and 16,000 ft).
In the South Atlantic the Walvis Ridge and Rio Grande Rise form barriers to ocean currents. The Laurentian Abyss is found off the eastern coast of Canada.

Surface water temperatures, which vary with latitude, current systems, and season and reflect the latitudinal distribution of solar energy, range from below −2 °C (28 °F) to over 30 °C (86 °F). Maximum temperatures occur north of the equator, and minimum values are found in the polar regions. In the middle latitudes, the area of maximum temperature variations, values may vary by 7–8 °C (13–14 °F).
From October to June the surface is usually covered with sea ice in the Labrador Sea, Denmark Strait, and Baltic Sea.
The Coriolis effect circulates North Atlantic water in a clockwise direction, whereas South Atlantic water circulates counter-clockwise. The south tides in the Atlantic Ocean are semi-diurnal; that is, two high tides occur during each 24 lunar hours. In latitudes above 40° North some east-west oscillation, known as the North Atlantic Oscillation, occurs.

On average, the Atlantic is the saltiest major ocean; surface water salinity in the open ocean ranges from 33 to 37 parts per thousand (3.3 – 3.7%) by mass and varies with latitude and season. Evaporation, precipitation, river inflow and sea ice melting influence surface salinity values. Although the lowest salinity values are just north of the equator (because of heavy tropical rainfall), in general the lowest values are in the high latitudes and along coasts where large rivers enter. Maximum salinity values occur at about 25° north and south, in subtropical regions with low rainfall and high evaporation.
The high surface salinity in the Atlantic, on which the Atlantic thermohaline circulation is dependent, is maintained by two processes: the Agulhas Leakage/Rings, which brings salty Indian Ocean waters into the South Atlantic, and the "Atmospheric Bridge", which evaporates subtropical Atlantic waters and exports it to the Pacific.

The Atlantic Ocean consists of four major, upper water masses with distinct temperature and salinity. The Atlantic Subarctic Upper Water in the northern-most North Atlantic is the source for Subarctic Intermediate Water and North Atlantic Intermediate Water. North Atlantic Central Water can be divided into the Eastern and Western North Atlantic central Water since the western part is strongly affected by the Gulf Stream and therefore the upper layer is closer to underlying fresher subpolar intermediate water. The eastern water is saltier because of its proximity to Mediterranean Water. North Atlantic Central Water flows into South Atlantic Central Water at 15°N.
There are five intermediate waters: four low-salinity waters formed at subpolar latitudes and one high-salinity formed through evaporation. Arctic Intermediate Water, flows from north to become the source for North Atlantic Deep Water south of the Greenland-Scotland sill. These two intermediate waters have different salinity in the western and eastern basins. The wide range of salinities in the North Atlantic is caused by the asymmetry of the northern subtropical gyre and the large number of contributions from a wide range of sources: Labrador Sea, Norwegian-Greenland Sea, Mediterranean, and South Atlantic Intermediate Water.
The North Atlantic Deep Water (NADW) is a complex of four water masses, two that form by deep convection in the open ocean — Classical and Upper Labrador Sea Water — and two that form from the inflow of dense water across the Greenland-Iceland-Scotland sill — Denmark Strait and Iceland-Scotland Overflow Water. Along its path across Earth the composition of the NADW is affected by other water masses, especially Antarctic Bottom Water and Mediterranean Overflow Water. The NADW is fed by a flow of warm shallow water into the northern North Atlantic which is responsible for the anomalous warm climate in Europe. Changes in the formation of NADW have been linked to global climate changes in the past. Since man-made substances were introduced into the environment, the path of the NADW can be traced throughout its course by measuring tritium and radiocarbon from nuclear weapon tests in the 1960s and CFCs.

The clockwise warm-water North Atlantic Gyre occupies the northern Atlantic, and the counter-clockwise warm-water South Atlantic Gyre appears in the southern Atlantic.
In the North Atlantic surface circulation is dominated by three inter-connected currents: the Gulf Stream which flows north-east from the North American coast at Cape Hatteras; the North Atlantic Current, a branch of the Gulf Stream which flows northward from the Grand Banks; and the Subpolar Front, an extension of the North Atlantic Current, a wide, vaguely defined region separating the subtropical gyre from the subpolar gyre. This system of currents transport warm water into the North Atlantic, without which temperatures in the North Atlantic and Europe would plunge dramatically.

North of the North Atlantic Gyre, the cyclonic North Atlantic Subpolar Gyre plays a key role in climate variability. It is governed by ocean currents from marginal seas and regional topography, rather than being steered by wind, both in the deep ocean and at sea level. The subpolar gyre forms an important part of the global thermohaline circulation. Its eastern portion includes eddying branches of the North Atlantic Current which transport warm, saline waters from the subtropics to the north-eastern Atlantic. There this water is cooled during winter and forms return currents that merge along the eastern continental slope of Greenland where they form an intense (40–50 Sv) current which flows around the continental margins of the Labrador Sea. A third of this water become parts of the deep portion of the North Atlantic Deep Water (NADW). The NADW, in its turn, feed the meridional overturning circulation (MOC), the northward heat transport of which is threatened by anthropogenic climate change. Large variations in the subpolar gyre on a decade-century scale, associated with the North Atlantic Oscillation, are especially pronounced in Labrador Sea Water, the upper layers of the MOC.
The South Atlantic is dominated by the anti-cyclonic southern subtropical gyre. The South Atlantic Central Water originates in this gyre, while Antarctic Intermediate Water originates in the upper layers of the circumpolar region, near the Drake Passage and Falkland Islands. Both these currents receive some contribution from the Indian Ocean. On the African east coast the small cyclonic Angola Gyre lies embedded in the large subtropical gyre. The southern subtropical gyre is partly masked by a wind-induced Ekman layer. The residence time of the gyre is 4.4–8.5 years. North Atlantic Deep Water flows southerward below the thermocline of the subtropical gyre.

The Sargasso Sea in the western North Atlantic can be defined as the area where two species of Sargassum (S. fluitans and natans) float, an area 4,000 km (2,500 mi) wide and encircled by the Gulf Stream, North Atlantic Drift, and North Equatorial Current. This population of seaweed probably originated from Tertiary ancestors on the European shores of the former Tethys Ocean and has, if so, maintained itself by vegetative growth, floating in the ocean for millions of years.

Other species endemic to the Sargasso Sea include the sargassum fish, a predator with algae-like appendages who hovers motionless among the Sargassum. Fossils of similar fishes have been found in fossil bays of the former Tethys Ocean, in what is now the Carpathian region, that were similar to the Sargasso Sea. It is possible that the population in the Sargasso Sea migrated to the Atlantic as the Tethys closed at the end of the Miocene around 17 Ma. The origin of the Sargasso fauna and flora remained enigmatic for centuries. The fossils found in the Carpathians in the mid-20th century, often called the "quasi-Sargasso assemblage", finally showed that this assemblage originated in the Carpathian Basin from were it migrated over Sicily to the Central Atlantic where it evolved into modern species of the Sargasso Sea.
The location of the spawning ground for European eels remained unknown for decades. In the early 19th century it was discovered that the southern Sargasso Sea is the spawning ground for both the European and American eel and that the former migrate more than 5,000 km (3,100 mi) and the latter 2,000 km (1,200 mi). Ocean currents such as the Gulf Stream transport eel larvae from the Sargasso Sea to foraging areas in North America, Europe, and Northern Africa.

Climate is influenced by the temperatures of the surface waters and water currents as well as winds. Because of the ocean's great capacity to store and release heat, maritime climates are more moderate and have less extreme seasonal variations than inland climates. Precipitation can be approximated from coastal weather data and air temperature from water temperatures.
The oceans are the major source of the atmospheric moisture that is obtained through evaporation. Climatic zones vary with latitude; the warmest zones stretch across the Atlantic north of the equator. The coldest zones are in high latitudes, with the coldest regions corresponding to the areas covered by sea ice. Ocean currents influence climate by transporting warm and cold waters to other regions. The winds that are cooled or warmed when blowing over these currents influence adjacent land areas.
The Gulf Stream and its northern extension towards Europe, the North Atlantic Drift, for example, warms the atmosphere of the British Isles and north-western Europe and influences weather and climate as far south as the northern Mediterranean. The cold water currents contribute to heavy fog off the coast of eastern Canada (the Grand Banks of Newfoundland area) and Africa's north-western coast. In general, winds transport moisture and air over land areas. More local particular weather examples could be found in examples such as the Azores High, Benguela Current, and Nor'easter.

Icebergs are common from February to August in the Davis Strait, Denmark Strait, and the northwestern Atlantic and have been spotted as far south as Bermuda and Madeira. Ships are subject to superstructure icing in the extreme north from October to May. Persistent fog can be a maritime hazard from May to September, as can hurricanes north of the equator (May to December).
The United States' southeast coast, especially the Virginia and North Carolina coasts, has a long history of shipwrecks due to its many shoals and reefs.
The Bermuda Triangle is popularly believed to be the site of numerous aviation and shipping incidents because of unexplained and supposedly mysterious causes, but Coast Guard records do not support this belief.
Hurricanes are also a natural hazard in the Atlantic, but mainly in the northern part of the ocean, rarely tropical cyclones form in the southern parts. Hurricanes usually form annually between June and November.

The break-up of Pangaea began in the Central Atlantic, between North America and Northwest Africa, where rift basins opened during the Late Triassic and Early Jurassic. This period also saw the first stages of the uplift of the Atlas Mountains. The exact timing is controversial with estimates ranging from 200 to 170 Ma.
The opening of the Atlantic Ocean coincided with the initial break-up of the supercontinent Pangaea, both of which were initiated by the eruption of the Central Atlantic Magmatic Province (CAMP), one of the most extensive and voluminous large igneous provinces in Earth's history associated with the Triassic–Jurassic extinction event, one of Earth's major extinction events. Theoliitic dikes, sills, and lava flows from the CAMP eruption at 200 Ma have been found in West Africa, eastern North America, and northern South America. The extent of the volcanism has been estimated to 4.5×106 km2 (1.7×106 sq mi) of which 2.5×106 km2 (9.7×105 sq mi) covered what is now northern and central Brazil.
The formation of the Central American Isthmus closed the Central American Seaway at the end of the Pliocene 2.8 Ma ago. The formation of the isthmus resulted in the migration and extinction of many land-living animals, known as the Great American Interchange, but the closure of the seaway resulted in a "Great American Schism" as it affected ocean currents, salinity, and temperatures in both the Atlantic and Pacific. Marine organisms on both sides of the isthmus became isolated and either diverged or went extinct.

Geologically the Northern Atlantic is the area delimited to the south by two conjugate margins, Newfoundland and Iberia, and to the north by the Arctic Eurasian Basin. The opening of the Northern Atlantic closely followed the margins of its predecessor, the Iapetus Ocean, and spread from the Central Atlantic in six stages: Iberia–Newfoundland, Porcupine–North America, Eurasia–Greenland, Eurasia–North America. Active and inactive spreading systems in this area are marked by the interaction with the Iceland hotspot.

West Gondwana (South America and Africa) broke up in the Early Cretaceous to form the South Atlantic. The apparent fit between the coastlines of the two continents was noted on the first maps that included the South Atlantic and it was also the subject of the first computer-assisted plate tectonic reconstructions in 1965. This magnificent fit, however, has since then proven problematic and later reconstructions have introduced various deformation zones along the shorelines to accommodate the northward-propagating break-up. Intra-continental rifts and deformations have also been introduced to subdivide both continental plates into sub-plates.
Geologically the South Atlantic can be divided into four segments: Equatorial segment, from 10°N to the Romanche Fracture Zone (RFZ);; Central segment, from RFZ to Florianopolis Fracture Zone (FFZ, north of Walvis Ridge and Rio Grande Rise); Southern segment, from FFZ to the Agulhas-Falkland Fracture Zone (AFFZ); and Falkland segment, south of AFFZ.
In the southern segment the Early Cretaceous (133–130 Ma) intensive magmatism of the Paraná–Etendeka Large Igneous Province produced by the Tristan hotspot resulted in an estimated volume of 1.5×106 to 2.0×106 km3 (3.6×105 to 4.8×105 cu mi). It covered an area of 1.2×106 to 1.6×106 km2 (4.6×105 to 6.2×105 sq mi) in Brazil, Paraguay, and Uruguay and 0.8×105 km2 (3.1×104 sq mi) in Africa. Dyke swarms in Brazil, Angola, eastern Paraguay, and Namibia, however, suggest the LIP originally covered a much larger area and also indicate failed rifts in all these areas. Associated offshore basaltic flows reach as far south as the Falkland Islands and South Africa. Traces of magmatism in both offshore and onshore basins in the central and southern segments have been dated to 147–49 Ma with two peaks between 143–121 Ma and 90–60 Ma.
In the Falkland segment rifting began with dextral movements between the Patagonia and Colorado sub-plates between the Early Jurassic (190 Ma) and the Early Cretaceous (126.7 Ma). Around 150 Ma sea-floor spreading propagated northward into the southern segment. No later than 130 Ma rifting had reached the Walvis Ridge–Rio Grande Rise.
In the central segment rifting started to break Africa in two by opening the Benue Trough around 118 Ma. Rifting in the central segment, however, coincided with the Cretaceous Normal Superchron (also known as the Cretaceous quiet period), a 40 Ma period without magnetic reversals, which makes it difficult to date sea-floor spreading in this segment.
The equatorial segment is the last phase of the break-up, but, because it is located on the Equator, magnetic anomalies cannot be used for dating. Various estimates date the propagation of sea-floor spreading in this segment to the period 120–96 Ma. This final stage, nevertheless, coincided with or resulted in the end of continental extension in Africa.
About 50 Ma the opening of the Drake Passage resulted from a change in the motions and separation rate of the South American and Antarctic plates. First small ocean basins opened and a shallow gateway appeared during the Middle Eocene. 34–30 Ma a deeper seaway developed, followed by an Eocene–Oligocene climatic deterioration and the growth of the Antarctic ice sheet.

An embryonic subduction margin is potentially developing west of Gibraltar. The Gibraltar Arc in the western Mediterranean is migrating westward into the Central Atlantic where it joins the converging African and Eurasian plates. Together these three tectonic forces are slowly developing into a new subduction system in the eastern Atlantic Basin. Meanwhile, the Scotia Arc and Caribbean Plate in the western Atlantic Basin are eastward-propagating subduction systems that might, together with the Gibraltar system, represent the beginning of the closure of the Atlantic Ocean and the final stage of the Atlantic Wilson Cycle.

Humans evolved in Africa; first by diverging from other apes around 7 Ma; then developing stone tools around 2.6 Ma; to finally evolve as modern humans around 100 kya. The earliest evidences for the complex behaviour associated with this behavioral modernity has been found in the Greater Cape Floristic Region (GCFR) along the coast of South Africa. During the latest glacial stages the now-submerged plains of the Agulhas Bank were exposed above sea level, extending the South African coastline farther south by hundreds of kilometres. A small population of modern humans — probably fewer than a thousand reproducing individuals — survived glacial maxima by exploring the high diversity offered by these Palaeo-Agulhas plains. The GCFR is delimited to the north by the Cape Fold Belt and the limited space south of it resulted in the development of social networks out of which complex Stone Age technologies emerged. Human history thus begins on the coasts of South Africa where the Atlantic Benguela Upwelling and Indian Ocean Agulhas Current meet to produce an intertidal zone on which shellfish, fur seal, fishes and sea birds provided the necessary protein sources. The African origin of this modern behaviour is evidenced by 70,000 years-old engravings from Blombos Cave, South Africa.

Mitochondrial DNA (mtDNA) studies indicate that 80–60,000 years ago a major demographic expansion within Africa, derived from a single, small population, coincided with the emergence of behavioural complexity and the rapid MIS 5–4 environmental changes. This group of people not only expanded over the whole of Africa, but also started to disperse out of Africa into Asia, Europe, and Australasia around 65.000 years ago and quickly replaced the archaic humans in these regions. During the Last Glacial Maximum (LGM) 20,000 years ago humans had to abandon their initial settlements along the European North Atlantic coast and retreat to the Mediterranean. Following rapid climate changes at the end of the LGM this region was repopulated by Magdalenian culture. Other hunter-gatherers followed in waves interrupted by large-scale hazards such as the Laacher See volcanic eruption, the inundation of Doggerland (now the North Sea), and the formation of the Baltic Sea. The European coasts of the North Atlantic were permanently populated about 9–8.5 thousand years ago.
This human dispersal left abundant traces along the coasts of the Atlantic Ocean. 50 ka-old, deeply stratified shell middens found in Ysterfontein on the western coast of South Africa are associated with the Middle Stone Age (MSA). The MSA population was small and dispersed and the rate of their reproduction and exploitation was less intense than those of later generations. While their middens resemble 12-11 ka-old Late Stone Age (LSA) middens found on every inhabited continent, the 50-45 ka-old Enkapune Ya Muto in Kenya probably represents the oldest traces of the first modern humans to disperse out of Africa.

The same development can be seen in Europe. In La Riera Cave (23-13 ka) in Asturias, Spain, only some 26,600 molluscs were deposited over 10 ka. In contrast, 8-7 ka-old shell middens in Portugal, Denmark, and Brazil generated thousands of tonnes of debris and artefacts. The Ertebølle middens in Denmark, for example, accumulated 2,000 m3 (71,000 cu ft) of shell deposits representing some 50 million molluscs over only a thousand years. This intensification in the exploitation of marine resources has been described as accompanied by new technologies — such as boats, harpoons, and fish-hooks — because many caves found in the Mediterranean and on the European Atlantic coast have increased quantities of marine shells in their upper levels and reduced quantities in their lower. The earliest exploitation, however, took place on the now submerged shelves, and most settlements now excavated were then located several kilometres from these shelves. The reduced quantities of shells in the lower levels can represent the few shells that were exported inland.

During the LGM the Laurentide Ice Sheet covered most of northern North America while Beringia connected Siberia to Alaska. In 1973 late U.S. geoscientist Paul S. Martin proposed a "blitzkrieg" colonization of America by which Clovis hunters migrated into North America around 13,000 years ago in a single wave through an ice-free corridor in the ice sheet and "spread southward explosively, briefly attaining a density sufficiently large to overkill much of their prey." Others later proposed a "three-wave" migration over the Bering Land Bridge. These hypotheses remained the long-held view regarding the settlement of the Americas, a view challenged by more recent archaeological discoveries: the oldest archaeological sites in the Americas have been found in South America; sites in north-east Siberia report virtually no human presence there during the LGM; and most Clovis artefacts have been found in eastern North America along the Atlantic coast. Furthermore, colonisation models based on mtDNA, yDNA, and atDNA data respectively support neither the "blitzkrieg" nor the "three-wave" hypotheses but they also deliver mutually ambiguous results. Contradictory data from archaeology and genetics will most likely deliver future hypotheses that will, eventually, confirm each other. A proposed route across the Pacific to South America could explain early South American finds and another hypothesis proposes a northern path, through the Canadian Arctic and down the North American Atlantic coast. Early settlements across the Atlantic have been suggested by alternative theories, ranging from purely hypothetical to mostly disputed, including the Solutrean hypothesis and some of the Pre-Columbian trans-oceanic contact theories.

The Norse settlement of the Faroe Islands and Iceland began during the 9th and 10th centuries. A settlement on Greenland was established before 1000 CE, but contact with it was lost in 1409 and it was finally abandoned during the early Little Ice Age. This setback was caused by a range of factors: an unsustainable economy resulted in erosion and denudation, while conflicts with the local Inuit resulted in the failure to adapt their Arctic technologies; a colder climate resulted in starvation; and the colony got economically marginalised as the Great Plague and Barbary pirates harvested its victims on Iceland in the 15th century. Iceland was initially settled 865–930 CE following a warm period when winter temperatures hovered around 2 °C (36 °F) which made farming favourable at high latitudes. This did not last, however, and temperatures quickly dropped; at 1080 CE summer temperatures had reached a maximum of 5 °C (41 °F). The Landnámabók (Book of Settlement) records disastrous famines during the first century of settlement — "men ate foxes and ravens" and "the old and helpless were killed and thrown over cliffs" — and by the early 1200s hay had to be abandoned for short-season crops such as barley.

Christopher Columbus discovered the Americas in 1492 under Spanish flag. Six years later Vasco da Gama reached India under Portuguese flag, by navigating south around the Cape of Good Hope, thus proving that the Atlantic and Indian Oceans are connected. In 1500, in his voyage to India following Vasco da Gama, Pedro Alvares Cabral reached Brazil, taken by the currents of the South Atlantic Gyre. Following these explorations, Spain and Portugal quickly conquered and colonized large territories in the New World and forced the Native American population into slavery in order to explore the vast quantities of silver and gold they found. Spain and Portugal monopolised this trade in order to keep other European nations out, but conflicting interests nevertheless lead to a series of Spanish-Portuguese wars. A peace treaty mediated by the Pope divided the conquered territories into Spanish and Portuguese sectors while keeping other colonial powers away. England, France, and the Dutch Republic enviously watched the Spanish and Portuguese wealth grow and allied themselves with pirates such as Henry Mainwaring and Alexandre Exquemelin. They could explore the convoys leaving America because prevailing winds and currents made the transport of heavy metals slow and predictable.

In the American colonies depredation, disease, and slavery quickly reduced the indigenous American population to the extent that the Atlantic slave trade had to be introduced to replace them — a trade that became norm and an integral part of the colonisation. Between the 15th century and 1888, when Brazil became the last part of America to end slave trade, an estimated ten million Africans were exported as slaves, most of them destined for agricultural labour. The slave trade was officially abolished in the British Empire and the United States in 1808, and slavery itself was abolished in the British Empire in 1838 and in the U.S. in 1865 after the Civil War.
From Columbus to the Industrial revolution Trans-Atlantic trade, including colonialism and slavery, became crucial for Western Europe. For European countries with a direct access to the Atlantic (including Britain, France, the Netherlands, Portugal, and Spain) 1500–1800 was a period of sustained growth during which these countries grew richer than those in Eastern Europe and Asia. Colonialism evolved as part of the Trans-Atlantic trade, but this trade also strengthened the position of merchant groups at the expense of monarchs. Growth was more rapid in non-absolutist countries, such as Britain and the Netherlands, and more limited in absolutist monarchies, such as Portugal, Spain, and France, where profit mostly or exclusively benefited the monarchy and its allies.
Trans-Atlantic trade also resulted in an increasing urbanisation: in European countries facing the Atlantic urbanisation grew from 8% in 1300, 10.1% in 1500, to 24.5% in 1850; in other European countries from 10% in 1300, 11.4% in 1500, to 17% in 1850. Likewise, GDP doubled in Atlantic countries but rose by only 30% in the rest of Europe. By end of the 17th century the volume of the Trans-Atlantic trade had surpassed that of the Mediterranean trade.

The Atlantic has contributed significantly to the development and economy of surrounding countries. Besides major transatlantic transportation and communication routes, the Atlantic offers abundant petroleum deposits in the sedimentary rocks of the continental shelves.
The Atlantic harbours petroleum and gas fields, fish, marine mammals (seals and whales), sand and gravel aggregates, placer deposits, polymetallic nodules, and precious stones. Gold deposits are a mile or two under water on the ocean floor, however the deposits are also encased in rock that must be mined through. Currently, there is no cost-effective way to mine or extract gold from the ocean to make a profit.
Various international treaties attempt to reduce pollution caused by environmental threats such as oil spills, marine debris, and the incineration of toxic wastes at sea.

The shelves of the Atlantic hosts one of the world's richest fishing resources. The most productive areas include the Grand Banks of Newfoundland, the Scotian Shelf, Georges Bank off Cape Cod, the Bahama Banks, the waters around Iceland, the Irish Sea, the Dogger Bank of the North Sea, and the Falkland Banks. Fisheries have, however, undergone significant changes since the 1950s and global catches can now be divided into three groups of which only two are observed in the Atlantic: fisheries in the Eastern Central and South-West Atlantic oscillate around a globally stable value, the rest of the Atlantic is in overall decline following historical peaks. The third group, "continuously increasing trend since 1950", is only found in the Indian Ocean and Western Pacific.

In the North-East Atlantic total catches decreased between the mid-1970s and the 1990s and reached 8.7 million tonnes in 2013. Blue whiting reached a 2.4 million tonnes peak in 2004 but was down to 628,000 tonnes in 2013. Recovery plans for cod, sole, and plaice have reduced mortality in these species. Arctic cod reached its lowest levels in the 1960s-1980s but is now recovered. Arctic saithe and haddock are considered fully fished; Sand eel is overfished as was capelin which has now recovered to fully fished. Limited data makes the state of redfishes and deep-water species difficult to assess but most likely they remain vulnerable to overfishing. Stocks of northern shrimp and Norwegian lobster are in good condition. In the North-East Atlantic 21% of stocks are considered overfished.

In the North-West Atlantic landings have decreased from 4.2 million tonnes in the early 1970s to 1.9 million tonnes in 2013. During the 21th century some species have shown weak signs of recovery, including Greenland halibut, yellowtail flounder, Atlantic halibut, haddock, spiny dogfish, while other stocks shown no such signs, including cod, witch flounder, and redfish. Stocks of invertebrates, in contrast, remain at record levels of abundance. 31% of stocks are overfished in the North-west Atlantic.

In 1497 John Cabot became the first to explore mainland North America and one of his major discoveries was the abundant resources of Atlantic cod off Newfoundland. Referred to as "Newfoundland Currency" this discovery supplied mankind with some 200 million tonnes of fish over five centuries. In the late 19th and early 20th centuries new fisheries started to exploit haddock, mackerel, and lobster. From the 1950s to the 1970s the introduction of European and Asian distant-water fleets in the area dramatically increased the fishing capacity and number of exploited species. It also expanded the exploited areas from near-shore to the open sea and to great depths to include deep-water species such as redfish, Greenland halibut, witch flounder, and grenadiers. Overfishing in the area was recognised as early as the 1960s but, because this was occurring on international waters, it took until the late 1970s before any attempts to regulate was made. In the early 1990s this finally resulted in the collapse of the Atlantic northwest cod fishery. The population of a number of deep-sea fishes also collapsed in the process, including American plaice, redfish, and Greenland halibut, together with flounder and grenadier.
In the Eastern Central Atlantic small pelagic fishes constitute about 50% of landings with sardine reaching 0.6–1.0 million tonnes per year. Pelagic fish stocks are considered fully fishes or overfished, with sardines south of Cape Bojador the notable exception. Almost half of stocks are fished at biologically unsustainable levels. Total catches have been fluctuating since the 1970s; reaching 3.9 million tonnes in 2013 or slightly less than the peak production in 2010.

In the Western Central Atlantic catches have been decreasing since 2000 and reached 1.3 million tonnes in 2013. The most important species in the area, Gulf menhaden, reached a million tonnes in the mid-1980s but only half a million tonnes in 2013 and is now considered fully fished. Round sardinella was an important species in the 1990s but is now considered overfished. Groupers and snappers are overfished and northern brown shrimp and American cupped oyster are considered fully fished approaching overfished. 44% of stocks are being fished at unsustainable levels.

In the South-East Atlantic catches have decreased from 3.3 million tonnes in the early 1970s to 1.3 million tonnes in 2013. Horse mackerel and hake are the most important species, together representing almost half of the landings. Off South Africa and Namibia deep-water hake and shallow-water Cape hake have recovered to sustainable levels since regulations were introduced in 2006 and the states of Southern African pilchard and anchovy have improved to fully fished in 2013.
In the South-West Atlantic a peak was reached in the mid-1980s and catches now fluctuate between 1.7 and 2.6 million tonnes. The most important species, the Argentine shortfin squid, which reached half a million tonnes in 2013 or half the peak value, is considered fully fished to overfished. Another important species was the Brazilian sardinella, with a production of 100,000 tonnes in 2013 it is now considered overfished. Half the stocks in this area are being fished at unsustainable levels: Whitehead’s round herring has not yet reached fully fished but Cunene horse mackerel is overfished. The sea snail perlemoen abalone is targeted by illegal fishing and remain overfished.

Endangered marine species include the manatee, seals, sea lions, turtles, and whales. Drift net fishing can kill dolphins, albatrosses and other seabirds (petrels, auks), hastening the fish stock decline and contributing to international disputes. Municipal pollution comes from the eastern United States, southern Brazil, and eastern Argentina; oil pollution in the Caribbean Sea, Gulf of Mexico, Lake Maracaibo, Mediterranean Sea, and North Sea; and industrial waste and municipal sewage pollution in the Baltic Sea, North Sea, and Mediterranean Sea.
North Atlantic hurricane activity has increased over past decades because of increased sea surface temperature (SST) at tropical latitudes, changes that can be attributed to either the natural Atlantic Multidecadal Oscillation (AMO) or to anthropogenic climate change. A 2005 report indicated that the Atlantic meridional overturning circulation (AMOC) slowed down by 30% between 1957 and 2004. If the AMO was responsible for SST variability then the AMOC would have increased in strength, which is apparently not the case. Furthermore, it is clear from statistical analyses of annual tropical cyclones that these changes do not display multidecadal cyclicity. Therefore, these changes in SST must be caused by human activities.
The ocean mixed layer plays an important role heat storage over seasonal and decadal time-scales, whereas deeper layers are affected over millennia and has a heat capacity about 50 times that of the mixed layer. This heat uptake provides a time-lag for climate change but it also results in a thermal expansion of the oceans which contribute to sea-level rise. 21st century global warming will probably result in an equilibrium sea-level rise five times greater than today, whilst melting of glaciers, including that of the Greenland ice-sheet, expected to have virtually no effect during the 21st century, will probably result in a sea-level rise of 3–6 m over a millennium.
On 7 June 2006, Florida's wildlife commission voted to take the manatee off the state's endangered species list. Some environmentalists worry that this could erode safeguards for the popular sea creature.
Marine pollution is a generic term for the entry into the ocean of potentially hazardous chemicals or particles. The biggest culprits are rivers and with them many agriculture fertilizer chemicals as well as livestock and human waste. The excess of oxygen-depleting chemicals leads to hypoxia and the creation of a dead zone.
Marine debris, which is also known as marine litter, describes human-created waste floating in a body of water. Oceanic debris tends to accumulate at the centre of gyres and coastlines, frequently washing aground where it is known as beach litter.

List of countries and territories bordering the Atlantic Ocean
Seven Seas
Gulf Stream shutdown
Shipwrecks in the Atlantic Ocean
Atlantic hurricanes
Transatlantic crossing

Winchester, Simon (2010). Atlantic: A Vast Ocean of a Million Stories. HarperCollins UK. ISBN 978-0-00-734137-5.

Atlantic Ocean
"Map of Atlantic Coast of North America from the Chesapeake Bay to Florida" from 1639 via the World Digital LibraryEarth, otherwise known as the world, (Greek: Γαῖα Gaia; Latin: Terra) is the third planet from the Sun and the only object in the Universe known to harbor life. It is the densest planet in the Solar System and the largest of the four terrestrial planets.
According to radiometric dating and other sources of evidence, Earth formed about 4.54 billion years ago. Earth's gravity interacts with other objects in space, especially the Sun and the Moon, Earth's only natural satellite. During one orbit around the Sun, Earth rotates about its axis over 365 times, thus an Earth year is about 365.26 days long. Earth's axis of rotation is tilted, producing seasonal variations on the planet's surface. The gravitational interaction between the Earth and Moon causes ocean tides, stabilizes the Earth's orientation on its axis, and gradually slows its rotation.
Earth's lithosphere is divided into several rigid tectonic plates that migrate across the surface over periods of many millions of years. About 71% of Earth's surface is covered with water, mostly by its oceans. The remaining 29% is land consisting of continents and islands that together have many lakes, rivers and other sources of water that contribute to the hydrosphere. The majority of Earth's polar regions are covered in ice, including the Antarctic ice sheet and the sea ice of the Arctic ice pack. Earth's interior remains active with a solid iron inner core, a liquid outer core that generates the Earth's magnetic field, and a convecting mantle that drives plate tectonics.
Within the first billion years of Earth's history, life appeared in the oceans and began to affect the Earth's atmosphere and surface, leading to the proliferation of aerobic and anaerobic organisms. Some geological evidence indicates that life may have arisen as much as 4.1 billion years ago. Since then, the combination of Earth's distance from the Sun, physical properties, and geological history have allowed life to evolve and thrive. In the history of the Earth, biodiversity has gone through long periods of expansion, occasionally punctuated by mass extinction events. Over 99% of all species that ever lived on Earth are extinct. Estimates of the number of species on Earth today vary widely; most species have not been described. Over 7.4 billion humans live on Earth and depend on its biosphere and minerals for their survival. Humans have developed diverse societies and cultures; politically, the world has about 200 sovereign states.

The modern English word  Earth developed from a wide variety of Middle English forms, which derived from an Old English noun most often spelled eorðe. It has cognates in every Germanic language, and their proto-Germanic root has been reconstructed as *erþō. In its earliest appearances, eorðe was already being used to translate the many senses of Latin terra and Greek γῆ (gē): the ground, its soil, dry land, the human world, the surface of the world (including the sea), and the globe itself. As with Terra and Gaia, Earth was a personified goddess in Germanic paganism: the Angles were listed by Tacitus as among the devotees of Nerthus, and later Norse mythology included Jörð, a giantess often given as the mother of Thor.
Originally, earth was written in lowercase, and from early Middle English, its definite sense as "the globe" was expressed as the earth. By early Modern English, many nouns were capitalized, and the earth became (and often remained) the Earth, particularly when referenced along with other heavenly bodies. More recently, the name is sometimes simply given as Earth, by analogy with the names of the other planets. House styles now vary: Oxford spelling recognizes the lowercase form as the most common, with the capitalized form an acceptable variant. Another convention capitalizes "Earth" when appearing as a name (e.g. "Earth's atmosphere") but writes it in lowercase when preceded by the (e.g. "the atmosphere of the earth"). It almost always appears in lowercase in colloquial expressions such as "what on earth are you doing?"

The oldest material found in the Solar System is dated to 7000456720000000000♠4.5672±0.0006 billion years ago (Gya). By 7000454000000000000♠4.54±0.04 Gya the primordial Earth had formed. The formation and evolution of Solar System bodies occurred along with the Sun. In theory, a solar nebula partitions a volume out of a molecular cloud by gravitational collapse, which begins to spin and flatten into a circumstellar disk, and then the planets grow out of that disk along with the Sun. A nebula contains gas, ice grains, and dust (including primordial nuclides). According to nebular theory, planetesimals formed by accretion, with the primordial Earth taking 10–7001200000000000000♠20 million years (Ma) to form.
A subject of on-going research is the formation of the Moon, some 4.53 billion years ago. A working hypothesis is that it was formed by accretion from material loosed from Earth after a Mars-sized object, named Theia, impacted Earth. In this scenario, the mass of Theia was approximately 10% of that of Earth, it impacted Earth with a glancing blow, and some of its mass merged with Earth. Between approximately 4.1 and 7000380000000000000♠3.8 Gya, numerous asteroid impacts during the Late Heavy Bombardment caused significant changes to the greater surface environment of the Moon, and by inference, to that of Earth.

Earth's atmosphere and oceans were formed by volcanic activity and outgassing that included water vapor. The origin of the world's oceans was condensation augmented by water and ice delivered by asteroids, protoplanets, and comets. In this model, atmospheric "greenhouse gases" kept the oceans from freezing when the newly forming Sun had only 70% of its current luminosity. By 7000350000000000000♠3.5 Gya, Earth's magnetic field was established, which helped prevent the atmosphere from being stripped away by the solar wind.
A crust formed when the molten outer layer of Earth cooled to form a solid. The two models that explain land mass propose either a steady growth to the present-day forms or, more likely, a rapid growth early in Earth history followed by a long-term steady continental area. Continents formed by plate tectonics, a process ultimately driven by the continuous loss of heat from Earth's interior. On time scales lasting hundreds of millions of years, the supercontinents have assembled and broken apart. Roughly 7016236682000000000♠750 mya (million years ago), one of the earliest known supercontinents, Rodinia, began to break apart. The continents later recombined to form Pannotia, 600–7016170411040000000♠540 mya, then finally Pangaea, which also broke apart 7015568036800000000♠180 mya.
The present pattern of ice ages began about 7015126230400000000♠40 mya and then intensified during the Pleistocene about 7013946728000000000♠3 mya. High-latitude regions have since undergone repeated cycles of glaciation and thaw, repeating about every 40,000–7012315576000000000♠100000 years. The last continental glaciation ended 10,000 years ago.

Chemical reactions led to the first self–replicating molecules about four billion years ago. A half billion years later, the last common ancestor of all life arose. The evolution of photosynthesis allowed the Sun's energy to be harvested directly by life forms. The resultant molecular oxygen (O2) accumulated in the atmosphere and due to interaction with ultraviolet solar radiation, formed a protective ozone layer (O3) in the upper atmosphere. The incorporation of smaller cells within larger ones resulted in the development of complex cells called eukaryotes. True multicellular organisms formed as cells within colonies became increasingly specialized. Aided by the absorption of harmful ultraviolet radiation by the ozone layer, life colonized Earth's surface. Among the earliest fossil evidence for life is microbial mat fossils found in 3.48 billion-year-old sandstone in Western Australia, biogenic graphite found in 3.7 billion-year-old metasedimentary rocks in Western Greenland, remains of biotic material found in 4.1 billion-year-old rocks in Western Australia.
During the Neoproterozoic, 7016236682000000000♠750 to 580 mya, much of Earth might have been covered in ice. This hypothesis has been termed "Snowball Earth", and it is of particular interest because it preceded the Cambrian explosion, when multicellular life forms significantly increased in complexity. Following the Cambrian explosion, 7016168833160000000♠535 mya, there have been five major mass extinctions. The most recent such event was 7015208280160000000♠66 mya, when an asteroid impact triggered the extinction of the non-avian dinosaurs and other large reptiles, but spared some small animals such as mammals, which then resembled shrews. Over the past 7015208280160000000♠66 Ma, mammalian life has diversified, and several million years ago an African ape-like animal such as Orrorin tugenensis gained the ability to stand upright. This facilitated tool use and encouraged communication that provided the nutrition and stimulation needed for a larger brain, which allowed the evolution of humans. The development of agriculture, and then civilization, led to humans having an influence on Earth and the nature and quantity of other life forms that continues today.

Earth's long-term future is closely tied to that of the Sun. Over the next 7016347133600000000♠1.1 Ga, solar luminosity will increase by 10%, and over the next 7017110451600000000♠3.5 Ga by 40%. The Earth's increasing surface temperature will accelerate the inorganic CO2 cycle, reducing its concentration to levels lethally low for plants (6995099999999999999♠10 ppm for C4 photosynthesis) in approximately 500–7016284018400000000♠900 Ma. The lack of vegetation will result in the loss of oxygen in the atmosphere, and animal life will become extinct. After another billion years all surface water will have disappeared and the mean global temperature will reach 7002343150000000000♠70 °C (7002343150000000000♠158 °F). From that point, the Earth is expected to be habitable for another 7016157788000000000♠500 Ma, possibly up to 7016725824800000000♠2.3 Ga if nitrogen is removed from the atmosphere. Even if the Sun were eternal and stable, 27% of the water in the modern oceans will descend to the mantle in one billion years, due to reduced steam venting from mid-ocean ridges.
The Sun will evolve to become a red giant in about 7017157788000000000♠5 Ga. Models predict that the Sun will expand to roughly 1 AU (150,000,000 km), which is about 250 times its present radius. Earth's fate is less clear. As a red giant, the Sun will lose roughly 30% of its mass, so, without tidal effects, Earth will move to an orbit 1.7 AU from the Sun when the star reaches its maximum radius. Most, if not all, remaining life will be destroyed by the Sun's increased luminosity (peaking at about 5,000 times its present level). A 2008 simulation indicates that Earth's orbit will eventually decay due to tidal effects and drag, causing it to enter the Sun's atmosphere and be vaporized.

The shape of Earth is approximately oblate spheroidal. Due to rotation, the Earth is flattened along the geographic axis and bulging around the equator. The diameter of the Earth at the equator is 43 kilometres (27 mi) larger than the pole-to-pole diameter. Thus the point on the surface farthest from Earth's center of mass is the summit of the equatorial Chimborazo volcano in Ecuador. The average diameter of the reference spheroid is 12,742 kilometres (7,918 mi). Local topography deviates from this idealized spheroid, although on a global scale these deviations are small compared to Earth's radius: The maximum deviation of only 0.17% is at the Mariana Trench (10,911 metres (35,797 ft) below local sea level), whereas Mount Everest (8,848 metres (29,029 ft) above local sea level) represents a deviation of 0.14%.

Earth's mass is approximately 7024597000000000000♠5.97×1024 kg (5,970 Yg). It is composed mostly of iron (32.1%), oxygen (30.1%), silicon (15.1%), magnesium (13.9%), sulfur (2.9%), nickel (1.8%), calcium (1.5%), and aluminium (1.4%), with the remaining 1.2% consisting of trace amounts of other elements. Due to mass segregation, the core region is estimated to be primarily composed of iron (88.8%), with smaller amounts of nickel (5.8%), sulfur (4.5%), and less than 1% trace elements.
A little more than 47% of Earth's crust consists of oxygen. The most common rock constituents of the crust are nearly all oxides: chlorine, sulfur, and fluorine are the important exceptions to this and their total amount in any rock is usually much less than 1%. The principal oxides are silica, alumina, iron oxides, lime, magnesia, potash, and soda. The silica functions principally as an acid, forming silicates, and all the most common minerals of igneous rocks are of this nature. 99.22% of all rocks are composed of 11 oxides (see the table at right), with the other constituents occurring in minute quantities.

Earth's interior, like that of the other terrestrial planets, is divided into layers by their chemical or physical (rheological) properties. The outer layer is a chemically distinct silicate solid crust, which is underlain by a highly viscous solid mantle. The crust is separated from the mantle by the Mohorovičić discontinuity. The thickness of the crust varies from about 7003600000000000000♠6 km (kilometers) under the oceans to 30–50 km for the continents. The crust and the cold, rigid, top of the upper mantle are collectively known as the lithosphere, and it is of the lithosphere that the tectonic plates are composed. Beneath the lithosphere is the asthenosphere, a relatively low-viscosity layer on which the lithosphere rides. Important changes in crystal structure within the mantle occur at 410 and 7005660000000000000♠660 km below the surface, spanning a transition zone that separates the upper and lower mantle. Beneath the mantle, an extremely low viscosity liquid outer core lies above a solid inner core. The Earth's inner core might rotate at a slightly higher angular velocity than the remainder of the planet, advancing by 0.1–0.5° per year. The radius of the inner core is about one fifth of that of Earth.

Earth's internal heat comes from a combination of residual heat from planetary accretion (about 20%) and heat produced through radioactive decay (80%). The major heat-producing isotopes within Earth are potassium-40, uranium-238, and thorium-232. At the center, the temperature may be up to 6,000 °C (10,830 °F), and the pressure could reach 360 GPa. Because much of the heat is provided by radioactive decay, scientists postulate that early in Earth's history, before isotopes with short half-lives were depleted, Earth's heat production was much higher. At approximately 7016946728000000000♠3 Ga, twice the present-day heat would have been produced, increasing the rates of mantle convection and plate tectonics, and allowing the production of uncommon igneous rocks such as komatiites that are rarely formed today.
The mean heat loss from Earth is 87 mW m−2, for a global heat loss of 4.42 × 1013 W. A portion of the core's thermal energy is transported toward the crust by mantle plumes, a form of convection consisting of upwellings of higher-temperature rock. These plumes can produce hotspots and flood basalts. More of the heat in Earth is lost through plate tectonics, by mantle upwelling associated with mid-ocean ridges. The final major mode of heat loss is through conduction through the lithosphere, the majority of which occurs under the oceans because the crust there is much thinner than that of the continents.

The mechanically rigid outer layer of Earth, the lithosphere, is divided into pieces called tectonic plates. These plates are rigid segments that move in relation to one another at one of three types of plate boundaries: convergent boundaries, at which two plates come together, divergent boundaries, at which two plates are pulled apart, and transform boundaries, in which two plates slide past one another laterally. Earthquakes, volcanic activity, mountain-building, and oceanic trench formation can occur along these plate boundaries. The tectonic plates ride on top of the asthenosphere, the solid but less-viscous part of the upper mantle that can flow and move along with the plates.
As the tectonic plates migrate, oceanic crust is subducted under the leading edges of the plates at convergent boundaries. At the same time, the upwelling of mantle material at divergent boundaries creates mid-ocean ridges. The combination of these processes recycles the oceanic crust back into the mantle. Due to this recycling, most of the ocean floor is less than 7015315576000000000♠100 Ma old in age. The oldest oceanic crust is located in the Western Pacific and has an estimated age of 7015631152000000000♠200 Ma. By comparison, the oldest dated continental crust is 7017127177128000000♠4030 Ma.
The seven major plates are the Pacific, North American, Eurasian, African, Antarctic, Indo-Australian, and South American. Other notable plates include the Arabian Plate, the Caribbean Plate, the Nazca Plate off the west coast of South America and the Scotia Plate in the southern Atlantic Ocean. The Australian Plate fused with the Indian Plate between 50 and 7015173566800000000♠55 mya. The fastest-moving plates are the oceanic plates, with the Cocos Plate advancing at a rate of 75 mm/year and the Pacific Plate moving 52–69 mm/year. At the other extreme, the slowest-moving plate is the Eurasian Plate, progressing at a typical rate of 21 mm/year.

The total surface area of the Earth is about 7008510000000000000♠510 million km2 (197 million sq mi). Of this, 70.8%, or 7008361130000000000♠361.13 million km2 (139.43 million sq mi), is below sea level and covered by ocean water. Below the ocean's surface are much of the continental shelf, mountains, volcanoes, oceanic trenches, submarine canyons, oceanic plateaus, abyssal plains, and a globe-spanning mid-ocean ridge system. The remaining 29.2% (7008148940000000000♠148.94 million km2, or 57.51 million sq mi) not covered by water has terrain that varies greatly from place to place and consists of mountains, deserts, plains, plateaus, and other landforms. Tectonics and erosion, volcanic eruptions, flooding, weathering, glaciation, the growth of coral reefs, and meteorite impacts are among the processes that constantly reshape the Earth's surface over geological time.
The continental crust consists of lower density material such as the igneous rocks granite and andesite. Less common is basalt, a denser volcanic rock that is the primary constituent of the ocean floors. Sedimentary rock is formed from the accumulation of sediment that becomes buried and compacted together. Nearly 75% of the continental surfaces are covered by sedimentary rocks, although they form about 5% of the crust. The third form of rock material found on Earth is metamorphic rock, which is created from the transformation of pre-existing rock types through high pressures, high temperatures, or both. The most abundant silicate minerals on Earth's surface include quartz, feldspars, amphibole, mica, pyroxene and olivine. Common carbonate minerals include calcite (found in limestone) and dolomite.
The elevation of the land surface varies from the low point of −418 m at the Dead Sea, to a maximum altitude of 8,848 m at the top of Mount Everest. The mean height of land above sea level is 840 m.
The pedosphere is the outermost layer of Earth's continental surface and is composed of soil and subject to soil formation processes. The total arable land is 10.9% of the land surface, with 1.3% being permanent cropland. Close to 40% of Earth's land surface is used for cropland and pasture, or an estimated 1.3×107 km2 of cropland and 3.4×107 km2 of pastureland.

The abundance of water on Earth's surface is a unique feature that distinguishes the "Blue Planet" from other planets in the Solar System. Earth's hydrosphere consists chiefly of the oceans, but technically includes all water surfaces in the world, including inland seas, lakes, rivers, and underground waters down to a depth of 2,000 m. The deepest underwater location is Challenger Deep of the Mariana Trench in the Pacific Ocean with a depth of 10,911.4 m.
The mass of the oceans is approximately 1.35×1018 metric tons or about 1/4400 of Earth's total mass. The oceans cover an area of 7014361800000000000♠3.618×108 km2 with a mean depth of 7003368200000000000♠3682 m, resulting in an estimated volume of 7018133200000000000♠1.332×109 km3. If all of Earth's crustal surface were at the same elevation as a smooth sphere, the depth of the resulting world ocean would be 2.7 to 2.8 km.
About 97.5% of the water is saline; the remaining 2.5% is fresh water. Most fresh water, about 68.7%, is present as ice in ice caps and glaciers.
The average salinity of Earth's oceans is about 35 grams of salt per kilogram of sea water (3.5% salt). Most of this salt was released from volcanic activity or extracted from cool igneous rocks. The oceans are also a reservoir of dissolved atmospheric gases, which are essential for the survival of many aquatic life forms. Sea water has an important influence on the world's climate, with the oceans acting as a large heat reservoir. Shifts in the oceanic temperature distribution can cause significant weather shifts, such as the El Niño-Southern Oscillation.

The atmospheric pressure on Earth's surface averages 101.325 kPa, with a scale height of about 8.5 km. It has a composition of 78% nitrogen and 21% oxygen, with trace amounts of water vapor, carbon dioxide, and other gaseous molecules. The height of the troposphere varies with latitude, ranging between 8 km at the poles to 17 km at the equator, with some variation resulting from weather and seasonal factors.
Earth's biosphere has significantly altered its atmosphere. Oxygenic photosynthesis evolved 7000270000000000000♠2.7 Gya, forming the primarily nitrogen–oxygen atmosphere of today. This change enabled the proliferation of aerobic organisms and, indirectly, the formation of the ozone layer due to the subsequent conversion of atmospheric O2 into O3. The ozone layer blocks ultraviolet solar radiation, permitting life on land. Other atmospheric functions important to life include transporting water vapor, providing useful gases, causing small meteors to burn up before they strike the surface, and moderating temperature. This last phenomenon is known as the greenhouse effect: trace molecules within the atmosphere serve to capture thermal energy emitted from the ground, thereby raising the average temperature. Water vapor, carbon dioxide, methane, and ozone are the primary greenhouse gases in the atmosphere. Without this heat-retention effect, the average surface temperature would be −18 °C, in contrast to the current +15 °C, and life would likely not exist.

Earth's atmosphere has no definite boundary, slowly becoming thinner and fading into outer space. Three-quarters of the atmosphere's mass is contained within the first 11 km of the surface. This lowest layer is called the troposphere. Energy from the Sun heats this layer, and the surface below, causing expansion of the air. This lower-density air then rises and is replaced by cooler, higher-density air. The result is atmospheric circulation that drives the weather and climate through redistribution of thermal energy.
The primary atmospheric circulation bands consist of the trade winds in the equatorial region below 30° latitude and the westerlies in the mid-latitudes between 30° and 60°. Ocean currents are also important factors in determining climate, particularly the thermohaline circulation that distributes thermal energy from the equatorial oceans to the polar regions.
Water vapor generated through surface evaporation is transported by circulatory patterns in the atmosphere. When atmospheric conditions permit an uplift of warm, humid air, this water condenses and falls to the surface as precipitation. Most of the water is then transported to lower elevations by river systems and usually returned to the oceans or deposited into lakes. This water cycle is a vital mechanism for supporting life on land and is a primary factor in the erosion of surface features over geological periods. Precipitation patterns vary widely, ranging from several meters of water per year to less than a millimeter. Atmospheric circulation, topographic features, and temperature differences determine the average precipitation that falls in each region.
The amount of solar energy reaching Earth's surface decreases with increasing latitude. At higher latitudes, the sunlight reaches the surface at lower angles, and it must pass through thicker columns of the atmosphere. As a result, the mean annual air temperature at sea level decreases by about 0.4 °C (0.7 °F) per degree of latitude from the equator. Earth's surface can be subdivided into specific latitudinal belts of approximately homogeneous climate. Ranging from the equator to the polar regions, these are the tropical (or equatorial), subtropical, temperate and polar climates.
This latitudinal rule has several anomalies:
Proximity to oceans moderates the climate. For example, the Scandinavian peninsula has more moderate climate than similarly northern latitudes of northern Canada.
The wind enables this moderating effect. The windward side of a land mass experiences more moderation than the leeward side. In the Northern Hemisphere, the prevailing wind is west-to-east, and western coasts tend to be milder than eastern coasts. This is seen in Eastern North America and Western Europe, where rough continental climates appear on the east coast on parallels with mild climates on the other side of the ocean. In the Southern Hemisphere, the prevailing wind is east-to-west, and the eastern coasts are milder.
The distance from the Earth to the Sun varies. The Earth is closest to the Sun (at perihelion) in January, which is summer in the Southern Hemisphere. It is furthest away (at aphelion) in July, which is summer in the Northern Hemisphere, and only 93.55% of the solar radiation from the Sun falls on a given square area of land than at perihelion. Despite this, there are larger land masses in the Northern Hemisphere, which are easier to heat than the seas. Consequently, summers are 2.3 °C (4 °F) warmer in the Northern Hemisphere than in the Southern Hemisphere under similar conditions.
The climate is colder at high altitudes than at sea level because of the decreased air density.
The commonly used Köppen climate classification system has five broad groups (humid tropics, arid, humid middle latitudes, continental and cold polar), which are further divided into more specific subtypes. The Köppen system rates regions of terrain based on observed temperature and precipitation.
The highest air temperature ever measured on Earth was 56.7 °C (134.1 °F) in Furnace Creek, California, in Death Valley, in 1913. The lowest air temperature ever directly measured on Earth was −89.2 °C (−128.6 °F) at Vostok Station in 1983, but satellites have used remote sensing to measure temperatures as low as −94.7 °C (−138.5 °F) in East Antarctica. These temperature records are only measurements made with modern instruments from the 20th century onwards and likely do not reflect the full range of temperature on Earth.

Above the troposphere, the atmosphere is usually divided into the stratosphere, mesosphere, and thermosphere. Each layer has a different lapse rate, defining the rate of change in temperature with height. Beyond these, the exosphere thins out into the magnetosphere, where the geomagnetic fields interact with the solar wind. Within the stratosphere is the ozone layer, a component that partially shields the surface from ultraviolet light and thus is important for life on Earth. The Kármán line, defined as 100 km above Earth's surface, is a working definition for the boundary between the atmosphere and outer space.
Thermal energy causes some of the molecules at the outer edge of the atmosphere to increase their velocity to the point where they can escape from Earth's gravity. This causes a slow but steady loss of the atmosphere into space. Because unfixed hydrogen has a low molecular mass, it can achieve escape velocity more readily, and it leaks into outer space at a greater rate than other gases. The leakage of hydrogen into space contributes to the shifting of Earth's atmosphere and surface from an initially reducing state to its current oxidizing one. Photosynthesis provided a source of free oxygen, but the loss of reducing agents such as hydrogen is thought to have been a necessary precondition for the widespread accumulation of oxygen in the atmosphere. Hence the ability of hydrogen to escape from the atmosphere may have influenced the nature of life that developed on Earth. In the current, oxygen-rich atmosphere most hydrogen is converted into water before it has an opportunity to escape. Instead, most of the hydrogen loss comes from the destruction of methane in the upper atmosphere.

The main part of Earth's magnetic field is generated in the core, the site of a dynamo process that converts the kinetic energy of thermally and compositionally driven convection into electrical and magnetic field energy. The field extends outwards from the core, through the mantle, and up to Earth's surface, where it is, approximately, a dipole. The poles of the dipole are located close to Earth's geographic poles. At the equator of the magnetic field, the magnetic-field strength at the surface is 3.05 × 10−5 T, with global magnetic dipole moment of 7.91 × 1015 T m3. The convection movements in the core are chaotic; the magnetic poles drift and periodically change alignment. This causes secular variation of the main field and field reversals at irregular intervals averaging a few times every million years. The most recent reversal occurred approximately 700,000 years ago.

The extent of Earth's magnetic field in space defines the magnetosphere. Ions and electrons of the solar wind are deflected by the magnetosphere; solar wind pressure compresses the dayside of the magnetosphere, to about 10 Earth radii, and extends the nightside magnetosphere into a long tail. Because the velocity of the solar wind is greater than the speed at which wave propagate through the solar wind, a supersonic bowshock precedes the dayside magnetosphere within the solar wind. Charged particles are contained within the magnetosphere; the plasmasphere is defined by low-energy particles that essentially follow magnetic field lines as Earth rotates; the ring current is defined by medium-energy particles that drift relative to the geomagnetic field, but with paths that are still dominated by the magnetic field, and the Van Allen radiation belt are formed by high-energy particles whose motion is essentially random, but otherwise contained by the magnetosphere.
During magnetic storms and substorms, charged particles can be deflected from the outer magnetosphere and especially the magnetotail, directed along field lines into Earth's ionosphere, where atmospheric atoms can be excited and ionized, causing the aurora.

Earth's rotation period relative to the Sun—its mean solar day—is 86,400 seconds of mean solar time (86,400.0025 SI seconds). Because Earth's solar day is now slightly longer than it was during the 19th century due to tidal deceleration, each day varies between 0 and 2 SI ms longer.
Earth's rotation period relative to the fixed stars, called its stellar day by the International Earth Rotation and Reference Systems Service (IERS), is 86,164.098903691 seconds of mean solar time (UT1), or 23h 56m 4.098903691s. Earth's rotation period relative to the precessing or moving mean vernal equinox, misnamed its sidereal day, is 86,164.09053083288 seconds of mean solar time (UT1) (23h 56m 4.09053083288s) as of 1982. Thus the sidereal day is shorter than the stellar day by about 8.4 ms. The length of the mean solar day in SI seconds is available from the IERS for the periods 1623–2005 and 1962–2005.
Apart from meteors within the atmosphere and low-orbiting satellites, the main apparent motion of celestial bodies in Earth's sky is to the west at a rate of 15°/h = 15'/min. For bodies near the celestial equator, this is equivalent to an apparent diameter of the Sun or the Moon every two minutes; from Earth's surface, the apparent sizes of the Sun and the Moon are approximately the same.

Earth orbits the Sun at an average distance of about 150 million km (93 million mi) every 365.2564 mean solar days, or one sidereal year. This gives an apparent movement of the Sun eastward with respect to the stars at a rate of about 1°/day, which is one apparent Sun or Moon diameter every 12 hours. Due to this motion, on average it takes 24 hours—a solar day—for Earth to complete a full rotation about its axis so that the Sun returns to the meridian. The orbital speed of Earth averages about 29.78 km/s (107,200 km/h; 66,600 mph), which is fast enough to travel a distance equal to Earth's diameter, about 12,742 km (7,918 mi), in seven minutes, and the distance to the Moon, 384,000 km (239,000 mi), in about 3.5 hours.
The Moon and Earth orbit a common barycenter every 27.32 days relative to the background stars. When combined with the Earth–Moon system's common orbit around the Sun, the period of the synodic month, from new moon to new moon, is 29.53 days. Viewed from the celestial north pole, the motion of Earth, the Moon, and their axial rotations are all counterclockwise. Viewed from a vantage point above the north poles of both the Sun and Earth, Earth orbits in a counterclockwise direction about the Sun. The orbital and axial planes are not precisely aligned: Earth's axis is tilted some 23.44 degrees from the perpendicular to the Earth–Sun plane (the ecliptic), and the Earth–Moon plane is tilted up to ±5.1 degrees against the Earth–Sun plane. Without this tilt, there would be an eclipse every two weeks, alternating between lunar eclipses and solar eclipses.
The Hill sphere, or the sphere of gravitational influence, of the Earth is about 1.5 million kilometres (930,000 mi) in radius. This is the maximum distance at which the Earth's gravitational influence is stronger than the more distant Sun and planets. Objects must orbit the Earth within this radius, or they can become unbound by the gravitational perturbation of the Sun.
Earth, along with the Solar System, is situated in the Milky Way and orbits about 28,000 light-years from its center. It is about 20 light-years above the galactic plane in the Orion Arm.

The axial tilt of the Earth is approximately 23.439281° with the axis of its orbit plane, always pointing towards the Celestial Poles. Due to Earth's axial tilt, the amount of sunlight reaching any given point on the surface varies over the course of the year. This causes the seasonal change in climate, with summer in the Northern Hemisphere occurring when the Tropic of Cancer is facing the Sun, and winter taking place when the Tropic of Capricorn in the Southern Hemisphere faces the Sun. During the summer, the day lasts longer, and the Sun climbs higher in the sky. In winter, the climate becomes cooler and the days shorter. In northern temperate latitudes, the Sun rises north of true east during the summer solstice, and sets north of true west, reversing in the winter. The Sun rises south of true east in the summer for the southern temperate zone and sets south of true west.
Above the Arctic Circle, an extreme case is reached where there is no daylight at all for part of the year, up to six months at the North Pole itself, a polar night. In the Southern Hemisphere, the situation is exactly reversed, with the South Pole oriented opposite the direction of the North Pole. Six months later, this pole will experience a midnight sun, a day of 24 hours, again reversing with the South Pole.
By astronomical convention, the four seasons can be determined by the solstices—the points in the orbit of maximum axial tilt toward or away from the Sun—and the equinoxes, when the direction of the tilt and the direction to the Sun are perpendicular. In the Northern Hemisphere, winter solstice currently occurs around 21 December; summer solstice is near 21 June, spring equinox is around 20 March and autumnal equinox is about 22 or 23 September. In the Southern Hemisphere, the situation is reversed, with the summer and winter solstices exchanged and the spring and autumnal equinox dates swapped.
The angle of Earth's axial tilt is relatively stable over long periods of time. Its axial tilt does undergo nutation; a slight, irregular motion with a main period of 18.6 years. The orientation (rather than the angle) of Earth's axis also changes over time, precessing around in a complete circle over each 25,800 year cycle; this precession is the reason for the difference between a sidereal year and a tropical year. Both of these motions are caused by the varying attraction of the Sun and the Moon on Earth's equatorial bulge. The poles also migrate a few meters across Earth's surface. This polar motion has multiple, cyclical components, which collectively are termed quasiperiodic motion. In addition to an annual component to this motion, there is a 14-month cycle called the Chandler wobble. Earth's rotational velocity also varies in a phenomenon known as length-of-day variation.
In modern times, Earth's perihelion occurs around 3 January, and its aphelion around 4 July. These dates change over time due to precession and other orbital factors, which follow cyclical patterns known as Milankovitch cycles. The changing Earth–Sun distance causes an increase of about 6.9% in solar energy reaching Earth at perihelion relative to aphelion. Because the Southern Hemisphere is tilted toward the Sun at about the same time that Earth reaches the closest approach to the Sun, the Southern Hemisphere receives slightly more energy from the Sun than does the northern over the course of a year. This effect is much less significant than the total energy change due to the axial tilt, and most of the excess energy is absorbed by the higher proportion of water in the Southern Hemisphere.

A planet that can sustain life is termed habitable, even if life did not originate there. Earth provides liquid water—an environment where complex organic molecules can assemble and interact, and sufficient energy to sustain metabolism. The distance of Earth from the Sun, as well as its orbital eccentricity, rate of rotation, axial tilt, geological history, sustaining atmosphere, and magnetic field all contribute to the current climatic conditions at the surface.

A planet's life forms inhabit ecosystems, whose total is sometimes said to form a "biosphere". Earth's biosphere is thought to have begun evolving about 7000350000000000000♠3.5 Gya. The biosphere is divided into a number of biomes, inhabited by broadly similar plants and animals. On land, biomes are separated primarily by differences in latitude, height above sea level and humidity. Terrestrial biomes lying within the Arctic or Antarctic Circles, at high altitudes or in extremely arid areas are relatively barren of plant and animal life; species diversity reaches a peak in humid lowlands at equatorial latitudes.
In July 2016, scientists reported identifying a set of 355 genes from the Last Universal Common Ancestor (LUCA) of all organisms living on Earth.

Earth has resources that have been exploited by humans. Those termed non-renewable resources, such as fossil fuels, only renew over geological timescales.
Large deposits of fossil fuels are obtained from Earth's crust, consisting of coal, petroleum, and natural gas. These deposits are used by humans both for energy production and as feedstock for chemical production. Mineral ore bodies have also been formed within the crust through a process of ore genesis, resulting from actions of magmatism, erosion, and plate tectonics. These bodies form concentrated sources for many metals and other useful elements.
Earth's biosphere produces many useful biological products for humans, including food, wood, pharmaceuticals, oxygen, and the recycling of many organic wastes. The land-based ecosystem depends upon topsoil and fresh water, and the oceanic ecosystem depends upon dissolved nutrients washed down from the land. In 1980, 5,053 Mha (50.53 million km2) of Earth's land surface consisted of forest and woodlands, 6,788 Mha (67.88 million km2) was grasslands and pasture, and 1,501 Mha (15.01 million km2) was cultivated as croplands. The estimated amount of irrigated land in 1993 was 2,481,250 square kilometres (958,020 sq mi). Humans also live on the land by using building materials to construct shelters.

Large areas of Earth's surface are subject to extreme weather such as tropical cyclones, hurricanes, or typhoons that dominate life in those areas. From 1980 to 2000, these events caused an average of 11,800 human deaths per year. Many places are subject to earthquakes, landslides, tsunamis, volcanic eruptions, tornadoes, sinkholes, blizzards, floods, droughts, wildfires, and other calamities and disasters.
Many localized areas are subject to human-made pollution of the air and water, acid rain and toxic substances, loss of vegetation (overgrazing, deforestation, desertification), loss of wildlife, species extinction, soil degradation, soil depletion and erosion.
There is a scientific consensus linking human activities to global warming due to industrial carbon dioxide emissions. This is predicted to produce changes such as the melting of glaciers and ice sheets, more extreme temperature ranges, significant changes in weather and a global rise in average sea levels.

Cartography, the study and practice of map-making, and geography, the study of the lands, features, inhabitants and phenomena on Earth, have historically been the disciplines devoted to depicting Earth. Surveying, the determination of locations and distances, and to a lesser extent navigation, the determination of position and direction, have developed alongside cartography and geography, providing and suitably quantifying the requisite information.
Earth's human population reached approximately seven billion on 31 October 2011. Projections indicate that the world's human population will reach 9.2 billion in 2050. Most of the growth is expected to take place in developing nations. Human population density varies widely around the world, but a majority live in Asia. By 2020, 60% of the world's population is expected to be living in urban, rather than rural, areas.
It is estimated that one-eighth of Earth's surface is suitable for humans to live on – three-quarters of Earth's surface is covered by oceans, leaving one-quarter as land. Half of that land area is desert (14%), high mountains (27%), or other unsuitable terrains. The northernmost permanent settlement in the world is Alert, on Ellesmere Island in Nunavut, Canada. (82°28′N) The southernmost is the Amundsen–Scott South Pole Station, in Antarctica, almost exactly at the South Pole. (90°S)
Independent sovereign nations claim the planet's entire land surface, except for some parts of Antarctica, a few land parcels along the Danube river's western bank, and the unclaimed area of Bir Tawil between Egypt and Sudan. As of 2015, there are 193 sovereign states that are member states of the United Nations, plus two observer states and 72 dependent territories and states with limited recognition. Earth has never had a sovereign government with authority over the entire globe, although some nation-states have striven for world domination and failed.
The United Nations is a worldwide intergovernmental organization that was created with the goal of intervening in the disputes between nations, thereby avoiding armed conflict. The U.N. serves primarily as a forum for international diplomacy and international law. When the consensus of the membership permits, it provides a mechanism for armed intervention.
The first human to orbit Earth was Yuri Gagarin on 12 April 1961. In total, about 487 people have visited outer space and reached orbit as of 30 July 2010, and, of these, twelve have walked on the Moon. Normally, the only humans in space are those on the International Space Station. The station's crew, made up of six people, is usually replaced every six months. The farthest that humans have traveled from Earth is 400,171 km, achieved during the Apollo 13 mission in 1970.

The Moon is a relatively large, terrestrial, planet-like natural satellite, with a diameter about one-quarter of Earth's. It is the largest moon in the Solar System relative to the size of its planet, although Charon is larger relative to the dwarf planet Pluto. The natural satellites of other planets are also referred to as "moons", after Earth's.
The gravitational attraction between Earth and the Moon causes tides on Earth. The same effect on the Moon has led to its tidal locking: its rotation period is the same as the time it takes to orbit Earth. As a result, it always presents the same face to the planet. As the Moon orbits Earth, different parts of its face are illuminated by the Sun, leading to the lunar phases; the dark part of the face is separated from the light part by the solar terminator.

Due to their tidal interaction, the Moon recedes from Earth at the rate of approximately 38 mm/yr. Over millions of years, these tiny modifications—and the lengthening of Earth's day by about 23 µs/yr—add up to significant changes. During the Devonian period, for example, (approximately 7016129386160000000♠410 mya) there were 400 days in a year, with each day lasting 21.8 hours.
The Moon may have dramatically affected the development of life by moderating the planet's climate. Paleontological evidence and computer simulations show that Earth's axial tilt is stabilized by tidal interactions with the Moon. Some theorists think that without this stabilization against the torques applied by the Sun and planets to Earth's equatorial bulge, the rotational axis might be chaotically unstable, exhibiting chaotic changes over millions of years, as appears to be the case for Mars.
Viewed from Earth, the Moon is just far enough away to have almost the same apparent-sized disk as the Sun. The angular size (or solid angle) of these two bodies match because, although the Sun's diameter is about 400 times as large as the Moon's, it is also 400 times more distant. This allows total and annular solar eclipses to occur on Earth.
The most widely accepted theory of the Moon's origin, the giant-impact hypothesis, states that it formed from the collision of a Mars-size protoplanet called Theia with the early Earth. This hypothesis explains (among other things) the Moon's relative lack of iron and volatile elements and the fact that its composition is nearly identical to that of Earth's crust.

Earth has at least five co-orbital asteroids, including 3753 Cruithne and 2002 AA29. A trojan asteroid companion, 2010 TK7, is librating around the leading Lagrange triangular point, L4, in the Earth's orbit around the Sun.
The tiny near-Earth asteroid 2006 RH120 makes close approaches to the Earth–Moon system roughly every twenty years. During these approaches, it can orbit Earth for brief periods of time.
As of June 2016, there were 1,419 operational, human-made satellites orbiting Earth. There are also inoperative satellites, including Vanguard 1, the oldest satellite currently in orbit, and over 16,000 pieces of tracked space debris. Earth's largest artificial satellite is the International Space Station.

The standard astronomical symbol of Earth consists of a cross circumscribed by a circle, , representing the four quadrants of the world.
Human cultures have developed many views of the planet. Earth is sometimes personified as a deity. In many cultures it is a mother goddess that is also the primary fertility deity, and by the mid-20th century, the Gaia Principle compared Earth's environments and life as a single self-regulating organism leading to broad stabilization of the conditions of habitability. Creation myths in many religions involve the creation of Earth by a supernatural deity or deities.
Scientific investigation has resulted in several culturally transformative shifts in our view of the planet. In the West, belief in a flat Earth was displaced by the idea of spherical Earth, credited to Pythagoras in the 6th century BC. Earth was further believed to be the center of the universe until the 16th century when scientists first theorized that it was a moving object, comparable to the other planets in the Solar System. Due to the efforts of influential Christian scholars and clerics such as James Ussher, who sought to determine the age of Earth through analysis of genealogies in Scripture, Westerners before the 19th century generally believed Earth to be a few thousand years old at most. It was only during the 19th century that geologists realized Earth's age was at least many millions of years. Lord Kelvin used thermodynamics to estimate the age of Earth to be between 20 million and 400 million years in 1864, sparking a vigorous debate on the subject; it was only when radioactivity and radioactive dating were discovered in the late 19th and early 20th centuries that a reliable mechanism for determining Earth's age was established, proving the planet to be billions of years old. The perception of Earth shifted again in the 20th century when humans first viewed it from orbit, and especially with photographs of Earth returned by the Apollo program.

Celestial sphere
Earth physical characteristics tables
Earth science
Earth system science
Timeline of the far future

Comins, Neil F. (2001). Discovering the Essential Universe (2nd ed.). New York: W. H. Freeman. Bibcode:2003deu..book.....C. ISBN 0-7167-5804-0. OCLC 52082611.

National Geographic encyclopedic entry about Earth
Earth – Profile – Solar System Exploration – NASA
Earth – Climate Changes Cause Shape to Change – NASA
United States Geological Survey – USGS
Earth – Astronaut Photography Gateway – NASA
Earth Observatory – NASA
Earth – Audio (29:28) – Cain/Gay – Astronomy Cast (2007)
Earth – Videos – International Space Station:
Video (01:02) – Earth (time-lapse)
Video (00:27) – Earth and Auroras (time-lapse)Climate is the statistics of weather, usually over a 30-year interval. It is measured by assessing the patterns of variation in temperature, humidity, atmospheric pressure, wind, precipitation, atmospheric particle count and other meteorological variables in a given region over long periods of time. Climate differs from weather, in that weather only describes the short-term conditions of these variables in a given region.
A region's climate is generated by the climate system, which has five components: atmosphere, hydrosphere, cryosphere, lithosphere, and biosphere.
The climate of a location is affected by its latitude, terrain, and altitude, as well as nearby water bodies and their currents. Climates can be classified according to the average and the typical ranges of different variables, most commonly temperature and precipitation. The most commonly used classification scheme was the Köppen climate classification. The Thornthwaite system, in use since 1948, incorporates evapotranspiration along with temperature and precipitation information and is used in studying biological diversity and how climate change effects it. The Bergeron and Spatial Synoptic Classification systems focus on the origin of air masses that define the climate of a region.
Paleoclimatology is the study of ancient climates. Since direct observations of climate are not available before the 19th century, paleoclimates are inferred from proxy variables that include non-biotic evidence such as sediments found in lake beds and ice cores, and biotic evidence such as tree rings and coral. Climate models are mathematical models of past, present and future climates. Climate change may occur over long and short timescales from a variety of factors; recent warming is discussed in global warming. Global warming results in redistributions. For example, "a 3°C change in mean annual temperature corresponds to a shift in isotherms of approximately 300–400 km in latitude (in the temperate zone) or 500 m in elevation. Therefore, species are expected to move upwards in elevation or towards the poles in latitude in response to shifting climate zones".

Climate (from Ancient Greek klima, meaning inclination) is commonly defined as the weather averaged over a long period. The standard averaging period is 30 years, but other periods may be used depending on the purpose. Climate also includes statistics other than the average, such as the magnitudes of day-to-day or year-to-year variations. The Intergovernmental Panel on Climate Change (IPCC) 2001 glossary definition is as follows:

Climate in a narrow sense is usually defined as the "average weather," or more rigorously, as the statistical description in terms of the mean and variability of relevant quantities over a period ranging from months to thousands or millions of years. The classical period is 30 years, as defined by the World Meteorological Organization (WMO). These quantities are most often surface variables such as temperature, precipitation, and wind. Climate in a wider sense is the state, including a statistical description, of the climate system.

The World Meteorological Organization (WMO) describes climate "normals" as "reference points used by climatologists to compare current climatological trends to that of the past or what is considered 'normal'. A Normal is defined as the arithmetic average of a climate element (e.g. temperature) over a 30-year period. A 30 year period is used, as it is long enough to filter out any interannual variation or anomalies, but also short enough to be able to show longer climatic trends." The WMO originated from the International Meteorological Organization which set up a technical commission for climatology in 1929. At its 1934 Wiesbaden meeting the technical commission designated the thirty-year period from 1901 to 1930 as the reference time frame for climatological standard normals. In 1982 the WMO agreed to update climate normals, and in these were subsequently completed on the basis of climate data from 1 January 1961 to 31 December 1990.
The difference between climate and weather is usefully summarized by the popular phrase "Climate is what you expect, weather is what you get." Over historical time spans there are a number of nearly constant variables that determine climate, including latitude, altitude, proportion of land to water, and proximity to oceans and mountains. These change only over periods of millions of years due to processes such as plate tectonics. Other climate determinants are more dynamic: the thermohaline circulation of the ocean leads to a 5 °C (9 °F) warming of the northern Atlantic Ocean compared to other ocean basins. Other ocean currents redistribute heat between land and water on a more regional scale. The density and type of vegetation coverage affects solar heat absorption, water retention, and rainfall on a regional level. Alterations in the quantity of atmospheric greenhouse gases determines the amount of solar energy retained by the planet, leading to global warming or global cooling. The variables which determine climate are numerous and the interactions complex, but there is general agreement that the broad outlines are understood, at least insofar as the determinants of historical climate change are concerned.

There are several ways to classify climates into similar regimes. Originally, climes were defined in Ancient Greece to describe the weather depending upon a location's latitude. Modern climate classification methods can be broadly divided into genetic methods, which focus on the causes of climate, and empiric methods, which focus on the effects of climate. Examples of genetic classification include methods based on the relative frequency of different air mass types or locations within synoptic weather disturbances. Examples of empiric classifications include climate zones defined by plant hardiness, evapotranspiration, or more generally the Köppen climate classification which was originally designed to identify the climates associated with certain biomes. A common shortcoming of these classification schemes is that they produce distinct boundaries between the zones they define, rather than the gradual transition of climate properties more common in nature.

The simplest classification is that involving air masses. The Bergeron classification is the most widely accepted form of air mass classification. Air mass classification involves three letters. The first letter describes its moisture properties, with c used for continental air masses (dry) and m for maritime air masses (moist). The second letter describes the thermal characteristic of its source region: T for tropical, P for polar, A for Arctic or Antarctic, M for monsoon, E for equatorial, and S for superior air (dry air formed by significant downward motion in the atmosphere). The third letter is used to designate the stability of the atmosphere. If the air mass is colder than the ground below it, it is labeled k. If the air mass is warmer than the ground below it, it is labeled w. While air mass identification was originally used in weather forecasting during the 1950s, climatologists began to establish synoptic climatologies based on this idea in 1973.
Based upon the Bergeron classification scheme is the Spatial Synoptic Classification system (SSC). There are six categories within the SSC scheme: Dry Polar (similar to continental polar), Dry Moderate (similar to maritime superior), Dry Tropical (similar to continental tropical), Moist Polar (similar to maritime polar), Moist Moderate (a hybrid between maritime polar and maritime tropical), and Moist Tropical (similar to maritime tropical, maritime monsoon, or maritime equatorial).

The Köppen classification depends on average monthly values of temperature and precipitation. The most commonly used form of the Köppen classification has five primary types labeled A through E. These primary types are A) tropical, B) dry, C) mild mid-latitude, D) cold mid-latitude, and E) polar. The five primary classifications can be further divided into secondary classifications such as rain forest, monsoon, tropical savanna, humid subtropical, humid continental, oceanic climate, Mediterranean climate, desert, steppe, subarctic climate, tundra, and polar ice cap.
Rain forests are characterized by high rainfall, with definitions setting minimum normal annual rainfall between 1,750 millimetres (69 in) and 2,000 millimetres (79 in). Mean monthly temperatures exceed 18 °C (64 °F) during all months of the year.
A monsoon is a seasonal prevailing wind which lasts for several months, ushering in a region's rainy season. Regions within North America, South America, Sub-Saharan Africa, Australia and East Asia are monsoon regimes.

A tropical savanna is a grassland biome located in semiarid to semi-humid climate regions of subtropical and tropical latitudes, with average temperatures remain at or above 18 °C (64 °F) year round and rainfall between 750 millimetres (30 in) and 1,270 millimetres (50 in) a year. They are widespread on Africa, and are found in India, the northern parts of South America, Malaysia, and Australia.

The humid subtropical climate zone where winter rainfall (and sometimes snowfall) is associated with large storms that the westerlies steer from west to east. Most summer rainfall occurs during thunderstorms and from occasional tropical cyclones. Humid subtropical climates lie on the east side continents, roughly between latitudes 20° and 40° degrees away from the equator.

A humid continental climate is marked by variable weather patterns and a large seasonal temperature variance. Places with more than three months of average daily temperatures above 10 °C (50 °F) and a coldest month temperature below −3 °C (27 °F) and which do not meet the criteria for an arid or semiarid climate, are classified as continental.
An oceanic climate is typically found along the west coasts at the middle latitudes of all the world's continents, and in southeastern Australia, and is accompanied by plentiful precipitation year-round.
The Mediterranean climate regime resembles the climate of the lands in the Mediterranean Basin, parts of western North America, parts of Western and South Australia, in southwestern South Africa and in parts of central Chile. The climate is characterized by hot, dry summers and cool, wet winters.
A steppe is a dry grassland with an annual temperature range in the summer of up to 40 °C (104 °F) and during the winter down to −40 °C (−40 °F).
A subarctic climate has little precipitation, and monthly temperatures which are above 10 °C (50 °F) for one to three months of the year, with permafrost in large parts of the area due to the cold winters. Winters within subarctic climates usually include up to six months of temperatures averaging below 0 °C (32 °F).

Tundra occurs in the far Northern Hemisphere, north of the taiga belt, including vast areas of northern Russia and Canada.
A polar ice cap, or polar ice sheet, is a high-latitude region of a planet or moon that is covered in ice. Ice caps form because high-latitude regions receive less energy as solar radiation from the sun than equatorial regions, resulting in lower surface temperatures.
A desert is a landscape form or region that receives very little precipitation. Deserts usually have a large diurnal and seasonal temperature range, with high or low, depending on location daytime temperatures (in summer up to 45 °C or 113 °F), and low nighttime temperatures (in winter down to 0 °C or 32 °F) due to extremely low humidity. Many deserts are formed by rain shadows, as mountains block the path of moisture and precipitation to the desert.

Devised by the American climatologist and geographer C. W. Thornthwaite, this climate classification method monitors the soil water budget using evapotranspiration. It monitors the portion of total precipitation used to nourish vegetation over a certain area. It uses indices such as a humidity index and an aridity index to determine an area's moisture regime based upon its average temperature, average rainfall, and average vegetation type. The lower the value of the index in any given area, the drier the area is.
The moisture classification includes climatic classes with descriptors such as hyperhumid, humid, subhumid, subarid, semi-arid (values of −20 to −40), and arid (values below −40). Humid regions experience more precipitation than evaporation each year, while arid regions experience greater evaporation than precipitation on an annual basis. A total of 33 percent of the Earth's landmass is considered either arid or semi-arid, including southwest North America, southwest South America, most of northern and a small part of southern Africa, southwest and portions of eastern Asia, as well as much of Australia. Studies suggest that precipitation effectiveness (PE) within the Thornthwaite moisture index is overestimated in the summer and underestimated in the winter. This index can be effectively used to determine the number of herbivore and mammal species numbers within a given area. The index is also used in studies of climate change.
Thermal classifications within the Thornthwaite scheme include microthermal, mesothermal, and megathermal regimes. A microthermal climate is one of low annual mean temperatures, generally between 0 °C (32 °F) and 14 °C (57 °F) which experiences short summers and has a potential evaporation between 14 centimetres (5.5 in) and 43 centimetres (17 in). A mesothermal climate lacks persistent heat or persistent cold, with potential evaporation between 57 centimetres (22 in) and 114 centimetres (45 in). A megathermal climate is one with persistent high temperatures and abundant rainfall, with potential annual evaporation in excess of 114 centimetres (45 in).

Details of the modern climate record are known through the taking of measurements from such weather instruments as thermometers, barometers, and anemometers during the past few centuries. The instruments used to study weather over the modern time scale, their known error, their immediate environment, and their exposure have changed over the years, which must be considered when studying the climate of centuries past.

Paleoclimatology is the study of past climate over a great period of the Earth's history. It uses evidence from ice sheets, tree rings, sediments, coral, and rocks to determine the past state of the climate. It demonstrates periods of stability and periods of change and can indicate whether changes follow patterns such as regular cycles.

Climate change is the variation in global or regional climates over time. It reflects changes in the variability or average state of the atmosphere over time scales ranging from decades to millions of years. These changes can be caused by processes internal to the Earth, external forces (e.g. variations in sunlight intensity) or, more recently, human activities.

In recent usage, especially in the context of environmental policy, the term "climate change" often refers only to changes in modern climate, including the rise in average surface temperature known as global warming. In some cases, the term is also used with a presumption of human causation, as in the United Nations Framework Convention on Climate Change (UNFCCC). The UNFCCC uses "climate variability" for non-human caused variations.
Earth has undergone periodic climate shifts in the past, including four major ice ages. These consisting of glacial periods where conditions are colder than normal, separated by interglacial periods. The accumulation of snow and ice during a glacial period increases the surface albedo, reflecting more of the Sun's energy into space and maintaining a lower atmospheric temperature. Increases in greenhouse gases, such as by volcanic activity, can increase the global temperature and produce an interglacial period. Suggested causes of ice age periods include the positions of the continents, variations in the Earth's orbit, changes in the solar output, and volcanism.

Climate models use quantitative methods to simulate the interactions of the atmosphere, oceans, land surface and ice. They are used for a variety of purposes; from the study of the dynamics of the weather and climate system, to projections of future climate. All climate models balance, or very nearly balance, incoming energy as short wave (including visible) electromagnetic radiation to the earth with outgoing energy as long wave (infrared) electromagnetic radiation from the earth. Any imbalance results in a change in the average temperature of the earth.
The most talked-about applications of these models in recent years have been their use to infer the consequences of increasing greenhouse gases in the atmosphere, primarily carbon dioxide (see greenhouse gas). These models predict an upward trend in the global mean surface temperature, with the most rapid increase in temperature being projected for the higher latitudes of the Northern Hemisphere.
Models can range from relatively simple to quite complex:
Simple radiant heat transfer model that treats the earth as a single point and averages outgoing energy
this can be expanded vertically (radiative-convective models), or horizontally
finally, (coupled) atmosphere–ocean–sea ice global climate models discretise and solve the full equations for mass and energy transfer and radiant exchange.
Climate forecasting is a way by some scientists are using to predict climate change. In 1997 the prediction division of the International Research Institute for Climate and Society at Columbia University began generating seasonal climate forecasts on a real-time basis. To produce these forecasts an extensive suite of forecasting tools was developed, including a multimodel ensemble approach that required thorough validation of each model's accuracy level in simulating interannual climate variability.

The Study of Climate on Alien Worlds; Characterizing atmospheres beyond our Solar System is now within our reach Kevin Heng July–August 2012 American Scientist
Reumert, Johannes: "Vahls climatic divisions. An explanation" (Geografisk Tidsskrift, Band 48; 1946)

NOAA Climate Services Portal
NOAA State of the Climate
NASA's Climate change and global warming portal
Climate Models and modeling groups
Climate Prediction Project
ESPERE Climate Encyclopaedia
Climate index and mode information – Arctic
A current view of the Bering Sea Ecosystem and Climate
Climate: Data and charts for world and US locations
MIL-HDBK-310, Global Climate Data U.S. Department of Defense – Aid to derive natural environmental design criteria
IPCC Data Distribution Centre – Climate data and guidance on use.
HistoricalClimatology.com – Past, present and future climates – 2013.
Globalclimatemonitor – Contains climatic information from 1901.
ClimateCharts – Webapplication to generate climate charts for recent and historical data.
International Disaster Database
Paris Climate ConferenceWeather is the state of the atmosphere, to the degree that it is hot or cold, wet or dry, calm or stormy, clear or cloudy. Most weather phenomena occur in the troposphere, just below the stratosphere. Weather refers to day-to-day temperature and precipitation activity, whereas climate is the term for the statistics of atmospheric conditions over longer periods of time. When used without qualification, "weather" is generally understood to mean the weather of Earth.
Weather is driven by air pressure, temperature and moisture differences between one place and another. These differences can occur due to the sun's angle at any particular spot, which varies by latitude from the tropics. The strong temperature contrast between polar and tropical air gives rise to the jet stream. Weather systems in the mid-latitudes, such as extratropical cyclones, are caused by instabilities of the jet stream flow. Because the Earth's axis is tilted relative to its orbital plane, sunlight is incident at different angles at different times of the year. On Earth's surface, temperatures usually range ±40 °C (−40 °F to 100 °F) annually. Over thousands of years, changes in Earth's orbit can affect the amount and distribution of solar energy received by the Earth, thus influencing long-term climate and global climate change.
Surface temperature differences in turn cause pressure differences. Higher altitudes are cooler than lower altitudes due to differences in compressional heating. Weather forecasting is the application of science and technology to predict the state of the atmosphere for a future time and a given location. The system is a chaotic system; so small changes to one part of the system can grow to have large effects on the system as a whole. Human attempts to control the weather have occurred throughout human history, and there is evidence that human activities such as agriculture and industry have modified weather patterns.
Studying how the weather works on other planets has been helpful in understanding how weather works on Earth. A famous landmark in the Solar System, Jupiter's Great Red Spot, is an anticyclonic storm known to have existed for at least 300 years. However, weather is not limited to planetary bodies. A star's corona is constantly being lost to space, creating what is essentially a very thin atmosphere throughout the Solar System. The movement of mass ejected from the Sun is known as the solar wind.

On Earth, the common weather phenomena include wind, cloud, rain, snow, fog and dust storms. Less common events include natural disasters such as tornadoes, hurricanes, typhoons and ice storms. Almost all familiar weather phenomena occur in the troposphere (the lower part of the atmosphere). Weather does occur in the stratosphere and can affect weather lower down in the troposphere, but the exact mechanisms are poorly understood.
Weather occurs primarily due to air pressure, temperature and moisture differences between one place to another. These differences can occur due to the sun angle at any particular spot, which varies by latitude from the tropics. In other words, the farther from the tropics one lies, the lower the sun angle is, which causes those locations to be cooler due to the indirect sunlight. The strong temperature contrast between polar and tropical air gives rise to the jet stream. Weather systems in the mid-latitudes, such as extratropical cyclones, are caused by instabilities of the jet stream flow (see baroclinity). Weather systems in the tropics, such as monsoons or organized thunderstorm systems, are caused by different processes.

Because the Earth's axis is tilted relative to its orbital plane, sunlight is incident at different angles at different times of the year. In June the Northern Hemisphere is tilted towards the sun, so at any given Northern Hemisphere latitude sunlight falls more directly on that spot than in December (see Effect of sun angle on climate). This effect causes seasons. Over thousands to hundreds of thousands of years, changes in Earth's orbital parameters affect the amount and distribution of solar energy received by the Earth and influence long-term climate. (See Milankovitch cycles).
The uneven solar heating (the formation of zones of temperature and moisture gradients, or frontogenesis) can also be due to the weather itself in the form of cloudiness and precipitation. Higher altitudes are typically cooler than lower altitudes, which is explained by the lapse rate. In some situations, the temperature actually increases with height. This phenomenon is known as an inversion and can cause mountaintops to be warmer than the valleys below. Inversions can lead to the formation of fog and often act as a cap that suppresses thunderstorm development. On local scales, temperature differences can occur because different surfaces (such as oceans, forests, ice sheets, or man-made objects) have differing physical characteristics such as reflectivity, roughness, or moisture content.
Surface temperature differences in turn cause pressure differences. A hot surface warms the air above it causing it to expand and lower the density and the resulting surface air pressure. The resulting horizontal pressure gradient moves the air from higher to lower pressure regions, creating a wind, and the Earth's rotation then causes deflection of this air flow due to the Coriolis effect. The simple systems thus formed can then display emergent behaviour to produce more complex systems and thus other weather phenomena. Large scale examples include the Hadley cell while a smaller scale example would be coastal breezes.
The atmosphere is a chaotic system, so small changes to one part of the system can grow to have large effects on the system as a whole. This makes it difficult to accurately predict weather more than a few days in advance, though weather forecasters are continually working to extend this limit through the scientific study of weather, meteorology. It is theoretically impossible to make useful day-to-day predictions more than about two weeks ahead, imposing an upper limit to potential for improved prediction skill.

Weather is one of the fundamental processes that shape the Earth. The process of weathering breaks down the rocks and soils into smaller fragments and then into their constituent substances. During rains precipitation, the water droplets absorb and dissolve carbon dioxide from the surrounding air. This causes the rainwater to be slightly acidic, which aids the erosive properties of water. The released sediment and chemicals are then free to take part in chemical reactions that can affect the surface further (such as acid rain), and sodium and chloride ions (salt) deposited in the seas/oceans. The sediment may reform in time and by geological forces into other rocks and soils. In this way, weather plays a major role in erosion of the surface.

EUMETSAT created "A Year in Weather 2015" a narrated video of the earth's weather photographed from weather satellites for the entire year 2015. Geostationary satellite photographs from EUMETSAT, the Japan Meteorological Agency and the National Oceanic and Atmospheric Administration were assembled to show weather changing on earth for 365 days in a time lapse video.

Weather, seen from an anthropological perspective, is something all humans in the world constantly experience through their senses, at least while being outside. There are socially and scientifically constructed understandings of what weather is, what makes it change, the effect it has on humans in different situations, etc. Therefore, weather is something people often communicate about.

Weather has played a large and sometimes direct part in human history. Aside from climatic changes that have caused the gradual drift of populations (for example the desertification of the Middle East, and the formation of land bridges during glacial periods), extreme weather events have caused smaller scale population movements and intruded directly in historical events. One such event is the saving of Japan from invasion by the Mongol fleet of Kublai Khan by the Kamikaze winds in 1281. French claims to Florida came to an end in 1565 when a hurricane destroyed the French fleet, allowing Spain to conquer Fort Caroline. More recently, Hurricane Katrina redistributed over one million people from the central Gulf coast elsewhere across the United States, becoming the largest diaspora in the history of the United States.
The Little Ice Age caused crop failures and famines in Europe. The 1690s saw the worst famine in France since the Middle Ages. Finland suffered a severe famine in 1696–1697, during which about one-third of the Finnish population died.

Weather forecasting is the application of science and technology to predict the state of the atmosphere for a future time and a given location. Human beings have attempted to predict the weather informally for millennia, and formally since at least the nineteenth century. Weather forecasts are made by collecting quantitative data about the current state of the atmosphere and using scientific understanding of atmospheric processes to project how the atmosphere will evolve.
Once an all-human endeavor based mainly upon changes in barometric pressure, current weather conditions, and sky condition, forecast models are now used to determine future conditions. Human input is still required to pick the best possible forecast model to base the forecast upon, which involves pattern recognition skills, teleconnections, knowledge of model performance, and knowledge of model biases. The chaotic nature of the atmosphere, the massive computational power required to solve the equations that describe the atmosphere, error involved in measuring the initial conditions, and an incomplete understanding of atmospheric processes mean that forecasts become less accurate as the difference in current time and the time for which the forecast is being made (the range of the forecast) increases. The use of ensembles and model consensus helps to narrow the error and pick the most likely outcome.
There are a variety of end users to weather forecasts. Weather warnings are important forecasts because they are used to protect life and property. Forecasts based on temperature and precipitation are important to agriculture, and therefore to commodity traders within stock markets. Temperature forecasts are used by utility companies to estimate demand over coming days. On an everyday basis, people use weather forecasts to determine what to wear on a given day. Since outdoor activities are severely curtailed by heavy rain, snow and the wind chill, forecasts can be used to plan activities around these events, and to plan ahead and survive them.

The aspiration to control the weather is evident throughout human history: from ancient rituals intended to bring rain for crops to the U.S. Military Operation Popeye, an attempt to disrupt supply lines by lengthening the North Vietnamese monsoon. The most successful attempts at influencing weather involve cloud seeding; they include the fog- and low stratus dispersion techniques employed by major airports, techniques used to increase winter precipitation over mountains, and techniques to suppress hail. A recent example of weather control was China's preparation for the 2008 Summer Olympic Games. China shot 1,104 rain dispersal rockets from 21 sites in the city of Beijing in an effort to keep rain away from the opening ceremony of the games on 8 August 2008. Guo Hu, head of the Beijing Municipal Meteorological Bureau (BMB), confirmed the success of the operation with 100 millimeters falling in Baoding City of Hebei Province, to the southwest and Beijing's Fangshan District recording a rainfall of 25 millimeters.
Whereas there is inconclusive evidence for these techniques' efficacy, there is extensive evidence that human activity such as agriculture and industry results in inadvertent weather modification:
Acid rain, caused by industrial emission of sulfur dioxide and nitrogen oxides into the atmosphere, adversely affects freshwater lakes, vegetation, and structures.
Anthropogenic pollutants reduce air quality and visibility.
Climate change caused by human activities that emit greenhouse gases into the air is expected to affect the frequency of extreme weather events such as drought, extreme temperatures, flooding, high winds, and severe storms.
Heat, generated by large metropolitan areas have been shown to minutely affect nearby weather, even at distances as far as 1,600 kilometres (990 mi).
The effects of inadvertent weather modification may pose serious threats to many aspects of civilization, including ecosystems, natural resources, food and fiber production, economic development, and human health.

Microscale meteorology is the study of short-lived atmospheric phenomena smaller than mesoscale, about 1 km or less. These two branches of meteorology are sometimes grouped together as "mesoscale and microscale meteorology" (MMM) and together study all phenomena smaller than synoptic scale; that is they study features generally too small to be depicted on a weather map. These include small and generally fleeting cloud "puffs" and other small cloud features.

On Earth, temperatures usually range ±40 °C (100 °F to −40 °F) annually. The range of climates and latitudes across the planet can offer extremes of temperature outside this range. The coldest air temperature ever recorded on Earth is −89.2 °C (−128.6 °F), at Vostok Station, Antarctica on 21 July 1983. The hottest air temperature ever recorded was 57.7 °C (135.9 °F) at 'Aziziya, Libya, on 13 September 1922, but that reading is queried. The highest recorded average annual temperature was 34.4 °C (93.9 °F) at Dallol, Ethiopia. The coldest recorded average annual temperature was −55.1 °C (−67.2 °F) at Vostok Station, Antarctica.
The coldest average annual temperature in a permanently inhabited location is at Eureka, Nunavut, in Canada, where the annual average temperature is −19.7 °C (−3.5 °F).

Studying how the weather works on other planets has been seen as helpful in understanding how it works on Earth. Weather on other planets follows many of the same physical principles as weather on Earth, but occurs on different scales and in atmospheres having different chemical composition. The Cassini–Huygens mission to Titan discovered clouds formed from methane or ethane which deposit rain composed of liquid methane and other organic compounds. Earth's atmosphere includes six latitudinal circulation zones, three in each hemisphere. In contrast, Jupiter's banded appearance shows many such zones, Titan has a single jet stream near the 50th parallel north latitude, and Venus has a single jet near the equator.
One of the most famous landmarks in the Solar System, Jupiter's Great Red Spot, is an anticyclonic storm known to have existed for at least 300 years. On other gas giants, the lack of a surface allows the wind to reach enormous speeds: gusts of up to 600 metres per second (about 2,100 km/h or 1,300 mph) have been measured on the planet Neptune. This has created a puzzle for planetary scientists. The weather is ultimately created by solar energy and the amount of energy received by Neptune is only about  1⁄900 of that received by Earth, yet the intensity of weather phenomena on Neptune is far greater than on Earth. The strongest planetary winds discovered so far are on the extrasolar planet HD 189733 b, which is thought to have easterly winds moving at more than 9,600 kilometres per hour (6,000 mph).

Weather is not limited to planetary bodies. Like all stars, the sun's corona is constantly being lost to space, creating what is essentially a very thin atmosphere throughout the Solar System. The movement of mass ejected from the Sun is known as the solar wind. Inconsistencies in this wind and larger events on the surface of the star, such as coronal mass ejections, form a system that has features analogous to conventional weather systems (such as pressure and wind) and is generally known as space weather. Coronal mass ejections have been tracked as far out in the solar system as Saturn. The activity of this system can affect planetary atmospheres and occasionally surfaces. The interaction of the solar wind with the terrestrial atmosphere can produce spectacular aurorae, and can play havoc with electrically sensitive systems such as electricity grids and radio signals.

Weather station
Outline of meteorology

Chemistry is a branch of physical science that studies the composition, structure, properties and change of matter. Chemistry includes topics such as the properties of individual atoms, how atoms form chemical bonds to create chemical compounds, the interactions of substances through intermolecular forces that give matter its general properties, and the interactions between substances through chemical reactions to form different substances.
Chemistry is sometimes called the central science because it bridges other natural sciences, including physics, geology and biology. For the differences between chemistry and physics see comparison of chemistry and physics.
Scholars disagree about the etymology of the word chemistry. The history of chemistry can be traced to alchemy, which had been practiced for several millennia in various parts of the world.

The word chemistry comes from alchemy, which referred to an earlier set of practices that encompassed elements of chemistry, metallurgy, philosophy, astrology, astronomy, mysticism and medicine. It is often seen as linked to the quest to turn lead or another common starting material into gold, though in ancient times the study encompassed many of the questions of modern chemistry being defined as the study of the composition of waters, movement, growth, embodying, disembodying, drawing the spirits from bodies and bonding the spirits within bodies by the early 4th century Greek-Egyptian alchemist Zosimos. An alchemist was called a 'chemist' in popular speech, and later the suffix "-ry" was added to this to describe the art of the chemist as "chemistry".
The modern word alchemy in turn is derived from the Arabic word al-kīmīā (الکیمیاء). In origin, the term is borrowed from the Greek χημία or χημεία. This may have Egyptian origins since al-kīmīā is derived from the Greek χημία, which is in turn derived from the word Chemi or Kimi, which is the ancient name of Egypt in Egyptian. Alternately, al-kīmīā may derive from χημεία, meaning "cast together".

In retrospect, the definition of chemistry has changed over time, as new discoveries and theories add to the functionality of the science. The term "chymistry", in the view of noted scientist Robert Boyle in 1661, meant the subject of the material principles of mixed bodies. In 1663 the chemist Christopher Glaser described "chymistry" as a scientific art, by which one learns to dissolve bodies, and draw from them the different substances on their composition, and how to unite them again, and exalt them to a higher perfection.
The 1730 definition of the word "chemistry", as used by Georg Ernst Stahl, meant the art of resolving mixed, compound, or aggregate bodies into their principles; and of composing such bodies from those principles. In 1837, Jean-Baptiste Dumas considered the word "chemistry" to refer to the science concerned with the laws and effects of molecular forces. This definition further evolved until, in 1947, it came to mean the science of substances: their structure, their properties, and the reactions that change them into other substances - a characterization accepted by Linus Pauling. More recently, in 1998, Professor Raymond Chang broadened the definition of "chemistry" to mean the study of matter and the changes it undergoes.

Early civilizations, such as the Egyptians Babylonians, Indians amassed practical knowledge concerning the arts of metallurgy, pottery and dyes, but didn't develop a systematic theory.
A basic chemical hypothesis first emerged in Classical Greece with the theory of four elements as propounded definitively by Aristotle stating that fire, air, earth and water were the fundamental elements from which everything is formed as a combination. Greek atomism dates back to 440 BC, arising in works by philosophers such as Democritus and Epicurus. In 50 BC, the Roman philosopher Lucretius expanded upon the theory in his book De rerum natura (On The Nature of Things). Unlike modern concepts of science, Greek atomism was purely philosophical in nature, with little concern for empirical observations and no concern for chemical experiments.
In the Hellenistic world the art of alchemy first proliferated, mingling magic and occultism into the study of natural substances with the ultimate goal of transmuting elements into gold and discovering the elixir of eternal life. Work, particularly the development of distillation, continued in the early Byzantine period with the most famous practitioner being the 4th century Greek-Egyptian Zosimos of Panopolis. Alchemy continued to be developed and practised throughout the Arab world after the Muslim conquests, and from there, and from the Byzantine remnants, diffused into medieval and Renaissance Europe through Latin translations. Some influential Muslim chemists, Abū al-Rayhān al-Bīrūnī, Avicenna and Al-Kindi refuted the theories of alchemy, particularly the theory of the transmutation of metals; and al-Tusi described a version of the conservation of mass, noting that a body of matter is able to change but is not able to disappear.

The development of the modern scientific method was slow and arduous, but an early scientific method for chemistry began emerging among early Muslim chemists, beginning with the 9th century Persian or Arabian chemist Jābir ibn Hayyān (known as "Geber" in Europe), who is sometimes referred to as "the father of chemistry". He introduced a systematic and experimental approach to scientific research based in the laboratory, in contrast to the ancient Greek and Egyptian alchemists whose works were largely allegorical and often unintelligble. Under the influence of the new empirical methods propounded by Sir Francis Bacon and others, a group of chemists at Oxford, Robert Boyle, Robert Hooke and John Mayow began to reshape the old alchemical traditions into a scientific discipline. Boyle in particular is regarded as the founding father of chemistry due to his most important work, the classic chemistry text The Sceptical Chymist where the differentiation is made between the claims of alchemy and the empirical scientific discoveries of the new chemistry. He formulated Boyle's law, rejected the classical "four elements" and proposed a mechanistic alternative of atoms and chemical reactions that could be subject to rigorous experiment.

The theory of phlogiston (a substance at the root of all combustion) was propounded by the German Georg Ernst Stahl in the early 18th century and was only overturned by the end of the century by the French chemist Antoine Lavoisier, the chemical analogue of Newton in physics; who did more than any other to establish the new science on proper theoretical footing, by elucidating the principle of conservation of mass and developing a new system of chemical nomenclature used to this day.
Before his work, though, many important discoveries had been made, specifically relating to the nature of 'air' which was discovered to be composed of many different gases. The Scottish chemist Joseph Black (the first experimental chemist) and the Dutchman J. B. van Helmont discovered carbon dioxide, or what Black called 'fixed air' in 1754; Henry Cavendish discovered hydrogen and elucidated its properties and Joseph Priestley and, independently, Carl Wilhelm Scheele isolated pure oxygen.

English scientist John Dalton proposed the modern theory of atoms; that all substances are composed of indivisible 'atoms' of matter and that different atoms have varying atomic weights.
The development of the electrochemical theory of chemical combinations occurred in the early 19th century as the result of the work of two scientists in particular, J. J. Berzelius and Humphry Davy, made possible by the prior invention of the voltaic pile by Alessandro Volta. Davy discovered nine new elements including the alkali metals by extracting them from their oxides with electric current.
British William Prout first proposed ordering all the elements by their atomic weight as all atoms had a weight that was an exact multiple of the atomic weight of hydrogen. J. A. R. Newlands devised an early table of elements, which was then developed into the modern periodic table of elements in the 1860s by Dmitri Mendeleev and independently by several other scientists including Julius Lothar Meyer. The inert gases, later called the noble gases were discovered by William Ramsay in collaboration with Lord Rayleigh at the end of the century, thereby filling in the basic structure of the table.
Organic chemistry was developed by Justus von Liebig and others, following Friedrich Wöhler's synthesis of urea which proved that living organisms were, in theory, reducible to chemistry. Other crucial 19th century advances were; an understanding of valence bonding (Edward Frankland in 1852) and the application of thermodynamics to chemistry (J. W. Gibbs and Svante Arrhenius in the 1870s).

At the turn of the twentieth century the theoretical underpinnings of chemistry were finally understood due to a series of remarkable discoveries that succeeded in probing and discovering the very nature of the internal structure of atoms. In 1897, J. J. Thomson of Cambridge University discovered the electron and soon after the French scientist Becquerel as well as the couple Pierre and Marie Curie investigated the phenomenon of radioactivity. In a series of pioneering scattering experiments Ernest Rutherford at the University of Manchester discovered the internal structure of the atom and the existence of the proton, classified and explained the different types of radioactivity and successfully transmuted the first element by bombarding nitrogen with alpha particles.
His work on atomic structure was improved on by his students, the Danish physicist Niels Bohr and Henry Moseley. The electronic theory of chemical bonds and molecular orbitals was developed by the American scientists Linus Pauling and Gilbert N. Lewis.
The year 2011 was declared by the United Nations as the International Year of Chemistry. It was an initiative of the International Union of Pure and Applied Chemistry, and of the United Nations Educational, Scientific, and Cultural Organization and involves chemical societies, academics, and institutions worldwide and relied on individual initiatives to organize local and regional activities.

The current model of atomic structure is the quantum mechanical model. Traditional chemistry starts with the study of elementary particles, atoms, molecules, substances, metals, crystals and other aggregates of matter. This matter can be studied in solid, liquid, or gas states, in isolation or in combination. The interactions, reactions and transformations that are studied in chemistry are usually the result of interactions between atoms, leading to rearrangements of the chemical bonds which hold atoms together. Such behaviors are studied in a chemistry laboratory.
The chemistry laboratory stereotypically uses various forms of laboratory glassware. However glassware is not central to chemistry, and a great deal of experimental (as well as applied/industrial) chemistry is done without it.
A chemical reaction is a transformation of some substances into one or more different substances. The basis of such a chemical transformation is the rearrangement of electrons in the chemical bonds between atoms. It can be symbolically depicted through a chemical equation, which usually involves atoms as subjects. The number of atoms on the left and the right in the equation for a chemical transformation is equal. (When the number of atoms on either side is unequal, the transformation is referred to as a nuclear reaction or radioactive decay.) The type of chemical reactions a substance may undergo and the energy changes that may accompany it are constrained by certain basic rules, known as chemical laws.
Energy and entropy considerations are invariably important in almost all chemical studies. Chemical substances are classified in terms of their structure, phase, as well as their chemical compositions. They can be analyzed using the tools of chemical analysis, e.g. spectroscopy and chromatography. Scientists engaged in chemical research are known as chemists. Most chemists specialize in one or more sub-disciplines. Several concepts are essential for the study of chemistry; some of them are:

In chemistry, matter is defined as anything that has rest mass and volume (it takes up space) and is made up of particles. The particles that make up matter have rest mass as well - not all particles have rest mass, such as the photon. Matter can be a pure chemical substance or a mixture of substances.

The atom is the basic unit of chemistry. It consists of a dense core called the atomic nucleus surrounded by a space called the electron cloud. The nucleus is made up of positively charged protons and uncharged neutrons (together called nucleons), while the electron cloud consists of negatively charged electrons which orbit the nucleus. In a neutral atom, the negatively charged electrons balance out the positive charge of the protons. The nucleus is dense; the mass of a nucleon is 1,836 times that of an electron, yet the radius of an atom is about 10,000 times that of its nucleus.
The atom is also the smallest entity that can be envisaged to retain the chemical properties of the element, such as electronegativity, ionization potential, preferred oxidation state(s), coordination number, and preferred types of bonds to form (e.g., metallic, ionic, covalent).

A chemical element is a pure substance which is composed of a single type of atom, characterized by its particular number of protons in the nuclei of its atoms, known as the atomic number and represented by the symbol Z. The mass number is the sum of the number of protons and neutrons in a nucleus. Although all the nuclei of all atoms belonging to one element will have the same atomic number, they may not necessarily have the same mass number; atoms of an element which have different mass numbers are known as isotopes. For example, all atoms with 6 protons in their nuclei are atoms of the chemical element carbon, but atoms of carbon may have mass numbers of 12 or 13.
The standard presentation of the chemical elements is in the periodic table, which orders elements by atomic number. The periodic table is arranged in groups, or columns, and periods, or rows. The periodic table is useful in identifying periodic trends.

A compound is a pure chemical substance composed of more than one element. The properties of a compound bear little similarity to those of its elements. The standard nomenclature of compounds is set by the International Union of Pure and Applied Chemistry (IUPAC). Organic compounds are named according to the organic nomenclature system. Inorganic compounds are named according to the inorganic nomenclature system. In addition the Chemical Abstracts Service has devised a method to index chemical substances. In this scheme each chemical substance is identifiable by a number known as its CAS registry number.

A molecule is the smallest indivisible portion of a pure chemical substance that has its unique set of chemical properties, that is, its potential to undergo a certain set of chemical reactions with other substances. However, this definition only works well for substances that are composed of molecules, which is not true of many substances (see below). Molecules are typically a set of atoms bound together by covalent bonds, such that the structure is electrically neutral and all valence electrons are paired with other electrons either in bonds or in lone pairs.
Thus, molecules exist as electrically neutral units, unlike ions. When this rule is broken, giving the "molecule" a charge, the result is sometimes named a molecular ion or a polyatomic ion. However, the discrete and separate nature of the molecular concept usually requires that molecular ions be present only in well-separated form, such as a directed beam in a vacuum in a mass spectrometer. Charged polyatomic collections residing in solids (for example, common sulfate or nitrate ions) are generally not considered "molecules" in chemistry.

The "inert" or noble gas elements (helium, neon, argon, krypton, xenon and radon) are composed of lone atoms as their smallest discrete unit, but the other isolated chemical elements consist of either molecules or networks of atoms bonded to each other in some way. Identifiable molecules compose familiar substances such as water, air, and many organic compounds like alcohol, sugar, gasoline, and the various pharmaceuticals.
However, not all substances or chemical compounds consist of discrete molecules, and indeed most of the solid substances that make up the solid crust, mantle, and core of the Earth are chemical compounds without molecules. These other types of substances, such as ionic compounds and network solids, are organized in such a way as to lack the existence of identifiable molecules per se. Instead, these substances are discussed in terms of formula units or unit cells as the smallest repeating structure within the substance. Examples of such substances are mineral salts (such as table salt), solids like carbon and diamond, metals, and familiar silica and silicate minerals such as quartz and granite.
One of the main characteristics of a molecule is its geometry often called its structure. While the structure of diatomic, triatomic or tetra atomic molecules may be trivial, (linear, angular pyramidal etc.) the structure of polyatomic molecules, that are constituted of more than six atoms (of several elements) can be crucial for its chemical nature.

A chemical substance is a kind of matter with a definite composition and set of properties. A collection of substances is called a mixture. Examples of mixtures are air and alloys.

The mole is a unit of measurement that denotes an amount of substance (also called chemical amount). The mole is defined as the number of atoms found in exactly 0.012 kilogram (or 12 grams) of carbon-12, where the carbon-12 atoms are unbound, at rest and in their ground state. The number of entities per mole is known as the Avogadro constant, and is determined empirically to be approximately 6.022×1023 mol−1. Molar concentration is the amount of a particular substance per volume of solution, and is commonly reported in moldm−3.

In addition to the specific chemical properties that distinguish different chemical classifications, chemicals can exist in several phases. For the most part, the chemical classifications are independent of these bulk phase classifications; however, some more exotic phases are incompatible with certain chemical properties. A phase is a set of states of a chemical system that have similar bulk structural properties, over a range of conditions, such as pressure or temperature.
Physical properties, such as density and refractive index tend to fall within values characteristic of the phase. The phase of matter is defined by the phase transition, which is when energy put into or taken out of the system goes into rearranging the structure of the system, instead of changing the bulk conditions.
Sometimes the distinction between phases can be continuous instead of having a discrete boundary, in this case the matter is considered to be in a supercritical state. When three states meet based on the conditions, it is known as a triple point and since this is invariant, it is a convenient way to define a set of conditions.
The most familiar examples of phases are solids, liquids, and gases. Many substances exhibit multiple solid phases. For example, there are three phases of solid iron (alpha, gamma, and delta) that vary based on temperature and pressure. A principal difference between solid phases is the crystal structure, or arrangement, of the atoms. Another phase commonly encountered in the study of chemistry is the aqueous phase, which is the state of substances dissolved in aqueous solution (that is, in water).
Less familiar phases include plasmas, Bose–Einstein condensates and fermionic condensates and the paramagnetic and ferromagnetic phases of magnetic materials. While most familiar phases deal with three-dimensional systems, it is also possible to define analogs in two-dimensional systems, which has received attention for its relevance to systems in biology.

Atoms sticking together in molecules or crystals are said to be bonded with one another. A chemical bond may be visualized as the multipole balance between the positive charges in the nuclei and the negative charges oscillating about them. More than simple attraction and repulsion, the energies and distributions characterize the availability of an electron to bond to another atom.
A chemical bond can be a covalent bond, an ionic bond, a hydrogen bond or just because of Van der Waals force. Each of these kinds of bonds is ascribed to some potential. These potentials create the interactions which hold atoms together in molecules or crystals. In many simple compounds, valence bond theory, the Valence Shell Electron Pair Repulsion model (VSEPR), and the concept of oxidation number can be used to explain molecular structure and composition.
An ionic bond is formed when a metal loses one or more of its electrons, becoming a positively charged cation, and the electrons are then gained by the non-metal atom, becoming a negatively charged anion. The two oppositely charged ions attract one another, and the ionic bond is the electrostatic force of attraction between them. For example, sodium (Na), a metal, loses one electron to become an Na+ cation while chlorine (Cl), a non-metal, gains this electron to become Cl−. The ions are held together due to electrostatic attraction, and that compound sodium chloride (NaCl), or common table salt, is formed.

In a covalent bond, one or more pairs of valence electrons are shared by two atoms: the resulting electrically neutral group of bonded atoms is termed a molecule. Atoms will share valence electrons in such a way as to create a noble gas electron configuration (eight electrons in their outermost shell) for each atom. Atoms that tend to combine in such a way that they each have eight electrons in their valence shell are said to follow the octet rule. However, some elements like hydrogen and lithium need only two electrons in their outermost shell to attain this stable configuration; these atoms are said to follow the duet rule, and in this way they are reaching the electron configuration of the noble gas helium, which has two electrons in its outer shell.
Similarly, theories from classical physics can be used to predict many ionic structures. With more complicated compounds, such as metal complexes, valence bond theory is less applicable and alternative approaches, such as the molecular orbital theory, are generally used. See diagram on electronic orbitals.

In the context of chemistry, energy is an attribute of a substance as a consequence of its atomic, molecular or aggregate structure. Since a chemical transformation is accompanied by a change in one or more of these kinds of structures, it is invariably accompanied by an increase or decrease of energy of the substances involved. Some energy is transferred between the surroundings and the reactants of the reaction in the form of heat or light; thus the products of a reaction may have more or less energy than the reactants.
A reaction is said to be exergonic if the final state is lower on the energy scale than the initial state; in the case of endergonic reactions the situation is the reverse. A reaction is said to be exothermic if the reaction releases heat to the surroundings; in the case of endothermic reactions, the reaction absorbs heat from the surroundings.
Chemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy. The speed of a chemical reaction (at given temperature T) is related to the activation energy E, by the Boltzmann's population factor




          e

            −
            E

              /

            k
            T




    {\displaystyle e^{-E/kT}}
   - that is the probability of a molecule to have energy greater than or equal to E at the given temperature T. This exponential dependence of a reaction rate on temperature is known as the Arrhenius equation. The activation energy necessary for a chemical reaction to occur can be in the form of heat, light, electricity or mechanical force in the form of ultrasound.
A related concept free energy, which also incorporates entropy considerations, is a very useful means for predicting the feasibility of a reaction and determining the state of equilibrium of a chemical reaction, in chemical thermodynamics. A reaction is feasible only if the total change in the Gibbs free energy is negative,



        Δ
        G
        ≤
        0



    {\displaystyle \Delta G\leq 0\,}
  ; if it is equal to zero the chemical reaction is said to be at equilibrium.
There exist only limited possible states of energy for electrons, atoms and molecules. These are determined by the rules of quantum mechanics, which require quantization of energy of a bound system. The atoms/molecules in a higher energy state are said to be excited. The molecules/atoms of substance in an excited energy state are often much more reactive; that is, more amenable to chemical reactions.
The phase of a substance is invariably determined by its energy and the energy of its surroundings. When the intermolecular forces of a substance are such that the energy of the surroundings is not sufficient to overcome them, it occurs in a more ordered phase like liquid or solid as is the case with water (H2O); a liquid at room temperature because its molecules are bound by hydrogen bonds. Whereas hydrogen sulfide (H2S) is a gas at room temperature and standard pressure, as its molecules are bound by weaker dipole-dipole interactions.
The transfer of energy from one chemical substance to another depends on the size of energy quanta emitted from one substance. However, heat energy is often transferred more easily from almost any substance to another because the phonons responsible for vibrational and rotational energy levels in a substance have much less energy than photons invoked for the electronic energy transfer. Thus, because vibrational and rotational energy levels are more closely spaced than electronic energy levels, heat is more easily transferred between substances relative to light or other forms of electronic energy. For example, ultraviolet electromagnetic radiation is not transferred with as much efficacy from one substance to another as thermal or electrical energy.
The existence of characteristic energy levels for different chemical substances is useful for their identification by the analysis of spectral lines. Different kinds of spectra are often used in chemical spectroscopy, e.g. IR, microwave, NMR, ESR, etc. Spectroscopy is also used to identify the composition of remote objects - like stars and distant galaxies - by analyzing their radiation spectra.

The term chemical energy is often used to indicate the potential of a chemical substance to undergo a transformation through a chemical reaction or to transform other chemical substances.

When a chemical substance is transformed as a result of its interaction with another substance or with energy, a chemical reaction is said to have occurred. A chemical reaction is therefore a concept related to the "reaction" of a substance when it comes in close contact with another, whether as a mixture or a solution; exposure to some form of energy, or both. It results in some energy exchange between the constituents of the reaction as well as with the system environment, which may be designed vessels—often laboratory glassware.
Chemical reactions can result in the formation or dissociation of molecules, that is, molecules breaking apart to form two or more smaller molecules, or rearrangement of atoms within or across molecules. Chemical reactions usually involve the making or breaking of chemical bonds. Oxidation, reduction, dissociation, acid-base neutralization and molecular rearrangement are some of the commonly used kinds of chemical reactions.
A chemical reaction can be symbolically depicted through a chemical equation. While in a non-nuclear chemical reaction the number and kind of atoms on both sides of the equation are equal, for a nuclear reaction this holds true only for the nuclear particles viz. protons and neutrons.
The sequence of steps in which the reorganization of chemical bonds may be taking place in the course of a chemical reaction is called its mechanism. A chemical reaction can be envisioned to take place in a number of steps, each of which may have a different speed. Many reaction intermediates with variable stability can thus be envisaged during the course of a reaction. Reaction mechanisms are proposed to explain the kinetics and the relative product mix of a reaction. Many physical chemists specialize in exploring and proposing the mechanisms of various chemical reactions. Several empirical rules, like the Woodward–Hoffmann rules often come in handy while proposing a mechanism for a chemical reaction.
According to the IUPAC gold book, a chemical reaction is "a process that results in the interconversion of chemical species." Accordingly, a chemical reaction may be an elementary reaction or a stepwise reaction. An additional caveat is made, in that this definition includes cases where the interconversion of conformers is experimentally observable. Such detectable chemical reactions normally involve sets of molecular entities as indicated by this definition, but it is often conceptually convenient to use the term also for changes involving single molecular entities (i.e. 'microscopic chemical events').

An ion is a charged species, an atom or a molecule, that has lost or gained one or more electrons. When an atom loses an electron and thus has more protons than electrons, the atom is a positively charged ion or cation. When an atom gains an electron and thus has more electrons than protons, the atom is a negatively charged ion or anion. Cations and anions can form a crystalline lattice of neutral salts, such as the Na+ and Cl− ions forming sodium chloride, or NaCl. Examples of polyatomic ions that do not split up during acid-base reactions are hydroxide (OH−) and phosphate (PO43−).
Plasma is composed of gaseous matter that has been completely ionized, usually through high temperature.

A substance can often be classified as an acid or a base. There are several different theories which explain acid-base behavior. The simplest is Arrhenius theory, which states than an acid is a substance that produces hydronium ions when it is dissolved in water, and a base is one that produces hydroxide ions when dissolved in water. According to Brønsted–Lowry acid–base theory, acids are substances that donate a positive hydrogen ion to another substance in a chemical reaction; by extension, a base is the substance which receives that hydrogen ion.
A third common theory is Lewis acid-base theory, which is based on the formation of new chemical bonds. Lewis theory explains that an acid is a substance which is capable of accepting a pair of electrons from another substance during the process of bond formation, while a base is a substance which can provide a pair of electrons to form a new bond. According to this theory, the crucial things being exchanged are charges. There are several other ways in which a substance may be classified as an acid or a base, as is evident in the history of this concept.
Acid strength is commonly measured by two methods. One measurement, based on the Arrhenius definition of acidity, is pH, which is a measurement of the hydronium ion concentration in a solution, as expressed on a negative logarithmic scale. Thus, solutions that have a low pH have a high hydronium ion concentration, and can be said to be more acidic. The other measurement, based on the Brønsted–Lowry definition, is the acid dissociation constant (Ka), which measures the relative ability of a substance to act as an acid under the Brønsted–Lowry definition of an acid. That is, substances with a higher Ka are more likely to donate hydrogen ions in chemical reactions than those with lower Ka values.

Redox (reduction-oxidation) reactions include all chemical reactions in which atoms have their oxidation state changed by either gaining electrons (reduction) or losing electrons (oxidation). Substances that have the ability to oxidize other substances are said to be oxidative and are known as oxidizing agents, oxidants or oxidizers. An oxidant removes electrons from another substance. Similarly, substances that have the ability to reduce other substances are said to be reductive and are known as reducing agents, reductants, or reducers.
A reductant transfers electrons to another substance, and is thus oxidized itself. And because it "donates" electrons it is also called an electron donor. Oxidation and reduction properly refer to a change in oxidation number—the actual transfer of electrons may never occur. Thus, oxidation is better defined as an increase in oxidation number, and reduction as a decrease in oxidation number.

Although the concept of equilibrium is widely used across sciences, in the context of chemistry, it arises whenever a number of different states of the chemical composition are possible, as for example, in a mixture of several chemical compounds that can react with one another, or when a substance can be present in more than one kind of phase.
A system of chemical substances at equilibrium, even though having an unchanging composition, is most often not static; molecules of the substances continue to react with one another thus giving rise to a dynamic equilibrium. Thus the concept describes the state in which the parameters such as chemical composition remain unchanged over time.

Chemical reactions are governed by certain laws, which have become fundamental concepts in chemistry. Some of them are:

Chemistry is typically divided into several major sub-disciplines. There are also several main cross-disciplinary and more specialized fields of chemistry.
Analytical chemistry is the analysis of material samples to gain an understanding of their chemical composition and structure. Analytical chemistry incorporates standardized experimental methods in chemistry. These methods may be used in all subdisciplines of chemistry, excluding purely theoretical chemistry.
Biochemistry is the study of the chemicals, chemical reactions and chemical interactions that take place in living organisms. Biochemistry and organic chemistry are closely related, as in medicinal chemistry or neurochemistry. Biochemistry is also associated with molecular biology and genetics.
Inorganic chemistry is the study of the properties and reactions of inorganic compounds. The distinction between organic and inorganic disciplines is not absolute and there is much overlap, most importantly in the sub-discipline of organometallic chemistry.
Materials chemistry is the preparation, characterization, and understanding of substances with a useful function. The field is a new breadth of study in graduate programs, and it integrates elements from all classical areas of chemistry with a focus on fundamental issues that are unique to materials. Primary systems of study include the chemistry of condensed phases (solids, liquids, polymers) and interfaces between different phases.
Neurochemistry is the study of neurochemicals; including transmitters, peptides, proteins, lipids, sugars, and nucleic acids; their interactions, and the roles they play in forming, maintaining, and modifying the nervous system.
Nuclear chemistry is the study of how subatomic particles come together and make nuclei. Modern Transmutation is a large component of nuclear chemistry, and the table of nuclides is an important result and tool for this field.
Organic chemistry is the study of the structure, properties, composition, mechanisms, and reactions of organic compounds. An organic compound is defined as any compound based on a carbon skeleton.
Physical chemistry is the study of the physical and fundamental basis of chemical systems and processes. In particular, the energetics and dynamics of such systems and processes are of interest to physical chemists. Important areas of study include chemical thermodynamics, chemical kinetics, electrochemistry, statistical mechanics, spectroscopy, and more recently, astrochemistry. Physical chemistry has large overlap with molecular physics. Physical chemistry involves the use of infinitesimal calculus in deriving equations. It is usually associated with quantum chemistry and theoretical chemistry. Physical chemistry is a distinct discipline from chemical physics, but again, there is very strong overlap.
Theoretical chemistry is the study of chemistry via fundamental theoretical reasoning (usually within mathematics or physics). In particular the application of quantum mechanics to chemistry is called quantum chemistry. Since the end of the Second World War, the development of computers has allowed a systematic development of computational chemistry, which is the art of developing and applying computer programs for solving chemical problems. Theoretical chemistry has large overlap with (theoretical and experimental) condensed matter physics and molecular physics.
Other disciplines within chemistry are traditionally grouped by the type of matter being studied or the kind of study. These include inorganic chemistry, the study of inorganic matter; organic chemistry, the study of organic (carbon-based) matter; biochemistry, the study of substances found in biological organisms; physical chemistry, the study of chemical processes using physical concepts such as thermodynamics and quantum mechanics; and analytical chemistry, the analysis of material samples to gain an understanding of their chemical composition and structure. Many more specialized disciplines have emerged in recent years, e.g. neurochemistry the chemical study of the nervous system (see subdisciplines).
Other fields include agrochemistry, astrochemistry (and cosmochemistry), atmospheric chemistry, chemical engineering, chemical biology, chemo-informatics, electrochemistry, environmental chemistry, femtochemistry, flavor chemistry, flow chemistry, geochemistry, green chemistry, histochemistry, history of chemistry, hydrogenation chemistry, immunochemistry, marine chemistry, materials science, mathematical chemistry, mechanochemistry, medicinal chemistry, molecular biology, molecular mechanics, nanotechnology, natural product chemistry, oenology, organometallic chemistry, petrochemistry, pharmacology, photochemistry, physical organic chemistry, phytochemistry, polymer chemistry, radiochemistry, solid-state chemistry, sonochemistry, supramolecular chemistry, surface chemistry, synthetic chemistry, thermochemistry, and many others.

The chemical industry represents an important economic activity worldwide. The global top 50 chemical producers in 2013 had sales of US$980.5 billion with a profit margin of 10.3%.

Outline of chemistry
Glossary of chemistry terms
Common chemicals
International Year of Chemistry
List of chemists
List of compounds
List of important publications in chemistry
Comparison of software for molecular mechanics modeling
List of unsolved problems in chemistry
Periodic systems of small molecules
Philosophy of chemistry

Atkins, Peter; de Paula, Julio (2009) [1992]. Elements of Physical Chemistry (5th ed.). New York: Oxford University Press. ISBN 978-0-19-922672-6.
Burrows, Andrew; Holman, John; Parsons, Andrew; Pilling, Gwen; Price, Gareth (2009). Chemistry3. Italy: Oxford University Press. ISBN 978-0-19-927789-6.
Housecroft, Catherine E.; Sharpe, Alan G. (2008) [2001]. Inorganic Chemistry (3rd ed.). Harlow, Essex: Pearson Education. ISBN 978-0-13-175553-6.

Popular reading
Atkins, P.W. Galileo's Finger (Oxford University Press) ISBN 0-19-860941-8
Atkins, P.W. Atkins' Molecules (Cambridge University Press) ISBN 0-521-82397-8
Kean, Sam. The Disappearing Spoon - and other true tales from the Periodic Table (Black Swan) London, 2010 ISBN 978-0-552-77750-6
Levi, Primo The Periodic Table (Penguin Books) [1975] translated from the Italian by Raymond Rosenthal (1984) ISBN 978-0-14-139944-7
Stwertka, A. A Guide to the Elements (Oxford University Press) ISBN 0-19-515027-9
"Dictionary of the History of Ideas". Archived from the original on March 10, 2008.
 "Chemistry". Encyclopædia Britannica. 6 (11th ed.). 1911. pp. 33–76.
Introductory undergraduate text books
Atkins, P.W., Overton, T., Rourke, J., Weller, M. and Armstrong, F. Shriver and Atkins inorganic chemistry (4th edition) 2006 (Oxford University Press) ISBN 0-19-926463-5
Chang, Raymond. Chemistry 6th ed. Boston: James M. Smith, 1998. ISBN 0-07-115221-0.
Clayden, Jonathan; Greeves, Nick; Warren, Stuart; Wothers, Peter (2001). Organic Chemistry (1st ed.). Oxford University Press. ISBN 978-0-19-850346-0.
Voet and Voet Biochemistry (Wiley) ISBN 0-471-58651-X
Advanced undergraduate-level or graduate text books
Atkins, P.W. Physical Chemistry (Oxford University Press) ISBN 0-19-879285-9
Atkins, P.W. et al. Molecular Quantum Mechanics (Oxford University Press)
McWeeny, R. Coulson's Valence (Oxford Science Publications) ISBN 0-19-855144-4
Pauling, L. The Nature of the chemical bond (Cornell University Press) ISBN 0-8014-0333-2
Pauling, L., and Wilson, E. B. Introduction to Quantum Mechanics with Applications to Chemistry (Dover Publications) ISBN 0-486-64871-0
Smart and Moore Solid State Chemistry: An Introduction (Chapman and Hall) ISBN 0-412-40040-5
Stephenson, G. Mathematical Methods for Science Students (Longman) ISBN 0-582-44416-0

General Chemistry principles, patterns and applications.A chemical compound (or just compound if used in the context of chemistry) is an entity consisting of two or more atoms, at least two from different elements, which associate via chemical bonds. There are four types of compounds, depending on how the constituent atoms are held together: molecules held together by covalent bonds, salts held together by ionic bonds, intermetallic compounds held together by metallic bonds, and certain complexes held together by coordinate covalent bonds. Many chemical compounds have a unique numerical identifier assigned by the Chemical Abstracts Service (CAS): its CAS number.
A chemical formula is a way of expressing information about the proportions of atoms that constitute a particular chemical compound, using the standard abbreviations for the chemical elements, and subscripts to indicate the number of atoms involved. For example, water is composed of two hydrogen atoms bonded to one oxygen atom: the chemical formula is H2O.
A compound can be converted to a different chemical composition by interaction with a second chemical compound via a chemical reaction. In this process, bonds between atoms are broken in both of the interacting compounds, and then bonds are reformed so that new associations are made between atoms. Schematically, this reaction could be described as AB + CD --> AC + BD, where A, B, C, and D are each unique atoms; and AB, CD, AC, and BD are each unique compounds.
A chemical element bonded to an identical chemical element is not a chemical compound since only one element, not two different elements, is involved. Examples are the diatomic molecule hydrogen (H2) and the polyatomic molecule sulfur (S8).

Any substance consisting of two or more different types of atoms (chemical elements) in a fixed proportion of its atoms (i.e., stoichiometry) can be termed a chemical compound; the concept is most readily understood when considering pure chemical substances.  It follows from their being composed of fixed proportions of two or more types of atoms that chemical compounds can be converted, via chemical reaction, into compounds or substances each having fewer atoms. In the case of non-stoichiometric compounds, the proportions may be reproducible with regard to their preparation, and give fixed proportions of their component elements, but proportions that are not integral [e.g., for palladium hydride, PdHx (0.02 < x < 0.58)]. Chemical compounds have a unique and defined chemical structure held together in a defined spatial arrangement by chemical bonds. Chemical compounds can be molecular compounds held together by covalent bonds, salts held together by ionic bonds, intermetallic compounds held together by metallic bonds, or the subset of chemical complexes that are held together by coordinate covalent bonds. Pure chemical elements are generally not considered chemical compounds, failing the two or more atom requirement, though they often consist of molecules composed of multiple atoms (such as in the diatomic molecule H2, or the polyatomic molecule S8, etc.).
There is varying and sometimes inconsistent nomenclature differentiating substances, which include truly non-stoichiometric examples, from chemical compounds, which require the fixed ratios. Many solid chemical substances—for example many silicate minerals—are chemical substances, but do not have simple formulae reflecting chemically bonding of elements to one another in fixed ratios; even so, these crystalline substances are often called "non-stoichiometric compounds". It may be argued that they are related to, rather than being chemical compounds, insofar as the variability in their compositions is often due to either the presence of foreign elements trapped within the crystal structure of an otherwise known true chemical compound, or due to perturbations in structure relative to the known compound that arise because of an excess of deficit of the constituent elements at places in its structure; such non-stoichiometric substances form most of the crust and mantle of the Earth. Other compounds regarded as chemically identical may have varying amounts of heavy or light isotopes of the constituent elements, which changes the ratio of elements by mass slightly.

Characteristic properties of compounds include that elements in a compound are present in a definite proportion. For example, the molecule of the compound water is composed of hydrogen and oxygen in a ratio of 2:1. In addition, compounds have a definite set of properties, and the elements that comprise a compound do not retain their original properties. For example, hydrogen, which is combustible and non-supportive of combustion, combines with oxygen, which is non-combustible and supportive of combustion, to produce the compound water, which is non-combustible and non-supportive of combustion.

The physical and chemical properties of compounds differ from those of their constituent elements. This is one of the main criteria that distinguish a compound from a mixture of elements or other substances—in general, a mixture's properties are closely related to, and depend on, the properties of its constituents. Another criterion that distinguishes a compound from a mixture is that constituents of a mixture can usually be separated by simple mechanical means, such as filtering, evaporation, or magnetic force, but components of a compound can be separated only by a chemical reaction. However, mixtures can be created by mechanical means alone, but a compound can be created (either from elements or from other compounds, or a combination of the two) only by a chemical reaction.
Some mixtures are so intimately combined that they have some properties similar to compounds and may easily be mistaken for compounds. One example is alloys. Alloys are made mechanically, most commonly by heating the constituent metals to a liquid state, mixing them thoroughly, and then cooling the mixture quickly so that the constituents are trapped in the base metal. Other examples of compound-like mixtures include intermetallic compounds and solutions of alkali metals in a liquid form of ammonia.

A chemical formula is a way of expressing information about the proportions of atoms that constitute a particular chemical compound, using a single line of chemical element symbols, numbers, and sometimes also other symbols, such as parentheses, dashes, brackets, commas and plus (+) and minus (−) signs.
Compounds may be described using formulas in various formats. For compounds that exist as molecules, the formula for the molecular unit is shown. For polymeric materials, such as minerals and many metal oxides, the empirical formula is normally given, e.g. NaCl for table salt.
The elements in a chemical formula are normally listed in a specific order, called the Hill system. In this system, the carbon atoms (if there are any) are usually listed first, any hydrogen atoms are listed next, and all other elements follow in alphabetical order. If the formula contains no carbon, then all of the elements, including hydrogen, are listed alphabetically. There are, however, several important exceptions to the normal rules. For ionic compounds, the positive ion is almost always listed first and the negative ion is listed second. For oxides, oxygen is usually listed last.
In general, organic acids follow the normal rules with C and H coming first in the formula. For example, the formula for trifluoroacetic acid is usually written as C2HF3O2. More descriptive formulas can convey structural information, such as writing the formula for trifluoroacetic acid as CF3CO2H. On the other hand, the chemical formulas for most inorganic acids and bases are exceptions to the normal rules. They are written according to the rules for ionic compounds (positive first, negative second), but they also follow rules that emphasize their Arrhenius definitions. To be specific, the formula for most inorganic acids begins with hydrogen and the formula for most bases ends with the hydroxide ion (OH−). Formulas for inorganic compounds do not often convey structural information, as illustrated by the common use of the formula H2SO4 for a molecule (sulfuric acid) that contains no H-S bonds. A more descriptive presentation would be O2S(OH)2, but it is almost never written this way.

Compounds may have several possible phases. All compounds can exist as solids, at least at low enough temperatures. Molecular compounds may also exist as liquids, gases, and, in some cases, even plasmas. All compounds decompose upon applying heat. The temperature at which such fragmentation occurs is often called the decomposition temperature. Decomposition temperatures are not sharp and depend on pressure, temperature, and the concentration of each species in the compound.

Chemical element
Chemical revolution
Chemical structure
IUPAC nomenclature
Dictionary of chemical formulas
List of compounds
Addition to pi ligands

Robert Siegfried (2002), From elements to atoms: a history of chemical composition, American Philosophical Society, ISBN 978-0-87169-924-4

A molecule is an electrically neutral group of two or more atoms held together by chemical bonds. Molecules are distinguished from ions by their lack of electrical charge. However, in quantum physics, organic chemistry, and biochemistry, the term molecule is often used less strictly, also being applied to polyatomic ions.
In the kinetic theory of gases, the term molecule is often used for any gaseous particle regardless of its composition. According to this definition, noble gas atoms are considered molecules as they are in fact monoatomic molecules.
A molecule may be homonuclear, that is, it consists of atoms of one chemical element, as with oxygen (O2); or it may be heteronuclear, a chemical compound composed of more than one element, as with water (H2O). Atoms and complexes connected by non-covalent interactions, such as hydrogen bonds or ionic bonds, are generally not considered single molecules.
Molecules as components of matter are common in organic substances (and therefore biochemistry). They also make up most of the oceans and atmosphere. However, the majority of familiar solid substances on Earth, including most of the minerals that make up the crust, mantle, and core of the Earth, contain many chemical bonds, but are not made of identifiable molecules. Also, no typical molecule can be defined for ionic crystals (salts) and covalent crystals (network solids), although these are often composed of repeating unit cells that extend either in a plane (such as in graphene) or three-dimensionally (such as in diamond, quartz, or sodium chloride). The theme of repeated unit-cellular-structure also holds for most condensed phases with metallic bonding, which means that solid metals are also not made of molecules. In glasses (solids that exist in a vitreous disordered state), atoms may also be held together by chemical bonds with no presence of any definable molecule, nor any of the regularity of repeating units that characterizes crystals.

The science of molecules is called molecular chemistry or molecular physics, depending on whether the focus is on chemistry or physics. Molecular chemistry deals with the laws governing the interaction between molecules that results in the formation and breakage of chemical bonds, while molecular physics deals with the laws governing their structure and properties. In practice, however, this distinction is vague. In molecular sciences, a molecule consists of a stable system (bound state) composed of two or more atoms. Polyatomic ions may sometimes be usefully thought of as electrically charged molecules. The term unstable molecule is used for very reactive species, i.e., short-lived assemblies (resonances) of electrons and nuclei, such as radicals, molecular ions, Rydberg molecules, transition states, van der Waals complexes, or systems of colliding atoms as in Bose–Einstein condensate.

According to Merriam-Webster and the Online Etymology Dictionary, the word "molecule" derives from the Latin "moles" or small unit of mass.
Molecule (1794) – "extremely minute particle", from French molécule (1678), from New Latin molecula, diminutive of Latin moles "mass, barrier". A vague meaning at first; the vogue for the word (used until the late 18th century only in Latin form) can be traced to the philosophy of Descartes.
The definition of the molecule has evolved as knowledge of the structure of molecules has increased. Earlier definitions were less precise, defining molecules as the smallest particles of pure chemical substances that still retain their composition and chemical properties. This definition often breaks down since many substances in ordinary experience, such as rocks, salts, and metals, are composed of large crystalline networks of chemically bonded atoms or ions, but are not made of discrete molecules.

Molecules are held together by either covalent bonding or ionic bonding. Several types of non-metal elements exist only as molecules in the environment. For example, hydrogen only exists as hydrogen molecule. A molecule of a compound is made out of two or more elements.

A covalent bond is a chemical bond that involves the sharing of electron pairs between atoms. These electron pairs are termed shared pairs or bonding pairs, and the stable balance of attractive and repulsive forces between atoms, when they share electrons, is termed covalent bonding.

Ionic bonding is a type of chemical bond that involves the electrostatic attraction between oppositely charged ions, and is the primary interaction occurring in ionic compounds. The ions are atoms that have lost one or more electrons (termed cations) and atoms that have gained one or more electrons (termed anions). This transfer of electrons is termed electrovalence in contrast to covalence. In the simplest case, the cation is a metal atom and the anion is a nonmetal atom, but these ions can be of a more complicated nature, e.g. molecular ions like NH4+ or SO42−. In simpler words, an ionic bond is the transfer of electrons from a metal to a non-metal for both atoms to obtain a full valence shell.

Most molecules are far too small to be seen with the naked eye, but there are exceptions. DNA, a macromolecule, can reach macroscopic sizes, as can molecules of many polymers. Molecules commonly used as building blocks for organic synthesis have a dimension of a few angstroms (Å) to several dozen Å, or around one billionth of a meter. Single molecules cannot usually be observed by light (as noted above), but small molecules and even the outlines of individual atoms may be traced in some circumstances by use of an atomic force microscope. Some of the largest molecules are macromolecules or supermolecules.
The smallest molecule is the diatomic hydrogen (H2), with a bond length of 0.74 Å.
Effective molecular radius is the size a molecule displays in solution. The table of permselectivity for different substances contains examples.

The chemical formula for a molecule uses one line of chemical element symbols, numbers, and sometimes also other symbols, such as parentheses, dashes, brackets, and plus (+) and minus (−) signs. These are limited to one typographic line of symbols, which may include subscripts and superscripts.
A compound's empirical formula is a very simple type of chemical formula. It is the simplest integer ratio of the chemical elements that constitute it. For example, water is always composed of a 2:1 ratio of hydrogen to oxygen atoms, and ethyl alcohol or ethanol is always composed of carbon, hydrogen, and oxygen in a 2:6:1 ratio. However, this does not determine the kind of molecule uniquely – dimethyl ether has the same ratios as ethanol, for instance. Molecules with the same atoms in different arrangements are called isomers. Also carbohydrates, for example, have the same ratio (carbon:hydrogen:oxygen= 1:2:1) (and thus the same empirical formula) but different total numbers of atoms in the molecule.
The molecular formula reflects the exact number of atoms that compose the molecule and so characterizes different molecules. However different isomers can have the same atomic composition while being different molecules.
The empirical formula is often the same as the molecular formula but not always. For example, the molecule acetylene has molecular formula C2H2, but the simplest integer ratio of elements is CH.
The molecular mass can be calculated from the chemical formula and is expressed in conventional atomic mass units equal to 1/12 of the mass of a neutral carbon-12 (12C isotope) atom. For network solids, the term formula unit is used in stoichiometric calculations.

For molecules with a complicated 3-dimensional structure, especially involving atoms bonded to four different substituents, a simple molecular formula or even semi-structural chemical formula may not be enough to completely specify the molecule. In this case, a graphical type of formula called a structural formula may be needed. Structural formulas may in turn be represented with a one-dimensional chemical name, but such chemical nomenclature requires many words and terms which are not part of chemical formulas.

Molecules have fixed equilibrium geometries—bond lengths and angles— about which they continuously oscillate through vibrational and rotational motions. A pure substance is composed of molecules with the same average geometrical structure. The chemical formula and the structure of a molecule are the two important factors that determine its properties, particularly its reactivity. Isomers share a chemical formula but normally have very different properties because of their different structures. Stereoisomers, a particular type of isomer, may have very similar physico-chemical properties and at the same time different biochemical activities.

Molecular spectroscopy deals with the response (spectrum) of molecules interacting with probing signals of known energy (or frequency, according to Planck's formula). Molecules have quantized energy levels that can be analyzed by detecting the molecule's energy exchange through absorbance or emission. Spectroscopy does not generally refer to diffraction studies where particles such as neutrons, electrons, or high energy X-rays interact with a regular arrangement of molecules (as in a crystal).
Microwave spectroscopy commonly measures changes in the rotation of molecules, and can be used to identify molecules in outer space. Infrared spectroscopy measures changes in vibration of molecules, including stretching, bending or twisting motions. It is commonly used to identify the kinds of bonds or functional groups in molecules. Changes in the arrangements of electrons yield absorption or emission lines in ultraviolet, visible or near infrared light, and result in colour. Nuclear resonance spectroscopy actually measures the environment of particular nuclei in the molecule, and can be used to characterise the numbers of atoms in different positions in a molecule.

The study of molecules by molecular physics and theoretical chemistry is largely based on quantum mechanics and is essential for the understanding of the chemical bond. The simplest of molecules is the hydrogen molecule-ion, H2+, and the simplest of all the chemical bonds is the one-electron bond. H2+ is composed of two positively charged protons and one negatively charged electron, which means that the Schrödinger equation for the system can be solved more easily due to the lack of electron–electron repulsion. With the development of fast digital computers, approximate solutions for more complicated molecules became possible and are one of the main aspects of computational chemistry.
When trying to define rigorously whether an arrangement of atoms is sufficiently stable to be considered a molecule, IUPAC suggests that it "must correspond to a depression on the potential energy surface that is deep enough to confine at least one vibrational state". This definition does not depend on the nature of the interaction between the atoms, but only on the strength of the interaction. In fact, it includes weakly bound species that would not traditionally be considered molecules, such as the helium dimer, He2, which has one vibrational bound state and is so loosely bound that it is only likely to be observed at very low temperatures.
Whether or not an arrangement of atoms is sufficiently stable to be considered a molecule is inherently an operational definition. Philosophically, therefore, a molecule is not a fundamental entity (in contrast, for instance, to an elementary particle); rather, the concept of a molecule is the chemist's way of making a useful statement about the strengths of atomic-scale interactions in the world that we observe.

Molecule of the Month – School of Chemistry, University of BristolThe scientific method is a body of techniques for investigating phenomena, acquiring new knowledge, or correcting and integrating previous knowledge. To be termed scientific, a method of inquiry is commonly based on empirical or measurable evidence subject to specific principles of reasoning. The Oxford Dictionaries Online define the scientific method as "a method or procedure that has characterized natural science since the 17th century, consisting in systematic observation, measurement, and experiment, and the formulation, testing, and modification of hypotheses". Experiments need to be designed to test hypotheses. The most important part of the scientific method is the experiment.
The scientific method is a continuous process, which usually begins with observations about the natural world. Human beings are naturally inquisitive, so they often come up with questions about things they see or hear and often develop ideas (hypotheses) about why things are the way they are. The best hypotheses lead to predictions that can be tested in various ways, including making further observations about nature. In general, the strongest tests of hypotheses come from carefully controlled and replicated experiments that gather empirical data. Depending on how well the tests match the predictions, the original hypothesis may require refinement, alteration, expansion or even rejection. If a particular hypothesis becomes very well supported a general theory may be developed.
Although procedures vary from one field of inquiry to another, identifiable features are frequently shared in common between them. The overall process of the scientific method involves making conjectures (hypotheses), deriving predictions from them as logical consequences, and then carrying out experiments based on those predictions. A hypothesis is a conjecture, based on knowledge obtained while formulating the question. The hypothesis might be very specific or it might be broad. Scientists then test hypotheses by conducting experiments. Under modern interpretations, a scientific hypothesis must be falsifiable, implying that it is possible to identify a possible outcome of an experiment that conflicts with predictions deduced from the hypothesis; otherwise, the hypothesis cannot be meaningfully tested.
The purpose of an experiment is to determine whether observations agree with or conflict with the predictions derived from a hypothesis. Experiments can take place anywhere from a college lab to CERN's Large Hadron Collider. There are difficulties in a formulaic statement of method, however. Though the scientific method is often presented as a fixed sequence of steps, it represents rather a set of general principles. Not all steps take place in every scientific inquiry (or to the same degree), and are not always in the same order. Some philosophers and scientists have argued that there is no scientific method, such as Lee Smolin and Paul Feyerabend (in his Against Method). Nola and Sankey remark that "For some, the whole idea of a theory of scientific method is yester-year's debate".

The DNA example below is a synopsis of this method
The scientific method is the process by which science is carried out. As in other areas of inquiry, science (through the scientific method) can build on previous knowledge and develop a more sophisticated understanding of its topics of study over time. This model can be seen to underlay the scientific revolution. One thousand years ago, Alhazen argued the importance of forming questions and subsequently testing them, an approach which was advocated by Galileo in 1638 with the publication of Two New Sciences. The current method is based on a hypothetico-deductive model formulated in the 20th century, although it has undergone significant revision since first proposed (for a more formal discussion, see below).

The overall process involves making conjectures (hypotheses), deriving predictions from them as logical consequences, and then carrying out experiments based on those predictions to determine whether the original conjecture was correct. There are difficulties in a formulaic statement of method, however. Though the scientific method is often presented as a fixed sequence of steps, they are better considered as general principles. Not all steps take place in every scientific inquiry (or to the same degree), and are not always in the same order. As noted by William Whewell (1794–1866), "invention, sagacity, [and] genius" are required at every step.

The question can refer to the explanation of a specific observation, as in "Why is the sky blue?", but can also be open-ended, as in "How can I design a drug to cure this particular disease?" This stage frequently involves finding and evaluating evidence from previous experiments, personal scientific observations or assertions, and/or the work of other scientists. If the answer is already known, a different question that builds on the previous evidence can be posed. When applying the scientific method to scientific research, determining a good question can be very difficult and affects the final outcome of the investigation.

A hypothesis is a conjecture, based on knowledge obtained while formulating the question, that may explain the observed behavior of a part of our universe. The hypothesis might be very specific, e.g., Einstein's equivalence principle or Francis Crick's "DNA makes RNA makes protein", or it might be broad, e.g., unknown species of life dwell in the unexplored depths of the oceans. A statistical hypothesis is a conjecture about some population. For example, the population might be people with a particular disease. The conjecture might be that a new drug will cure the disease in some of those people. Terms commonly associated with statistical hypotheses are null hypothesis and alternative hypothesis. A null hypothesis is the conjecture that the statistical hypothesis is false, e.g., that the new drug does nothing and that any cures are due to chance effects. Researchers normally want to show that the null hypothesis is false. The alternative hypothesis is the desired outcome, e.g., that the drug does better than chance. A final point: a scientific hypothesis must be falsifiable, meaning that one can identify a possible outcome of an experiment that conflicts with predictions deduced from the hypothesis; otherwise, it cannot be meaningfully tested.

This step involves determining the logical consequences of the hypothesis. One or more predictions are then selected for further testing. The more unlikely that a prediction would be correct simply by coincidence, then the more convincing it would be if the prediction were fulfilled; evidence is also stronger if the answer to the prediction is not already known, due to the effects of hindsight bias (see also postdiction). Ideally, the prediction must also distinguish the hypothesis from likely alternatives; if two hypotheses make the same prediction, observing the prediction to be correct is not evidence for either one over the other. (These statements about the relative strength of evidence can be mathematically derived using Bayes' Theorem).

This is an investigation of whether the real world behaves as predicted by the hypothesis. Scientists (and other people) test hypotheses by conducting experiments. The purpose of an experiment is to determine whether observations of the real world agree with or conflict with the predictions derived from a hypothesis. If they agree, confidence in the hypothesis increases; otherwise, it decreases. Agreement does not assure that the hypothesis is true; future experiments may reveal problems. Karl Popper advised scientists to try to falsify hypotheses, i.e., to search for and test those experiments that seem most doubtful. Large numbers of successful confirmations are not convincing if they arise from experiments that avoid risk. Experiments should be designed to minimize possible errors, especially through the use of appropriate scientific controls. For example, tests of medical treatments are commonly run as double-blind tests. Test personnel, who might unwittingly reveal to test subjects which samples are the desired test drugs and which are placebos, are kept ignorant of which are which. Such hints can bias the responses of the test subjects. Furthermore, failure of an experiment does not necessarily mean the hypothesis is false. Experiments always depend on several hypotheses, e.g., that the test equipment is working properly, and a failure may be a failure of one of the auxiliary hypotheses. (See the Duhem–Quine thesis.) Experiments can be conducted in a college lab, on a kitchen table, at CERN's Large Hadron Collider, at the bottom of an ocean, on Mars (using one of the working rovers), and so on. Astronomers do experiments, searching for planets around distant stars. Finally, most individual experiments address highly specific topics for reasons of practicality. As a result, evidence about broader topics is usually accumulated gradually.

This involves determining what the results of the experiment show and deciding on the next actions to take. The predictions of the hypothesis are compared to those of the null hypothesis, to determine which is better able to explain the data. In cases where an experiment is repeated many times, a statistical analysis such as a chi-squared test may be required. If the evidence has falsified the hypothesis, a new hypothesis is required; if the experiment supports the hypothesis but the evidence is not strong enough for high confidence, other predictions from the hypothesis must be tested. Once a hypothesis is strongly supported by evidence, a new question can be asked to provide further insight on the same topic. Evidence from other scientists and experience are frequently incorporated at any stage in the process. Depending on the complexity of the experiment, many iterations may be required to gather sufficient evidence to answer a question with confidence, or to build up many answers to highly specific questions in order to answer a single broader question.

The discovery became the starting point for many further studies involving the genetic material, such as the field of molecular genetics, and it was awarded the Nobel Prize in 1962. Each step of the example is examined in more detail later in the article.

The scientific method also includes other components required even when all the iterations of the steps above have been completed:

If an experiment cannot be repeated to produce the same results, this implies that the original results might have been in error. As a result, it is common for a single experiment to be performed multiple times, especially when there are uncontrolled variables or other indications of experimental error. For significant or surprising results, other scientists may also attempt to replicate the results for themselves, especially if those results would be important to their own work.

The process of peer review involves evaluation of the experiment by experts, who typically give their opinions anonymously. Some journals request that the experimenter provide lists of possible peer reviewers, especially if the field is highly specialized. Peer review does not certify correctness of the results, only that, in the opinion of the reviewer, the experiments themselves were sound (based on the description supplied by the experimenter). If the work passes peer review, which occasionally may require new experiments requested by the reviewers, it will be published in a peer-reviewed scientific journal. The specific journal that publishes the results indicates the perceived quality of the work.

Scientists typically are careful in recording their data, a requirement promoted by Ludwik Fleck (1896–1961) and others. Though not typically required, they might be requested to supply this data to other scientists who wish to replicate their original results (or parts of their original results), extending to the sharing of any experimental samples that may be difficult to obtain.

Scientific inquiry generally aims to obtain knowledge in the form of testable explanations that scientists can use to predict the results of future experiments. This allows scientists to gain a better understanding of the topic under study, and later to use that understanding to intervene in its causal mechanisms (such as to cure disease). The better an explanation is at making predictions, the more useful it frequently can be, and the more likely it will continue to explain a body of evidence better than its alternatives. The most successful explanations - those which explain and make accurate predictions in a wide range of circumstances - are often called scientific theories.
Most experimental results do not produce large changes in human understanding; improvements in theoretical scientific understanding typically result from a gradual process of development over time, sometimes across different domains of science. Scientific models vary in the extent to which they have been experimentally tested and for how long, and in their acceptance in the scientific community. In general, explanations become accepted over time as evidence accumulates on a given topic, and the explanation in question proves more powerful than its alternatives at explaining the evidence. Often subsequent researchers re-formulate the explanations over time, or combined explanations to produce new explanations.
Tow sees the scientific method in terms of an evolutionary algorithm applied to science and technology.

Scientific knowledge is closely tied to empirical findings, and can remain subject to falsification if new experimental observation incompatible with it is found. That is, no theory can ever be considered final, since new problematic evidence might be discovered. If such evidence is found, a new theory may be proposed, or (more commonly) it is found that modifications to the previous theory are sufficient to explain the new evidence. The strength of a theory can be argued to relate to how long it has persisted without major alteration to its core principles.
Theories can also become subsumed by other theories. For example, Newton's laws explained thousands of years of scientific observations of the planets almost perfectly. However, these laws were then determined to be special cases of a more general theory (relativity), which explained both the (previously unexplained) exceptions to Newton's laws and predicted and explained other observations such as the deflection of light by gravity. Thus, in certain cases independent, unconnected, scientific observations can be connected to each other, unified by principles of increasing explanatory power.
Since new theories might be more comprehensive than what preceded them, and thus be able to explain more than previous ones, successor theories might be able to meet a higher standard by explaining a larger body of observations than their predecessors. For example, the theory of evolution explains the diversity of life on Earth, how species adapt to their environments, and many other patterns observed in the natural world; its most recent major modification was unification with genetics to form the modern evolutionary synthesis. In subsequent modifications, it has also subsumed aspects of many other fields such as biochemistry and molecular biology.

Scientific methodology often directs that hypotheses be tested in controlled conditions wherever possible. This is frequently possible in certain areas, such as in the biological sciences, and more difficult in other areas, such as in astronomy. The practice of experimental control and reproducibility can have the effect of diminishing the potentially harmful effects of circumstance, and to a degree, personal bias. For example, pre-existing beliefs can alter the interpretation of results, as in confirmation bias; this is a heuristic that leads a person with a particular belief to see things as reinforcing their belief, even if another observer might disagree (in other words, people tend to observe what they expect to observe).
A historical example is the belief that the legs of a galloping horse are splayed at the point when none of the horse's legs touches the ground, to the point of this image being included in paintings by its supporters. However, the first stop-action pictures of a horse's gallop by Eadweard Muybridge showed this to be false, and that the legs are instead gathered together. Another important human bias that plays a role is a preference for new, surprising statements (see appeal to novelty), which can result in a search for evidence that the new is true. In contrast to this standard in the scientific method, poorly attested beliefs can be believed and acted upon via a less rigorous heuristic, sometimes taking advantage of the narrative fallacy that when narrative is constructed its elements become easier to believe. Sometimes, these have their elements assumed a priori, or contain some other logical or methodological flaw in the process that ultimately produced them.

There are different ways of outlining the basic method used for scientific inquiry. The scientific community and philosophers of science generally agree on the following classification of method components. These methodological elements and organization of procedures tend to be more characteristic of natural sciences than social sciences. Nonetheless, the cycle of formulating hypotheses, testing and analyzing the results, and formulating new hypotheses, will resemble the cycle described below.
Four essential elements of the scientific method are iterations, recursions, interleavings, or orderings of the following:
Characterizations (observations, definitions, and measurements of the subject of inquiry)
Hypotheses (theoretical, hypothetical explanations of observations and measurements of the subject)
Predictions (reasoning including deductive reasoning from the hypothesis or theory)
Experiments (tests of all of the above)

Each element of the scientific method is subject to peer review for possible mistakes. These activities do not describe all that scientists do (see below) but apply mostly to experimental sciences (e.g., physics, chemistry, and biology). The elements above are often taught in the educational system as "the scientific method".
The scientific method is not a single recipe: it requires intelligence, imagination, and creativity. In this sense, it is not a mindless set of standards and procedures to follow, but is rather an ongoing cycle, constantly developing more useful, accurate and comprehensive models and methods. For example, when Einstein developed the Special and General Theories of Relativity, he did not in any way refute or discount Newton's Principia. On the contrary, if the astronomically large, the vanishingly small, and the extremely fast are removed from Einstein's theories – all phenomena Newton could not have observed – Newton's equations are what remain. Einstein's theories are expansions and refinements of Newton's theories and, thus, increase our confidence in Newton's work.
A linearized, pragmatic scheme of the four points above is sometimes offered as a guideline for proceeding:
Define a question
Gather information and resources (observe)
Form an explanatory hypothesis
Test the hypothesis by performing an experiment and collecting data in a reproducible manner
Analyze the data
Interpret the data and draw conclusions that serve as a starting point for new hypothesis
Publish results
Retest (frequently done by other scientists)
The iterative cycle inherent in this step-by-step method goes from point 3 to 6 back to 3 again.
While this schema outlines a typical hypothesis/testing method, it should also be noted that a number of philosophers, historians and sociologists of science (perhaps most notably Paul Feyerabend) claim that such descriptions of scientific method have little relation to the ways that science is actually practiced.

The scientific method depends upon increasingly sophisticated characterizations of the subjects of investigation. (The subjects can also be called unsolved problems or the unknowns.) For example, Benjamin Franklin conjectured, correctly, that St. Elmo's fire was electrical in nature, but it has taken a long series of experiments and theoretical changes to establish this. While seeking the pertinent properties of the subjects, careful thought may also entail some definitions and observations; the observations often demand careful measurements and/or counting.
The systematic, careful collection of measurements or counts of relevant quantities is often the critical difference between pseudo-sciences, such as alchemy, and science, such as chemistry or biology. Scientific measurements are usually tabulated, graphed, or mapped, and statistical manipulations, such as correlation and regression, performed on them. The measurements might be made in a controlled setting, such as a laboratory, or made on more or less inaccessible or unmanipulatable objects such as stars or human populations. The measurements often require specialized scientific instruments such as thermometers, spectroscopes, particle accelerators, or voltmeters, and the progress of a scientific field is usually intimately tied to their invention and improvement.

I am not accustomed to saying anything with certainty after only one or two observations.

Measurements in scientific work are also usually accompanied by estimates of their uncertainty. The uncertainty is often estimated by making repeated measurements of the desired quantity. Uncertainties may also be calculated by consideration of the uncertainties of the individual underlying quantities used. Counts of things, such as the number of people in a nation at a particular time, may also have an uncertainty due to data collection limitations. Or counts may represent a sample of desired quantities, with an uncertainty that depends upon the sampling method used and the number of samples taken.

Measurements demand the use of operational definitions of relevant quantities. That is, a scientific quantity is described or defined by how it is measured, as opposed to some more vague, inexact or "idealized" definition. For example, electric current, measured in amperes, may be operationally defined in terms of the mass of silver deposited in a certain time on an electrode in an electrochemical device that is described in some detail. The operational definition of a thing often relies on comparisons with standards: the operational definition of "mass" ultimately relies on the use of an artifact, such as a particular kilogram of platinum-iridium kept in a laboratory in France.
The scientific definition of a term sometimes differs substantially from its natural language usage. For example, mass and weight overlap in meaning in common discourse, but have distinct meanings in mechanics. Scientific quantities are often characterized by their units of measure which can later be described in terms of conventional physical units when communicating the work.
New theories are sometimes developed after realizing certain terms have not previously been sufficiently clearly defined. For example, Albert Einstein's first paper on relativity begins by defining simultaneity and the means for determining length. These ideas were skipped over by Isaac Newton with, "I do not define time, space, place and motion, as being well known to all." Einstein's paper then demonstrates that they (viz., absolute time and length independent of motion) were approximations. Francis Crick cautions us that when characterizing a subject, however, it can be premature to define something when it remains ill-understood. In Crick's study of consciousness, he actually found it easier to study awareness in the visual system, rather than to study free will, for example. His cautionary example was the gene; the gene was much more poorly understood before Watson and Crick's pioneering discovery of the structure of DNA; it would have been counterproductive to spend much time on the definition of the gene, before them.

The history of the discovery of the structure of DNA is a classic example of the elements of the scientific method: in 1950 it was known that genetic inheritance had a mathematical description, starting with the studies of Gregor Mendel, and that DNA contained genetic information (Oswald Avery's transforming principle). But the mechanism of storing genetic information (i.e., genes) in DNA was unclear. Researchers in Bragg's laboratory at Cambridge University made X-ray diffraction pictures of various molecules, starting with crystals of salt, and proceeding to more complicated substances. Using clues painstakingly assembled over decades, beginning with its chemical composition, it was determined that it should be possible to characterize the physical structure of DNA, and the X-ray images would be the vehicle. ..2. DNA-hypotheses

The characterization element can require extended and extensive study, even centuries. It took thousands of years of measurements, from the Chaldean, Indian, Persian, Greek, Arabic and European astronomers, to fully record the motion of planet Earth. Newton was able to include those measurements into consequences of his laws of motion. But the perihelion of the planet Mercury's orbit exhibits a precession that cannot be fully explained by Newton's laws of motion (see diagram to the right), as Leverrier pointed out in 1859. The observed difference for Mercury's precession between Newtonian theory and observation was one of the things that occurred to Einstein as a possible early test of his theory of General Relativity. His relativistic calculations matched observation much more closely than did Newtonian theory. The difference is approximately 43 arc-seconds per century.

A hypothesis is a suggested explanation of a phenomenon, or alternately a reasoned proposal suggesting a possible correlation between or among a set of phenomena.
Normally hypotheses have the form of a mathematical model. Sometimes, but not always, they can also be formulated as existential statements, stating that some particular instance of the phenomenon being studied has some characteristic and causal explanations, which have the general form of universal statements, stating that every instance of the phenomenon has a particular characteristic.
Scientists are free to use whatever resources they have – their own creativity, ideas from other fields, inductive reasoning, Bayesian inference, and so on – to imagine possible explanations for a phenomenon under study. Charles Sanders Peirce, borrowing a page from Aristotle (Prior Analytics, 2.25) described the incipient stages of inquiry, instigated by the "irritation of doubt" to venture a plausible guess, as abductive reasoning. The history of science is filled with stories of scientists claiming a "flash of inspiration", or a hunch, which then motivated them to look for evidence to support or refute their idea. Michael Polanyi made such creativity the centerpiece of his discussion of methodology.
William Glen observes that
the success of a hypothesis, or its service to science, lies not simply in its perceived "truth", or power to displace, subsume or reduce a predecessor idea, but perhaps more in its ability to stimulate the research that will illuminate ... bald suppositions and areas of vagueness.
In general scientists tend to look for theories that are "elegant" or "beautiful". In contrast to the usual English use of these terms, they here refer to a theory in accordance with the known facts, which is nevertheless relatively simple and easy to handle. Occam's Razor serves as a rule of thumb for choosing the most desirable amongst a group of equally explanatory hypotheses.

Linus Pauling proposed that DNA might be a triple helix. This hypothesis was also considered by Francis Crick and James D. Watson but discarded. When Watson and Crick learned of Pauling's hypothesis, they understood from existing data that Pauling was wrong and that Pauling would soon admit his difficulties with that structure. So, the race was on to figure out the correct structure (except that Pauling did not realize at the time that he was in a race) ..3. DNA-predictions

Any useful hypothesis will enable predictions, by reasoning including deductive reasoning. It might predict the outcome of an experiment in a laboratory setting or the observation of a phenomenon in nature. The prediction can also be statistical and deal only with probabilities.
It is essential that the outcome of testing such a prediction be currently unknown. Only in this case does a successful outcome increase the probability that the hypothesis is true. If the outcome is already known, it is called a consequence and should have already been considered while formulating the hypothesis.
If the predictions are not accessible by observation or experience, the hypothesis is not yet testable and so will remain to that extent unscientific in a strict sense. A new technology or theory might make the necessary experiments feasible. Thus, much scientifically based speculation might convince one (or many) that the hypothesis that other intelligent species exist is true. But since there no experiment now known which can test this hypothesis, science itself can have little to say about the possibility. In future, some new technique might lead to an experimental test and the speculation would then become part of accepted science.

James D. Watson, Francis Crick, and others hypothesized that DNA had a helical structure. This implied that DNA's X-ray diffraction pattern would be 'x shaped'. This prediction followed from the work of Cochran, Crick and Vand (and independently by Stokes). The Cochran-Crick-Vand-Stokes theorem provided a mathematical explanation for the empirical observation that diffraction from helical structures produces x shaped patterns.
In their first paper, Watson and Crick also noted that the double helix structure they proposed provided a simple mechanism for DNA replication, writing, "It has not escaped our notice that the specific pairing we have postulated immediately suggests a possible copying mechanism for the genetic material". ..4. DNA-experiments

Einstein's theory of General Relativity makes several specific predictions about the observable structure of space-time, such as that light bends in a gravitational field, and that the amount of bending depends in a precise way on the strength of that gravitational field. Arthur Eddington's observations made during a 1919 solar eclipse supported General Relativity rather than Newtonian gravitation.

Once predictions are made, they can be sought by experiments. If the test results contradict the predictions, the hypotheses which entailed them are called into question and become less tenable. Sometimes the experiments are conducted incorrectly or are not very well designed, when compared to a crucial experiment. If the experimental results confirm the predictions, then the hypotheses are considered more likely to be correct, but might still be wrong and continue to be subject to further testing. The experimental control is a technique for dealing with observational error. This technique uses the contrast between multiple samples (or observations) under differing conditions to see what varies or what remains the same. We vary the conditions for each measurement, to help isolate what has changed. Mill's canons can then help us figure out what the important factor is. Factor analysis is one technique for discovering the important factor in an effect.
Depending on the predictions, the experiments can have different shapes. It could be a classical experiment in a laboratory setting, a double-blind study or an archaeological excavation. Even taking a plane from New York to Paris is an experiment which tests the aerodynamical hypotheses used for constructing the plane.
Scientists assume an attitude of openness and accountability on the part of those conducting an experiment. Detailed record keeping is essential, to aid in recording and reporting on the experimental results, and supports the effectiveness and integrity of the procedure. They will also assist in reproducing the experimental results, likely by others. Traces of this approach can be seen in the work of Hipparchus (190–120 BCE), when determining a value for the precession of the Earth, while controlled experiments can be seen in the works of Jābir ibn Hayyān (721–815 CE), al-Battani (853–929) and Alhazen (965–1039).

Watson and Crick showed an initial (and incorrect) proposal for the structure of DNA to a team from Kings College – Rosalind Franklin, Maurice Wilkins, and Raymond Gosling. Franklin immediately spotted the flaws which concerned the water content. Later Watson saw Franklin's detailed X-ray diffraction images which showed an X-shape and was able to confirm the structure was helical. This rekindled Watson and Crick's model building and led to the correct structure. ..1. DNA-characterizations

The scientific method is iterative. At any stage it is possible to refine its accuracy and precision, so that some consideration will lead the scientist to repeat an earlier part of the process. Failure to develop an interesting hypothesis may lead a scientist to re-define the subject under consideration. Failure of a hypothesis to produce interesting and testable predictions may lead to reconsideration of the hypothesis or of the definition of the subject. Failure of an experiment to produce interesting results may lead a scientist to reconsider the experimental method, the hypothesis, or the definition of the subject.
Other scientists may start their own research and enter the process at any stage. They might adopt the characterization and formulate their own hypothesis, or they might adopt the hypothesis and deduce their own predictions. Often the experiment is not done by the person who made the prediction, and the characterization is based on experiments done by someone else. Published results of experiments can also serve as a hypothesis predicting their own reproducibility.

After considerable fruitless experimentation, being discouraged by their superior from continuing, and numerous false starts, Watson and Crick were able to infer the essential structure of DNA by concrete modeling of the physical shapes of the nucleotides which comprise it. They were guided by the bond lengths which had been deduced by Linus Pauling and by Rosalind Franklin's X-ray diffraction images. ..DNA Example

Science is a social enterprise, and scientific work tends to be accepted by the scientific community when it has been confirmed. Crucially, experimental and theoretical results must be reproduced by others within the scientific community. Researchers have given their lives for this vision; Georg Wilhelm Richmann was killed by ball lightning (1753) when attempting to replicate the 1752 kite-flying experiment of Benjamin Franklin.
To protect against bad science and fraudulent data, government research-granting agencies such as the National Science Foundation, and science journals, including Nature and Science, have a policy that researchers must archive their data and methods so that other researchers can test the data and methods and build on the research that has gone before. Scientific data archiving can be done at a number of national archives in the U.S. or in the World Data Center.

The classical model of scientific inquiry derives from Aristotle, who distinguished the forms of approximate and exact reasoning, set out the threefold scheme of abductive, deductive, and inductive inference, and also treated the compound forms such as reasoning by analogy.

In 1877, Charles Sanders Peirce (/ˈpɜːrs/ like "purse"; 1839–1914) characterized inquiry in general not as the pursuit of truth per se but as the struggle to move from irritating, inhibitory doubts born of surprises, disagreements, and the like, and to reach a secure belief, belief being that on which one is prepared to act. He framed scientific inquiry as part of a broader spectrum and as spurred, like inquiry generally, by actual doubt, not mere verbal or hyperbolic doubt, which he held to be fruitless. He outlined four methods of settling opinion, ordered from least to most successful:
The method of tenacity (policy of sticking to initial belief) – which brings comforts and decisiveness but leads to trying to ignore contrary information and others' views as if truth were intrinsically private, not public. It goes against the social impulse and easily falters since one may well notice when another's opinion is as good as one's own initial opinion. Its successes can shine but tend to be transitory.
The method of authority – which overcomes disagreements but sometimes brutally. Its successes can be majestic and long-lived, but it cannot operate thoroughly enough to suppress doubts indefinitely, especially when people learn of other societies present and past.
The method of the a priori – which promotes conformity less brutally but fosters opinions as something like tastes, arising in conversation and comparisons of perspectives in terms of "what is agreeable to reason." Thereby it depends on fashion in paradigms and goes in circles over time. It is more intellectual and respectable but, like the first two methods, sustains accidental and capricious beliefs, destining some minds to doubt it.
The scientific method – the method wherein inquiry regards itself as fallible and purposely tests itself and criticizes, corrects, and improves itself.
Peirce held that slow, stumbling ratiocination can be dangerously inferior to instinct and traditional sentiment in practical matters, and that the scientific method is best suited to theoretical research, which in turn should not be trammeled by the other methods and practical ends; reason's "first rule" is that, in order to learn, one must desire to learn and, as a corollary, must not block the way of inquiry. The scientific method excels the others by being deliberately designed to arrive – eventually – at the most secure beliefs, upon which the most successful practices can be based. Starting from the idea that people seek not truth per se but instead to subdue irritating, inhibitory doubt, Peirce showed how, through the struggle, some can come to submit to truth for the sake of belief's integrity, seek as truth the guidance of potential practice correctly to its given goal, and wed themselves to the scientific method.
For Peirce, rational inquiry implies presuppositions about truth and the real; to reason is to presuppose (and at least to hope), as a principle of the reasoner's self-regulation, that the real is discoverable and independent of our vagaries of opinion. In that vein he defined truth as the correspondence of a sign (in particular, a proposition) to its object and, pragmatically, not as actual consensus of some definite, finite community (such that to inquire would be to poll the experts), but instead as that final opinion which all investigators would reach sooner or later but still inevitably, if they were to push investigation far enough, even when they start from different points. In tandem he defined the real as a true sign's object (be that object a possibility or quality, or an actuality or brute fact, or a necessity or norm or law), which is what it is independently of any finite community's opinion and, pragmatically, depends only on the final opinion destined in a sufficient investigation. That is a destination as far, or near, as the truth itself to you or me or the given finite community. Thus, his theory of inquiry boils down to "Do the science." Those conceptions of truth and the real involve the idea of a community both without definite limits (and thus potentially self-correcting as far as needed) and capable of definite increase of knowledge. As inference, "logic is rooted in the social principle" since it depends on a standpoint that is, in a sense, unlimited.
Paying special attention to the generation of explanations, Peirce outlined the scientific method as a coordination of three kinds of inference in a purposeful cycle aimed at settling doubts, as follows (in §III–IV in "A Neglected Argument" except as otherwise noted):
Abduction (or retroduction). Guessing, inference to explanatory hypotheses for selection of those best worth trying. From abduction, Peirce distinguishes induction as inferring, on the basis of tests, the proportion of truth in the hypothesis. Every inquiry, whether into ideas, brute facts, or norms and laws, arises from surprising observations in one or more of those realms (and for example at any stage of an inquiry already underway). All explanatory content of theories comes from abduction, which guesses a new or outside idea so as to account in a simple, economical way for a surprising or complicative phenomenon. Oftenest, even a well-prepared mind guesses wrong. But the modicum of success of our guesses far exceeds that of sheer luck and seems born of attunement to nature by instincts developed or inherent, especially insofar as best guesses are optimally plausible and simple in the sense, said Peirce, of the "facile and natural", as by Galileo's natural light of reason and as distinct from "logical simplicity". Abduction is the most fertile but least secure mode of inference. Its general rationale is inductive: it succeeds often enough and, without it, there is no hope of sufficiently expediting inquiry (often multi-generational) toward new truths. Coordinative method leads from abducing a plausible hypothesis to judging it for its testability and for how its trial would economize inquiry itself. Peirce calls his pragmatism "the logic of abduction". His pragmatic maxim is: "Consider what effects that might conceivably have practical bearings you conceive the objects of your conception to have. Then, your conception of those effects is the whole of your conception of the object". His pragmatism is a method of reducing conceptual confusions fruitfully by equating the meaning of any conception with the conceivable practical implications of its object's conceived effects—a method of experimentational mental reflection hospitable to forming hypotheses and conducive to testing them. It favors efficiency. The hypothesis, being insecure, needs to have practical implications leading at least to mental tests and, in science, lending themselves to scientific tests. A simple but unlikely guess, if uncostly to test for falsity, may belong first in line for testing. A guess is intrinsically worth testing if it has instinctive plausibility or reasoned objective probability, while subjective likelihood, though reasoned, can be misleadingly seductive. Guesses can be chosen for trial strategically, for their caution (for which Peirce gave as example the game of Twenty Questions), breadth, and incomplexity. One can hope to discover only that which time would reveal through a learner's sufficient experience anyway, so the point is to expedite it; the economy of research is what demands the leap, so to speak, of abduction and governs its art.
Deduction. Two stages:
Explication. Unclearly premissed, but deductive, analysis of the hypothesis in order to render its parts as clear as possible.
Demonstration: Deductive Argumentation, Euclidean in procedure. Explicit deduction of hypothesis's consequences as predictions, for induction to test, about evidence to be found. Corollarial or, if needed, theorematic.

Induction. The long-run validity of the rule of induction is deducible from the principle (presuppositional to reasoning in general) that the real is only the object of the final opinion to which adequate investigation would lead; anything to which no such process would ever lead would not be real. Induction involving ongoing tests or observations follows a method which, sufficiently persisted in, will diminish its error below any predesignate degree. Three stages:
Classification. Unclearly premissed, but inductive, classing of objects of experience under general ideas.
Probation: direct inductive argumentation. Crude (the enumeration of instances) or gradual (new estimate of proportion of truth in the hypothesis after each test). Gradual induction is qualitative or quantitative; if qualitative, then dependent on weightings of qualities or characters; if quantitative, then dependent on measurements, or on statistics, or on countings.
Sentential Induction. "...which, by inductive reasonings, appraises the different probations singly, then their combinations, then makes self-appraisal of these very appraisals themselves, and passes final judgment on the whole result".

Frequently the scientific method is employed not only by a single person, but also by several people cooperating directly or indirectly. Such cooperation can be regarded as an important element of a scientific community. Various standards of scientific methodology are used within such an environment.

Scientific journals use a process of peer review, in which scientists' manuscripts are submitted by editors of scientific journals to (usually one to three) fellow (usually anonymous) scientists familiar with the field for evaluation. In certain journals, the journal itself selects the referees; while in others (especially journals that are extremely specialized), the manuscript author might recommend referees. The referees may or may not recommend publication, or they might recommend publication with suggested modifications, or sometimes, publication in another journal. This standard is practiced to various degrees by different journals, and can have the effect of keeping the literature free of obvious errors and to generally improve the quality of the material, especially in the journals who use the standard most rigorously. The peer review process can have limitations when considering research outside the conventional scientific paradigm: problems of "groupthink" can interfere with open and fair deliberation of some new research.

Sometimes experimenters may make systematic errors during their experiments, veer from standard methods and practices (Pathological science) for various reasons, or, in rare cases, deliberately report false results. Occasionally because of this then, other scientists might attempt to repeat the experiments in order to duplicate the results.

Researchers sometimes practice scientific data archiving, such as in compliance with the policies of government funding agencies and scientific journals. In these cases, detailed records of their experimental procedures, raw data, statistical analyses and source code can be preserved in order to provide evidence of the methodology and practice of the procedure and assist in any potential future attempts to reproduce the result. These procedural records may also assist in the conception of new experiments to test the hypothesis, and may prove useful to engineers who might examine the potential practical applications of a discovery.

When additional information is needed before a study can be reproduced, the author of the study might be asked to provide it. They might provide it, or if the author refuses to share data, appeals can be made to the journal editors who published the study or to the institution which funded the research.

Since it is impossible for a scientist to record everything that took place in an experiment, facts selected for their apparent relevance are reported. This may lead, unavoidably, to problems later if some supposedly irrelevant feature is questioned. For example, Heinrich Hertz did not report the size of the room used to test Maxwell's equations, which later turned out to account for a small deviation in the results. The problem is that parts of the theory itself need to be assumed in order to select and report the experimental conditions. The observations are hence sometimes described as being 'theory-laden'.

The primary constraints on contemporary science are:
Publication, i.e. Peer review
Resources (mostly funding)
It has not always been like this: in the old days of the "gentleman scientist" funding (and to a lesser extent publication) were far weaker constraints.
Both of these constraints indirectly require scientific method – work that violates the constraints will be difficult to publish and difficult to get funded. Journals require submitted papers to conform to "good scientific practice" and to a degree this can be enforced by peer review. Originality, importance and interest are more important – see for example the author guidelines for Nature.
Smaldino and McElreath 2016 have noted that our need to reward scientific understanding is being nullified by poor research design and poor data analysis, which is leading to false-positive findings.

Philosophy of science looks at the underpinning logic of the scientific method, at what separates science from non-science, and the ethic that is implicit in science. There are basic assumptions, derived from philosophy by at least one prominent scientist, that form the base of the scientific method – namely, that reality is objective and consistent, that humans have the capacity to perceive reality accurately, and that rational explanations exist for elements of the real world. These assumptions from methodological naturalism form a basis on which science may be grounded. Logical Positivist, empiricist, falsificationist, and other theories have criticized these assumptions and given alternative accounts of the logic of science, but each has also itself been criticized. More generally, the scientific method can be recognized as an idealization.
Thomas Kuhn examined the history of science in his The Structure of Scientific Revolutions, and found that the actual method used by scientists differed dramatically from the then-espoused method. His observations of science practice are essentially sociological and do not speak to how science is or can be practiced in other times and other cultures.
Norwood Russell Hanson, Imre Lakatos and Thomas Kuhn have done extensive work on the "theory laden" character of observation. Hanson (1958) first coined the term for the idea that all observation is dependent on the conceptual framework of the observer, using the concept of gestalt to show how preconceptions can affect both observation and description. He opens Chapter 1 with a discussion of the Golgi bodies and their initial rejection as an artefact of staining technique, and a discussion of Brahe and Kepler observing the dawn and seeing a "different" sun rise despite the same physiological phenomenon. Kuhn and Feyerabend acknowledge the pioneering significance of his work.
Kuhn (1961) said the scientist generally has a theory in mind before designing and undertaking experiments so as to make empirical observations, and that the "route from theory to measurement can almost never be traveled backward". This implies that the way in which theory is tested is dictated by the nature of the theory itself, which led Kuhn (1961, p. 166) to argue that "once it has been adopted by a profession ... no theory is recognized to be testable by any quantitative tests that it has not already passed".
Paul Feyerabend similarly examined the history of science, and was led to deny that science is genuinely a methodological process. In his book Against Method he argues that scientific progress is not the result of applying any particular method. In essence, he says that for any specific method or norm of science, one can find a historic episode where violating it has contributed to the progress of science. Thus, if believers in scientific method wish to express a single universally valid rule, Feyerabend jokingly suggests, it should be 'anything goes'. Criticisms such as his led to the strong programme, a radical approach to the sociology of science.
The postmodernist critiques of science have themselves been the subject of intense controversy. This ongoing debate, known as the science wars, is the result of conflicting values and assumptions between the postmodernist and realist camps. Whereas postmodernists assert that scientific knowledge is simply another discourse (note that this term has special meaning in this context) and not representative of any form of fundamental truth, realists in the scientific community maintain that scientific knowledge does reveal real and fundamental truths about reality. Many books have been written by scientists which take on this problem and challenge the assertions of the postmodernists while defending science as a legitimate method of deriving truth.

Somewhere between 33% and 50% of all scientific discoveries are estimated to have been stumbled upon, rather than sought out. This may explain why scientists so often express that they were lucky. Louis Pasteur is credited with the famous saying that "Luck favours the prepared mind", but some psychologists have begun to study what it means to be 'prepared for luck' in the scientific context. Research is showing that scientists are taught various heuristics that tend to harness chance and the unexpected. This is what Nassim Nicholas Taleb calls "Anti-fragility"; while some systems of investigation are fragile in the face of human error, human bias, and randomness, the scientific method is more than resistant or tough – it actually benefits from such randomness in many ways (it is anti-fragile). Taleb believes that the more anti-fragile the system, the more it will flourish in the real world.
Psychologist Kevin Dunbar says the process of discovery often starts with researchers finding bugs in their experiments. These unexpected results lead researchers to try to fix what they think is an error in their method. Eventually, the researcher decides the error is too persistent and systematic to be a coincidence. The highly controlled, cautious and curious aspects of the scientific method are thus what make it well suited for identifying such persistent systematic errors. At this point, the researcher will begin to think of theoretical explanations for the error, often seeking the help of colleagues across different domains of expertise.

The history of scientific method considers changes in the methodology of scientific inquiry, as distinct from the history of science itself. The development of rules for scientific reasoning has not been straightforward; scientific method has been the subject of intense and recurring debate throughout the history of science, and eminent natural philosophers and scientists have argued for the primacy of one or another approach to establishing scientific knowledge. Despite the disagreements about approaches, scientific method has advanced in definite steps. Rationalist explanations of nature, including atomism, appeared both in ancient Greece in the thought of Leucippus and Democritus, and in ancient India, in the Nyaya, Vaisesika and Buddhist schools, while Charvaka materialism rejected inference as a source of knowledge in favour of an empiricism that was always subject to doubt. Aristotle pioneered scientific method in ancient Greece alongside his empirical biology and his work on logic, rejecting a purely deductive framework in favour of generalisations made from observations of nature. Important debates in the history of scientific method center on rationalism, especially as advocated by René Descartes, inductivism, which rose to particular prominence with Isaac Newton and his followers, and hypothetico-deductivism, which came to the fore in the early 19th century. In the late 19th and early 20th centuries, a debate over realism vs. antirealism was conducted as powerful scientific theories extended beyond the realm of the observable, while in the mid-20th century, prominent philosophers such as Paul Feyerabend argued against any universal rules of science at all.

Science is the process of gathering, comparing, and evaluating proposed models against observables. A model can be a simulation, mathematical or chemical formula, or set of proposed steps. Science is like mathematics in that researchers in both disciplines can clearly distinguish what is known from what is unknown at each stage of discovery. Models, in both science and mathematics, need to be internally consistent and also ought to be falsifiable (capable of disproof). In mathematics, a statement need not yet be proven; at such a stage, that statement would be called a conjecture. But when a statement has attained mathematical proof, that statement gains a kind of immortality which is highly prized by mathematicians, and for which some mathematicians devote their lives.
Mathematical work and scientific work can inspire each other. For example, the technical concept of time arose in science, and timelessness was a hallmark of a mathematical topic. But today, the Poincaré conjecture has been proven using time as a mathematical concept in which objects can flow (see Ricci flow).
Nevertheless, the connection between mathematics and reality (and so science to the extent it describes reality) remains obscure. Eugene Wigner's paper, The Unreasonable Effectiveness of Mathematics in the Natural Sciences, is a very well known account of the issue from a Nobel Prize-winning physicist. In fact, some observers (including some well known mathematicians such as Gregory Chaitin, and others such as Lakoff and Núñez) have suggested that mathematics is the result of practitioner bias and human limitation (including cultural ones), somewhat like the post-modernist view of science.
George Pólya's work on problem solving, the construction of mathematical proofs, and heuristic show that the mathematical method and the scientific method differ in detail, while nevertheless resembling each other in using iterative or recursive steps.
In Pólya's view, understanding involves restating unfamiliar definitions in your own words, resorting to geometrical figures, and questioning what we know and do not know already; analysis, which Pólya takes from Pappus, involves free and heuristic construction of plausible arguments, working backward from the goal, and devising a plan for constructing the proof; synthesis is the strict Euclidean exposition of step-by-step details of the proof; review involves reconsidering and re-examining the result and the path taken to it.
Gauss, when asked how he came about his theorems, once replied "durch planmässiges Tattonieren" (through systematic palpable experimentation).
Imre Lakatos argued that mathematicians actually use contradiction, criticism and revision as principles for improving their work. In like manner to science, where truth is sought, but certainty is not found, in Proofs and refutations (1976), what Lakatos tried to establish was that no theorem of informal mathematics is final or perfect. This means that we should not think that a theorem is ultimately true, only that no counterexample has yet been found. Once a counterexample, i.e. an entity contradicting/not explained by the theorem is found, we adjust the theorem, possibly extending the domain of its validity. This is a continuous way our knowledge accumulates, through the logic and process of proofs and refutations. (If axioms are given for a branch of mathematics, however, Lakatos claimed that proofs from those axioms were tautological, i.e. logically true, by rewriting them, as did Poincaré (Proofs and Refutations, 1976).)
Lakatos proposed an account of mathematical knowledge based on Polya's idea of heuristics. In Proofs and Refutations, Lakatos gave several basic rules for finding proofs and counterexamples to conjectures. He thought that mathematical 'thought experiments' are a valid way to discover mathematical conjectures and proofs.

The scientific method has been extremely successful in bringing the world out of medieval thinking, especially once it was combined with industrial processes. However, when the scientific method employs statistics as part of its arsenal, there are mathematical and practical issues that can have a deleterious effect on the reliability of the output of scientific methods. This is described in a popular 2005 scientific paper "Why Most Published Research Findings Are False" by John Ioannidis.
The particular points raised are statistical ("The smaller the studies conducted in a scientific field, the less likely the research findings are to be true" and "The greater the flexibility in designs, definitions, outcomes, and analytical modes in a scientific field, the less likely the research findings are to be true.") and economical ("The greater the financial and other interests and prejudices in a scientific field, the less likely the research findings are to be true" and "The hotter a scientific field (with more scientific teams involved), the less likely the research findings are to be true.") Hence: "Most research findings are false for most research designs and for most fields" and "As shown, the majority of modern biomedical research is operating in areas with very low pre- and poststudy probability for true findings." However: "Nevertheless, most new discoveries will continue to stem from hypothesis-generating research with low or very low pre-study odds," which means that *new* discoveries will come from research that, when that research started, had low or very low odds (a low or very low chance) of succeeding. Hence, if the scientific method is used to expand the frontiers of knowledge, research into areas that are outside the mainstream will yield most new discoveries.

Andersen, Anne; Hepburn, Brian. "Scientific Method". Stanford Encyclopedia of Philosophy.
"Confirmation and Induction". Internet Encyclopedia of Philosophy.
Scientific method at PhilPapers
Scientific method at the Indiana Philosophy Ontology Project
An Introduction to Science: Scientific Thinking and a scientific method by Steven D. Schafersman.
Introduction to the scientific method at the University of Rochester
Theory-ladenness by Paul Newall at The Galilean Library
Lecture on Scientific Method by Greg Anderson
Using the scientific method for designing science fair projects
SCIENTIFIC METHODS an online book by Richard D. Jarrard
Richard Feynman on the Key to Science (one minute, three seconds), from the Cornell Lectures.
Lectures on the Scientific Method by Nick Josh Karean, Kevin Padian, Michael Shermer and Richard DawkinsA computer network or data network is a telecommunications network which allows nodes to share resources. In computer networks, networked computing devices exchange data with each other using a data link. The connections between nodes are established using either cable media or wireless media. The best-known computer network is the Internet.
Network computer devices that originate, route and terminate the data are called network nodes. Nodes can include hosts such as personal computers, phones, servers as well as networking hardware. Two such devices can be said to be networked together when one device is able to exchange information with the other device, whether or not they have a direct connection to each other.
Computer networks differ in the transmission medium used to carry their signals, communications protocols to organize network traffic, the network's size, topology and organizational intent.
Computer networks support an enormous number of applications and services such as access to the World Wide Web, digital video, digital audio, shared use of application and storage servers, printers, and fax machines, and use of email and instant messaging applications as well as many others. In most cases, application-specific communications protocols are layered (i.e. carried as payload) over other more general communications protocols.

The chronology of significant computer-network developments includes:
In the late 1950s, early networks of computers included the military radar system Semi-Automatic Ground Environment (SAGE).
In 1959, Anatolii Ivanovich Kitov proposed to the Central Committee of the Communist Party of the Soviet Union a detailed plan for the re-organisation of the control of the Soviet armed forces and of the Soviet economy on the basis of a network of computing centres.
In 1960, the commercial airline reservation system semi-automatic business research environment (SABRE) went online with two connected mainframes.
In 1962, J.C.R. Licklider developed a working group he called the "Intergalactic Computer Network", a precursor to the ARPANET, at the Advanced Research Projects Agency (ARPA).
In 1964, researchers at Dartmouth College developed the Dartmouth Time Sharing System for distributed users of large computer systems. The same year, at Massachusetts Institute of Technology, a research group supported by General Electric and Bell Labs used a computer to route and manage telephone connections.
Throughout the 1960s, Leonard Kleinrock, Paul Baran, and Donald Davies independently developed network systems that used packets to transfer information between computers over a network.
In 1965, Thomas Marill and Lawrence G. Roberts created the first wide area network (WAN). This was an immediate precursor to the ARPANET, of which Roberts became program manager.
Also in 1965, Western Electric introduced the first widely used telephone switch that implemented true computer control.
In 1969, the University of California at Los Angeles, the Stanford Research Institute, the University of California at Santa Barbara, and the University of Utah became connected as the beginning of the ARPANET network using 50 kbit/s circuits.
In 1972, commercial services using X.25 were deployed, and later used as an underlying infrastructure for expanding TCP/IP networks.
In 1973, Robert Metcalfe wrote a formal memo at Xerox PARC describing Ethernet, a networking system that was based on the Aloha network, developed in the 1960s by Norman Abramson and colleagues at the University of Hawaii. In July 1976, Robert Metcalfe and David Boggs published their paper "Ethernet: Distributed Packet Switching for Local Computer Networks" and collaborated on several patents received in 1977 and 1978. In 1979, Robert Metcalfe pursued making Ethernet an open standard.
In 1976, John Murphy of Datapoint Corporation created ARCNET, a token-passing network first used to share storage devices.
In 1995, the transmission speed capacity for Ethernet increased from 10 Mbit/s to 100 Mbit/s. By 1998, Ethernet supported transmission speeds of a Gigabit. Subsequently, higher speeds of up to 100 Gbit/s were added (as of 2016). The ability of Ethernet to scale easily (such as quickly adapting to support new fiber optic cable speeds) is a contributing factor to its continued use.

Computer networking may be considered a branch of electrical engineering, telecommunications, computer science, information technology or computer engineering, since it relies upon the theoretical and practical application of the related disciplines.
A computer network facilitates interpersonal communications allowing users to communicate efficiently and easily via various means: email, instant messaging, chat rooms, telephone, video telephone calls, and video conferencing. Providing access to information on shared storage devices is an important feature of many networks. A network allows sharing of files, data, and other types of information giving authorized users the ability to access information stored on other computers on the network. A network allows sharing of network and computing resources. Users may access and use resources provided by devices on the network, such as printing a document on a shared network printer. Distributed computing uses computing resources across a network to accomplish tasks. A computer network may be used by computer crackers to deploy computer viruses or computer worms on devices connected to the network, or to prevent these devices from accessing the network via a denial of service attack.

Computer communication links that do not support packets, such as traditional point-to-point telecommunication links, simply transmit data as a bit stream. However, most information in computer networks is carried in packets. A network packet is a formatted unit of data (a list of bits or bytes, usually a few tens of bytes to a few kilobytes long) carried by a packet-switched network.
In packet networks, the data is formatted into packets that are sent through the network to their destination. Once the packets arrive they are reassembled into their original message. With packets, the bandwidth of the transmission medium can be better shared among users than if the network were circuit switched. When one user is not sending packets, the link can be filled with packets from other users, and so the cost can be shared, with relatively little interference, provided the link isn't overused.
Packets consist of two kinds of data: control information, and user data (payload). The control information provides data the network needs to deliver the user data, for example: source and destination network addresses, error detection codes, and sequencing information. Typically, control information is found in packet headers and trailers, with payload data in between.
Often the route a packet needs to take through a network is not immediately available. In that case the packet is queued and waits until a link is free.

The physical layout of a network is usually less important than the topology that connects network nodes. Most diagrams that describe a physical network are therefore topological, rather than geographic. The symbols on these diagrams usually denote network links and network nodes.

The transmission media (often referred to in the literature as the physical media) used to link devices to form a computer network include electrical cable (Ethernet, HomePNA, power line communication, G.hn), optical fiber (fiber-optic communication), and radio waves (wireless networking). In the OSI model, these are defined at layers 1 and 2 — the physical layer and the data link layer.
A widely adopted family of transmission media used in local area network (LAN) technology is collectively known as Ethernet. The media and protocol standards that enable communication between networked devices over Ethernet are defined by IEEE 802.3. Ethernet transmits data over both copper and fiber cables. Wireless LAN standards (e.g. those defined by IEEE 802.11) use radio waves, or others use infrared signals as a transmission medium. Power line communication uses a building's power cabling to transmit data.

The orders of the following wired technologies are, roughly, from slowest to fastest transmission speed.
Coaxial cable is widely used for cable television systems, office buildings, and other work-sites for local area networks. The cables consist of copper or aluminum wire surrounded by an insulating layer (typically a flexible material with a high dielectric constant), which itself is surrounded by a conductive layer. The insulation helps minimize interference and distortion. Transmission speed ranges from 200 million bits per second to more than 500 million bits per second.
ITU-T G.hn technology uses existing home wiring (coaxial cable, phone lines and power lines) to create a high-speed (up to 1 Gigabit/s) local area network
Twisted pair wire is the most widely used medium for all telecommunication. Twisted-pair cabling consist of copper wires that are twisted into pairs. Ordinary telephone wires consist of two insulated copper wires twisted into pairs. Computer network cabling (wired Ethernet as defined by IEEE 802.3) consists of 4 pairs of copper cabling that can be utilized for both voice and data transmission. The use of two wires twisted together helps to reduce crosstalk and electromagnetic induction. The transmission speed ranges from 2 million bits per second to 10 billion bits per second. Twisted pair cabling comes in two forms: unshielded twisted pair (UTP) and shielded twisted-pair (STP). Each form comes in several category ratings, designed for use in various scenarios.

An optical fiber is a glass fiber. It carries pulses of light that represent data. Some advantages of optical fibers over metal wires are very low transmission loss and immunity from electrical interference. Optical fibers can simultaneously carry multiple wavelengths of light, which greatly increases the rate that data can be sent, and helps enable data rates of up to trillions of bits per second. Optic fibers can be used for long runs of cable carrying very high data rates, and are used for undersea cables to interconnect continents.
Price is a main factor distinguishing wired- and wireless-technology options in a business. Wireless options command a price premium that can make purchasing wired computers, printers and other devices a financial benefit. Before making the decision to purchase hard-wired technology products, a review of the restrictions and limitations of the selections is necessary. Business and employee needs may override any cost considerations.

Terrestrial microwave – Terrestrial microwave communication uses Earth-based transmitters and receivers resembling satellite dishes. Terrestrial microwaves are in the low-gigahertz range, which limits all communications to line-of-sight. Relay stations are spaced approximately 48 km (30 mi) apart.
Communications satellites – Satellites communicate via microwave radio waves, which are not deflected by the Earth's atmosphere. The satellites are stationed in space, typically in geosynchronous orbit 35,400 km (22,000 mi) above the equator. These Earth-orbiting systems are capable of receiving and relaying voice, data, and TV signals.
Cellular and PCS systems use several radio communications technologies. The systems divide the region covered into multiple geographic areas. Each area has a low-power transmitter or radio relay antenna device to relay calls from one area to the next area.
Radio and spread spectrum technologies – Wireless local area networks use a high-frequency radio technology similar to digital cellular and a low-frequency radio technology. Wireless LANs use spread spectrum technology to enable communication between multiple devices in a limited area. IEEE 802.11 defines a common flavor of open-standards wireless radio-wave technology known as Wifi.
Free-space optical communication uses visible or invisible light for communications. In most cases, line-of-sight propagation is used, which limits the physical positioning of communicating devices.

There have been various attempts at transporting data over exotic media:
IP over Avian Carriers was a humorous April fool's Request for Comments, issued as RFC 1149. It was implemented in real life in 2001.
Extending the Internet to interplanetary dimensions via radio waves, the Interplanetary Internet.
Both cases have a large round-trip delay time, which gives slow two-way communication, but doesn't prevent sending large amounts of information.

Apart from any physical transmission medium there may be, networks comprise additional basic system building blocks, such as network interface controller (NICs), repeaters, hubs, bridges, switches, routers, modems, and firewalls.

A network interface controller (NIC) is computer hardware that provides a computer with the ability to access the transmission media, and has the ability to process low-level network information. For example, the NIC may have a connector for accepting a cable, or an aerial for wireless transmission and reception, and the associated circuitry.
The NIC responds to traffic addressed to a network address for either the NIC or the computer as a whole.
In Ethernet networks, each network interface controller has a unique Media Access Control (MAC) address—usually stored in the controller's permanent memory. To avoid address conflicts between network devices, the Institute of Electrical and Electronics Engineers (IEEE) maintains and administers MAC address uniqueness. The size of an Ethernet MAC address is six octets. The three most significant octets are reserved to identify NIC manufacturers. These manufacturers, using only their assigned prefixes, uniquely assign the three least-significant octets of every Ethernet interface they produce.

A repeater is an electronic device that receives a network signal, cleans it of unnecessary noise and regenerates it. The signal is retransmitted at a higher power level, or to the other side of an obstruction, so that the signal can cover longer distances without degradation. In most twisted pair Ethernet configurations, repeaters are required for cable that runs longer than 100 meters. With fiber optics, repeaters can be tens or even hundreds of kilometers apart.
A repeater with multiple ports is known as a hub. Repeaters work on the physical layer of the OSI model. Repeaters require a small amount of time to regenerate the signal. This can cause a propagation delay that affects network performance. As a result, many network architectures limit the number of repeaters that can be used in a row, e.g., the Ethernet 5-4-3 rule.
Hubs have been mostly obsoleted by modern switches; but repeaters are used for long distance links, notably undersea cabling.

A network bridge connects and filters traffic between two network segments at the data link layer (layer 2) of the OSI model to form a single network. This breaks the network's collision domain but maintains a unified broadcast domain. Network segmentation breaks down a large, congested network into an aggregation of smaller, more efficient networks.
Bridges come in three basic types:
Local bridges: Directly connect LANs
Remote bridges: Can be used to create a wide area network (WAN) link between LANs. Remote bridges, where the connecting link is slower than the end networks, largely have been replaced with routers.
Wireless bridges: Can be used to join LANs or connect remote devices to LANs.

A network switch is a device that forwards and filters OSI layer 2 datagrams (frames) between ports based on the destination MAC address in each frame. A switch is distinct from a hub in that it only forwards the frames to the physical ports involved in the communication rather than all ports connected. It can be thought of as a multi-port bridge. It learns to associate physical ports to MAC addresses by examining the source addresses of received frames. If an unknown destination is targeted, the switch broadcasts to all ports but the source. Switches normally have numerous ports, facilitating a star topology for devices, and cascading additional switches.
Multi-layer switches are capable of routing based on layer 3 addressing or additional logical levels. The term switch is often used loosely to include devices such as routers and bridges, as well as devices that may distribute traffic based on load or based on application content (e.g., a Web URL identifier).

A router is an internetworking device that forwards packets between networks by processing the routing information included in the packet or datagram (Internet protocol information from layer 3). The routing information is often processed in conjunction with the routing table (or forwarding table). A router uses its routing table to determine where to forward packets. A destination in a routing table can include a "null" interface, also known as the "black hole" interface because data can go into it, however, no further processing is done for said data, i.e. the packets are dropped.

Modems (MOdulator-DEModulator) are used to connect network nodes via wire not originally designed for digital network traffic, or for wireless. To do this one or more carrier signals are modulated by the digital signal to produce an analog signal that can be tailored to give the required properties for transmission. Modems are commonly used for telephone lines, using a Digital Subscriber Line technology.

A firewall is a network device for controlling network security and access rules. Firewalls are typically configured to reject access requests from unrecognized sources while allowing actions from recognized ones. The vital role firewalls play in network security grows in parallel with the constant increase in cyber attacks.

Network topology is the layout or organizational hierarchy of interconnected nodes of a computer network. Different network topologies can affect throughput, but reliability is often more critical. With many technologies, such as bus networks, a single failure can cause the network to fail entirely. In general the more interconnections there are, the more robust the network is; but the more expensive it is to install.

Common layouts are:
A bus network: all nodes are connected to a common medium along this medium. This was the layout used in the original Ethernet, called 10BASE5 and 10BASE2.
A star network: all nodes are connected to a special central node. This is the typical layout found in a Wireless LAN, where each wireless client connects to the central Wireless access point.
A ring network: each node is connected to its left and right neighbour node, such that all nodes are connected and that each node can reach each other node by traversing nodes left- or rightwards. The Fiber Distributed Data Interface (FDDI) made use of such a topology.
A mesh network: each node is connected to an arbitrary number of neighbours in such a way that there is at least one traversal from any node to any other.
A fully connected network: each node is connected to every other node in the network.
A tree network: nodes are arranged hierarchically.
Note that the physical layout of the nodes in a network may not necessarily reflect the network topology. As an example, with FDDI, the network topology is a ring (actually two counter-rotating rings), but the physical topology is often a star, because all neighboring connections can be routed via a central physical location.

An overlay network is a virtual computer network that is built on top of another network. Nodes in the overlay network are connected by virtual or logical links. Each link corresponds to a path, perhaps through many physical links, in the underlying network. The topology of the overlay network may (and often does) differ from that of the underlying one. For example, many peer-to-peer networks are overlay networks. They are organized as nodes of a virtual system of links that run on top of the Internet.
Overlay networks have been around since the invention of networking when computer systems were connected over telephone lines using modems, before any data network existed.
The most striking example of an overlay network is the Internet itself. The Internet itself was initially built as an overlay on the telephone network. Even today, each Internet node can communicate with virtually any other through an underlying mesh of sub-networks of wildly different topologies and technologies. Address resolution and routing are the means that allow mapping of a fully connected IP overlay network to its underlying network.
Another example of an overlay network is a distributed hash table, which maps keys to nodes in the network. In this case, the underlying network is an IP network, and the overlay network is a table (actually a map) indexed by keys.
Overlay networks have also been proposed as a way to improve Internet routing, such as through quality of service guarantees to achieve higher-quality streaming media. Previous proposals such as IntServ, DiffServ, and IP Multicast have not seen wide acceptance largely because they require modification of all routers in the network. On the other hand, an overlay network can be incrementally deployed on end-hosts running the overlay protocol software, without cooperation from Internet service providers. The overlay network has no control over how packets are routed in the underlying network between two overlay nodes, but it can control, for example, the sequence of overlay nodes that a message traverses before it reaches its destination.
For example, Akamai Technologies manages an overlay network that provides reliable, efficient content delivery (a kind of multicast). Academic research includes end system multicast, resilient routing and quality of service studies, among others.

A communications protocol is a set of rules for exchanging information over network links. In a protocol stack (also see the OSI model), each protocol leverages the services of the protocol below it. An important example of a protocol stack is HTTP (the World Wide Web protocol) running over TCP over IP (the Internet protocols) over IEEE 802.11 (the Wi-Fi protocol). This stack is used between the wireless router and the home user's personal computer when the user is surfing the web.
Whilst the use of protocol layering is today ubiquitous across the field of computer networking, it has been historically criticized by many researchers for two principal reasons. Firstly, abstracting the protocol stack in this way may cause a higher layer to duplicate functionality of a lower layer, a prime example being error recovery on both a per-link basis and an end-to-end basis. Secondly, it is common that a protocol implementation at one layer may require data, state or addressing information that is only present at another layer, thus defeating the point of separating the layers in the first place. For example, TCP uses the ECN field in the IPv4 header as an indication of congestion; IP is a network layer protocol whereas TCP is a transport layer protocol.
Communication protocols have various characteristics. They may be connection-oriented or connectionless, they may use circuit mode or packet switching, and they may use hierarchical addressing or flat addressing.
There are many communication protocols, a few of which are described below.

IEEE 802 is a family of IEEE standards dealing with local area networks and metropolitan area networks. The complete IEEE 802 protocol suite provides a diverse set of networking capabilities. The protocols have a flat addressing scheme. They operate mostly at levels 1 and 2 of the OSI model.
For example, MAC bridging (IEEE 802.1D) deals with the routing of Ethernet packets using a Spanning Tree Protocol. IEEE 802.1Q describes VLANs, and IEEE 802.1X defines a port-based Network Access Control protocol, which forms the basis for the authentication mechanisms used in VLANs (but it is also found in WLANs) – it is what the home user sees when the user has to enter a "wireless access key".

Ethernet, sometimes simply called LAN, is a family of protocols used in wired LANs, described by a set of standards together called IEEE 802.3 published by the Institute of Electrical and Electronics Engineers.

Wireless LAN, also widely known as WLAN or WiFi, is probably the most well-known member of the IEEE 802 protocol family for home users today. It is standarized by IEEE 802.11 and shares many properties with wired Ethernet.

The Internet Protocol Suite, also called TCP/IP, is the foundation of all modern networking. It offers connection-less as well as connection-oriented services over an inherently unreliable network traversed by data-gram transmission at the Internet protocol (IP) level. At its core, the protocol suite defines the addressing, identification, and routing specifications for Internet Protocol Version 4 (IPv4) and for IPv6, the next generation of the protocol with a much enlarged addressing capability.

Synchronous optical networking (SONET) and Synchronous Digital Hierarchy (SDH) are standardized multiplexing protocols that transfer multiple digital bit streams over optical fiber using lasers. They were originally designed to transport circuit mode communications from a variety of different sources, primarily to support real-time, uncompressed, circuit-switched voice encoded in PCM (Pulse-Code Modulation) format. However, due to its protocol neutrality and transport-oriented features, SONET/SDH also was the obvious choice for transporting Asynchronous Transfer Mode (ATM) frames.

Asynchronous Transfer Mode (ATM) is a switching technique for telecommunication networks. It uses asynchronous time-division multiplexing and encodes data into small, fixed-sized cells. This differs from other protocols such as the Internet Protocol Suite or Ethernet that use variable sized packets or frames. ATM has similarity with both circuit and packet switched networking. This makes it a good choice for a network that must handle both traditional high-throughput data traffic, and real-time, low-latency content such as voice and video. ATM uses a connection-oriented model in which a virtual circuit must be established between two endpoints before the actual data exchange begins.
While the role of ATM is diminishing in favor of next-generation networks, it still plays a role in the last mile, which is the connection between an Internet service provider and the home user.

A network can be characterized by its physical capacity or its organizational purpose. Use of the network, including user authorization and access rights, differ accordingly.
Nanoscale network
A nanoscale communication network has key components implemented at the nanoscale including message carriers and leverages physical principles that differ from macroscale communication mechanisms. Nanoscale communication extends communication to very small sensors and actuators such as those found in biological systems and also tends to operate in environments that would be too harsh for classical communication.
Personal area network
A personal area network (PAN) is a computer network used for communication among computer and different information technological devices close to one person. Some examples of devices that are used in a PAN are personal computers, printers, fax machines, telephones, PDAs, scanners, and even video game consoles. A PAN may include wired and wireless devices. The reach of a PAN typically extends to 10 meters. A wired PAN is usually constructed with USB and FireWire connections while technologies such as Bluetooth and infrared communication typically form a wireless PAN.
Local area network
A local area network (LAN) is a network that connects computers and devices in a limited geographical area such as a home, school, office building, or closely positioned group of buildings. Each computer or device on the network is a node. Wired LANs are most likely based on Ethernet technology. Newer standards such as ITU-T G.hn also provide a way to create a wired LAN using existing wiring, such as coaxial cables, telephone lines, and power lines.
The defining characteristics of a LAN, in contrast to a wide area network (WAN), include higher data transfer rates, limited geographic range, and lack of reliance on leased lines to provide connectivity. Current Ethernet or other IEEE 802.3 LAN technologies operate at data transfer rates up to 100 Gbit/s, standarized by IEEE in 2010. Currently, 400 Gbit/s Ethernet is being developed.
A LAN can be connected to a WAN using a router.
Home area network
A home area network (HAN) is a residential LAN used for communication between digital devices typically deployed in the home, usually a small number of personal computers and accessories, such as printers and mobile computing devices. An important function is the sharing of Internet access, often a broadband service through a cable TV or digital subscriber line (DSL) provider.
Storage area network
A storage area network (SAN) is a dedicated network that provides access to consolidated, block level data storage. SANs are primarily used to make storage devices, such as disk arrays, tape libraries, and optical jukeboxes, accessible to servers so that the devices appear like locally attached devices to the operating system. A SAN typically has its own network of storage devices that are generally not accessible through the local area network by other devices. The cost and complexity of SANs dropped in the early 2000s to levels allowing wider adoption across both enterprise and small to medium-sized business environments.
Campus area network
A campus area network (CAN) is made up of an interconnection of LANs within a limited geographical area. The networking equipment (switches, routers) and transmission media (optical fiber, copper plant, Cat5 cabling, etc.) are almost entirely owned by the campus tenant / owner (an enterprise, university, government, etc.).
For example, a university campus network is likely to link a variety of campus buildings to connect academic colleges or departments, the library, and student residence halls.
Backbone network
A backbone network is part of a computer network infrastructure that provides a path for the exchange of information between different LANs or sub-networks. A backbone can tie together diverse networks within the same building, across different buildings, or over a wide area.
For example, a large company might implement a backbone network to connect departments that are located around the world. The equipment that ties together the departmental networks constitutes the network backbone. When designing a network backbone, network performance and network congestion are critical factors to take into account. Normally, the backbone network's capacity is greater than that of the individual networks connected to it.
Another example of a backbone network is the Internet backbone, which is the set of wide area networks (WANs) and core routers that tie together all networks connected to the Internet.
Metropolitan area network
A Metropolitan area network (MAN) is a large computer network that usually spans a city or a large campus.
Wide area network
A wide area network (WAN) is a computer network that covers a large geographic area such as a city, country, or spans even intercontinental distances. A WAN uses a communications channel that combines many types of media such as telephone lines, cables, and air waves. A WAN often makes use of transmission facilities provided by common carriers, such as telephone companies. WAN technologies generally function at the lower three layers of the OSI reference model: the physical layer, the data link layer, and the network layer.
Enterprise private network
An enterprise private network is a network that a single organization builds to interconnect its office locations (e.g., production sites, head offices, remote offices, shops) so they can share computer resources.
Virtual private network
A virtual private network (VPN) is an overlay network in which some of the links between nodes are carried by open connections or virtual circuits in some larger network (e.g., the Internet) instead of by physical wires. The data link layer protocols of the virtual network are said to be tunneled through the larger network when this is the case. One common application is secure communications through the public Internet, but a VPN need not have explicit security features, such as authentication or content encryption. VPNs, for example, can be used to separate the traffic of different user communities over an underlying network with strong security features.
VPN may have best-effort performance, or may have a defined service level agreement (SLA) between the VPN customer and the VPN service provider. Generally, a VPN has a topology more complex than point-to-point.
Global area network
A global area network (GAN) is a network used for supporting mobile across an arbitrary number of wireless LANs, satellite coverage areas, etc. The key challenge in mobile communications is handing off user communications from one local coverage area to the next. In IEEE Project 802, this involves a succession of terrestrial wireless LANs.

Networks are typically managed by the organizations that own them. Private enterprise networks may use a combination of intranets and extranets. They may also provide network access to the Internet, which has no single owner and permits virtually unlimited global connectivity.

An intranet is a set of networks that are under the control of a single administrative entity. The intranet uses the IP protocol and IP-based tools such as web browsers and file transfer applications. The administrative entity limits use of the intranet to its authorized users. Most commonly, an intranet is the internal LAN of an organization. A large intranet typically has at least one web server to provide users with organizational information. An intranet is also anything behind the router on a local area network.

An extranet is a network that is also under the administrative control of a single organization, but supports a limited connection to a specific external network. For example, an organization may provide access to some aspects of its intranet to share data with its business partners or customers. These other entities are not necessarily trusted from a security standpoint. Network connection to an extranet is often, but not always, implemented via WAN technology.

An internetwork is the connection of multiple computer networks via a common routing technology using routers.

The Internet is the largest example of an internetwork. It is a global system of interconnected governmental, academic, corporate, public, and private computer networks. It is based on the networking technologies of the Internet Protocol Suite. It is the successor of the Advanced Research Projects Agency Network (ARPANET) developed by DARPA of the United States Department of Defense. The Internet is also the communications backbone underlying the World Wide Web (WWW).
Participants in the Internet use a diverse array of methods of several hundred documented, and often standardized, protocols compatible with the Internet Protocol Suite and an addressing system (IP addresses) administered by the Internet Assigned Numbers Authority and address registries. Service providers and large enterprises exchange information about the reachability of their address spaces through the Border Gateway Protocol (BGP), forming a redundant worldwide mesh of transmission paths.

A darknet is an overlay network, typically running on the internet, that is only accessible through specialized software. A darknet is an anonymizing network where connections are made only between trusted peers — sometimes called "friends" (F2F) — using non-standard protocols and ports.
Darknets are distinct from other distributed peer-to-peer networks as sharing is anonymous (that is, IP addresses are not publicly shared), and therefore users can communicate with little fear of governmental or corporate interference.

Routing is the process of selecting network paths to carry network traffic. Routing is performed for many kinds of networks, including circuit switching networks and packet switched networks.
In packet switched networks, routing directs packet forwarding (the transit of logically addressed network packets from their source toward their ultimate destination) through intermediate nodes. Intermediate nodes are typically network hardware devices such as routers, bridges, gateways, firewalls, or switches. General-purpose computers can also forward packets and perform routing, though they are not specialized hardware and may suffer from limited performance. The routing process usually directs forwarding on the basis of routing tables, which maintain a record of the routes to various network destinations. Thus, constructing routing tables, which are held in the router's memory, is very important for efficient routing.
There are usually multiple routes that can be taken, and to choose between them, different elements can be considered to decide which routes get installed into the routing table, such as (sorted by priority):
Prefix-Length: where longer subnet masks are preferred (independent if it is within a routing protocol or over different routing protocol)
Metric: where a lower metric/cost is preferred (only valid within one and the same routing protocol)
Administrative distance: where a lower distance is preferred (only valid between different routing protocols)
Most routing algorithms use only one network path at a time. Multipath routing techniques enable the use of multiple alternative paths.
Routing, in a more narrow sense of the term, is often contrasted with bridging in its assumption that network addresses are structured and that similar addresses imply proximity within the network. Structured addresses allow a single routing table entry to represent the route to a group of devices. In large networks, structured addressing (routing, in the narrow sense) outperforms unstructured addressing (bridging). Routing has become the dominant form of addressing on the Internet. Bridging is still widely used within localized environments.

Network services are applications hosted by servers on a computer network, to provide some functionality for members or users of the network, or to help the network itself to operate.
The World Wide Web, E-mail, printing and network file sharing are examples of well-known network services. Network services such as DNS (Domain Name System) give names for IP and MAC addresses (people remember names like “nm.lan” better than numbers like “210.121.67.18”), and DHCP to ensure that the equipment on the network has a valid IP address.
Services are usually based on a service protocol that defines the format and sequencing of messages between clients and servers of that network service.

Depending on the installation requirements, network performance is usually measured by the quality of service of a telecommunications product. The parameters that affect this typically can include throughput, jitter, bit error rate and latency.
The following list gives examples of network performance measures for a circuit-switched network and one type of packet-switched network, viz. ATM:
Circuit-switched networks: In circuit switched networks, network performance is synonymous with the grade of service. The number of rejected calls is a measure of how well the network is performing under heavy traffic loads. Other types of performance measures can include the level of noise and echo.
ATM: In an Asynchronous Transfer Mode (ATM) network, performance can be measured by line rate, quality of service (QoS), data throughput, connect time, stability, technology, modulation technique and modem enhancements.
There are many ways to measure the performance of a network, as each network is different in nature and design. Performance can also be modelled instead of measured. For example, state transition diagrams are often used to model queuing performance in a circuit-switched network. The network planner uses these diagrams to analyze how the network performs in each state, ensuring that the network is optimally designed.

Network congestion occurs when a link or node is carrying so much data that its quality of service deteriorates. Typical effects include queueing delay, packet loss or the blocking of new connections. A consequence of these latter two is that incremental increases in offered load lead either only to small increase in network throughput, or to an actual reduction in network throughput.
Network protocols that use aggressive retransmissions to compensate for packet loss tend to keep systems in a state of network congestion—even after the initial load is reduced to a level that would not normally induce network congestion. Thus, networks using these protocols can exhibit two stable states under the same level of load. The stable state with low throughput is known as congestive collapse.
Modern networks use congestion control and congestion avoidance techniques to try to avoid congestion collapse. These include: exponential backoff in protocols such as 802.11's CSMA/CA and the original Ethernet, window reduction in TCP, and fair queueing in devices such as routers. Another method to avoid the negative effects of network congestion is implementing priority schemes, so that some packets are transmitted with higher priority than others. Priority schemes do not solve network congestion by themselves, but they help to alleviate the effects of congestion for some services. An example of this is 802.1p. A third method to avoid network congestion is the explicit allocation of network resources to specific flows. One example of this is the use of Contention-Free Transmission Opportunities (CFTXOPs) in the ITU-T G.hn standard, which provides high-speed (up to 1 Gbit/s) Local area networking over existing home wires (power lines, phone lines and coaxial cables).
For the Internet RFC 2914 addresses the subject of congestion control in detail.

Network resilience is "the ability to provide and maintain an acceptable level of service in the face of faults and challenges to normal operation.”

Network security consists of provisions and policies adopted by the network administrator to prevent and monitor unauthorized access, misuse, modification, or denial of the computer network and its network-accessible resources. Network security is the authorization of access to data in a network, which is controlled by the network administrator. Users are assigned an ID and password that allows them access to information and programs within their authority. Network security is used on a variety of computer networks, both public and private, to secure daily transactions and communications among businesses, government agencies and individuals.

Network surveillance is the monitoring of data being transferred over computer networks such as the Internet. The monitoring is often done surreptitiously and may be done by or at the behest of governments, by corporations, criminal organizations, or individuals. It may or may not be legal and may or may not require authorization from a court or other independent agency.
Computer and network surveillance programs are widespread today, and almost all Internet traffic is or could potentially be monitored for clues to illegal activity.
Surveillance is very useful to governments and law enforcement to maintain social control, recognize and monitor threats, and prevent/investigate criminal activity. With the advent of programs such as the Total Information Awareness program, technologies such as high speed surveillance computers and biometrics software, and laws such as the Communications Assistance For Law Enforcement Act, governments now possess an unprecedented ability to monitor the activities of citizens.
However, many civil rights and privacy groups—such as Reporters Without Borders, the Electronic Frontier Foundation, and the American Civil Liberties Union—have expressed concern that increasing surveillance of citizens may lead to a mass surveillance society, with limited political and personal freedoms. Fears such as this have led to numerous lawsuits such as Hepting v. AT&T. The hacktivist group Anonymous has hacked into government websites in protest of what it considers "draconian surveillance".

End-to-end encryption (E2EE) is a digital communications paradigm of uninterrupted protection of data traveling between two communicating parties. It involves the originating party encrypting data so only the intended recipient can decrypt it, with no dependency on third parties. End-to-end encryption prevents intermediaries, such as Internet providers or application service providers, from discovering or tampering with communications. End-to-end encryption generally protects both confidentiality and integrity.
Examples of end-to-end encryption include PGP for email, OTR for instant messaging, ZRTP for telephony, and TETRA for radio.
Typical server-based communications systems do not include end-to-end encryption. These systems can only guarantee protection of communications between clients and servers, not between the communicating parties themselves. Examples of non-E2EE systems are Google Talk, Yahoo Messenger, Facebook, and Dropbox. Some such systems, for example LavaBit and SecretInk, have even described themselves as offering "end-to-end" encryption when they do not. Some systems that normally offer end-to-end encryption have turned out to contain a back door that subverts negotiation of the encryption key between the communicating parties, for example Skype or Hushmail.
The end-to-end encryption paradigm does not directly address risks at the communications endpoints themselves, such as the technical exploitation of clients, poor quality random number generators, or key escrow. E2EE also does not address traffic analysis, which relates to things such as the identities of the end points and the times and quantities of messages that are sent.

Users and network administrators typically have different views of their networks. Users can share printers and some servers from a workgroup, which usually means they are in the same geographic location and are on the same LAN, whereas a Network Administrator is responsible to keep that network up and running. A community of interest has less of a connection of being in a local area, and should be thought of as a set of arbitrarily located users who share a set of servers, and possibly also communicate via peer-to-peer technologies.
Network administrators can see networks from both physical and logical perspectives. The physical perspective involves geographic locations, physical cabling, and the network elements (e.g., routers, bridges and application layer gateways) that interconnect via the transmission media. Logical networks, called, in the TCP/IP architecture, subnets, map onto one or more transmission media. For example, a common practice in a campus of buildings is to make a set of LAN cables in each building appear to be a common subnet, using virtual LAN (VLAN) technology.
Both users and administrators are aware, to varying extents, of the trust and scope characteristics of a network. Again using TCP/IP architectural terminology, an intranet is a community of interest under private administration usually by an enterprise, and is only accessible by authorized users (e.g. employees). Intranets do not have to be connected to the Internet, but generally have a limited connection. An extranet is an extension of an intranet that allows secure communications to users outside of the intranet (e.g. business partners, customers).
Unofficially, the Internet is the set of users, enterprises, and content providers that are interconnected by Internet Service Providers (ISP). From an engineering viewpoint, the Internet is the set of subnets, and aggregates of subnets, which share the registered IP address space and exchange information about the reachability of those IP addresses using the Border Gateway Protocol. Typically, the human-readable names of servers are translated to IP addresses, transparently to users, via the directory function of the Domain Name System (DNS).
Over the Internet, there can be business-to-business (B2B), business-to-consumer (B2C) and consumer-to-consumer (C2C) communications. When money or sensitive information is exchanged, the communications are apt to be protected by some form of communications security mechanism. Intranets and extranets can be securely superimposed onto the Internet, without any access by general Internet users and administrators, using secure Virtual Private Network (VPN) technology.

Comparison of network diagram software
Cyberspace
History of the Internet
Network simulation
Network planning and design

 This article incorporates public domain material from the General Services Administration document "Federal Standard 1037C".

Shelly, Gary, et al. "Discovering Computers" 2003 Edition.
Wendell Odom, Rus Healy, Denise Donohue. (2010) CCIE Routing and Switching. Indianapolis, IN: Cisco Press
Kurose James F and Keith W. Ross : Computer Networking: A Top-Down Approach Featuring the Internet, Pearson Education 2005.
William Stallings, Computer Networking with Internet Protocols and Technology, Pearson Education 2004.
Important publications in computer networks
Network Communication Architecture and Protocols: OSI Network Architecture 7 Layers Model
Dimitri Bertsekas, and Robert Gallager, "Data Networks," Prentice Hall, 1992.

Networking at DMOZ
IEEE Ethernet manufacturer information
A computer networking acronym guideLogic programming is a type of programming paradigm which is largely based on formal logic. Any program written in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain. Major logic programming language families include Prolog, Answer set programming (ASP) and Datalog. In all of these languages, rules are written in the form of clauses:
H :- B1, …, Bn.
and are read declaratively as logical implications:
H if B1 and … and Bn.
H is called the head of the rule and B1, …, Bn is called the body. Facts are rules that have no body, and are written in the simplified form:
H.
In the simplest case in which H, B1, …, Bn are all atomic formulae, these clauses are called definite clauses or Horn clauses. However, there exist many extensions of this simple case, the most important one being the case in which conditions in the body of a clause can also be negations of atomic formulae. Logic programming languages that include this extension have the knowledge representation capabilities of a non-monotonic logic.
In ASP and Datalog, logic programs have only a declarative reading, and their execution is performed by means of a proof procedure or model generator whose behaviour is not meant to be under the control of the programmer. However, in the Prolog family of languages, logic programs also have a procedural interpretation as goal-reduction procedures:
to solve H, solve B1, and ... and solve Bn.
Consider, for example, the following clause:
fallible(X) :- human(X).
based on an example used by Terry Winograd  to illustrate the programming language Planner. As a clause in a logic program, it can be used both as a procedure to test whether X is fallible by testing whether X is human, and as a procedure to find an X that is fallible by finding an X that is human. Even facts have a procedural interpretation. For example, the clause:
human(socrates).
can be used both as a procedure to show that socrates is human, and as a procedure to find an X that is human by "assigning" socrates to X.
The declarative reading of logic programs can be used by a programmer to verify their correctness. Moreover, logic-based program transformation techniques can also be used to transform logic programs into logically equivalent programs that are more efficient. In the Prolog family of logic programming languages, the programmer can also use the known problem-solving behaviour of the execution mechanism to improve the efficiency of programs.

The use of mathematical logic to represent and execute computer programs is also a feature of the lambda calculus, developed by Alonzo Church in the 1930s. However, the first proposal to use the clausal form of logic for representing computer programs was made by Cordell Green. This used an axiomatization of a subset of LISP, together with a representation of an input-output relation, to compute the relation by simulating the execution of the program in LISP. Foster and Elcock's Absys, on the other hand, employed a combination of equations and lambda calculus in an assertional programming language which places no constraints on the order in which operations are performed.
Logic programming in its present form can be traced back to debates in the late 1960s and early 1970s about declarative versus procedural representations of knowledge in Artificial Intelligence. Advocates of declarative representations were notably working at Stanford, associated with John McCarthy, Bertram Raphael and Cordell Green, and in Edinburgh, with John Alan Robinson (an academic visitor from Syracuse University), Pat Hayes, and Robert Kowalski. Advocates of procedural representations were mainly centered at MIT, under the leadership of Marvin Minsky and Seymour Papert.
Although it was based on the proof methods of logic, Planner, developed at MIT, was the first language to emerge within this proceduralist paradigm. Planner featured pattern-directed invocation of procedural plans from goals (i.e. goal-reduction or backward chaining) and from assertions (i.e. forward chaining). The most influential implementation of Planner was the subset of Planner, called Micro-Planner, implemented by Gerry Sussman, Eugene Charniak and Terry Winograd. It was used to implement Winograd's natural-language understanding program SHRDLU, which was a landmark at that time. To cope with the very limited memory systems at the time, Planner used a backtracking control structure so that only one possible computation path had to be stored at a time. Planner gave rise to the programming languages QA-4, Popler, Conniver, QLISP, and the concurrent language Ether.
Hayes and Kowalski in Edinburgh tried to reconcile the logic-based declarative approach to knowledge representation with Planner's procedural approach. Hayes (1973) developed an equational language, Golux, in which different procedures could be obtained by altering the behavior of the theorem prover. Kowalski, on the other hand, developed SLD resolution, a variant of SL-resolution, and showed how it treats implications as goal-reduction procedures. Kowalski collaborated with Colmerauer in Marseille, who developed these ideas in the design and implementation of the programming language Prolog.
The Association for Logic Programming was founded to promote Logic Programming in 1986.
Prolog gave rise to the programming languages ALF, Fril, Gödel, Mercury, Oz, Ciao, Visual Prolog, XSB, and λProlog, as well as a variety of concurrent logic programming languages, constraint logic programming languages and datalog.

Logic programming can be viewed as controlled deduction. An important concept in logic programming is the separation of programs into their logic component and their control component. With pure logic programming languages, the logic component alone determines the solutions produced. The control component can be varied to provide alternative ways of executing a logic program. This notion is captured by the slogan
Algorithm = Logic + Control
where "Logic" represents a logic program and "Control" represents different theorem-proving strategies.

In the simplified, propositional case in which a logic program and a top-level atomic goal contain no variables, backward reasoning determines an and-or tree, which constitutes the search space for solving the goal. The top-level goal is the root of the tree. Given any node in the tree and any clause whose head matches the node, there exists a set of child nodes corresponding to the sub-goals in the body of the clause. These child nodes are grouped together by an "and". The alternative sets of children corresponding to alternative ways of solving the node are grouped together by an "or".
Any search strategy can be used to search this space. Prolog uses a sequential, last-in-first-out, backtracking strategy, in which only one alternative and one sub-goal is considered at a time. Other search strategies, such as parallel search, intelligent backtracking, or best-first search to find an optimal solution, are also possible.
In the more general case, where sub-goals share variables, other strategies can be used, such as choosing the subgoal that is most highly instantiated or that is sufficiently instantiated so that only one procedure applies. Such strategies are used, for example, in concurrent logic programming.

For most practical applications, as well as for applications that require non-monotonic reasoning in artificial intelligence, Horn clause logic programs need to be extended to normal logic programs, with negative conditions. A clause in a normal logic program has the form:
H :- A1, …, An, not B1, …, not Bn.
and is read declaratively as a logical implication:
H if A1 and … and An and not B1 and … and not Bn.
where H and all the Ai and Bi are atomic formulas. The negation in the negative literals not Bi is commonly referred to as "negation as failure", because in most implementations, a negative condition not Bi is shown to hold by showing that the positive condition Bi fails to hold. For example:

Given the goal of finding something that can fly:

there are two candidate solutions, which solve the first subgoal bird(X), namely X  mary is the only solution of the goal.
Micro-Planner had a construct, called "thnot", which when applied to an expression returns the value true if (and only if) the evaluation of the expression fails. An equivalent operator is normally built-in in modern Prolog's implementations. It is normally written as not(Goal) or \+ Goal, where Goal is some goal (proposition) to be proved by the program. This operator differs from negation in first-order logic: a negation such as \+ X  1 can succeed, binding X to 1, depending on whether X was initially bound (note that standard Prolog executes goals in left-to-right order).
The logical status of negation as failure was unresolved until Keith Clark [1978] showed that, under certain natural conditions, it is a correct (and sometimes complete) implementation of classical negation with respect to the completion of the program. Completion amounts roughly to regarding the set of all the program clauses with the same predicate on the left hand side, say
H :- Body1.
…
H :- Bodyk.
as a definition of the predicate
H iff (Body1 or … or Bodyk)
where "iff" means "if and only if". Writing the completion also requires explicit use of the equality predicate and the inclusion of a set of appropriate axioms for equality. However, the implementation of negation by failure needs only the if-halves of the definitions without the axioms of equality.
For example, the completion of the program above is:
canfly(X) iff bird(X), not abnormal(X).
abnormal(X) iff wounded(X).
bird(X) iff X  mary.
X = X.
not john = mary.
not mary = john.
The notion of completion is closely related to McCarthy's circumscription semantics for default reasoning, and to the closed world assumption.
As an alternative to the completion semantics, negation as failure can also be interpreted epistemically, as in the stable model semantics of answer set programming. In this interpretation not(Bi) means literally that Bi is not known or not believed. The epistemic interpretation has the advantage that it can be combined very simply with classical negation, as in "extended logic programming", to formalise such phrases as "the contrary can not be shown", where "contrary" is classical negation and "can not be shown" is the epistemic interpretation of negation as failure.

The fact that Horn clauses can be given a procedural interpretation and, vice versa, that goal-reduction procedures can be understood as Horn clauses + backward reasoning means that logic programs combine declarative and procedural representations of knowledge. The inclusion of negation as failure means that logic programming is a kind of non-monotonic logic.
Despite its simplicity compared with classical logic, this combination of Horn clauses and negation as failure has proved to be surprisingly expressive. For example, it provides a natural representation for the common-sense laws of cause and effect, as formalised by both the situation calculus and event calculus. It has also been shown to correspond quite naturally to the semi-formal language of legislation. In particular, Prakken and Sartor  credit the representation of the British Nationality Act as a logic program  with being "hugely influential for the development of computational representations of legislation, showing how logic programming enables intuitively appealing representations that can be directly deployed to generate automatic inferences".

The programming language Prolog was developed in 1972 by Alain Colmerauer. It emerged from a collaboration between Colmerauer in Marseille and Robert Kowalski in Edinburgh. Colmerauer was working on natural language understanding, using logic to represent semantics and using resolution for question-answering. During the summer of 1971, Colmerauer and Kowalski discovered that the clausal form of logic could be used to represent formal grammars and that resolution theorem provers could be used for parsing. They observed that some theorem provers, like hyper-resolution, behave as bottom-up parsers and others, like SL-resolution (1971), behave as top-down parsers.
It was in the following summer of 1972, that Kowalski, again working with Colmerauer, developed the procedural interpretation of implications. This dual declarative/procedural interpretation later became formalised in the Prolog notation
H :- B1, …, Bn.
which can be read (and used) both declaratively and procedurally. It also became clear that such clauses could be restricted to definite clauses or Horn clauses, where H, B1, …, Bn are all atomic predicate logic formulae, and that SL-resolution could be restricted (and generalised) to LUSH or SLD-resolution. Kowalski's procedural interpretation and LUSH were described in a 1973 memo, published in 1974.
Colmerauer, with Philippe Roussel, used this dual interpretation of clauses as the basis of Prolog, which was implemented in the summer and autumn of 1972. The first Prolog program, also written in 1972 and implemented in Marseille, was a French question-answering system. The use of Prolog as a practical programming language was given great momentum by the development of a compiler by David Warren in Edinburgh in 1977. Experiments demonstrated that Edinburgh Prolog could compete with the processing speed of other symbolic programming languages such as Lisp. Edinburgh Prolog became the de facto standard and strongly influenced the definition of ISO standard Prolog.

Abductive logic programming is an extension of normal Logic Programming that allows some predicates, declared as abducible predicates, to be "open" or undefined. A clause in an abductive logic program has the form:
H :- B1, …, Bn, A1, …, An.
where H is an atomic formula that is not abducible, all the Bi are literals whose predicates are not abducible, and the Ai are atomic formulas whose predicates are abducible. The abducible predicates can be constrained by integrity constraints, which can have the form:
false :- B1, …, Bn.
where the Bi are arbitrary literals (defined or abducible, and atomic or negated). For example:

where the predicate normal is abducible.
Problem solving is achieved by deriving hypotheses expressed in terms of the abducible predicates as solutions of problems to be solved. These problems can be either observations that need to be explained (as in classical abductive reasoning) or goals to be solved (as in normal logic programming). For example, the hypothesis normal(mary) explains the observation canfly(mary). Moreover, the same hypothesis entails the only solution X = mary of the goal of finding something that can fly:

Abductive logic programming has been used for fault diagnosis, planning, natural language processing and machine learning. It has also been used to interpret Negation as Failure as a form of abductive reasoning.

Because mathematical logic has a long tradition of distinguishing between object language and metalanguage, logic programming also allows metalevel programming. The simplest metalogic program is the so-called "vanilla" meta-interpreter:

where true represents an empty conjunction, and clause(A,B) means there is an object-level clause of the form A :- B.
Metalogic programming allows object-level and metalevel representations to be combined, as in natural language. It can also be used to implement any logic that is specified by means of inference rules. Metalogic is used in logic programming to implement metaprograms, which manipulate other programs, databases, knowledge bases or axiomatic theories as data.

Constraint logic programming combines Horn clause logic programming with constraint solving. It extends Horn clauses by allowing some predicates, declared as constraint predicates, to occur as literals in the body of clauses. A constraint logic program is a set of clauses of the form:
H :- C1, …, Cn



        ◊


    {\displaystyle \Diamond }
   B1, …, Bn.
where H and all the Bi are atomic formulas, and the Ci are constraints. Declaratively, such clauses are read as ordinary logical implications:
H if C1 and … and Cn and B1 and … and Bn.
However, whereas the predicates in the heads of clauses are defined by the constraint logic program, the predicates in the constraints are predefined by some domain-specific model-theoretic structure or theory.
Procedurally, subgoals whose predicates are defined by the program are solved by goal-reduction, as in ordinary logic programming, but constraints are checked for satisfiability by a domain-specific constraint-solver, which implements the semantics of the constraint predicates. An initial problem is solved by reducing it to a satisfiable conjunction of constraints.
The following constraint logic program represents a toy temporal database of john's history as a teacher:
teaches(john, hardware, T) :- 1990 ≤ T, T < 1999.
teaches(john, software, T) :- 1999 ≤ T, T < 2005.
teaches(john, logic, T) :- 2005 ≤ T, T ≤ 2012.
rank(john, instructor, T) :- 1990 ≤ T, T < 2010.
rank(john, professor, T) :- 2010 ≤ T, T < 2014.
Here ≤ and < are constraint predicates, with their usual intended semantics. The following goal clause queries the database to find out when john both taught logic and was a professor:
:- teaches(john, logic, T), rank(john, professor, T).
The solution is 2010 ≤ T, T ≤ 2012.
Constraint logic programming has been used to solve problems in such fields as civil engineering, mechanical engineering, digital circuit verification, automated timetabling, air traffic control, and finance. It is closely related to abductive logic programming.

Concurrent logic programming integrates concepts of logic programming with concurrent programming. Its development was given a big impetus in the 1980s by its choice for the systems programming language of the Japanese Fifth Generation Project (FGCS).
A concurrent logic program is a set of guarded Horn clauses of the form:

H :- G1, …, Gn | B1, …, Bn.

The conjunction G1, … , Gn is called the guard of the clause, and | is the commitment operator. Declaratively, guarded Horn clauses are read as ordinary logical implications:

H if G1 and … and Gn and B1 and … and Bn.

However, procedurally, when there are several clauses whose heads H match a given goal, then all of the clauses are executed in parallel, checking whether their guards G1, … , Gn hold. If the guards of more than one clause hold, then a committed choice is made to one of the clauses, and execution proceedes with the subgoals B1, …, Bn of the chosen clause. These subgoals can also be executed in parallel. Thus concurrent logic programming implements a form of "don't care nondeterminism", rather than "don't know nondeterminism".
For example, the following concurrent logic program defines a predicate shuffle(Left, Right, Merge) , which can be used to shuffle two lists Left and Right, combining them into a single list Merge that preserves the ordering of the two lists Left and Right:

Here, [] represents the empty list, and [Head | Tail] represents a list with first element Head followed by list Tail, as in Prolog. (Notice that the first occurrence of | in the second and third clauses is the list constructor, whereas the second occurrence of | is the commitment operator.) The program can be used, for example, to shuffle the lists [ace, queen, king] and [1, 4, 2] by invoking the goal clause:

The program will non-deterministically generate a single solution, for example Merge = [ace, queen, 1, king, 4, 2].
Arguably, concurrent logic programming is based on message passing and consequently is subject to the same indeterminacy as other concurrent message-passing systems, such as Actors (see Indeterminacy in concurrent computation). Carl Hewitt has argued that, concurrent logic programming is not based on logic in his sense that computational steps cannot be logically deduced. However, in concurrent logic programming, any result of a terminating computation is a logical consequence of the program, and any partial result of a partial computation is a logical consequence of the program and the residual goal (process network). Consequently, the indeterminacy of computations implies that not all logical consequences of the program can be deduced.

Concurrent constraint logic programming combines concurrent logic programming and constraint logic programming, using constraints to control concurrency. A clause can contain a guard, which is a set of constraints that may block the applicability of the clause. When the guards of several clauses are satisfied, concurrent constraint logic programming makes a committed choice to the use of only one.

Inductive logic programming is concerned with generalizing positive and negative examples in the context of background knowledge: machine learning of logic programs. Recent work in this area, combining logic programming, learning and probability, has given rise to the new field of statistical relational learning and probabilistic inductive logic programming.

Several researchers have extended logic programming with higher-order programming features derived from higher-order logic, such as predicate variables. Such languages include the Prolog extensions HiLog and λProlog.

Basing logic programming within linear logic has resulted in the design of logic programming languages that are considerably more expressive than those based on classical logic. Horn clause programs can only represent state change by the change in arguments to predicates. In linear logic programming, one can use the ambient linear logic to support state change. Some early designs of logic programming languages based on linear logic include LO [Andreoli & Pareschi, 1991], Lolli, ACL, and Forum [Miller, 1996]. Forum provides a goal-directed interpretation of all of linear logic.

F-logic extends logic programming with objects and the frame syntax. A number of systems are based on F-logic, including Flora-2, FLORID, and a highly scalable commercial system Ontobroker.
Logtalk extends the Prolog programming language with support for objects, protocols, and other OOP concepts. Highly portable, it supports most standard-complaint Prolog systems as backend compilers.

Transaction logic is an extension of logic programming with a logical theory of state-modifying updates. It has both a model-theoretic semantics and a procedural one. An implementation of a subset of Transaction logic is available in the Flora-2 system. Other prototypes are also available.

Boolean satisfiability problem
Constraint logic programming
Datalog
Fril
Functional programming
Fuzzy logic
Inductive logic programming
Logic in computer science (includes Formal methods)
Logic programming languages
Programming paradigm
R++
Reasoning system
Rule-based machine learning
Satisfiability

Baral, C.; Gelfond, M. (1994). "Logic programming and knowledge representation" (PDF). The Journal of Logic Programming. 19-20: 73–148. doi:10.1016/0743-1066(94)90025-6.
Robert Kowalski. The Early Years of Logic Programming Kowalski, R. A. (1988). "The early years of logic programming" (PDF). Communications of the ACM. 31: 38–43. doi:10.1145/35043.35046.
Lloyd, J. W. (1987). Foundations of Logic Programming. (2nd edition). Springer-Verlag.

John McCarthy. Programs with common sense Symposium on Mechanization of Thought Processes. National Physical Laboratory. Teddington, England. 1958.
D. Miller, G. Nadathur, F. Pfenning, A. Scedrov. Uniform proofs as a foundation for logic programming, Annals of Pure and Applied Logic, vol. 51, pp 125–157, 1991.
Ehud Shapiro (Editor). Concurrent Prolog MIT Press. 1987.
James Slagle. Experiments with a Deductive Question-Answering Program CACM. December 1965.

Carl Hewitt. Procedural Embedding of Knowledge In Planner IJCAI 1971.
Carl Hewitt. The repeated demise of logic programming and why it will be reincarnated
Evgeny Dantsin, Thomas Eiter, Georg Gottlob, Andrei Voronkov: Complexity and expressive power of logic programming. ACM Comput. Surv. 33(3): 374-425 (2001)
Ulf Nilsson and Jan Maluszynski, Logic, Programming and Prolog

Logic Programming Virtual Library entry
Bibliographies on Logic Programming
Association for Logic Programming (ALP)
Theory and Practice of Logic Programming journal
Logic programming in C++ with Castor
Logic programming in Oz
Prolog Development Center
Racklog: Logic Programming in RacketSemantics (from Ancient Greek: σημαντικός sēmantikos, "significant") is primarily the linguistic, and also philosophical study of meaning—in language, programming languages, formal logics, and semiotics. It focuses on the relationship between signifiers—like words, phrases, signs, and symbols—and what they stand for, their denotation.
In international scientific vocabulary semantics is also called semasiology. The word semantics was first used by Michel Bréal, a French philologist. It denotes a range of ideas—from the popular to the highly technical. It is often used in ordinary language for denoting a problem of understanding that comes down to word selection or connotation. This problem of understanding has been the subject of many formal enquiries, over a long period of time, especially in the field of formal semantics. In linguistics, it is the study of the interpretation of signs or symbols used in agents or communities within particular circumstances and contexts. Within this view, sounds, facial expressions, body language, and proxemics have semantic (meaningful) content, and each comprises several branches of study. In written language, things like paragraph structure and punctuation bear semantic content; other forms of language bear other semantic content.
The formal study of semantics intersects with many other fields of inquiry, including lexicology, syntax, pragmatics, etymology and others. Independently, semantics is also a well-defined field in its own right, often with synthetic properties. In the philosophy of language, semantics and reference are closely connected. Further related fields include philology, communication, and semiotics. The formal study of semantics can therefore be manifold and complex.
Semantics contrasts with syntax, the study of the combinatorics of units of a language (without reference to their meaning), and pragmatics, the study of the relationships between the symbols of a language, their meaning, and the users of the language. Semantics as a field of study also has significant ties to various representational theories of meaning including truth theories of meaning, coherence theories of meaning, and correspondence theories of meaning. Each of these is related to the general philosophical study of reality and the representation of meaning. In 1960s psychosemantic studies became popular after Osgood's massive cross-cultural studies using his Semantic differential (SD) method that used thousands of nouns and adjective bipolar scales. A specific form of the SD, Projective Semantics method  uses only most common and neutral nouns that correspond to the 7 groups (factors) of adjective-scales most consistently found in cross-cultural studies (Evaluation, Potency, Activity as found by Osgood, and Reality, Organization, Complexity, Limitation as found in other studies). In this method, seven groups of bipolar adjective scales corresponded to seven types of nouns so the method was thought to have the object-scale symmetry (OSS) between the scales and nouns for evaluation using these scales. For example, the nouns corresponding to the listed 7 factors would be: Beauty, Power, Motion, Life, Work, Chaos. Beauty was expected to be assessed unequivocally as “very good” on adjectives of Evaluation-related scales, Life as “very real” on Reality-related scales, etc. However, deviations in this symmetric and very basic matrix might show underlying biases of two types: scales-related bias and objects-related bias. This OSS design meant to increase the sensitivity of the SD method to any semantic biases in responses of people within the same culture and educational background.

In linguistics, semantics is the subfield that is devoted to the study of meaning, as inherent at the levels of words, phrases, sentences, and larger units of discourse (termed texts, or narratives). The study of semantics is also closely linked to the subjects of representation, reference and denotation. The basic study of semantics is oriented to the examination of the meaning of signs, and the study of relations between different linguistic units and compounds: homonymy, synonymy, antonymy, hypernymy, hyponymy, meronymy, metonymy, holonymy, paronyms. A key concern is how meaning attaches to larger chunks of text, possibly as a result of the composition from smaller units of meaning. Traditionally, semantics has included the study of sense and denotative reference, truth conditions, argument structure, thematic roles, discourse analysis, and the linkage of all of these to syntax.

In the late 1960s, Richard Montague proposed a system for defining semantic entries in the lexicon in terms of the lambda calculus. In these terms, the syntactic parse of the sentence John ate every bagel would consist of a subject (John) and a predicate (ate every bagel); Montague demonstrated that the meaning of the sentence altogether could be decomposed into the meanings of its parts and in relatively few rules of combination. The logical predicate thus obtained would be elaborated further, e.g. using truth theory models, which ultimately relate meanings to a set of Tarskiian universals, which may lie outside the logic. The notion of such meaning atoms or primitives is basic to the language of thought hypothesis from the 1970s.
Despite its elegance, Montague grammar was limited by the context-dependent variability in word sense, and led to several attempts at incorporating context, such as:
Situation semantics (1980s): truth-values are incomplete, they get assigned based on context
Generative lexicon (1990s): categories (types) are incomplete, and get assigned based on context

In Chomskyan linguistics there was no mechanism for the learning of semantic relations, and the nativist view considered all semantic notions as inborn. Thus, even novel concepts were proposed to have been dormant in some sense. This view was also thought unable to address many issues such as metaphor or associative meanings, and semantic change, where meanings within a linguistic community change over time, and qualia or subjective experience. Another issue not addressed by the nativist model was how perceptual cues are combined in thought, e.g. in mental rotation.
This view of semantics, as an innate finite meaning inherent in a lexical unit that can be composed to generate meanings for larger chunks of discourse, is now being fiercely debated in the emerging domain of cognitive linguistics and also in the non-Fodorian camp in philosophy of language. The challenge is motivated by:
factors internal to language, such as the problem of resolving indexical or anaphora (e.g. this x, him, last week). In these situations context serves as the input, but the interpreted utterance also modifies the context, so it is also the output. Thus, the interpretation is necessarily dynamic and the meaning of sentences is viewed as contexts changing potentials instead of propositions.
factors external to language, i.e. language is not a set of labels stuck on things, but "a toolbox, the importance of whose elements lie in the way they function rather than their attachments to things." This view reflects the position of the later Wittgenstein and his famous game example, and is related to the positions of Quine, Davidson, and others.
A concrete example of the latter phenomenon is semantic underspecification – meanings are not complete without some elements of context. To take an example of one word, red, its meaning in a phrase such as red book is similar to many other usages, and can be viewed as compositional. However, the colours implied in phrases such as red wine (very dark), and red hair (coppery), or red soil, or red skin are very different. Indeed, these colours by themselves would not be called red by native speakers. These instances are contrastive, so red wine is so called only in comparison with the other kind of wine (which also is not white for the same reasons). This view goes back to de Saussure:

Each of a set of synonyms like redouter ('to dread'), craindre ('to fear'), avoir peur ('to be afraid') has its particular value only because they stand in contrast with one another. No word has a value that can be identified independently of what else is in its vicinity.

and may go back to earlier Indian views on language, especially the Nyaya view of words as indicators and not carriers of meaning.
An attempt to defend a system based on propositional meaning for semantic underspecification can be found in the generative lexicon model of James Pustejovsky, who extends contextual operations (based on type shifting) into the lexicon. Thus meanings are generated "on the fly" (as you go), based on finite context.

Another set of concepts related to fuzziness in semantics is based on prototypes. The work of Eleanor Rosch in the 1970s led to a view that natural categories are not characterizable in terms of necessary and sufficient conditions, but are graded (fuzzy at their boundaries) and inconsistent as to the status of their constituent members. One may compare it with Jung's archetype, though the concept of archetype sticks to static concept. Some post-structuralists are against the fixed or static meaning of the words. Derrida, following Nietzsche, talked about slippages in fixed meanings.
Systems of categories are not objectively out there in the world but are rooted in people's experience. These categories evolve as learned concepts of the world – meaning is not an objective truth, but a subjective construct, learned from experience, and language arises out of the "grounding of our conceptual systems in shared embodiment and bodily experience". A corollary of this is that the conceptual categories (i.e. the lexicon) will not be identical for different cultures, or indeed, for every individual in the same culture. This leads to another debate (see the Sapir–Whorf hypothesis or Eskimo words for snow).

Originates from Montague's work (see above). A highly formalized theory of natural language semantics in which expressions are assigned denotations (meanings) such as individuals, truth values, or functions from one of these to another. The truth of a sentence, and more interestingly, its logical relation to other sentences, is then evaluated relative to a model.

Pioneered by the philosopher Donald Davidson, another formalized theory, which aims to associate each natural language sentence with a meta-language description of the conditions under which it is true, for example: 'Snow is white' is true if and only if snow is white. The challenge is to arrive at the truth conditions for any sentences from fixed meanings assigned to the individual words and fixed rules for how to combine them. In practice, truth-conditional semantics is similar to model-theoretic semantics; conceptually, however, they differ in that truth-conditional semantics seeks to connect language with statements about the real world (in the form of meta-language statements), rather than with abstract models.

This theory is an effort to explain properties of argument structure. The assumption behind this theory is that syntactic properties of phrases reflect the meanings of the words that head them. With this theory, linguists can better deal with the fact that subtle differences in word meaning correlate with other differences in the syntactic structure that the word appears in. The way this is gone about is by looking at the internal structure of words. These small parts that make up the internal structure of words are termed semantic primitives.

A linguistic theory that investigates word meaning. This theory understands that the meaning of a word is fully reflected by its context. Here, the meaning of a word is constituted by its contextual relations. Therefore, a distinction between degrees of participation as well as modes of participation are made. In order to accomplish this distinction any part of a sentence that bears a meaning and combines with the meanings of other constituents is labeled as a semantic constituent. Semantic constituents that cannot be broken down into more elementary constituents are labeled minimal semantic constituents.

Computational semantics is focused on the processing of linguistic meaning. In order to do this concrete algorithms and architectures are described. Within this framework the algorithms and architectures are also analyzed in terms of decidability, time/space complexity, data structures they require and communication protocols.

In computer science, the term semantics refers to the meaning of languages, as opposed to their form (syntax). According to Euzenat, semantics "provides the rules for interpreting the syntax which do not provide the meaning directly but constrains the possible interpretations of what is declared." In other words, semantics is about interpretation of an expression. Additionally, the term is applied to certain types of data structures specifically designed and used for representing information content.

The semantics of programming languages and other languages is an important issue and area of study in computer science. Like the syntax of a language, its semantics can be defined exactly.
For instance, the following statements use different syntaxes, but cause the same instructions to be executed:
Generally these operations would all perform an arithmetical addition of 'y' to 'x' and store the result in a variable called 'x'.
Various ways have been developed to describe the semantics of programming languages formally, building on mathematical logic:
Operational semantics: The meaning of a construct is specified by the computation it induces when it is executed on a machine. In particular, it is of interest how the effect of a computation is produced.
Denotational semantics: Meanings are modelled by mathematical objects that represent the effect of executing the constructs. Thus only the effect is of interest, not how it is obtained.
Axiomatic semantics: Specific properties of the effect of executing the constructs are expressed as assertions. Thus there may be aspects of the executions that are ignored.

Terms such as semantic network and semantic data model are used to describe particular types of data model characterized by the use of directed graphs in which the vertices denote concepts or entities in the world, and the arcs denote relationships between them.
The Semantic Web refers to the extension of the World Wide Web via embedding added semantic metadata, using semantic data modelling techniques such as Resource Description Framework (RDF) and Web Ontology Language (OWL).

In psychology, semantic memory is memory for meaning – in other words, the aspect of memory that preserves only the gist, the general significance, of remembered experience – while episodic memory is memory for the ephemeral details – the individual features, or the unique particulars of experience. The term 'episodic memory' was introduced by Tulving and Schacter in the context of 'declarative memory' which involved simple association of factual or objective information concerning its object. Word meaning is measured by the company they keep, i.e. the relationships among words themselves in a semantic network. The memories may be transferred intergenerationally or isolated in one generation due to a cultural disruption. Different generations may have different experiences at similar points in their own time-lines. This may then create a vertically heterogeneous semantic net for certain words in an otherwise homogeneous culture. In a network created by people analyzing their understanding of the word (such as Wordnet) the links and decomposition structures of the network are few in number and kind, and include part of, kind of, and similar links. In automated ontologies the links are computed vectors without explicit meaning. Various automated technologies are being developed to compute the meaning of words: latent semantic indexing and support vector machines as well as natural language processing, neural networks and predicate calculus techniques.
Ideasthesia is a psychological phenomenon in which activation of concepts evokes sensory experiences. For example, in synesthesia, activation of a concept of a letter (e.g., that of the letter A) evokes sensory-like experiences (e.g., of red color).

semanticsarchive.net
Teaching page for A-level semantics
Chomsky, Noam; On Referring, Harvard University, 30 October 2007 (video)
Jackendoff, Ray; Conceptual Semantics, Harvard University, 13 November 2007 (video)
Semantics: an interview with Jerry Fodor (ReVEL, vol. 5, no. 8 (2007))Physics (from Ancient Greek: φυσική (ἐπιστήμη) phusikḗ (epistḗmē) "knowledge of nature", from φύσις phúsis "nature") is the natural science that involves the study of matter and its motion and behavior through space and time, along with related concepts such as energy and force. One of the most fundamental scientific disciplines, the main goal of physics is to understand how the universe behaves.
Physics is one of the oldest academic disciplines, perhaps the oldest through its inclusion of astronomy. Over the last two millennia, physics was a part of natural philosophy along with chemistry, biology, and certain branches of mathematics, but during the scientific revolution in the 17th century, the natural sciences emerged as unique research programs in their own right. Physics intersects with many interdisciplinary areas of research, such as biophysics and quantum chemistry, and the boundaries of physics are not rigidly defined. New ideas in physics often explain the fundamental mechanisms of other sciences while opening new avenues of research in areas such as mathematics and philosophy.
Physics also makes significant contributions through advances in new technologies that arise from theoretical breakthroughs. For example, advances in the understanding of electromagnetism or nuclear physics led directly to the development of new products that have dramatically transformed modern-day society, such as television, computers, domestic appliances, and nuclear weapons; advances in thermodynamics led to the development of industrialization, and advances in mechanics inspired the development of calculus.
The United Nations named 2005 the World Year of Physics.

Astronomy is the oldest of the natural sciences. The earliest civilizations dating back to beyond 3000 BCE, such as the Sumerians, ancient Egyptians, and the Indus Valley Civilization, all had a predictive knowledge and a basic understanding of the motions of the Sun, Moon, and stars. The stars and planets were often a target of worship, believed to represent their gods. While the explanations for these phenomena were often unscientific and lacking in evidence, these early observations laid the foundation for later astronomy.
According to Asger Aaboe, the origins of Western astronomy can be found in Mesopotamia, and all Western efforts in the exact sciences are descended from late Babylonian astronomy. Egyptian astronomers left monuments showing knowledge of the constellations and the motions of the celestial bodies, while Greek poet Homer wrote of various celestial objects in his Iliad and Odyssey; later Greek astronomers provided names, which are still used today, for most constellations visible from the northern hemisphere.

Natural philosophy has its origins in Greece during the Archaic period, (650 BCE – 480 BCE), when pre-Socratic philosophers like Thales rejected non-naturalistic explanations for natural phenomena and proclaimed that every event had a natural cause. They proposed ideas verified by reason and observation, and many of their hypotheses proved successful in experiment; for example, atomism was found to be correct approximately 2000 years after it was first proposed by Leucippus and his pupil Democritus.

Islamic scholarship had inherited Aristotelian physics from the Greeks and during the Islamic Golden Age developed it further, especially placing emphasis on observation and a priori reasoning, developing early forms of the scientific method.
The most notable innovations were in the field of optics and vision, which came from the works of many scientists like Ibn Sahl, Al-Kindi, Ibn al-Haytham, Al-Farisi and Avicenna. The most notable work was The Book of Optics (also known as Kitāb al-Manāẓir), written by Ibn Al-Haitham, in which he was not only the first to disprove the ancient Greek idea about vision, but also came up with a new theory. In the book, he was also the first to study the phenomenon of the pinhole camera and delved further into the way the eye itself works. Using dissections and the knowledge of previous scholars, he was able to begin to explain how light enters the eye, is focused, and is projected to the back of the eye: and built then the world's first camera obscura hundreds of years before the modern development of photography.

The seven-volume Book of Optics (Kitab al-Manathir) hugely influenced thinking across disciplines from the theory of visual perception to the nature of perspective in medieval art, in both the East and the West, for more than 600 years. Many later European scholars and fellow polymaths, from Robert Grosseteste and Leonardo da Vinci to René Descartes, Johannes Kepler and Isaac Newton, were in his debt. Indeed, the influence of Ibn al-Haytham's Optics ranks alongside that of Newton's work of the same title, published 700 years later.
The translation of The Book of Optics had a huge impact on Europe. From it, later European scholars were able to build the same devices as what Ibn al-Haytham did, and understand the way light works. From this, such important things as eyeglasses, magnifying glasses, telescopes, and cameras were developed.

Physics became a separate science when early modern Europeans used experimental and quantitative methods to discover what are now considered to be the laws of physics.
Major developments in this period include the replacement of the geocentric model of the solar system with the heliocentric Copernican model, the laws governing the motion of planetary bodies determined by Johannes Kepler between 1609 and 1619, pioneering work on telescopes and observational astronomy by Galileo Galilei in the 16th and 17th Centuries, and Isaac Newton's discovery and unification of the laws of motion and universal gravitation that would come to bear his name. Newton also developed calculus, the mathematical study of change, which provided new mathematical methods for solving physical problems.
The discovery of new laws in thermodynamics, chemistry, and electromagnetics resulted from greater research efforts during the Industrial Revolution as energy needs increased. The laws comprising classical physics remain very widely used for objects on everyday scales travelling at non-relativistic speeds, since they provide a very close approximation in such situations, and theories such as quantum mechanics and the theory of relativity simplify to their classical equivalents at such scales. However, inaccuracies in classical mechanics for very small objects and very high velocities led to the development of modern physics in the 20th century.

Modern physics began in the early 20th century with the work of Max Planck in quantum theory and Albert Einstein's theory of relativity. Both of these theories came about due to inaccuracies in classical mechanics in certain situations. Classical mechanics predicted a varying speed of light, which could not be resolved with the constant speed predicted by Maxwell's equations of electromagnetism; this discrepancy was corrected by Einstein's theory of special relativity, which replaced classical mechanics for fast-moving bodies and allowed for a constant speed of light. Black body radiation provided another problem for classical physics, which was corrected when Planck proposed that the excitation of material oscillators is possible only in discrete steps proportional to their frequency; this, along with the photoelectric effect and a complete theory predicting discrete energy levels of electron orbitals, led to the theory of quantum mechanics taking over from classical physics at very small scales.
Quantum mechanics would come to be pioneered by Werner Heisenberg, Erwin Schrödinger and Paul Dirac. From this early work, and work in related fields, the Standard Model of particle physics was derived. Following the discovery of a particle with properties consistent with the Higgs boson at CERN in 2012, all fundamental particles predicted by the standard model, and no others, appear to exist; however, physics beyond the Standard Model, with theories such as supersymmetry, is an active area of research. Areas of mathematics in general are important to this field, such as the study of probabilities and groups.

In many ways, physics stems from ancient Greek philosophy. From Thales' first attempt to characterise matter, to Democritus' deduction that matter ought to reduce to an invariant state, the Ptolemaic astronomy of a crystalline firmament, and Aristotle's book Physics (an early book on physics, which attempted to analyze and define motion from a philosophical point of view), various Greek philosophers advanced their own theories of nature. Physics was known as natural philosophy until the late 18th century.
By the 19th century, physics was realised as a discipline distinct from philosophy and the other sciences. Physics, as with the rest of science, relies on philosophy of science and its "scientific method" to advance our knowledge of the physical world. The scientific method employs a priori reasoning as well as a posteriori reasoning and the use of Bayesian inference to measure the validity of a given theory.
The development of physics has answered many questions of early philosophers, but has also raised new questions. Study of the philosophical issues surrounding physics, the philosophy of physics, involves issues such as the nature of space and time, determinism, and metaphysical outlooks such as empiricism, naturalism and realism.
Many physicists have written about the philosophical implications of their work, for instance Laplace, who championed causal determinism, and Erwin Schrödinger, who wrote on quantum mechanics. The mathematical physicist Roger Penrose has been called a Platonist by Stephen Hawking, a view Penrose discusses in his book, The Road to Reality. Hawking refers to himself as an "unashamed reductionist" and takes issue with Penrose's views.

Though physics deals with a wide variety of systems, certain theories are used by all physicists. Each of these theories were experimentally tested numerous times and found to be an adequate approximation of nature. For instance, the theory of classical mechanics accurately describes the motion of objects, provided they are much larger than atoms and moving at much less than the speed of light. These theories continue to be areas of active research today. Chaos theory, a remarkable aspect of classical mechanics was discovered in the 20th century, three centuries after the original formulation of classical mechanics by Isaac Newton (1642–1727).
These central theories are important tools for research into more specialised topics, and any physicist, regardless of their specialisation, is expected to be literate in them. These include classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, and special relativity.

Classical physics includes the traditional branches and topics that were recognised and well-developed before the beginning of the 20th century—classical mechanics, acoustics, optics, thermodynamics, and electromagnetism. Classical mechanics is concerned with bodies acted on by forces and bodies in motion and may be divided into statics (study of the forces on a body or bodies not subject to an acceleration), kinematics (study of motion without regard to its causes), and dynamics (study of motion and the forces that affect it); mechanics may also be divided into solid mechanics and fluid mechanics (known together as continuum mechanics), the latter include such branches as hydrostatics, hydrodynamics, aerodynamics, and pneumatics. Acoustics is the study of how sound is produced, controlled, transmitted and received. Important modern branches of acoustics include ultrasonics, the study of sound waves of very high frequency beyond the range of human hearing; bioacoustics, the physics of animal calls and hearing, and electroacoustics, the manipulation of audible sound waves using electronics.
Optics, the study of light, is concerned not only with visible light but also with infrared and ultraviolet radiation, which exhibit all of the phenomena of visible light except visibility, e.g., reflection, refraction, interference, diffraction, dispersion, and polarization of light. Heat is a form of energy, the internal energy possessed by the particles of which a substance is composed; thermodynamics deals with the relationships between heat and other forms of energy. Electricity and magnetism have been studied as a single branch of physics since the intimate connection between them was discovered in the early 19th century; an electric current gives rise to a magnetic field, and a changing magnetic field induces an electric current. Electrostatics deals with electric charges at rest, electrodynamics with moving charges, and magnetostatics with magnetic poles at rest.

Classical physics is generally concerned with matter and energy on the normal scale of observation, while much of modern physics is concerned with the behavior of matter and energy under extreme conditions or on a very large or very small scale. For example, atomic and nuclear physics studies matter on the smallest scale at which chemical elements can be identified. The physics of elementary particles is on an even smaller scale since it is concerned with the most basic units of matter; this branch of physics is also known as high-energy physics because of the extremely high energies necessary to produce many types of particles in particle accelerators. On this scale, ordinary, commonsense notions of space, time, matter, and energy are no longer valid.
The two chief theories of modern physics present a different picture of the concepts of space, time, and matter from that presented by classical physics. Classical mechanics approximates nature as continuous, while quantum theory is concerned with the discrete nature of many phenomena at the atomic and subatomic level and with the complementary aspects of particles and waves in the description of such phenomena. The theory of relativity is concerned with the description of phenomena that take place in a frame of reference that is in motion with respect to an observer; the special theory of relativity is concerned with relative uniform motion in a straight line and the general theory of relativity with accelerated motion and its connection with gravitation. Both quantum theory and the theory of relativity find applications in all areas of modern physics.

While physics aims to discover universal laws, its theories lie in explicit domains of applicability. Loosely speaking, the laws of classical physics accurately describe systems whose important length scales are greater than the atomic scale and whose motions are much slower than the speed of light. Outside of this domain, observations do not match predictions provided by classical mechanics. Albert Einstein contributed the framework of special relativity, which replaced notions of absolute time and space with spacetime and allowed an accurate description of systems whose components have speeds approaching the speed of light. Max Planck, Erwin Schrödinger, and others introduced quantum mechanics, a probabilistic notion of particles and interactions that allowed an accurate description of atomic and subatomic scales. Later, quantum field theory unified quantum mechanics and special relativity. General relativity allowed for a dynamical, curved spacetime, with which highly massive systems and the large-scale structure of the universe can be well-described. General relativity has not yet been unified with the other fundamental descriptions; several candidate theories of quantum gravity are being developed.

Mathematics provides a compact and exact language used to describe of the order in nature. This was noted and advocated by Pythagoras, Plato, Galileo, and Newton.
Physics uses mathematics to organise and formulate experimental results. From those results, precise or estimated solutions, quantitative results from which new predictions can be made and experimentally confirmed or negated. The results from physics experiments are numerical measurements. Technologies based on mathematics, like computation have made computational physics an active area of research.

Ontology is a prerequisite for physics, but not for mathematics. It means physics is ultimately concerned with descriptions of the real world, while mathematics is concerned with abstract patterns, even beyond the real world. Thus physics statements are synthetic, while mathematical statements are analytic. Mathematics contains hypotheses, while physics contains theories. Mathematics statements have to be only logically true, while predictions of physics statements must match observed and experimental data.
The distinction is clear-cut, but not always obvious. For example, mathematical physics is the application of mathematics in physics. Its methods are mathematical, but its subject is physical. The problems in this field start with a "mathematical model of a physical situation" (system) and a "mathematical description of a physical law" that will be applied to that system. Every mathematical statement used for solving has a hard-to-find physical meaning. The final mathematical solution has an easier-to-find meaning, because it is what the solver is looking for.
Physics is a branch of fundamental science, not practical science. Physics is also called "the fundamental science" because the subject of study of all branches of natural science like chemistry, astronomy, geology, and biology are constrained by laws of physics, similar to how chemistry is often called the central science because of its role in linking the physical sciences. For example, chemistry studies properties, structures, and reactions of matter (chemistry's focus on the atomic scale distinguishes it from physics). Structures are formed because particles exert electrical forces on each other, properties include physical characteristics of given substances, and reactions are bound by laws of physics, like conservation of energy, mass, and charge.
Physics is applied in industries like engineering and medicine.

Applied physics is a general term for physics research which is intended for a particular use. An applied physics curriculum usually contains a few classes in an applied discipline, like geology or electrical engineering. It usually differs from engineering in that an applied physicist may not be designing something in particular, but rather is using physics or conducting physics research with the aim of developing new technologies or solving a problem.
The approach is similar to that of applied mathematics. Applied physicists use physics in scientific research. For instance, people working on accelerator physics might seek to build better particle detectors for research in theoretical physics.
Physics is used heavily in engineering. For example, statics, a subfield of mechanics, is used in the building of bridges and other static structures. The understanding and use of acoustics results in sound control and better concert halls; similarly, the use of optics creates better optical devices. An understanding of physics makes for more realistic flight simulators, video games, and movies, and is often critical in forensic investigations.
With the standard consensus that the laws of physics are universal and do not change with time, physics can be used to study things that would ordinarily be mired in uncertainty. For example, in the study of the origin of the earth, one can reasonably model earth's mass, temperature, and rate of rotation, as a function of time allowing one to extrapolate forward or backward in time and so predict future or prior events. It also allows for simulations in engineering which drastically speed up the development of a new technology.
But there is also considerable interdisciplinarity in the physicist's methods, so many other important fields are influenced by physics (e.g., the fields of econophysics and sociophysics).

Physicists use the scientific method to test the validity of a physical theory. By using a methodical approach to compare the implications of a theory with the conclusions drawn from its related experiments and observations, physicists are better able to test the validity of a theory in a logical, unbiased, and repeatable way. To that end, experiments are performed and observations are made in order to determine the validity or invalidity of the theory.
A scientific law is a concise verbal or mathematical statement of a relation which expresses a fundamental principle of some theory, such as Newton's law of universal gravitation.

Theorists seek to develop mathematical models that both agree with existing experiments and successfully predict future experimental results, while experimentalists devise and perform experiments to test theoretical predictions and explore new phenomena. Although theory and experiment are developed separately, they are strongly dependent upon each other. Progress in physics frequently comes about when experimentalists make a discovery that existing theories cannot explain, or when new theories generate experimentally testable predictions, which inspire new experiments.
Physicists who work at the interplay of theory and experiment are called phenomenologists, who study complex phenomena observed in experiment and work to relate them to a fundamental theory.
Theoretical physics has historically taken inspiration from philosophy; electromagnetism was unified this way. Beyond the known universe, the field of theoretical physics also deals with hypothetical issues, such as parallel universes, a multiverse, and higher dimensions. Theorists invoke these ideas in hopes of solving particular problems with existing theories. They then explore the consequences of these ideas and work toward making testable predictions.
Experimental physics expands, and is expanded by, engineering and technology. Experimental physicists involved in basic research design and perform experiments with equipment such as particle accelerators and lasers, whereas those involved in applied research often work in industry developing technologies such as magnetic resonance imaging (MRI) and transistors. Feynman has noted that experimentalists may seek areas which are not well-explored by theorists.

Physics covers a wide range of phenomena, from elementary particles (such as quarks, neutrinos, and electrons) to the largest superclusters of galaxies. Included in these phenomena are the most basic objects composing all other things. Therefore, physics is sometimes called the "fundamental science". Physics aims to describe the various phenomena that occur in nature in terms of simpler phenomena. Thus, physics aims to both connect the things observable to humans to root causes, and then connect these causes together.
For example, the ancient Chinese observed that certain rocks (lodestone and magnetite) were attracted to one another by an invisible force. This effect was later called magnetism, which was first rigorously studied in the 17th century. But even before the Chinese discovered magnetism, the ancient Greeks knew of other objects such as amber, that when rubbed with fur would cause a similar invisible attraction between the two. This was also first studied rigorously in the 17th century and came to be called electricity. Thus, physics had come to understand two observations of nature in terms of some root cause (electricity and magnetism). However, further work in the 19th century revealed that these two forces were just two different aspects of one force—electromagnetism. This process of "unifying" forces continues today, and electromagnetism and the weak nuclear force are now considered to be two aspects of the electroweak interaction. Physics hopes to find an ultimate reason (Theory of Everything) for why nature is as it is (see section Current research below for more information).

Contemporary research in physics can be broadly divided into particle physics; condensed matter physics; atomic, molecular, and optical physics; astrophysics; and applied physics. Some physics departments also support physics education research and physics outreach.
Since the 20th century, the individual fields of physics have become increasingly specialised, and today most physicists work in a single field for their entire careers. "Universalists" such as Albert Einstein (1879–1955) and Lev Landau (1908–1968), who worked in multiple fields of physics, are now very rare.
The major fields of physics, along with their subfields and the theories and concepts they employ, are shown in the following table.

Particle physics is the study of the elementary constituents of matter and energy and the interactions between them. In addition, particle physicists design and develop the high energy accelerators, detectors, and computer programs necessary for this research. The field is also called "high-energy physics" because many elementary particles do not occur naturally but are created only during high-energy collisions of other particles.
Currently, the interactions of elementary particles and fields are described by the Standard Model. The model accounts for the 12 known particles of matter (quarks and leptons) that interact via the strong, weak, and electromagnetic fundamental forces. Dynamics are described in terms of matter particles exchanging gauge bosons (gluons, W and Z bosons, and photons, respectively). The Standard Model also predicts a particle known as the Higgs boson. In July 2012 CERN, the European laboratory for particle physics, announced the detection of a particle consistent with the Higgs boson, an integral part of a Higgs mechanism.
Nuclear physics is the field of physics that studies the constituents and interactions of atomic nuclei. The most commonly known applications of nuclear physics are nuclear power generation and nuclear weapons technology, but the research has provided application in many fields, including those in nuclear medicine and magnetic resonance imaging, ion implantation in materials engineering, and radiocarbon dating in geology and archaeology.

Atomic, molecular, and optical physics (AMO) is the study of matter–matter and light–matter interactions on the scale of single atoms and molecules. The three areas are grouped together because of their interrelationships, the similarity of methods used, and the commonality of their relevant energy scales. All three areas include both classical, semi-classical and quantum treatments; they can treat their subject from a microscopic view (in contrast to a macroscopic view).
Atomic physics studies the electron shells of atoms. Current research focuses on activities in quantum control, cooling and trapping of atoms and ions, low-temperature collision dynamics and the effects of electron correlation on structure and dynamics. Atomic physics is influenced by the nucleus (see, e.g., hyperfine splitting), but intra-nuclear phenomena such as fission and fusion are considered part of high-energy physics.
Molecular physics focuses on multi-atomic structures and their internal and external interactions with matter and light. Optical physics is distinct from optics in that it tends to focus not on the control of classical light fields by macroscopic objects but on the fundamental properties of optical fields and their interactions with matter in the microscopic realm.

Condensed matter physics is the field of physics that deals with the macroscopic physical properties of matter. In particular, it is concerned with the "condensed" phases that appear whenever the number of particles in a system is extremely large and the interactions between them are strong.
The most familiar examples of condensed phases are solids and liquids, which arise from the bonding by way of the electromagnetic force between atoms. More exotic condensed phases include the superfluid and the Bose–Einstein condensate found in certain atomic systems at very low temperature, the superconducting phase exhibited by conduction electrons in certain materials, and the ferromagnetic and antiferromagnetic phases of spins on atomic lattices.
Condensed matter physics is the largest field of contemporary physics. Historically, condensed matter physics grew out of solid-state physics, which is now considered one of its main subfields. The term condensed matter physics was apparently coined by Philip Anderson when he renamed his research group—previously solid-state theory—in 1967. In 1978, the Division of Solid State Physics of the American Physical Society was renamed as the Division of Condensed Matter Physics. Condensed matter physics has a large overlap with chemistry, materials science, nanotechnology and engineering.

Astrophysics and astronomy are the application of the theories and methods of physics to the study of stellar structure, stellar evolution, the origin of the Solar System, and related problems of cosmology. Because astrophysics is a broad subject, astrophysicists typically apply many disciplines of physics, including mechanics, electromagnetism, statistical mechanics, thermodynamics, quantum mechanics, relativity, nuclear and particle physics, and atomic and molecular physics.
The discovery by Karl Jansky in 1931 that radio signals were emitted by celestial bodies initiated the science of radio astronomy. Most recently, the frontiers of astronomy have been expanded by space exploration. Perturbations and interference from the earth's atmosphere make space-based observations necessary for infrared, ultraviolet, gamma-ray, and X-ray astronomy.
Physical cosmology is the study of the formation and evolution of the universe on its largest scales. Albert Einstein's theory of relativity plays a central role in all modern cosmological theories. In the early 20th century, Hubble's discovery that the universe is expanding, as shown by the Hubble diagram, prompted rival explanations known as the steady state universe and the Big Bang.
The Big Bang was confirmed by the success of Big Bang nucleosynthesis and the discovery of the cosmic microwave background in 1964. The Big Bang model rests on two theoretical pillars: Albert Einstein's general relativity and the cosmological principle. Cosmologists have recently established the ΛCDM model of the evolution of the universe, which includes cosmic inflation, dark energy, and dark matter.
Numerous possibilities and discoveries are anticipated to emerge from new data from the Fermi Gamma-ray Space Telescope over the upcoming decade and vastly revise or clarify existing models of the universe. In particular, the potential for a tremendous discovery surrounding dark matter is possible over the next several years. Fermi will search for evidence that dark matter is composed of weakly interacting massive particles, complementing similar experiments with the Large Hadron Collider and other underground detectors.
IBEX is already yielding new astrophysical discoveries: "No one knows what is creating the ENA (energetic neutral atoms) ribbon" along the termination shock of the solar wind, "but everyone agrees that it means the textbook picture of the heliosphere—in which the Solar System's enveloping pocket filled with the solar wind's charged particles is plowing through the onrushing 'galactic wind' of the interstellar medium in the shape of a comet—is wrong."

Research in physics is continually progressing on a large number of fronts.
In condensed matter physics, an important unsolved theoretical problem is that of high-temperature superconductivity. Many condensed matter experiments are aiming to fabricate workable spintronics and quantum computers.
In particle physics, the first pieces of experimental evidence for physics beyond the Standard Model have begun to appear. Foremost among these are indications that neutrinos have non-zero mass. These experimental results appear to have solved the long-standing solar neutrino problem, and the physics of massive neutrinos remains an area of active theoretical and experimental research. Large Hadron Collider had already found the Higgs Boson. Future research aims to prove or disprove the supersymmetry, which extends the Standard Model of particle physics. The research on dark matter and dark energy is also on the agenda.
Theoretical attempts to unify quantum mechanics and general relativity into a single theory of quantum gravity, a program ongoing for over half a century, have not yet been decisively resolved. The current leading candidates are M-theory, superstring theory and loop quantum gravity.
Many astronomical and cosmological phenomena have yet to be satisfactorily explained, including the existence of ultra-high energy cosmic rays, the baryon asymmetry, the acceleration of the universe and the anomalous rotation rates of galaxies.
Although much progress has been made in high-energy, quantum, and astronomical physics, many everyday phenomena involving complexity, chaos, or turbulence are still poorly understood. Complex problems that seem like they could be solved by a clever application of dynamics and mechanics remain unsolved; examples include the formation of sandpiles, nodes in trickling water, the shape of water droplets, mechanisms of surface tension catastrophes, and self-sorting in shaken heterogeneous collections.
These complex phenomena have received growing attention since the 1970s for several reasons, including the availability of modern mathematical methods and computers, which enabled complex systems to be modeled in new ways. Complex physics has become part of increasingly interdisciplinary research, as exemplified by the study of turbulence in aerodynamics and the observation of pattern formation in biological systems. In the 1932 Annual Review of Fluid Mechanics, Horace Lamb said:

I am an old man now, and when I die and go to heaven there are two matters on which I hope for enlightenment. One is quantum electrodynamics, and the other is the turbulent motion of fluids. And about the former I am rather optimistic.

Peter Woit (January 2017). Fake Physics,

General
Encyclopedia of Physics at Scholarpedia
de Haas, Paul, Historic Papers in Physics (20th Century) at the Wayback Machine (archived 26 August 2009)
PhysicsCentral – Web portal run by the American Physical Society
Physics.org – Web portal run by the Institute of Physics
The Skeptic's Guide to Physics
Usenet Physics FAQ – A FAQ compiled by sci.physics and other physics newsgroups
Website of the Nobel Prize in physics
World of Physics An online encyclopedic dictionary of physics
Nature: Physics
Physics announced 17 July 2008 by the American Physical Society
Physics/Publications at DMOZ
Physicsworld.com – News website from Institute of Physics Publishing
Physics Central – includes articles on astronomy, particle physics, and mathematics.
The Vega Science Trust – science videos, including physics
Video: Physics "Lightning" Tour with Justin Morgan
52-part video course: The Mechanical Universe...and Beyond Note: also available at 01 – Introduction at Google Videos
HyperPhysics website – HyperPhysics, a physics and astronomy mind-map from Georgia State University
Organizations
AIP.org – Website of the American Institute of Physics
APS.org – Website of the American Physical Society
IOP.org – Website of the Institute of Physics
PlanetPhysics.org
Royal Society – Although not exclusively a physics institution, it has a strong history of physics
SPS National – Website of the Society of Physics StudentsA particle accelerator is a machine that uses electromagnetic fields to propel charged particles to nearly light speed and to contain them in well-defined beams. Large accelerators are used in particle physics as colliders (e.g. the LHC at CERN, KEKB at KEK in Japan, RHIC at Brookhaven National Laboratory, and Tevatron at Fermilab), or as synchrotron light sources for the study of condensed matter physics. Smaller particle accelerators are used in a wide variety of applications, including particle therapy for oncological purposes, radioisotope production for medical diagnostics, ion implanters for manufacture of semiconductors, and accelerator mass spectrometers for measurements of rare isotopes such as radiocarbon. There are currently more than 30,000 accelerators in operation around the world.
There are two basic classes of accelerators: electrostatic and electrodynamic (or electromagnetic) accelerators.  Electrostatic accelerators use static electric fields to accelerate particles. The most common types are the Cockcroft–Walton generator and the Van de Graaff generator. A small-scale example of this class is the cathode ray tube in an ordinary old television set. The achievable kinetic energy for particles in these devices is determined by the accelerating voltage, which is limited by electrical breakdown. Electrodynamic or electromagnetic accelerators, on the other hand, use changing electromagnetic fields (either magnetic induction or oscillating radio frequency fields) to accelerate particles. Since in these types the particles can pass through the same accelerating field multiple times, the output energy is not limited by the strength of the accelerating field. This class, which was first developed in the 1920s, is the basis for most modern large-scale accelerators.
Rolf Widerøe, Gustav Ising, Leó Szilárd, Max Steenbeck, and Ernest Lawrence are considered pioneers of this field, conceiving and building the first operational linear particle accelerator, the betatron, and the cyclotron.
Because colliders can give evidence of the structure of the subatomic world, accelerators were commonly referred to as atom smashers in the 20th century. Despite the fact that most accelerators (but not ion facilities) actually propel subatomic particles, the term persists in popular usage when referring to particle accelerators in general.

Beams of high-energy particles are useful for both fundamental and applied research in the sciences, and also in many technical and industrial fields unrelated to fundamental research. It has been estimated that there are approximately 30,000 accelerators worldwide. Of these, only about 1% are research machines with energies above 1 GeV, while about 44% are for radiotherapy, 41% for ion implantation, 9% for industrial processing and research, and 4% for biomedical and other low-energy research. The bar graph shows the breakdown of the number of industrial accelerators according to their applications. The numbers are based on 2012 statistics available from various sources, including production and sales data published in presentations or market surveys, and data provided by a number of manufacturers.

For the most basic inquiries into the dynamics and structure of matter, space, and time, physicists seek the simplest kinds of interactions at the highest possible energies. These typically entail particle energies of many GeV, and the interactions of the simplest kinds of particles: leptons (e.g. electrons and positrons) and quarks for the matter, or photons and gluons for the field quanta. Since isolated quarks are experimentally unavailable due to color confinement, the simplest available experiments involve the interactions of, first, leptons with each other, and second, of leptons with nucleons, which are composed of quarks and gluons. To study the collisions of quarks with each other, scientists resort to collisions of nucleons, which at high energy may be usefully considered as essentially 2-body interactions of the quarks and gluons of which they are composed. Thus elementary particle physicists tend to use machines creating beams of electrons, positrons, protons, and antiprotons, interacting with each other or with the simplest nuclei (e.g., hydrogen or deuterium) at the highest possible energies, generally hundreds of GeV or more.
The largest and highest energy particle accelerator used for elementary particle physics is the Large Hadron Collider (LHC) at CERN, operating since 2009.

Nuclear physicists and cosmologists may use beams of bare atomic nuclei, stripped of electrons, to investigate the structure, interactions, and properties of the nuclei themselves, and of condensed matter at extremely high temperatures and densities, such as might have occurred in the first moments of the Big Bang. These investigations often involve collisions of heavy nuclei – of atoms like iron or gold – at energies of several GeV per nucleon. The largest such particle accelerator is the Relativistic Heavy Ion Collider (RHIC) at Brookhaven National Laboratory.
Particle accelerators can also produce proton beams, which can produce proton-rich medical or research isotopes as opposed to the neutron-rich ones made in fission reactors; however, recent work has shown how to make 99Mo, usually made in reactors, by accelerating isotopes of hydrogen, although this method still requires a reactor to produce tritium. An example of this type of machine is LANSCE at Los Alamos.

Besides being of fundamental interest, high energy electrons may be coaxed into emitting extremely bright and coherent beams of high energy photons via synchrotron radiation, which have numerous uses in the study of atomic structure, chemistry, condensed matter physics, biology, and technology. A large number of synchrotron light sources exist worldwide. Examples in the US are SSRL and LCLS at SLAC National Accelerator Laboratory, APS at Argonne National Laboratory, ALS at Lawrence Berkeley National Laboratory, and NSLS at Brookhaven National Laboratory. The ESRF in Grenoble, France has been used to extract detailed 3-dimensional images of insects trapped in amber. Thus there is a great demand for electron accelerators of moderate (GeV) energy and high intensity.

Everyday examples of particle accelerators are cathode ray tubes found in television sets and X-ray generators. These low-energy accelerators use a single pair of electrodes with a DC voltage of a few thousand volts between them. In an X-ray generator, the target itself is one of the electrodes. A low-energy particle accelerator called an ion implanter is used in the manufacture of integrated circuits.
At lower energies, beams of accelerated nuclei are also used in medicine as particle therapy, for the treatment of cancer.
DC accelerator types capable of accelerating particles to speeds sufficient to cause nuclear reactions are Cockcroft-Walton generators or voltage multipliers, which convert AC to high voltage DC, or Van de Graaff generators that use static electricity carried by belts.

Historically, the first accelerators used simple technology of a single static high voltage to accelerate charged particles. The charged particle was accelerated through an evacuated tube with an electrode at either end, with the static potential across it. Since the particle passed only once through the potential difference, the output energy was limited to the accelerating voltage of the machine. While this method is still extremely popular today, with the electrostatic accelerators greatly out-numbering any other type, they are more suited to lower energy studies owing to the practical voltage limit of about 1 MV for air insulated machines, or 30 MV when the accelerator is operated in a tank of pressurized gas with high dielectric strength, such as sulfur hexafluoride. In a tandem accelerator the potential is used twice to accelerate the particles, by reversing the charge of the particles while they are inside the terminal. This is possible with the acceleration of atomic nuclei by using anions (negatively charged ions), and then passing the beam through a thin foil to strip electrons off the anions inside the high voltage terminal, converting them to cations (positively charged ions), which are accelerated again as they leave the terminal.
The two main types of electrostatic accelerator are the Cockcroft-Walton accelerator, which uses a diode-capacitor voltage multiplier to produce high voltage, and the Van de Graaff accelerator, which uses a moving fabric belt to carry charge to the high voltage electrode. Although electrostatic accelerators accelerate particles along a straight line, the term linear accelerator is more often used for accelerators that employ oscillating rather than static electric fields.

Due to the high voltage ceiling imposed by electrical discharge, in order to accelerate particles to higher energies, techniques involving dynamic fields rather than static fields are used. Electrodynamic acceleration can arise from either of two mechanisms: non-resonant magnetic induction, or resonant circuits or cavities excited by oscillating RF fields.  Electrodynamic accelerators can be linear, with particles accelerating in a straight line, or circular, using magnetic fields to bend particles in a roughly circular orbit.

Magnetic induction accelerators accelerate particles by induction from an increasing magnetic field, as if the particles were the secondary winding in a transformer. The increasing magnetic field creates a circulating electric field which can be configured to accelerate the particles. Induction accelerators can be either linear or circular.

Linear induction accelerators utilize ferrite-loaded, non-resonant induction cavities. Each cavity can be thought of as two large washer-shaped disks connected by an outer cylindrical tube. Between the disks is a ferrite toroid. A voltage pulse applied between the two disks causes an increasing magnetic field which inductively couples power into the charged particle beam.
The linear induction accelerator was invented by Christofilos in the 1960s. Linear induction accelerators are capable of accelerating very high beam currents (>1000 A) in a single short pulse. They have been used to generate X-rays for flash radiography (e.g. DARHT at LANL), and have been considered as particle injectors for magnetic confinement fusion and as drivers for free electron lasers.

The Betatron is circular magnetic induction accelerator, invented by Donald Kerst in 1940 for accelerating electrons. The concept originates ultimately from Norwegian-German scientist Rolf Widerøe. These machines, like synchrotrons, use a donut-shaped ring magnet (see below) with a cyclically increasing B field, but accelerate the particles by induction from the increasing magnetic field, as if they were the secondary winding in a transformer, due to the changing magnetic flux through the orbit.
Achieving constant orbital radius while supplying the proper accelerating electric field requires that the magnetic flux linking the orbit be somewhat independent of the magnetic field on the orbit, bending the particles into a constant radius curve. These machines have in practice been limited by the large radiative losses suffered by the electrons moving at nearly the speed of light in a relatively small radius orbit.

In a linear particle accelerator (linac), particles are accelerated in a straight line with a target of interest at one end. They are often used to provide an initial low-energy kick to particles before they are injected into circular accelerators. The longest linac in the world is the Stanford Linear Accelerator, SLAC, which is 3 km (1.9 mi) long. SLAC is an electron-positron collider.
Linear high-energy accelerators use a linear array of plates (or drift tubes) to which an alternating high-energy field is applied. As the particles approach a plate they are accelerated towards it by an opposite polarity charge applied to the plate. As they pass through a hole in the plate, the polarity is switched so that the plate now repels them and they are now accelerated by it towards the next plate. Normally a stream of "bunches" of particles are accelerated, so a carefully controlled AC voltage is applied to each plate to continuously repeat this process for each bunch.
As the particles approach the speed of light the switching rate of the electric fields becomes so high that they operate at radio frequencies, and so microwave cavities are used in higher energy machines instead of simple plates.
Linear accelerators are also widely used in medicine, for radiotherapy and radiosurgery. Medical grade linacs accelerate electrons using a klystron and a complex bending magnet arrangement which produces a beam of 6-30 MeV energy. The electrons can be used directly or they can be collided with a target to produce a beam of X-rays. The reliability, flexibility and accuracy of the radiation beam produced has largely supplanted the older use of cobalt-60 therapy as a treatment tool.

In the circular accelerator, particles move in a circle until they reach sufficient energy. The particle track is typically bent into a circle using electromagnets. The advantage of circular accelerators over linear accelerators (linacs) is that the ring topology allows continuous acceleration, as the particle can transit indefinitely. Another advantage is that a circular accelerator is smaller than a linear accelerator of comparable power (i.e. a linac would have to be extremely long to have the equivalent power of a circular accelerator).
Depending on the energy and the particle being accelerated, circular accelerators suffer a disadvantage in that the particles emit synchrotron radiation. When any charged particle is accelerated, it emits electromagnetic radiation and secondary emissions. As a particle traveling in a circle is always accelerating towards the center of the circle, it continuously radiates towards the tangent of the circle. This radiation is called synchrotron light and depends highly on the mass of the accelerating particle. For this reason, many high energy electron accelerators are linacs. Certain accelerators (synchrotrons) are however built specially for producing synchrotron light (X-rays).
Since the special theory of relativity requires that matter always travels slower than the speed of light in a vacuum, in high-energy accelerators, as the energy increases the particle speed approaches the speed of light as a limit, but never attains it. Therefore, particle physicists do not generally think in terms of speed, but rather in terms of a particle's energy or momentum, usually measured in electron volts (eV). An important principle for circular accelerators, and particle beams in general, is that the curvature of the particle trajectory is proportional to the particle charge and to the magnetic field, but inversely proportional to the (typically relativistic) momentum.

The earliest operational circular accelerators were cyclotrons, invented in 1929 by Ernest O. Lawrence at the University of California, Berkeley. Cyclotrons have a single pair of hollow 'D'-shaped plates to accelerate the particles and a single large dipole magnet to bend their path into a circular orbit. It is a characteristic property of charged particles in a uniform and constant magnetic field B that they orbit with a constant period, at a frequency called the cyclotron frequency, so long as their speed is small compared to the speed of light c. This means that the accelerating D's of a cyclotron can be driven at a constant frequency by a radio frequency (RF) accelerating power source, as the beam spirals outwards continuously. The particles are injected in the centre of the magnet and are extracted at the outer edge at their maximum energy.
Cyclotrons reach an energy limit because of relativistic effects whereby the particles effectively become more massive, so that their cyclotron frequency drops out of synch with the accelerating RF. Therefore, simple cyclotrons can accelerate protons only to an energy of around 15 million electron volts (15 MeV, corresponding to a speed of roughly 10% of c), because the protons get out of phase with the driving electric field. If accelerated further, the beam would continue to spiral outward to a larger radius but the particles would no longer gain enough speed to complete the larger circle in step with the accelerating RF. To accommodate relativistic effects the magnetic field needs to be increased to higher radii like it is done in isochronous cyclotrons. An example of an isochronous cyclotron is the PSI Ring cyclotron in Switzerland, which provides protons at the energy of 590 MeV which corresponds to roughly 80% of the speed of light. The advantage of such a cyclotron is the maximum achievable extracted proton current which is currently 2.2 mA. The energy and current correspond to 1.3 MW beam power which is the highest of any accelerator currently existing.

A classic cyclotron can be modified to increase its energy limit. The historically first approach was the synchrocyclotron, which accelerates the particles in bunches. It uses a constant magnetic field



        B


    {\displaystyle B}
  , but reduces the accelerating field's frequency so as to keep the particles in step as they spiral outward, matching their mass-dependent cyclotron resonance frequency. This approach suffers from low average beam intensity due to the bunching, and again from the need for a huge magnet of large radius and constant field over the larger orbit demanded by high energy.
The second approach to the problem of accelerating relativistic particles is the isochronous cyclotron. In such a structure, the accelerating field's frequency (and the cyclotron resonance frequency) is kept constant for all energies by shaping the magnet poles so to increase magnetic field with radius. Thus, all particles get accelerated in isochronous time intervals. Higher energy particles travel a shorter distance in each orbit than they would in a classical cyclotron, thus remaining in phase with the accelerating field. The advantage of the isochronous cyclotron is that it can deliver continuous beams of higher average intensity, which is useful for some applications. The main disadvantages are the size and cost of the large magnet needed, and the difficulty in achieving the high magnetic field values required at the outer edge of the structure.
Synchrocyclotrons have not been built since the isochronous cyclotron was developed.

To reach still higher energies, with relativistic mass approaching or exceeding the rest mass of the particles (for protons, billions of electron volts or GeV), it is necessary to use a synchrotron. This is an accelerator in which the particles are accelerated in a ring of constant radius. An immediate advantage over cyclotrons is that the magnetic field need only be present over the actual region of the particle orbits, which is much narrower than that of the ring. (The largest cyclotron built in the US had a 184-inch-diameter (4.7 m) magnet pole, whereas the diameter of synchrotrons such as the LEP and LHC is nearly 10 km. The aperture of the two beams of the LHC is of the order of a centimeter.)
However, since the particle momentum increases during acceleration, it is necessary to turn up the magnetic field B in proportion to maintain constant curvature of the orbit. In consequence, synchrotrons cannot accelerate particles continuously, as cyclotrons can, but must operate cyclically, supplying particles in bunches, which are delivered to a target or an external beam in beam "spills" typically every few seconds.
Since high energy synchrotrons do most of their work on particles that are already traveling at nearly the speed of light c, the time to complete one orbit of the ring is nearly constant, as is the frequency of the RF cavity resonators used to drive the acceleration.
In modern synchrotrons, the beam aperture is small and the magnetic field does not cover the entire area of the particle orbit as it does for a cyclotron, so several necessary functions can be separated. Instead of one huge magnet, one has a line of hundreds of bending magnets, enclosing (or enclosed by) vacuum connecting pipes. The design of synchrotrons was revolutionized in the early 1950s with the discovery of the strong focusing concept. The focusing of the beam is handled independently by specialized quadrupole magnets, while the acceleration itself is accomplished in separate RF sections, rather similar to short linear accelerators. Also, there is no necessity that cyclic machines be circular, but rather the beam pipe may have straight sections between magnets where beams may collide, be cooled, etc. This has developed into an entire separate subject, called "beam physics" or "beam optics".
More complex modern synchrotrons such as the Tevatron, LEP, and LHC may deliver the particle bunches into storage rings of magnets with a constant magnetic field, where they can continue to orbit for long periods for experimentation or further acceleration. The highest-energy machines such as the Tevatron and LHC are actually accelerator complexes, with a cascade of specialized elements in series, including linear accelerators for initial beam creation, one or more low energy synchrotrons to reach intermediate energy, storage rings where beams can be accumulated or "cooled" (reducing the magnet aperture required and permitting tighter focusing; see beam cooling), and a last large ring for final acceleration and experimentation.

Circular electron accelerators fell somewhat out of favor for particle physics around the time that SLAC's linear particle accelerator was constructed, because their synchrotron losses were considered economically prohibitive and because their beam intensity was lower than for the unpulsed linear machines. The Cornell Electron Synchrotron, built at low cost in the late 1970s, was the first in a series of high-energy circular electron accelerators built for fundamental particle physics, the last being LEP, built at CERN, which was used from 1989 until 2000.
A large number of electron synchrotrons have been built in the past two decades, as part of synchrotron light sources that emit ultraviolet light and X rays; see below.

For some applications, it is useful to store beams of high energy particles for some time (with modern high vacuum technology, up to many hours) without further acceleration. This is especially true for colliding beam accelerators, in which two beams moving in opposite directions are made to collide with each other, with a large gain in effective collision energy. Because relatively few collisions occur at each pass through the intersection point of the two beams, it is customary to first accelerate the beams to the desired energy, and then store them in storage rings, which are essentially synchrotron rings of magnets, with no significant RF power for acceleration.

Some circular accelerators have been built to deliberately generate radiation (called synchrotron light) as X-rays also called synchrotron radiation, for example the Diamond Light Source which has been built at the Rutherford Appleton Laboratory in England or the Advanced Photon Source at Argonne National Laboratory in Illinois, USA. High-energy X-rays are useful for X-ray spectroscopy of proteins or X-ray absorption fine structure (XAFS), for example.
Synchrotron radiation is more powerfully emitted by lighter particles, so these accelerators are invariably electron accelerators. Synchrotron radiation allows for better imaging as researched and developed at SLAC's SPEAR.

Fixed-Field Alternating Gradient accelerators (FFAG)s, in which a very strong radial field gradient, combined with strong focusing, allows the beam to be confined to a narrow ring, are an extension of the isochronous cyclotron idea that is lately under development. They use RF accelerating sections between the magnets, and so are isochronous for relativistic particles like electrons (which achieve essentially the speed of light at only a few MeV), but only over a limited energy range for protons and heavier particles at sub-relativistic energies. Like the isochronous cyclotrons, they achieve continuous beam operation, but without the need for a huge dipole bending magnet covering the entire radius of the orbits.

Ernest Lawrence's first cyclotron was a mere 4 inches (100 mm) in diameter. Later, in 1939, he built a machine with a 60-inch diameter pole face, and planned one with a 184-inch diameter in 1942, which was, however, taken over for World War II-related work connected with uranium isotope separation; after the war it continued in service for research and medicine over many years.
The first large proton synchrotron was the Cosmotron at Brookhaven National Laboratory, which accelerated protons to about 3 GeV (1953–1968). The Bevatron at Berkeley, completed in 1954, was specifically designed to accelerate protons to sufficient energy to create antiprotons, and verify the particle-antiparticle symmetry of nature, then only theorized. The Alternating Gradient Synchrotron (AGS) at Brookhaven (1960–) was the first large synchrotron with alternating gradient, "strong focusing" magnets, which greatly reduced the required aperture of the beam, and correspondingly the size and cost of the bending magnets. The Proton Synchrotron, built at CERN (1959–), was the first major European particle accelerator and generally similar to the AGS.
The Stanford Linear Accelerator, SLAC, became operational in 1966, accelerating electrons to 30 GeV in a 3 km long waveguide, buried in a tunnel and powered by hundreds of large klystrons. It is still the largest linear accelerator in existence, and has been upgraded with the addition of storage rings and an electron-positron collider facility. It is also an X-ray and UV synchrotron photon source.
The Fermilab Tevatron has a ring with a beam path of 4 miles (6.4 km). It has received several upgrades, and has functioned as a proton-antiproton collider until it was shut down due to budget cuts on September 30, 2011. The largest circular accelerator ever built was the LEP synchrotron at CERN with a circumference 26.6 kilometers, which was an electron/positron collider. It achieved an energy of 209 GeV before it was dismantled in 2000 so that the underground tunnel could be used for the Large Hadron Collider (LHC). The LHC is a proton collider, and currently the world's largest and highest-energy accelerator, achieving 6.5 TeV energy per beam (13 TeV in total).
The aborted Superconducting Super Collider (SSC) in Texas would have had a circumference of 87 km. Construction was started in 1991, but abandoned in 1993. Very large circular accelerators are invariably built in underground tunnels a few metres wide to minimize the disruption and cost of building such a structure on the surface, and to provide shielding against intense secondary radiations that occur, which are extremely penetrating at high energies.
Current accelerators such as the Spallation Neutron Source, incorporate superconducting cryomodules. The Relativistic Heavy Ion Collider, and Large Hadron Collider also make use of superconducting magnets and RF cavity resonators to accelerate particles.

The output of a particle accelerator can generally be directed towards multiple lines of experiments, one at a given time, by means of a deviating electromagnet. This makes it possible to operate multiple experiments without needing to move things around or shutting down the entire accelerator beam. Except for synchrotron radiation sources, the purpose of an accelerator is to generate high-energy particles for interaction with matter.
This is usually a fixed target, such as the phosphor coating on the back of the screen in the case of a television tube; a piece of uranium in an accelerator designed as a neutron source; or a tungsten target for an X-ray generator. In a linac, the target is simply fitted to the end of the accelerator. The particle track in a cyclotron is a spiral outwards from the centre of the circular machine, so the accelerated particles emerge from a fixed point as for a linear accelerator.
For synchrotrons, the situation is more complex. Particles are accelerated to the desired energy. Then, a fast acting dipole magnet is used to switch the particles out of the circular synchrotron tube and towards the target.
A variation commonly used for particle physics research is a collider, also called a storage ring collider. Two circular synchrotrons are built in close proximity – usually on top of each other and using the same magnets (which are then of more complicated design to accommodate both beam tubes). Bunches of particles travel in opposite directions around the two accelerators and collide at intersections between them. This can increase the energy enormously; whereas in a fixed-target experiment the energy available to produce new particles is proportional to the square root of the beam energy, in a collider the available energy is linear.

At present the highest energy accelerators are all circular colliders, but both hadron accelerators and electron accelerators are running into limits. Higher energy hadron and ion cyclic accelerators will require accelerator tunnels of larger physical size due to the increased beam rigidity.
For cyclic electron accelerators, a limit on practical bend radius is placed by synchrotron radiation losses and the next generation will probably be linear accelerators 10 times the current length. An example of such a next generation electron accelerator is the proposed 40 km long International Linear Collider.
It is believed that plasma wakefield acceleration in the form of electron-beam 'afterburners' and standalone laser pulsers might be able to provide dramatic increases in efficiency over RF accelerators within two to three decades. In plasma wakefield accelerators, the beam cavity is filled with a plasma (rather than vacuum). A short pulse of electrons or laser light either constitutes or immediately precedes the particles that are being accelerated. The pulse disrupts the plasma, causing the charged particles in the plasma to integrate into and move toward the rear of the bunch of particles that are being accelerated. This process transfers energy to the particle bunch, accelerating it further, and continues as long as the pulse is coherent.
Energy gradients as steep as 200 GeV/m have been achieved over millimeter-scale distances using laser pulsers and gradients approaching 1 GeV/m are being produced on the multi-centimeter-scale with electron-beam systems, in contrast to a limit of about 0.1 GeV/m for radio-frequency acceleration alone. Existing electron accelerators such as SLAC could use electron-beam afterburners to greatly increase the energy of their particle beams, at the cost of beam intensity. Electron systems in general can provide tightly collimated, reliable beams; laser systems may offer more power and compactness. Thus, plasma wakefield accelerators could be used – if technical issues can be resolved – to both increase the maximum energy of the largest accelerators and to bring high energies into university laboratories and medical centres.
Higher than 0.25 GeV/m gradients have been achieved by a dielectric laser accelerator, which may present another viable approach to building compact high-energy accelerators.

In the future, the possibility of black hole production at the highest energy accelerators may arise if certain predictions of superstring theory are accurate. This and other possibilities have led to public safety concerns that have been widely reported in connection with the LHC, which began operation in 2008. The various possible dangerous scenarios have been assessed as presenting "no conceivable danger" in the latest risk assessment produced by the LHC Safety Assessment Group. If black holes are produced, it is theoretically predicted that such small black holes should evaporate extremely quickly via Bekenstein-Hawking radiation, but which is as yet experimentally unconfirmed. If colliders can produce black holes, cosmic rays (and particularly ultra-high-energy cosmic rays, UHECRs) must have been producing them for eons, but they have yet to harm anybody. It has been argued that to conserve energy and momentum, any black holes created in a collision between an UHECR and local matter would necessarily be produced moving at relativistic speed with respect to the Earth, and should escape into space, as their accretion and growth rate should be very slow, while black holes produced in colliders (with components of equal mass) would have some chance of having a velocity less than Earth escape velocity, 11.2 km per sec, and would be liable to capture and subsequent growth. Yet even on such scenarios the collisions of UHECRs with white dwarfs and neutron stars would lead to their rapid destruction, but these bodies are observed to be common astronomical objects. Thus if stable micro black holes should be produced, they must grow far too slowly to cause any noticeable macroscopic effects within the natural lifetime of the solar system.

An accelerator operator controls the operation of a particle accelerator used in research experiments, reviews an experiment schedule to determine experiment parameters specified by an experimenter (physicist), adjust particle beam parameters such as aspect ratio, current intensity, and position on target, communicates with and assists accelerator maintenance personnel to ensure readiness of support systems, such as vacuum, magnet power supplies and controls, low conductivity water or LCW cooling, and radiofrequency power supplies and controls, and maintains a record of accelerator related events.

Accelerator physics
Atom smasher (disambiguation)
Dielectric wall accelerator
Nuclear transmutation
List of accelerators in particle physics
Rolf Widerøe
The idea of a particle accelerator has also been used in television shows such as The Flash.

Social media are computer-mediated technologies that allow the creating and sharing of information, ideas, career interests and other forms of expression via virtual communities and networks. The variety of stand-alone and built-in social media services currently available introduces challenges of definition. However, there are some common features.
Social media are interactive Web 2.0 Internet-based applications.
User-generated content, such as text posts or comments, digital photos or videos, and data generated through all online interactions, are the lifeblood of social media.
Users create service-specific profiles for the website or app that are designed and maintained by the social media organization.
Social media facilitate the development of online social networks by connecting a user's profile with those of other individuals and/or groups.
Social media use web-based technologies, desktop computers and mobile technologies (e.g., smartphones and tablet computers) to create highly interactive platforms through which individuals, communities and organizations can share, co-create, discuss, and modify user-generated content or pre-made content posted online. They introduce substantial and pervasive changes to communication between businesses, organizations, communities and individuals. Social media changes the way individuals and large organizations communicate. These changes are the focus of the emerging field of technoself studies. In America, a survey reported that 84 percent of adolescents in America have a Facebook account. Over 60% of 13 to 17-year-olds have at least one profile on social media, with many spending more than two hours a day on social networking sites. According to Nielsen, Internet users continue to spend more time on social media sites than on any other type of site. At the same time, the total time spent on social media sites in the U.S. across PCs as well as on mobile devices increased by 99 percent to 121 billion minutes in July 2012 compared to 66 billion minutes in July 2011. For content contributors, the benefits of participating in social media have gone beyond simply social sharing to building reputation and bringing in career opportunities and monetary income.
Social media differ from paper-based or traditional electronic media such as TV broadcasting in many ways, including quality, reach, frequency, usability, immediacy, and permanence. Social media operate in a dialogic transmission system (many sources to many receivers). This is in contrast to traditional media which operates under a monologic transmission model (one source to many receivers), such as a paper newspaper which is delivered to many subscribers. Some of the most popular social media websites are Facebook (and its associated Facebook Messenger), WhatsApp, Tumblr, Instagram, Twitter, Baidu Tieba, Pinterest, LinkedIn, Gab, Google+, YouTube, Viber, Snapchat, Weibo and WeChat. These social media websites have more than 100,000,000 registered users.
Observers have noted a range of positive and negative impacts from social media use. Social media can help to improve individuals' sense of connectedness with real and/or online communities and social media can be an effective communications (or marketing) tool for corporations, entrepreneurs, nonprofit organizations, including advocacy groups and political parties and governments. At the same time, concerns have been raised about possible links between heavy social media use and depression, and even the issues of cyberbullying, online harassment and "trolling". Currently, about half of young adults have been cyberbullied and of those, 20 percent said that they have been cyberbullied on a regular basis. Another survey was carried out among 7th grade students in America which is known as the Precaution Process Adoption Model. According to this study 69 percent of 7th grade students claim to have experienced cyberbullying and they also said that it is worse than face to face bullying.

The variety and evolving stand-alone and built-in social media services introduces a challenge of definition. The idea that social media are defined by their ability to bring people together has been seen as too broad a definition, as this would suggest that the telegraph and telephone were also social media – not the technologies scholars are intending to describe. The terminology is unclear, with some referring to social media as social networks.
A 2015 paper reviewed the prominent literature in the area and identified four commonalities unique to then-current social media services:
social media are Web 2.0 Internet-based applications,
user-generated content (UGC) is the lifeblood of the social media organism,
users create service-specific profiles for the site or app that are designed and maintained by the social media organization,
social media facilitate the development of online social networks by connecting a user's profile with those of other individuals and/or groups.
In 2016, Merriam-Webster defined social media as "Forms of electronic communication (such as Web sites) through which people create online communities to share information, ideas, personal messages, etc."

The term social media is usually used to describe social networking sites such as:
Facebook – an online social networking site that allows users to create their personal profiles, share photos and videos, and communicate with other users
Twitter – an internet service that allows users to post "tweets" for their followers to see updates in real-time
LinkedIn – a networking website for the business community that allows users to create professional profiles, post resumes, and communicate with other professionals and job-seekers.
Pinterest – an online community that allows users to display photos of items found on the web by "pinning" them and sharing ideas with others.
Snapchat – an app on mobile devices that allows users to send and share photos of themselves doing their daily activities.
Social media technologies take many different forms including blogs, business networks, enterprise social networks, forums, microblogs, photo sharing, products/services review, social bookmarking, social gaming, social networks, video sharing, and virtual worlds. The development of social media started off with simple platforms such as sixdegrees.com. Unlike instant messaging clients such as ICQ and AOL's AIM, or chat clients like IRC, iChat or Chat Television, sixdegrees.com was the first online business that was created for real people, using their real names. However, the first social networks were short-lived because their users lost interest. The Social Network Revolution has led to the rise of the networking sites. Research shows that the audience spends 22 percent of their time on social networking sites, thus proving how popular social media platforms have become. This increase is because of the smart phones that are now in the daily lives of most humans.

Some social media sites have greater potential for content that is posted there to spread virally over social networks. This is an analogy to the concept of a viral infectious disease in biology, some of which can spread rapidly from an infected person to another person. In a social media context, content or websites that are "viral" (or which "go viral") are those with a greater likelihood that users will reshare content posted (by another user) to their social network, leading to further sharing. In some cases, posts containing controversial content (e.g., Kim Kardashian's nude photos that "broke the Internet" and crashed servers) or fast-breaking news have been rapidly shared and re-shared by huge numbers of users. Many social media sites provide specific functionality to help users reshare content – for example, Twitter's retweet button, Pinterest's pin function, Facebook's share option or Tumblr's reblog function. Businesses have a particular interest in viral marketing tactics because such a campaign can achieve widespread advertising coverage (particularly if the "viral" reposting itself makes the news) for a fraction of the cost of a traditional marketing campaign (e.g., billboard ads, television commercials, magazine ads, etc.). Nonprofit organisations and activists may have similar interests in posting content online with the hopes that it goes viral. The social news website Slashdot sometimes has news stories that, once posted on its website, "go viral"; the Slashdot effect refers to this situation.

Mobile social media refers to the use of social media on mobile devices such as smartphones and tablet computers. This is a group of mobile marketing applications that allow the creation, exchange and circulation of user-generated content. Due to the fact that mobile social media run on mobile devices, they differ from traditional social media by incorporating new factors such as the current location of the user (location-sensitivity) or the time delay between sending and receiving messages (time-sensitivity). According to Andreas Kaplan, mobile social media applications can be differentiated among four types:
Space-timers (location and time sensitive): Exchange of messages with relevance mostly for one specific location at one specific point in time (e.g. Facebook Places; Foursquare)
Space-locators (only location sensitive): Exchange of messages, with relevance for one specific location, which are tagged to a certain place and read later by others (e.g. Yelp; Qype, Tumblr, Fishbrain)
Quick-timers (only time sensitive): Transfer of traditional social media applications to mobile devices to increase immediacy (e.g. posting Twitter messages or Facebook status updates)
Slow-timers (neither location, nor time sensitive): Transfer of traditional social media applications to mobile devices (e.g. watching a YouTube video or reading/editing a Wikipedia article)

Although social media accessed via desktop computers offer a variety of opportunities for companies in a wide range of business sectors, mobile social media, which users are accessing when they are "on the go" via tablet computer or smartphone can take advantage of the location- and time-sensitive awareness of users. Mobile social media tools can be used for marketing research, communication, sales promotions/discounts, and relationship development/loyalty programs.
Marketing research: Mobile social media applications offer data about offline consumer movements at a level of detail heretofore limited to online companies. Any firm can know the exact time at which a customer entered one of its outlets, as well as know the social media comments made during the visit.
Communication: Mobile social media communication takes two forms: company-to-consumer (in which a company may establish a connection to a consumer based on its location and provide reviews about locations nearby) and user-generated content. For example, McDonald's offered $5 and $10 gift-cards to 100 users randomly selected among those checking in at one of its restaurants. This promotion increased check-ins by 33% (from 2,146 to 2,865), resulted in over 50 articles and blog posts, and prompted several hundred thousand news feeds and Twitter messages.
Sales promotions and discounts: Although customers have had to use printed coupons in the past, mobile social media allows companies to tailor promotions to specific users at specific times. For example, when launching its California-Cancun service, Virgin America offered users who checked in through Loopt at one of three designated Border Grill taco trucks in San Francisco and Los Angeles between 11 a.m. and 3 p.m. on August 31, 2010, two tacos for $1 and two flights to Mexico for the price of one. This special promotion was only available to people who were at a certain location and at a certain time.
Relationship development and loyalty programs: In order to increase long-term relationships with customers, companies can develop loyalty programs that allow customers who check-in via social media regularly at a location to earn discounts or perks. For example, American Eagle Outfitters remunerates such customers with a tiered 10%, 15%, or 20% discount on their total purchase.
e-Commerce: Social media sites are increasingly implementing marketing-friendly strategies, creating platforms that are mutually beneficial for users, businesses, and the networks themselves in the popularity and accessibility of e-commerce, or online purchases. The user who posts her or his comments about a company's product or service benefits because they are able to share their views with their online friends and acquaintances. The company benefits because it obtains insight (positive or negative) about how their product or service is viewed by consumers. Mobile social media applications such as Amazon.com and Pinterest have started to influence an upward trend in the popularity and accessibility of e-commerce, or online purchases.
E-commerce businesses may refer to social media as consumer-generated media (CGM). A common thread running through all definitions of social media is a blending of technology and social interaction for the co-creation of value for the business or organization that is using it. People obtain valuable information, education, news, and other data from electronic and print media. Social media are distinct from industrial or traditional media such as newspapers, magazines, television, and film as they are comparatively inexpensive and accessible (at least once a person has already acquired Internet access and a computer). They enable anyone (even private individuals) to publish or access information. Industrial media generally require significant resources to publish information as in most cases the articles go through many revisions before being published. This process adds to the cost and the resulting market price. Originally social media was only used by individuals but now it is used by businesses, charities and also in government and politics.
One characteristic shared by both social and industrial media is the capability to reach small or large audiences; for example, either a blog post or a television show may reach no people or millions of people. Some of the properties that help describe the differences between social and industrial media are:
Quality: In industrial (traditional) publishing—mediated by a publisher—the typical range of quality is substantially narrower (skewing to the high quality side) than in niche, unmediated markets like user-generated social media posts. The main challenge posed by content in social media sites is the fact that the distribution of quality has high variance: from very high-quality items to low-quality, sometimes even abusive or inappropriate content.
Reach: Both industrial and social media technologies provide scale and are capable of reaching a global audience. Industrial media, however, typically use a centralized framework for organization, production, and dissemination, whereas social media are by their very nature more decentralized, less hierarchical, and distinguished by multiple points of production and utility.
Frequency: The number of times users access a type of media per day. Heavy social media users, such as young people, check their social media account numerous times throughout the day.
Accessibility: The means of production for industrial media are typically government and/or corporate (privately owned); social media tools are generally available to the public at little or no cost, or they are supported by advertising revenue. While social media tools are available to anyone with access to Internet and a computer or mobile device, due to the digital divide, the poorest segment of the population lacks access to the Internet and computer. Low-income people may have more access to traditional media (TV, radio, etc.), as an inexpensive TV and aerial or radio costs much less than an inexpensive computer or mobile device. Moreover, in many regions, TV or radio owners can tune into free over the air programming; computer or mobile device owners need Internet access to go on social media sites.
Usability: Industrial media production typically requires specialized skills and training. For example, in the 1970s, to record a pop song, an aspiring singer would have to rent time in an expensive professional recording studio and hire an audio engineer. Conversely, most social media activities, such as posting a video of oneself singing a song require only modest reinterpretation of existing skills (assuming a person understands Web 2.0 technologies); in theory, anyone with access to the Internet can operate the means of social media production, and post digital pictures, videos or text online.
Immediacy: The time lag between communications produced by industrial media can be long (days, weeks, or even months, by the time the content has been reviewed by various editors and fact checkers) compared to social media (which can be capable of virtually instantaneous responses). The immediacy of social media can be seen as a strength, in that it enables regular people to instantly communicate their opinions and information. At the same time, the immediacy of social media can also be seen as a weakness, as the lack of fact checking and editorial "gatekeepers" facilitates the circulation of hoaxes and fake news.
Permanence: Industrial media, once created, cannot be altered (e.g., once a magazine article or paper book is printed and distributed, changes cannot be made to that same article in that print run) whereas social media posts can be altered almost instantaneously, when the user decides to edit their post or due to comments from other readers.
Community media constitute a hybrid of industrial and social media. Though community-owned, some community radio, TV, and newspapers are run by professionals and some by amateurs. They use both social and industrial media frameworks. Social media have also been recognized for the way they have changed how public relations professionals conduct their jobs. They have provided an open arena where people are free to exchange ideas on companies, brands, and products. Doc Searls and David Wagner state that the "...best of the people in PR are not PR types at all. They understand that there aren't censors, they're the company's best conversationalists." Social media provides an environment where users and PR professionals can converse, and where PR professionals can promote their brand and improve their company's image by listening and responding to what the public is saying about their product.

Social media have a strong influence on business activities and business performance. There are four channels by which social media resources are transformed into business performance capabilities:
Social capital: represents the extent to which social media affects firms' and organizations' relationships with society and the degree to which the organizations' ue of social media increases corporate social performance capabilities.
Revealed preferences: represents the extent to which social media exposes customers' likings (e.g., "likes" and followers) and increases a firm's financial capabilities (e.g., stock price, revenue, profit), or for non-profits, increases their donations, volunteerism rate, etc.
Social marketing: represents the extent to which social marketing resources (e.g., online conversations, sharing links, online presence, sending text messages) are used to increase a firm's financial capabilities (e.g., sales, acquisition of new customers) or a non-profit's voluntary sector goals.
Social corporate networking: Social corporate networking refers to the informal ties and linkages of corporate/organizational staff with other people from their field or industry, clients, customers, and other members of the public, which were formed through social networks. Social corporate networking can increase operational performance capabilities in many ways, as it can enable sales staff to find new clients; marketing staff to learn about client/customer needs and demand; and management can learn about the public perceptions of their strategy or approach.
There are four tools or approaches that engage experts, customers, suppliers, and employees in the development of products and services using social media. Companies and other organizations can use these tools and approaches to improve their business capacity and performance.
Customer relationship management (CRM) is an approach to managing a company's interaction with current and potential future customers that tries to analyze data about customers' history with a company and to improve business relationships with customers, specifically focusing on customer retention and ultimately driving sales growth. One important aspect of the CRM approach is the systems of CRM that compile data from a range of different communication channels, including a company's website, telephone, email, live chat, marketing materials, and social media. Through the CRM approach and the systems used to facilitate CRM, businesses learn more about their target audiences and how to best cater to their needs. However, adopting the CRM approach may also occasionally lead to favoritism within an audience of consumers, resulting in dissatisfaction among customers and defeating the purpose of CRM.
Innovation can be defined simply as a "new idea, device, or method" or as the application of better solutions that meet new requirements, unarticulated needs, or existing market needs. This is accomplished through more-effective products, processes, services, technologies, or business models that are readily available to markets, governments and society. The term "innovation" can be defined as something original and more effective and, as a consequence, new, that "breaks into" the market or society.[3] It is related to, but not the same as, invention. Innovation is often manifested via the engineering process. Innovation is generally considered to be the result of a process that brings together various novel ideas in a way that they affect society. In industrial economics, innovations are created and found empirically from services to meet the growing consumer demand.
Training
Knowledge management

Companies are increasingly using social media monitoring tools to monitor, track, and analyze online conversations on the Web about their brand or products or about related topics of interest. This can be useful in public relations management and advertising campaign tracking, allowing the companies to measure return on investment for their social media ad spending, competitor-auditing, and for public engagement. Tools range from free, basic applications to subscription-based, more in-depth tools. Hootsuite is an example of a social media monitoring and tracking software that companies can use.
Social media tracking also enables companies to respond quickly to online posts that criticize their product or service. By responding quickly to critical online posts, and helping the user to resolve the concerns, this helps the company to lessen the negative effects that online complaints can have about company product or service sales. In the US, for example, if a customer criticizes a major hotel chain's cleanliness or service standards on a social media website, a company representative will usually quickly be alerted to this critical post, so that the company representative can go online and express concern for the sub-par service and offer the complaining person a coupon or discount on their next purchase, plus a promise to forward their concerns to the hotel manager so that the problem will not be repeated. This rapid response helps to show that the company cares about its customers.
The "honeycomb framework" defines how social media services focus on some or all of seven functional building blocks. These building blocks help explain the engagement needs of the social media audience. For instance, LinkedIn users are thought to care mostly about identity, reputation, and relationships, whereas YouTube's primary features are sharing, conversations, groups, and reputation. Many companies build their own social "containers" that attempt to link the seven functional building blocks around their brands. These are private communities that engage people around a more narrow theme, as in around a particular brand, vocation or hobby, rather than social media containers such as Google+, Facebook, and Twitter. PR departments face significant challenges in dealing with viral negative sentiment directed at organizations or individuals on social media platforms (dubbed "sentimentitis"), which may be a reaction to an announcement or event. In a 2011 article, Jan H. Kietzmann, Kristopher Hermkens, Ian P. McCarthy and Bruno S. Silvestre describe the honeycomb relationship as "present[ing] a framework that defines social media by using seven functional building blocks: identity, conversations, sharing, presence, relationships, reputation, and groups."
The elements of the honeycomb framework include:
Identity: This block represents the extent to which users reveal their identities in a social media setting. This can include disclosing information such as name, age, gender, profession, location, and also information that portrays users in certain ways.
Conversations: This block represents the extent to which users communicate with other users in a social media setting. Many social media sites are designed primarily to facilitate conversations among individuals and groups. These conversations happen for all sorts of reasons. People tweet, blog, make online comments and send messages to other users to meet new like-minded people, to ﬁnd a romantic partner, to build their self-esteem, or to be on the cutting edge of new ideas or trending topics. Yet others see social media as a way of making their message heard and positively impacting humanitarian causes, environmental problems, economic issues, or political debates.
Sharing: This block represents the extent to which users exchange, distribute, and receive content, ranging from a short text post to a link or a digital photo. The term 'social' implies that exchanges between people are crucial. In many cases, however, sociality is about the objects that mediate these ties between people—the reasons why they meet online and associate with each other.
Presence: This block represents the extent to which users can know if other users are accessible. It includes knowing where others are, in the virtual world and/or in the real world, and whether they are available. Some social media sites have icons that indicate when other users are online, such as Facebook.
Relationships: This block represents the extent to which users can be related or linked up to other users. Two or more users have some form of association that leads them to converse, share objects of sociality, send texts or messages, meet up, or simply just list each other as a friend or fan.
Reputation: This block represents the extent to which users can identify the standing of others, including themselves, in a social media setting. Reputation can have different meanings on social media platforms. In most cases, reputation is a matter of trust, but because information technologies are not yet good at determining such highly qualitative criteria, social media sites rely on 'mechanical Turks': tools that automatically aggregate user-generated information to determine trustworthiness. Reputation management is another aspect and use of social media.
Groups: This block represents the extent to which users can form communities and sub-communities of people with similar backgrounds, demographics or interests. The more 'social' a network becomes, the wider the group of friends, followers, and contacts can be developed. Some Facebook users develop a list of friends that includes people from all over the world.

Social media becomes effective through a process called "building social authority". One of the foundation concepts in social media has become that you cannot completely control your message through social media but rather you can simply begin to participate in the "conversation" expecting that you can achieve a significant influence in that conversation. However, this conversation participation must be cleverly executed because although people are resistant to marketing in general, they are even more resistant to direct or overt marketing through social media platforms. This may seem counterintuitive but it is the main reason building social authority with credibility is so important. A marketer can generally not expect people to be receptive to a marketing message in and of itself. In the Edelman Trust Barometer report in 2008, the majority (58%) of the respondents reported they most trusted company or product information coming from "people like me" inferred to be information from someone they trusted. In the 2010 Trust Report, the majority switched to 64% preferring their information from industry experts and academics. According to Inc. Technology's Brent Leary, "This loss of trust, and the accompanying turn towards experts and authorities, seems to be coinciding with the rise of social media and networks."

Social media "mining" is a type of data mining, a technique of analyzing data to detect patterns. Social media mining is a process of representing, analyzing, and extracting actionable patterns from data collected from people's activities on social media. Social media mining introduces basic concepts and principal algorithms suitable for investigating massive social media data; it discusses theories and methodologies from different disciplines such as computer science, data mining, machine learning, social network analysis, network science, sociology, ethnography, statistics, optimization, and mathematics. It encompasses the tools to formally represent, measure, model, and mine meaningful patterns from large-scale social media data. Detecting patterns in social media use by data mining is of particular interest to advertisers, major corporations and brands, governments and political parties, among others.

According to the article "The Emerging Role of Social Media in Political and Regime Change" by Rita Safranek, the Middle East and North Africa region has one of the most youthful populations in the world, with people under 25 making up between 35–45% of the population in each country. They make up the majority of social media users, including about 17 million Facebook users, 25,000 Twitter accounts and 40,000 active blogs, according to the Arab Advisors Group.

This is a list of the leading social networks based on number of active user accounts as of September 2016.
Facebook: 1,712,000,000 users
WhatsApp 1,000,000,000 users
Facebook Messenger: 1,000,000,000 users
QQ: 899,000,000 users
WeChat: 806,000,000 users
QZone: 652,000,000 users
Tumblr: 555,000,000 users
Instagram: 500,000,000 users
Twitter: 313,000,000 users
Baidu Tieba: 300,000,000 users
Skype: 300,000,000 users
Sina Weibo: 282,000,000 users
Viber: 249,000,000 users
Line: 218,000,000 users
Snapchat: 200,000,000 users

Just as television turned a nation of people who listened to media content into watchers of media content, the emergence of social media has created a nation of media content creators. According to 2011 Pew Research data, nearly 80% of American adults are online and nearly 60% of them use social networking sites. More Americans get their news via the Internet than from newspapers or radio, as well as three-fourths who say they get news from e-mail or social media sites updates, according to a report published by CNN. The survey suggests that Facebook and Twitter make news a more participatory experience than before as people share news articles and comment on other people's posts. According to CNN, in 2010 75% of people got their news forwarded through e-mail or social media posts, whereas 37% of people shared a news item via Facebook or Twitter. In the United States, 81% of people say they look online for news of the weather, first and foremost. National news at 73%, 52% for sports news, and 41% for entertainment or celebrity news. Based on this study, done for the Pew Center, two-thirds of the sample's online news users were younger than 50, and 30% were younger than 30. The survey involved tracking daily the habits of 2,259 adults 18 or older. Thirty-three percent of young adults get news from social networks. Thirty-four percent watched TV news and 13% read print or digital content. Nineteen percent of Americans got news from Facebook, Google+, or LinkedIn. Thirty-six percent of those who get news from social network got it yesterday from survey. More than 36% of Twitter users use accounts to follow news organizations or journalists. Nineteen percent of users say they got information from news organizations of journalists. TV remains most popular source of news, but audience is aging (only 34% of young people).
Of those younger than 25, 29% said they got no news yesterday either digitally or traditional news platforms. Only 5% under 30 said they follow news about political figures and events in DC. Only 14% of respondents could answer all four questions about which party controls the House, current unemployment rate, what nation Angela Merkel leads, and which presidential candidate favors taxing higher-income Americans. Facebook and Twitter now pathways to news, but are not replacements for traditional ones. Seventy percent get social media news from friends and family on Facebook.
Social media fosters communication. An Internet research company, PewResearch Center, claims that "more than half of internet users (52%) use two or more of the social media sites measured (Facebook, Twitter, Instagram, Pinterest) to communicate with their family or friends". For children, using social media sites can help promote creativity, interaction, and learning. It can also help them with homework and class work. Moreover, social media enable them to stay connected with their peers, and help them to interact with each other. Some can get involved with developing fundraising campaigns and political events. However, it can impact social skills due to the absence of face-to-face contact. Social media can affect mental health of teens. Teens who use Facebook frequently and especially who are susceptible may become more narcissistic, antisocial, and aggressive. Teens become strongly influenced by advertising, and it influences buying habits. Since the creation of Facebook in 2004, it has become a distraction and a way to waste time for many users. A head teacher in the United Kingdom commented in 2015 that social media caused more stress to teenage children than examinations, with constant interaction and monitoring by peers ending the past practice where what pupils did in the evening or at weekends was separate from the arguments and peer pressure at school.
In a 2014 study, high school students ages 18 and younger were examined in an effort to find their preference for receiving news. Based on interviews with 61 teenagers, conducted from December 2007 to February 2011, most of the teen participants reported reading print newspapers only "sometimes," with fewer than 10% reading them daily. The teenagers instead reported learning about current events from social media sites such as Facebook, MySpace, YouTube, and blogs. Another study showed that social media users read a set of news that is different from what newspaper editors feature in the print press. Using nanotechnology as an example, a study was conducted that studied tweets from Twitter and found that some 41% of the discourse about nanotechnology focused on its negative impacts, suggesting that a portion of the public may be concerned with how various forms of nanotechnology are used in the future. Although optimistic-sounding and neutral-sounding tweets were equally likely to express certainty or uncertainty, the pessimistic tweets were nearly twice as likely to appear certain of an outcome than uncertain. These results imply the possibility of a preconceived negative perception of many news articles associated with nanotechnology. Alternatively, these results could also imply that posts of a more pessimistic nature that are also written with an air of certainty are more likely to be shared or otherwise permeate groups on Twitter. Similar biases need to be considered when the utility of new media is addressed, as the potential for human opinion to over-emphasize any particular news story is greater despite the general improvement in addressed potential uncertainty and bias in news articles than in traditional media.
On October 2, 2013, the most common hashtag throughout the United States was "#governmentshutdown", as well as ones focusing on political parties, Obama, and healthcare. Most news sources have Twitter, and Facebook, pages, like CNN and the New York Times, providing links to their online articles, getting an increased readership. Additionally, several college news organizations and administrators have Twitter pages as a way to share news and connect to students. According to "Reuters Institute Digital News Report 2013", in the US, among those who use social media to find news, 47% of these people are under 45 years old, and 23% are above 45 years old. However social media as a main news gateway does not follow the same pattern across countries. For example, in this report, in Brazil, 60% of the respondents said social media was one of the five most important ways to find news online, 45% in Spain, 17% in the UK, 38% in Italy, 14% in France, 22% in Denmark, 30% in the U.S., and 12% in Japan. Moreover, there are differences among countries about commenting on news in social networks, 38% of the respondents in Brazil said they commented on news in social network in a week. These percentages are 21% in the U.S. and 10% in the UK. The authors argued that differences among countries may be due to culture difference rather than different levels of access to technical tools.

News media and television journalism have been instrumental in the shaping of American collective memory for much of the twentieth century. Indeed, since the United States' colonial era, news media has influenced collective memory and discourse about national development and trauma. In many ways, mainstream journalists have maintained an authoritative voice as the storytellers of the American past. Their documentary style narratives, detailed exposes, and their positions in the present make them prime sources for public memory. Specifically, news media journalists have shaped collective memory on nearly every major national event – from the deaths of social and political figures to the progression of political hopefuls. Journalists provide elaborate descriptions of commemorative events in U.S. history and contemporary popular cultural sensations. Many Americans learn the significance of historical events and political issues through news media, as they are presented on popular news stations. However, journalistic influence is growing less important, whereas social networking sites such as Facebook, YouTube and Twitter, provide a constant supply of alternative news sources for users.
As social networking becomes more popular among older and younger generations, sites such as Facebook and YouTube, gradually undermine the traditionally authoritative voices of news media. For example, American citizens contest media coverage of various social and political events as they see fit, inserting their voices into the narratives about America's past and present and shaping their own collective memories. An example of this is the public explosion of the Trayvon Martin shooting in Sanford, Florida. News media coverage of the incident was minimal until social media users made the story recognizable through their constant discussion of the case. Approximately one month after the fatal shooting of Trayvon Martin, its online coverage by everyday Americans garnered national attention from mainstream media journalists, in turn exemplifying media activism. In some ways, the spread of this tragic event through alternative news sources parallels that of the Emmitt Till – whose murder became a national story after it circulated African American and Communists newspapers. Social media was also influential in the widespread attention given to the revolutionary outbreaks in the Middle East and North Africa during 2011. However, there is some debate about the extent to which social media facilitated this kind of change. Another example of this shift is in the ongoing Kony 2012 campaign, which surfaced first on YouTube and later garnered a great amount of attention from mainstream news media journalists. These journalists now monitor social media sites to inform their reports on the movement. Lastly, in the past couple of presidential elections, the use of social media sites such as Facebook and Twitter were used to predict election results. U.S. President Barack Obama was more liked on Facebook than his opponent Mitt Romney and it was found by a study done by Oxford Institute Internet Experiment that more people liked to tweet about comments of President Obama rather than Romney.

Criticisms of social media range from criticisms of the ease of use of specific platforms and their capabilities, disparity of information available, issues with trustworthiness and reliability of information presented, the impact of social media use on an individual's concentration, ownership of media content, and the meaning of interactions created by social media. Although some social media platforms offer users the opportunity to cross-post simultaneously, some social network platforms have been criticized for poor interoperability between platforms, which leads to the creation of information silos, viz. isolated pockets of data contained in one social media platform. However, it is also argued that social media have positive effects such as allowing the democratization of the Internet while also allowing individuals to advertise themselves and form friendships. Others have noted that the term "social" cannot account for technological features of a platform alone, hence the level of sociability should be determined by the actual performances of its users. There has been a dramatic decrease in face-to-face interactions as more and more social media platforms have been introduced with the threat of cyber-bullying and online sexual predators being more prevalent. Social media may expose children to images of alcohol, tobacco, and sexual behaviors. In regards to cyber-bullying, it has been proven that individuals who have no experience with cyber-bullying often have a better well-being than individuals who have been bullied online.
Twitter is increasingly a target of heavy activity of marketers. Their actions, focused on gaining massive numbers of followers, include use of advanced scripts and manipulation techniques that distort the prime idea of social media by abusing human trustfulness. Twitter also promotes social connections among students. It can be used to enhance communication building and critical thinking. Domizi (2013) utilised Twitter in a graduate seminar requiring students to post weekly tweets to extend classroom discussions. Students reportedly used Twitter to connect with content and other students. Additionally, students found it "to be useful professionally and personally." British-American entrepreneur and author Andrew Keen criticizes social media in his book The Cult of the Amateur, writing, "Out of this anarchy, it suddenly became clear that what was governing the infinite monkeys now inputting away on the Internet was the law of digital Darwinism, the survival of the loudest and most opinionated. Under these rules, the only way to intellectually prevail is by infinite filibustering." This is also relative to the issue "justice" in the social network. For example, the phenomenon "Human flesh search engine" in Asia raised the discussion of "private-law" brought by social network platform. Comparative media professor José van Dijck contends in her book "The Culture of Connectivity" (2013) that to understand the full weight of social media, their technological dimensions should be connected to the social and the cultural. She critically describes six social media platforms. One of her findings is the way Facebook had been successful in framing the term 'sharing' in such a way that third party use of user data is neglected in favour of intra-user connectedness.

The digital divide is a measure of disparity in the level of access to technology between households, socioeconomic levels or other demographic categories. People who are homeless, living in poverty, elderly people and those living in rural or remote communities may have little or no access to computers and the Internet; in contrast, middle class and upper-class people in urban areas have very high rates of computer and Internet access. Other models argue that within a modern information society, some individuals produce Internet content while others only consume it, which could be a result of disparities in the education system where only some teachers integrate technology into the classroom and teach critical thinking. While social media has differences among age groups, a 2010 study in the United States found no racial divide. Some zero-rating programs offer subsidized data access to certain websites on low-cost plans. Critics say that this is an anti-competitive program that undermines net neutrality and creates a "walled garden" for platforms like Facebook Zero. A 2015 study found that 65% of Nigerians, 61% of Indonesians, and 58% of Indians agree with the statement that "Facebook is the Internet" compared with only 5% in the US.
Eric Ehrmann contends that social media in the form of public diplomacy create a patina of inclusiveness that covers traditional economic interests that are structured to ensure that wealth is pumped up to the top of the economic pyramid, perpetuating the digital divide and post Marxian class conflict. He also voices concern over the trend that finds social utilities operating in a quasi-libertarian global environment of oligopoly that requires users in economically challenged nations to spend high percentages of annual income to pay for devices and services to participate in the social media lifestyle. Neil Postman also contends that social media will increase an information disparity between "winners" – who are able to use the social media actively – and "losers" – who are not familiar with modern technologies or who do not have access to them. People with high social media skills may have better access to information about job opportunities, potential new friends, and social activities in their area, which may enable them to improve their standard of living and their quality of life.

Because large-scale collaborative co-creation is one of the main ways of forming information in the social network, the user generated content is sometimes viewed with skepticism; readers do not trust it as a reliable source of information. Aniket Kittur, Bongowon Suh, and Ed H. Chi took wikis under examination and indicated that, "One possibility is that distrust of wiki content is not due to the inherently mutable nature of the system but instead to the lack of available information for judging trustworthiness." To be more specific, the authors mention that reasons for distrusting collaborative systems with user-generated content, such as Wikipedia, include a lack of information regarding accuracy of contents, motives and expertise of editors, stability of content, coverage of topics and the absence of sources.
Social media is also an important source of news. According to 'Reuters Institute Digital News Report 2013', social media are one of the most important ways for people find news online (the others being traditional brands, search engines and news aggregators). The report suggested that in the United Kingdom, trust in news which comes from social media sources is low, compared to news from other sources (e.g. online news from traditional broadcaster or online news from national newspapers). People who aged at 24–35 trust social media most, whereas trust declined with the increase of age.
Rainie and Wellman have argued that media making now has become a participation work, which changes communication systems. The center of power is shifted from only the media (as the gatekeeper) to the peripheral area, which may include government, organizations, and out to the edge, the individual. These changes in communication systems raise empirical questions about trust to media effect. Prior empirical studies have shown that trust in information sources plays a major role in people's decision making. People's attitudes more easily change when they hear messages from trustworthy sources. In the Reuters report, 27% of respondents agree that they worry about the accuracy of a story on a blog. However, 40% of them believe the stories on blogs are more balanced than traditional papers because they are provided with a range of opinions. Recent research has shown that in the new social media communication environment, the civil or uncivil nature of comments will bias people's information processing even if the message is from a trustworthy source, which bring the practical and ethical question about the responsibility of communicator in the social media environment.

As media theorist Marshall McLuhan pointed out in the 1960s, media are not just passive channels of information or "dumb pipes". The media supply the stuff of thought, but they also shape the process of thought, as captured in his maxim "The medium is the message". For example, in the 1990s and 2000s, the increasing popularity of 24-hour all news channels such as CNN led to an increasing demand by news organizations for audience-grabbing headlines. As a result, even minor gaffes or misstatements by celebrities or public officials were made into leading news stories, to satisfy audience demand. Thus, in this example, the medium of 24-hour all-news channels started to shape the "message" that was sent on the media channel.

For Malcolm Gladwell, the role of social media, such as Twitter and Facebook, in revolutions and protests is overstated. On one hand, social media make it easier for individuals, and in this case activists, to express themselves. On the other hand, it is harder for that expression to have an impact. Gladwell distinguishes between social media activism and high risk activism, which brings real changes. Activism and especially high-risk activism involves strong-tie relationships, hierarchies, coordination, motivation, exposing oneself to high risks, making sacrifices. Gladwell discusses that social media are built around weak ties and he argues that "social networks are effective at increasing participation — by lessening the level of motivation that participation requires". According to him "Facebook activism succeeds not by motivating people to make a real sacrifice, but by motivating them to do the things that people do when they are not motivated enough to make a real sacrifice".
Furthermore, social media's role in democratizing media participation, which proponents herald as ushering in a new era of participatory democracy, with all users able to contribute news and comments, may fall short of the ideals. Social media has been championed as allowing anyone with an Internet connection to become a content creator and empowering the "active audience". But international survey data suggest online media audience members are largely passive consumers, while content creation is dominated by a small number of users who post comments and write new content. Others argue that the effect of social media will vary from one country to another, with domestic political structures playing a greater role than social media in determining how citizens express opinions about "current affairs stories involving the state". According to the "Reuters Institute Digital News Report 2013", the percent of online news users who blog about news issues ranges from 1–5%. Greater percentages use social media to comment on news, with participation ranging from 8% in Germany to 38% in Brazil. But online news users are most likely to just talk about online news with friends offline or use social media to share stories without creating content.

Evgeny Morozov, 2009–2010 Yahoo fellow at Georgetown University, contends that the information uploaded to Twitter may have little relevance to the rest of the people who do not use Twitter. In the article "Iran: Downside to the "Twitter Revolution"" in the magazine Dissent , he says:

"Twitter only adds to the noise: it's simply impossible to pack much context into its 140 characters. All other biases are present as well: in a country like Iran it's mostly pro-Western, technology-friendly and iPod-carrying young people who are the natural and most frequent users of Twitter. They are a tiny and, most important, extremely untypical segment of the Iranian population (the number of Twitter users in Iran — a country of more than seventy million people.)"

Even in the United States, the birth-country of Twitter, currently in 2015 the social network has 306 million accounts. Because there are likely to be many multi-account users, and the United States in 2012 had a population of 314.7 million, the adoption of Twitter is somewhat limited. Professor Matthew Auer of Bates College casts doubt on the conventional wisdom that social media are open and participatory. He also speculates on the emergence of "anti-social media" used as "instruments of pure control."

Social media content is generated through social media interactions done by the users through the site. There has always been a huge debate on the ownership of the content on social media platforms because it is generated by the users and hosted by the company. Added to this is the danger to security of information, which can be leaked to third parties with economic interests in the platform, or parasites who comb the data for their own databases. The author of Social Media Is Bullshit, Brandon Mendelson, claims that the "true" owners of content created on social media sites only benefits the large corporations who own those sites and rarely the users that created them.

Privacy rights advocates warn users on social media about the collection of their personal data. Some information is captured without the user's knowledge or consent through electronic tracking and third party applications. Data may also be collected for law enforcement and governmental purposes,[101] by social media intelligence using data mining techniques.[102] Data and information may also be collected for third party use. When information is shared on social media, that information is no longer private. There have been many cases in which young persons especially, share personal information, which can attract predators. It is very important to monitor what you share, and to be aware of who you could potentially be sharing that information with. Teens especially share significantly more information on the internet now than they have in the past. Teen are much more likely to share their personal information, such as email address, phone number, and school names. Studies suggest that teens are not aware of what they are posting and how much of that information can be accessed by third parties.
Other privacy concerns with employers and social media are when employers use social media as a tool to screen a prospective employee. This issue raises many ethical questions that some consider an employer's right and others consider discrimination. Except in the states of California, Maryland, and Illinois, there are no laws that prohibit employers from using social media profiles as a basis of whether or not someone should be hired.[105] Title VII also prohibits discrimination during any aspect of employment including hiring or firing, recruitment, or testing.[106] Social media has been integrating into the workplace and this has led to conflicts within employees and employers.[107] Particularly, Facebook has been seen as a popular platform for employers to investigate in order to learn more about potential employees. This conflict first started in Maryland when an employer requested and received an employee's Facebook username and password. State lawmakers first introduced legislation in 2012 to prohibit employers from requesting passwords to personal social accounts in order to get a job or to keep a job. This led to Canada, Germany, the U.S. Congress and 11 U.S. states to pass or propose legislation that prevents employers' access to private social accounts of employees.[108]
It is not only an issue in the workplace, but an issue in schools as well. There have been situations where students have been forced to give up their social media passwords to school administrators. There are inadequate laws to protect a student's social media privacy, and organizations such as the ACLU are pushing for more privacy protection, as it is an invasion. They urge students who are pressured to give up their account information to tell the administrators to contact a parent and/or lawyer before they take the matter any further. Although they are students, they still have the right to keep their password-protected information private.
Many Western European countries have already implemented laws that restrict the regulation of social media in the workplace. States including Arkansas, California, Colorado, Illinois, Maryland, Michigan, Nevada, New Jersey, New Mexico, Utah, Washington, and Wisconsin have passed legislation that protects potential employees and current employees from employers that demand them to give forth their username or password for a social media account.[109] Laws that forbid employers from disciplining an employee based on activity off the job on social media sites have also been put into act in states including California, Colorado, Connecticut, North Dakota, and New York. Several states have similar laws that protect students in colleges and universities from having to grant access to their social media accounts. Eight states have passed the law that prohibits post secondary institutions from demanding social media login information from any prospective or current students and privacy legislation has been introduced or is pending in at least 36 states as of July 2013.[110] As of May 2014, legislation has been introduced and is in the process of pending in at least 28 states and has been enacted in Maine and Wisconsin.[111] In addition, the National Labor Relations Board has been devoting a lot of their attention to attacking employer policies regarding social media that can discipline employees who seek to speak and post freely on social media sites.
There are arguments that "privacy is dead" and that with social media growing more and more, some heavy social media users appear to have become quite unconcerned with privacy. Others argue, however, that people are still very concerned about their privacy, but are being ignored by the companies running these social networks, who can sometimes make a profit off of sharing someone's personal information. There is also a disconnect between social media user's words and their actions. Studies suggest that surveys show that people want to keep their lives private, but their actions on social media suggest otherwise. Another factor is ignorance of how accessible social media posts are. Some social media users who have been criticized for inappropriate comments stated that they did not realize that anyone outside their circle of friends would read their post; in fact, on some social media sites, unless a user selects higher privacy settings, their content is shared with a wide audience.

Data suggest that participants use social media to fulfill perceived social needs, but are typically disappointed.  Lonely individuals are drawn to the Internet for emotional support. This could interfere with "real life socializing" by reducing face-to-face relationships. Some of these views are summed up in an Atlantic article by Stephen Marche entitled Is Facebook Making Us Lonely?, in which the author argues that social media provides more breadth, but not the depth of relationships that humans require and that users begin to find it difficult to distinguish between the meaningful relationships which we foster in the real world, and the numerous casual relationships that are formed through social media. Sherry Turkle explores similar issues in her book Alone Together as she discusses how people confuse social media usage with authentic communication. She posits that people tend to act differently online and are less afraid to hurt each other's feelings. Some online behaviors can cause stress and anxiety, due to the permanence of online posts, the fear of being hacked, or of colleges and employers exploring social media pages. Turkle also speculates that people are beginning to prefer texting to face-to-face communication, which can contribute to feelings of loneliness. Some researchers have also found that only exchanges that involved direct communication and reciprocation of messages to each other increased feelings of connectedness. However, passively using social media without sending or receiving messages to individuals does not make people feel less lonely unless they were lonely to begin with.
A study published in the Public Library of Science in 2013 revealed that the perception of Facebook being an important resource for social connection was diminished by the number of people found to have developed low self-esteem, and the more they used the network the lower their level of self-esteem. A current controversial topic is whether or not social media addiction should be explicitly categorized as a psychological ailment. Extended use of social media has led to increased Internet addiction, cyberbullying, sexting, sleep deprivation, and the decline of face-to-face interaction. Several clinics in the UK classify social media addiction is a certifiable medical condition with one psychiatric consultant claiming that he treats as many as one hundred cases a year. Lori Ann Wagner, a psychotherapist, argues that humans communicate best face to face with their five senses involved. In addition, a study on social media done by PhD's Hsuan-Ting Kim and Yonghwan Kim, suggests that social networking sites have begun to raise concern because of the expectations people seek to fulfill from these sites and the amount of time users are willing to invest.
However, there are also positive effects as there are negative ones. Social media is a great way to make sure that people know that one is in a relationship or not, advertising in their about section. This can reduce the complications and confusions that may have been a problem when social media was not so popular. It is a great way of sharing big dates that may have occurred in ones life, such as a pregnancy announcement or engagement. Not all aspects about social media negatively effect interpersonal relationships. Some aspects encourage the relationships and build them to be stronger.

As social media usage has become increasingly widespread, social media has to a large extent come to be subjected to commercialization by marketing companies and advertising agencies. Christofer Laurell, a digital marketing researcher, suggested that the social media landscape currently consists of three types of places becacuse of this development: consumer-dominated places, professionally dominated places and places undergoing commercialization. As social media becomes commercialized, this process have been shown to create novel forms of value networks stretching between consumer and producer in which a combination of personal, private and commercial contents are created. The commercial development of social media has been criticized as the actions of consumers in these settings has become increasingly value-creating, for example when consumers contribute to the marketing and branding of specific products by posting positive reviews. As such, value-creating activities also increase the value of a specific product, which could, according to the marketing professors Bernad Cova and Daniele Dalli, lead to what they refer to as "double exploitation". Companies are getting consumers to create content for the companies' websites for which the consumers are not paid.

There are several negative effects to social media which receive criticism, for example regarding privacy issues, information overload and Internet fraud. Social media can also have negative social effects on users. Angry or emotional conversations can lead to real-world interactions outside of the Internet, which can get users into dangerous situations. Some users have experienced threats of violence online and have feared these threats manifesting themselves offline. Studies also show that social media have negative effects on peoples' self-esteem and self-worth. The authors of "Who Compares and Despairs? The Effect of Social Comparison Orientation on Social Media Use and its Outcomes" found that people with a higher social comparison orientation appear to use social media more heavily than people with low social comparison orientation. This finding was consistent with other studies that found people with high social comparison orientation make more social comparisons once on social media. People compare their own lives to the lives of their friends through their friends' posts. People are motivated to portray themselves in a way that is appropriate to the situation and serves their best interest. Often the things posted online are the positive aspects of people's lives, making other people question why their own lives are not as exciting or fulfilling. This can lead to depression and other self-esteem issues.
Three researchers at Blanquerna University, Spain, examined how adolescents interact with social media and specifically Facebook.They suggest that interactions on the website encourage representing oneself in the traditional gender constructs, which helps maintain gender stereotypes. The authors noted that girls generally show more emotion in their posts and more frequently change their profile pictures, which according to some psychologists can lead to self-objectification. On the other hand, the researchers found that boys prefer to portray themselves as strong, independent, and powerful. For example, men often post pictures of objects and not themselves, and rarely change their profile pictures; using the pages more for entertainment and pragmatic reasons. In contrast girls generally post more images that include themselves, friends and things they have emotional ties to, which the researchers attributed that to the higher emotional intelligence of girls at a younger age. The authors sampled over 632 girls and boys from the ages of 12–16 from Spain in an effort to confirm their beliefs. The researchers concluded that masculinity is more commonly associated with a positive psychological well-being, while femininity displays less psychological well-being. Furthermore, the researchers discovered that people tend not to completely conform to either stereotype, and encompass desirable parts of both. Users of Facebook generally use their profile to reflect that they are a "normal" person. Social media was found to uphold gender stereotypes both feminine and masculine. The researchers also noted that the traditional stereotypes are often upheld by boys more so than girls. The authors described how neither stereotype was entirely positive, but most people viewed masculine values as more positive.
Terri H. Chan, the author of "Facebook and its Effects on Users' Empathic Social Skills and Life Satisfaction: A Double Edged Sword Effect", claims that the more time people spend on Facebook, the less satisfied they feel about their life. Self-presentational theory explains that people will consciously manage their self-image or identity related information in social contexts. According to Gina Chen, the author of Losing Face on Social Media: Threats to Positive Face Lead to an Indirect Effect on Retaliatory Aggression Through Negative Affect, when people are not accepted or are criticized online they feel emotional pain. This may lead to some form of online retaliation such as online bullying. Trudy Hui Hui Chua and Leanne Chang's article, "Follow Me and Like My Beautiful Selfies: Singapore Teenage Girls' Engagement in Self-Presentation and Peer Comparison on Social Media" states that teenage girls manipulate their self-presentation on social media to achieve a sense of beauty that is projected by their peers. These authors also discovered that teenage girls compare themselves to their peers on social media and present themselves in certain ways in effort to earn regard and acceptance, which can actually lead to problems with self-confidence and self-satisfaction.
According to writer Christine Rosen in "Virtual Friendship, and the New Narcissism," many social media sites encourage status-seeking. According to Rosen, the practice and definition of "friendship" changes in virtuality. Friendship "in these virtual spaces is thoroughly different from real-world friendship. In its traditional sense, friendship is a relationship which, broadly speaking, involves the sharing of mutual interests, reciprocity, trust, and the revelation of intimate details over time and within specific social (and cultural) contexts. Because friendship depends on mutual revelations that are concealed from the rest of the world, it can only flourish within the boundaries of privacy; the idea of public friendship is an oxymoron." Rosen also cites Brigham Young University researchers who "recently surveyed 184 users of social networking sites and found that heavy users 'feel less socially involved with the community around them.'" Critic Nicholas G. Carr in "Is Google Making Us Stupid?" questions how technology affects cognition and memory. "The kind of deep reading that a sequence of printed pages promotes is valuable not just for the knowledge we acquire from the author's words but for the intellectual vibrations those words set off within our own minds. In the quiet spaces opened up by the sustained, undistracted reading of a book, or by any other act of contemplation, for that matter, we make our own associations, draw our own inferences and analogies, foster our own ideas... If we lose those quiet spaces, or fill them up with "content," we will sacrifice something important not only in our selves but in our culture."
Bo Han, a social media researcher at Texas A&M University-Commerce, finds that users are likely to experience the "social media burnout" issue. Ambivalence, emotional exhaustion, and depersonalization are usually the main symptoms if a user experiences social media burnout. Ambivalence refers to a user's confusion about the benefits she can get from using a social media site. Emotional exhaustion refers to the stress a user has when using a social media site. Depersonalization refers to the emotional detachment from a social media site a user experiences. The three burnout factors can all negatively influence the user's social media continuance. This study provides an instrument to measure the burnout a user can experience, when her social media "friends" are generating an overwhelming amount of useless information (e.g., "what I had for dinner", "where I am now").

In the book Networked – The New Social Operating System by Lee Rainie and Barry Wellman, the two authors reflect on mainly positive effects of social media and other Internet-based social networks. According to the authors, social media are used to document memories, learn about and explore things, advertise oneself and form friendships. For instance, they claim that the communication through Internet based services can be done more privately than in real life. Furthermore, Rainie and Wellman discuss that everybody has the possibility to become a content creator. Content creation provides networked individuals opportunities to reach wider audiences. Moreover, it can positively affect their social standing and gain political support. This can lead to influence on issues that are important for someone. As a concrete example of the positive effects of social media, the authors use the Tunisian revolution in 2011, where people used Facebook to gather meetings, protest actions, etc. Rainie and Wellman (Ibid) also discuss that content creation is a voluntary and participatory act. What is important is that networked individuals create, edit, and manage content in collaboration with other networked individuals. This way they contribute in expanding knowledge. Wikis are examples of collaborative content creation.
A survey conducted (in 2011), by Pew Internet Research, discussed in Lee Rainie and Barry Wellman's Networked – The New Social Operating System, illustrates that 'networked individuals' are engaged to a further extent regarding numbers of content creation activities and that the 'networked individuals' are increasing over a larger age span. These are some of the content creation activities that networked individuals take part in:
writing material, such as text or online comments, on a social networking site such as Facebook: 65% of Internet users do this
sharing digital photos: 55%
contributing rankings and reviews of products or services: 37%
creating "tags" of content, such as tagging songs by genre: 33%
posting comments on third-party websites or blogs: 26%
taking online material and remixing it into a new creation: 15% of Internet users do this with photos, video, audio, or text
creating or working on a blog: 14%
Another survey conducted (in 2015) by Pew Internet Research shows that the Internet users among American adults who uses at least one social networking site has increased from 10% to 76% since 2005. Pew Internet Research illustrates furthermore that it nowadays is no real gender difference among Americans when it comes to social media usage. Women were even more active on social media a couple of years ago, however today's numbers point at women: 68%, and men: 62%. Social media have been used to assist in searches for missing persons. When 21-year-old University of Cincinnati student Brogan Dulle disappeared in May 2014 from near his apartment in the Clifton neighborhood of Cincinnati, Ohio, his friends and family used social media to organize and fund a search effort. The disappearance made international news when their efforts went viral on Facebook, Twitter, GoFundMe, and The Huffington Post during the week-long search. Dulle's body was eventually found in a building next door to his apartment.

Use of social media by young people has caused significant problems for some applicants who are active on social media when they try to enter the job market. A survey of 17,000 young people in six countries in 2013 found that 1 in 10 people aged 16 to 34 have been rejected for a job because of online comments they made on social media websites. A 2014 survey of recruiters found that 93% of them check candidates' social media postings. Moreover, professor Stijn Baert of Ghent University conducted a field experiment in which fictitious job candidates applied for real job vacancies in Belgium. They were identical except in one respect: their Facebook profile photos. It was found that candidates with the most wholesome photos were a lot more likely to receive invitations for job interviews than those with the more controversial photos. In addition, Facebook profile photos had a greater impact on hiring decisions when candidates were highly educated. These cases have created some privacy implications as to whether or not companies should have the right to look at employee's Facebook profiles. In March 2012, Facebook decided they might take legal action against employers for gaining access to employee's profiles through their passwords. According to Facebook Chief Privacy Officer for policy, Erin Egan, the company has worked hard to give its users the tools to control who sees their information. He also said users shouldn't be forced to share private information and communications just to get a job. According to the network's Statement of Rights and Responsibilities, sharing or soliciting a password is a violation of Facebook policy. Employees may still give their password information out to get a job, but according to Erin Egan, Facebook will continue to do their part to protect the privacy and security of their users.

Before social media, admissions officials in the United States used SAT and other standardized test scores, extra-curricular activities, letters of recommendation, and high school report cards to determine whether to accept or deny an applicant. In the 2010s, while colleges and universities still use these traditional methods to evaluate applicants, these institutions are increasingly accessing applicants' social media profiles to learn about their character and activities. According to Kaplan, Inc, a corporation that provides higher education preparation, in 2012 27% of admissions officers used Google to learn more about an applicant, with 26% checking Facebook. Students whose social media pages include offensive jokes or photos, racist or homophobic comments, photos depicting the applicant engaging in illegal drug use or drunkenness, and so on, may be screened out from admission processes.

People are increasingly getting political news and information from social media platforms. A 2014 study showed that 62% of web users turn to Facebook to find political news. This social phenomenon allows for political information, true or not, to spread quickly and easily among peer networks. Furthermore, social media sites are now encouraging political involvement by uniting like-minded people, reminding users to vote in elections, and analyzing users' political affiliation data to find cultural similarities and differences. Social media can help taint the reputation of political figures fairly quickly with information that may or may not be true. Information spreads like wildfire and before a politician can even get an opportunity to address the information, either to confirm, deny, or explain, the public has already formed an opinion about the politician based on that information. However, when conducted on purpose, the spread of information on social media for political means can help campaigns immensely. The Barack Obama presidential campaign, 2008, is considered to be one of the most successful in terms of social media. On the other hand, negative word-of-mouth in social media concerning a political figure can be very unfortunate for a politician and can cost the politician his/her career if the information is very damaging. For example, Anthony Weiner's misuse of the social media platform Twitter to send inappropriate messages eventually led to his resignation from U.S. Congress.
Open forums online have led to some negative effects in the political sphere. Some politicians have made the mistake of using open forums to try and reach a broader audience and thus more potential voters. What they forgot to account for was that the forums would be open to everyone, including those in opposition. Having no control over the comments being posted, negative included, has been damaging for some with unfortunate oversight. Additionally, a constraint of social media as a tool for public political discourse is that if oppressive governments recognize the ability social media has to cause change, they shut it down. During the peak of the Egyptian Revolution of 2011, the Internet and social media played a huge role in facilitating information. At that time, Hosni Mubarak was the president of Egypt and head the regime for almost 30 years. Mubarak was so threatened by the immense power that the Internet and social media gave the people that the government successfully shut down the Internet, using the Ramses Exchange, for a period of time in February 2011.
Social media as an open forum gives a voice to those who have previously not had the ability to be heard. In 2015, some countries are still becoming equipped with Internet accessibility and other technologies. Social media is giving everyone a voice to speak out against government regimes. In 2014, the rural areas in Paraguay were only just receiving access to social media, such as Facebook. In congruence with the users worldwide, teens and young adults in Paraguay are drawn to Facebook and others types of social media as a means to self-express. Social media is becoming a main conduit for social mobilization and government critiques because, "the government can't control what we say on the Internet."
Younger generations are becoming more involved in politics due to the increase of political news posted on various types of social media. Due to the heavier use of social media among younger generations, they are exposed to politics more frequently, and in a way that is integrated into their online social lives. While informing younger generations of political news is important, there are many biases within the realms of social media. It can be difficult for outsiders to truly understand the conditions of dissent when they are removed from direct involvement. Social media can create a false sense of understanding among people who are not directly involved in the issue. An example of social media creating misconceptions can be seen during the Arab Spring protests. Today's generation rely heavily on social media to understand what is happening in the world, and consequently people are exposed to both true and false information. For example, Americans have several misconceptions surrounding the events of the Arab Springs movement. Social media can be used to create political change, both major and minor. For example, in 2011 Egyptians used Facebook, Twitter, and YouTube as a means to communicate and organize demonstrations and rallies to overthrow President Hosni Mubarak. Statistics show that during this time the rate of Tweets from Egypt increased from 2,300 to 230,000 per day and the top 23 protest videos had approximately 5.5 million views.

People around the world are taking advantage of social media as one of their key components of communication. According to King, 67 percent of US citizens ages 12 and up use social media of some type. With the expansion of social media networks there are many positive and negative alternatives. As the use of Twitter increases, its influence impacts users as well. The potential role of Twitter as a means of both service feedback and a space in which mental health can be openly discussed and considered from a variety of perspectives. The study conducted shows a positive outlook for using Twitter to discuss health issues with a patient and a professional, in this case alcohol. On the other hand, there can be negatives that arise from the use of social media. If a clinician prescribes abstinence from alcohol but then posts pictures on social media of one's own drunken exploits, the clinician's credibility is potentially lost in the eyes of the patient. In these two studies, both negative and positive outcomes were examined. Although social media can be beneficial, it is important to understand the negative consequences as well.

As the world is becoming increasingly connected via the power of the Internet, political movements, including militant groups, have begun to see social media as a major organizing and recruiting tool. Islamic State of Iraq and the Levant, also known as ISIS, has used social media to promote their cause. ISIS produces an online magazine named the Islamic State Report to recruit more fighters. ISIS produces online materials in a number of languages and uses recruiters to contact potential recruitees over the Internet.
In Canada, two girls from Montreal left their country to join ISIS in Syria after exploring ISIS on social media and eventually being recruited. On Twitter, there is an app called the Dawn of Glad Tidings that users can download and keep up to date on news about ISIS. Hundreds of users around the world have signed up for the app which once downloaded will post tweets and hash-tags to your account that are in support of ISIS. As ISIS marched on the northern region of Iraq, tweets in support of their efforts reached a high of 40,000 a day. ISIS support online is a factor in the radicalization of youth. Mass media has yet to adopt the view that social media plays a vital link in the radicalization of people. When tweets supportive of ISIS make their way onto Twitter, they result in 72 re-tweets to the original, which further spreads the message of ISIS. These tweets have made their way to the account known as active hashtags, which further helps broadcast ISIS' message as the account sends out to its followers the most popular hashtags of the day. Other militant groups such as al-Qaeda and the Taliban are increasingly using social media to raise funds, recruit and radicalize persons, and it has become increasingly effective.

There has been rapid growth in the number of US patent applications that cover new technologies related to social media, and the number of them that are published has been growing rapidly over the past five years. There are now over 2000 published patent applications. As many as 7000 applications may be currently on file including those that haven't been published yet. Only slightly over 100 of these applications have issued as patents, however, largely due to the multi-year backlog in examination of business method patents, patents which outline and claim new methods of doing business.

Having social media in the classroom has been a controversial topic in the 2010s. Many parents and educators have been fearful of the repercussions of having social media in the classroom. There are concerns that social media tools can be misused for cyberbullying or sharing inappropriate content. As result, cell phones have been banned from some classrooms, and some schools have blocked many popular social media websites. However, despite apprehensions, students in industrialized countries are (or will be) active social media users. As a result, many schools have realized that they need to loosen restrictions, teach digital citizenship skills, and even incorporate these tools into classrooms. The Peel District School Board (PDSB) in Ontario is one of many school boards that has begun to accept the use of social media in the classroom. In 2013, the PDSB introduced a "Bring Your Own Device" (BYOD) policy and have unblocked many social media sites. Fewkes and McCabe (2012) have researched about the benefits of using Facebook in the classroom. Some schools permit students to use smartphones or tablet computers in class, as long as the students are using these devices for academic purposes, such as doing research.

In early 2013, Steve Joordens, a professor at the University of Toronto, encouraged the 1,900 students enrolled in his introductory psychology course to add content to Wikipedia pages featuring content that related to the course. Like other educators, Joordens argued that the assignment would not only strengthen the site's psychology-related content, but also provide an opportunity for students to engage in critical reflection about the negotiations involved in collaborative knowledge production. However, Wikipedia's all-volunteer editorial staff complained that the students' contributions resulted in an overwhelming number of additions to the site, and that some of the contributions were inaccurate.

Using Facebook in class allows for both an asynchronous and synchronous, open speech via a familiar and regularly accessed medium, and supports the integration of multimodal content such as student-created photographs and video and URLs to other texts, in a platform that many students are already familiar with. Further, it allows students to ask more minor questions that they might not otherwise feel motivated to visit a professor in person during office hours to ask. It also allows students to manage their own privacy settings, and often work with the privacy settings they have already established as registered users. Facebook is one alternative means for shyer students to be able to voice their thoughts in and outside of the classroom. It allows students to collect their thoughts and articulate them in writing before committing to their expression. Further, the level of informality typical to Facebook can also aid students in self-expression and encourage more frequent student-and-instructor and student-and-student communication. At the same time, Towner and Munoz note that this informality may actually drive many educators and students away from using Facebook for educational purposes.
From a course management perspective, Facebook may be less efficient as a replacement for more conventional course management systems, both because of its limitations with regards to uploading assignments and due to some students' (and educators') resistance to its use in education. Specifically, there are features of student-to-student collaboration that may be conducted more efficiently on dedicated course management systems, such as the organization of posts in a nested and linked format. That said, a number of studies suggest that students post to discussion forums more frequently and are generally more active discussants on Facebook posts versus conventional course management systems like WebCT or Blackboard (Chu and Meulemans, 2008; Salaway, et al., 2008; Schroeder and Greenbowe, 2009).
Further, familiarity and comfortability with Facebook is often divided by socio-economic class, with students whose parents obtained a college degree, or at least having attended college for some span of time, being more likely to already be active users. Instructors ought to seriously consider and respect these hesitancies, and refrain from "forcing" Facebook on their students for academic purposes. Instructors also ought to consider that rendering Facebook optional, but continuing to provide content through it to students who elect to use it, places an unfair burden on hesitant students, who then are forced to choose between using a technology they are uncomfortable with and participating fully in the course. A related limitation, particularly at the level of K-12 schooling, is the distrust (and in some cases, outright prohibition) of the use of Facebook in formal classroom settings in many educational jurisdictions. However, this hesitancy towards Facebook use is continually diminishing in the United States, as the Pew Internet & American Life Project's annual report for 2012 shows that the likelihood of a person to be a registered Facebook user only fluctuates by 13 percent between different levels of educational attainment, 9 percent between urban, suburban, and rural users, only 5 percent between different household income brackets. The largest gap occurs between age brackets, with 86 percent of 18- to 29-year-olds reported as registered users as opposed to only 35 percent of 65-and-up-year-old users.

Twitter can be used to enhance communication building and critical thinking. Domizi (2013) utilized Twitter in a graduate seminar requiring students to post weekly tweets to extend classroom discussions. Students reportedly used Twitter to connect with content and other students. Additionally, students found it "to be useful professionally and personally". Junco, Heibergert, and Loken (2011) completed a study of 132 students to examine the link between social media and student engagement and social media and grades. They divided the students into two groups, one used Twitter and the other did not. Twitter was used to discuss material, organize study groups, post class announcements, and connect with classmates. Junco and his colleagues (2011) found that the students in the Twitter group had higher GPAs and greater engagement scores than the control group.
Gao, Luo, and Zhang (2012) reviewed literature about Twitter published between 2008 and 2011. They concluded that Twitter allowed students to participate with each other in class (by creating an informal "back channel"), and extend discussion outside of class time. They also reported that students used Twitter to get up-to-date news and connect with professionals in their field. Students reported that microblogging encouraged students to "participate at a higher level". Because the posts cannot exceed 140 characters, students were required to express ideas, reflect, and focus on important concepts in a concise manner. Some students found this very beneficial. Other students did not like the character limit. Also, some students found microblogging to be overwhelming (information overload). The research indicated that many students did not actually participate in the discussions, "they just lurked" online and watched the other participants.

A popular component and feature of Twitter is retweeting. Twitter allows other people to keep up with important events, stay connected with their peers, and can contribute in various ways throughout social media. When certain posts become popular, they start to get tweeted over and over again, becoming viral. Ellen DeGeneres is a prime example of this. She was a host during the 86th Academy Awards, when she took the opportunity to take a selfie with about twelve other celebrities that joined in on the highlight of the night, including Jennifer Lawrence, Brad Pitt and Julia Roberts. This picture went viral within forty minutes and was retweeted 1.8 million times within the first hour. This was an astonishing record for Twitter and the use of selfies, which other celebrities have tried to recreate. Retweeting is beneficial strategy, which notifies individuals on Twitter about popular trends, posts, and events.

YouTube is a frequently used social media tool in the classroom (also the second most visited website in the world). Students can watch videos, answer questions, and discuss content. Additionally, students can create videos to share with others. Sherer and Shea (2011) claimed that YouTube increased participation, personalization (customization), and productivity. YouTube also improved students' digital skills and provided opportunity for peer learning and problem solving Eick et al. (2012) found that videos kept students' attention, generated interest in the subject, and clarified course content. Additionally, the students reported that the videos helped them recall information and visualize real world applications of course concepts.

LinkedIn is a professional social network that enables employers and job-seeking workers to connect. It was created by Reid Hoffman in 2002 and was launched on May 2003. LinkedIn is now the world's largest professional social network with over 300 million members in over 200 countries. The mission of LinkedIn is to "connect the world's professionals to make them more productive and successful." A lot of people describe LinkedIn as a "professional Facebook", but it is important to remember that LinkedIn is not Facebook. Users tend to avoid informal nicknames and any inappropriate pictures of their private lives in their profile. Instead, they use a standard headshot as a profile picture and keep the content and information as professional and career-focused as possible. Most LinkedIn users put their CV online. Some also provide a list of the courses they have taken in college or university. Users can also post articles that they have written or published, which enables prospective employers to see their written work.
There are over 39 million students and recent college graduates on LinkedIn, becoming the fastest-growing demographic on the site. There are many ways that LinkedIn can be used in the classroom. First and foremost, using LinkedIn in the classroom encourages students to have a professional online social presence and can help them become comfortable in searching for a job or internship. "The key to making LinkedIn a great social learning tool is to encourage learners to build credibility through their profiles, so that experts and professionals won't think twice about connecting with them and share knowledge." Dedicating class time solely for the purpose of setting up LinkedIn accounts and showing students how to navigate it and build their profile will set them up for success in the future. Next, professors can create assignments that involve using LinkedIn as a research tool. The search tool in LinkedIn gives students the opportunity to seek out organizations they are interested in and allow them to learn more.
Giving students the class time to work on their LinkedIn profile allows them to network with each other, and stresses the importance of networking. Finally, professors can design activities that revolve around resume building and interviews. A person's LinkedIn and resume are what employers look at first, and they need to know how to make a strong first impression. It's important to learn how to construct a strong resume as soon as possible, as well as learn strong interviewing skills. Not only is the information and skills learned in the classroom important, but it is also important to know how to apply the information and skills to their LinkedIn profile so they can get a job in their field of study. These skills can be gained while incorporating LinkedIn into the classroom.

In 2013, the United Kingdom Advertising Standards Authority (ASA) began to advise celebrities and sports stars to make it clear if they had been paid to tweet about a product or service by using the hashtag #spon or #ad within tweets containing endorsements. In July 2013, Wayne Rooney was accused of misleading followers by not including either of these tags in a tweet promoting Nike. The tweet read:"The pitches change. The killer instinct doesn't. Own the turf, anywhere. @NikeFootball #myground." The tweet was investigated by the ASA but no charges were pressed. The ASA stated that "We considered the reference to Nike Football was prominent and clearly linked the tweet with the Nike brand." When asked about whether the number of complaints regarding misleading social advertising had increased, the ASA stated that the number of complaints had risen marginally since 2011 but that complaints were "very low" in the "grand scheme."

Social media often features in political struggles to control public perception and online activity. In some countries, Internet police or secret police monitor or control citizens' use of social media. For example, in 2013 some social media was banned in Turkey after the Taksim Gezi Park protests. Both Twitter and YouTube were temporarily suspended in the country by a court's decision. A new law, passed by Turkish Parliament, has granted immunity to Telecommunications Directorate (TİB) personnel. The TİB was also given the authority to block access to specific websites without the need for a court order. Yet TİB's 2014 blocking of Twitter was ruled by the constitutional court to violate free speech. More recently, in the 2014 Thai coup d'état, the public was explicitly instructed not to 'share' or 'like' dissenting views on social media or face prison. In July that same year, in response to Wikileaks' release of a secret suppression order made by the Victorian Supreme Court, media lawyers were quoted in the Australian media to the effect that "anyone who tweets a link to the Wikileaks report, posts it on Facebook, or shares it in any way online could also face charges".

Social media has affected the way youth communicate, by introducing new forms of language. Abbreviations have been introduced to cut down on the time it takes to respond online. The commonly known "LOL" has become globally recognized as the abbreviation for "laugh out loud" thanks to social media. Online linguistics has changed the way youth communicate and will continue to do so in the future, as each year new catchphrases and neologisms such as "YOLO", which stands for "you only live once", and "BAE", which stands for "before anyone else" arise and start "trending" around the world.
Other trends that influence the way youth communicate is through hashtags. With the introduction of social media platforms such as Twitter, Facebook and Instagram, the hashtag was created to easily organize and search for information. As hashtags such as #tbt ("throwback Thursday") become a part of online communication, it influenced the way in which youth share and communicate in their daily lives. Because of these changes in linguistics and communication etiquette, researchers of media semiotics have found that this has altered youth's communications habits and more.
Social media also alters the way we understand each other. Social media has allowed for mass cultural exchange and intercultural communication. For example, people from different regions or even different countries can discuss current issues on Facebook. As different cultures have different value systems, cultural themes, grammar, and worldviews, they also communicate differently. The emergence of social media platforms collided different cultures and their communication methods together, forcing them to realign in order to communicate with ease with other cultures. As different cultures continue to connect through social media platforms, thinking patterns, expression styles and cultural content that influence cultural values are chipped away.

Benkler, Yochai (2006). The Wealth of Networks. New Haven: Yale University Press. ISBN 0-300-11056-1. OCLC 61881089.
Gentle, Anne (2012). Conversation and Community: The Social Web for Documentation (2nd ed.). Laguna Hills, CA: XML Press. ISBN 978-1-937434-10-6. OCLC 794490599.
Johnson, Steven Berlin (2005). Everything Bad Is Good for You. New York: Riverhead Books. ISBN 1-57322-307-7. OCLC 57514882.
Jue, Arthur L., Jackie Alcalde Marr, Mary Ellen Kassotakis (2010). Social media at work : how networking tools propel organizational performance (1st ed.). San Francisco, CA: Jossey-Bass. ISBN 978-0470405437.
Lardi, Kamales; Fuchs, Rainer (2013). Social Media Strategy – A step-by-step guide to building your social business (1st ed.). Zurich: vdf. ISBN 978-3-7281-3557-5.
Li, Charlene; Bernoff, Josh (2008). Groundswell: Winning in a World Transformed by Social Technologies. Boston: Harvard Business Press. ISBN 978-1-4221-2500-7. OCLC 423555651.
McHale, Robert; Garulay, Eric (2012). Navigating Social Media Legal Risks: Safeguarding Your Business. Que. ISBN 978-0-789-74953-6.
Piskorski, Mikołaj Jan (2014). A Social Strategy: How We Profit from Social Media. Princeton, NJ: Princeton University Press. ISBN 978-0-691-15339-1.
Powell, Guy R.; Groves, Steven W.; Dimos, Jerry (2011). ROI of Social Media: How to improve the return on your social marketing investment. New York: John Wiley & Sons. ISBN 978-0-470-82741-3. OCLC 0470827416.
Rheingold, Howard (2002). Smart mobs: The next social revolution (1st printing ed.). Cambridge, MA: Perseus Pub. p. 288. ISBN 978-0-7382-0608-0.
Scoble, Robert; Israel, Shel (2006). Naked Conversations: How Blogs are Changing the Way Businesses Talk with Customers. Hoboken, N.J: John Wiley. ISBN 0-471-74719-X. OCLC 61757953.
Shirky, Clay (2008). Here Comes Everybody. New York: Penguin Press. ISBN 978-1-59420-153-0. OCLC 458788924.
Siegel, Alyssa (September 7, 2015). "How Social Media Affects Our Relationships". Psychology Tomorrow.
Surowiecki, James (2004). The Wisdom of Crowds. New York: Anchor Books. ISBN 0-385-72170-6. OCLC 156770258.
Tapscott, Don; Williams, Anthony D. (2006). Wikinomics. New York: Portfolio. ISBN 1-59184-138-0. OCLC 318389282.
Watts, Duncan J. (2003). Six degrees: The science of a connected age. London: Vintage. p. 368. ISBN 978-0-09-944496-1.
Tedesco, Laura Anne. The Metropolitan Museum of Art. 200-2013. 12 02 2014
Agozzino, Alisa. "Building A Personal Relationship Through Social Media: A Study Of Millennial Students' Brand Engagement." Ohio Communication Journal 50. (2012): 181–204. Communication Abstracts. Web. 3 Dec. 2013.
Schoen, Harald, et al. "The Power Of Prediction With Social Media." Internet Research 23.5 (2013): 528–543. Communication Abstracts. Web. 3 Dec. 2013.
Mateus, Samuel (2012). "Social Networks Scopophilic dimension – social belonging through spectatorship".
Schrape, Jan-Felix (2016). "Social Media, Mass Media and the ›Public Sphere‹. Differentiation, Complementarity and Co-existence" (PDF). Stuttgart: Research Contributions to Organizational Sociology and Innovation Studies 2016-01. ISSN 2191-4990.Journalism is the production and distribution of reports on the interaction of events, facts, ideas, and people that are the "news of the day" and that informs society to at least some degree. The word applies to the occupation (professional or not), the methods of gathering information, and the organizing literary styles. Journalistic media include: print, television, radio, Internet, and, in the past, newsreels.
Concepts of the appropriate role for journalism varies between countries. In some nations, the news media is controlled by a government intervention, and is not a fully independent body. In others, the news media is independent from the government but the profit motive is in tension with constitutional protections of freedom of the press. Access to freely available information gathered by independent and competing journalistic enterprises with transparent editorial standards can enable citizens to effectively participate in the political process. In the United States, journalism is protected by the freedom of the press clause in the First Amendment.
The role and status of journalism, along with that of the mass media, has undergone changes over the last two decades with the advent of digital technology and publication of news on the Internet. This has created a shift in the consumption of print media channels, as people increasingly consume news through e-readers, smartphones, and other electronic devices, challenging news organizations to fully monetize their digital wing, as well as improvise on the context in which they publish news in print. Notably, in the American media landscape, newsrooms have reduced their staff and coverage as traditional media channels, such as television, grapple with declining audiences. For instance, between 2007 and 2012, CNN edited its story packages into nearly half of their original time length.
This compactness in coverage has been linked to broad audience attrition, as a large majority of respondents in recent studies show changing preferences in news consumption. The digital era has also ushered in a new kind of journalism in which ordinary citizens play a greater role in the process of news making, with the rise of citizen journalism being possible through the Internet. Using video camera equipped smartphones, active citizens are now enabled to record footage of news events and upload them onto channels like YouTube, which is often discovered and used by mainstream news media outlets. Meanwhile, easy access to news from a variety of online sources, like blogs and other social media, has resulted in readers being able to pick from a wider choice of official and unofficial sources, instead of only from traditional media organizations. Journalism is nonfiction.

Journalistic conventions vary by country. In the United States, journalism is produced by media organizations or by individuals. Bloggers are often, but not always, journalists. The Federal Trade Commission requires that bloggers who receive free promotional gifts, then write about products, must disclose that they received the products for free. This is to eliminate conflicts of interest and protect consumers.
Fake news is news that is not truthful or is produced by unreliable media organizations. Fake news is easily spread on social media. Readers can determine fake news by evaluating whether the news has been published by a credible news organization. In the US, a credible news organization is an incorporated entity; has an editorial board; and has a clear division between editorial and advertising departments. Credible news organizations, or their employees, belong to one or more professional organizations such as the American Society of News Editors, the Society of Professional Journalists, Investigative Reporters & Editors, or the Online News Association. All of these organizations have codes of ethics that members abide by. Many news organizations have their own codes of ethics that guide journalists' professional publications. The New York Times code of standards and ethics is considered particularly rigorous.
When they write stories, journalists are concerned with issues of objectivity and bias. Some types of stories are intended to represent the author's own opinion; other types of stories are intended to be more neutral or balanced. In a physical newspaper, information is organized into sections and it is easy to see which stories are supposed to be opinion and which are supposed to be neutral. Online, many of these distinctions break down. Readers should pay careful attention to headings and other design elements to ensure that they understand the journalist's intent. Opinion pieces generally are written by regular columnists or appear in a section titled "Op-ed." Feature stories, breaking news, and hard news stories are generally not opinion pieces.
Many debates center on whether journalists are "supposed" to be "objective" or "neutral." The idea of "journalistic objectivity" is considered out of date. Journalists are people who produce news out of and as part of a particular social context. They are guided by professional codes of ethics and do their best to represent all legitimate points of view.

There are several different forms of journalism, all with diverse audiences. Journalism is said to serve the role of a "fourth estate", acting as a watchdog on the workings of the government. A single publication (such as a newspaper) contains many forms of journalism, each of which may be presented in different formats. Each section of a newspaper, magazine, or website may cater to different audiences.

Some forms include:

Advocacy journalism – writing to advocate particular viewpoints or influence the opinions of the audience.
Broadcast journalism – written or spoken journalism for radio or television.
Citizen journalism -- participatory journalism.
Data journalism -- the practice of finding stories in numbers, and using numbers to tell stories. Data journalists may use data to support their reporting. They may also report about uses and misuses of data. The US news organization ProPublica is known as a pioneer of data journalism.
Drone journalism – use of drones to capture journalistic footage.
Gonzo journalism – first championed by Hunter S. Thompson, gonzo journalism is a "highly personal style of reporting".
Interactive journalism: a type of online journalism that is presented on the web
Investigative journalism: in-depth reporting that uncovers social problems. Often leads to major social problems being resolved.
Photojournalism: the practice of telling true stories through images
Sensor journalism: the use of sensors to support journalistic inquiry.
Tabloid journalism – writing that is light-hearted and entertaining. Considered less legitimate than mainstream journalism.
Yellow journalism (or sensationalism) – writing which emphasizes exaggerated claims or rumours.

The recent rise of social media has resulted in arguments to reconsider journalism as a process rather than attributing it to particular news products. From this perspective, journalism is participatory, a process distributed among multiple authors and involving journalists as well as the socially mediating public.

Johann Carolus's Relation aller Fürnemmen und gedenckwürdigen Historien, published in 1605 in Strassburg, is often recognized as the first newspaper. The first successful English daily, the Daily Courant, was published from 1702 to 1735. The reform of the Diário Carioca newspaper in the 1950s is usually referred to as the birth of modern journalism in Brazil.

 In the 1920s, as modern journalism was just taking form, writer Walter Lippmann and American philosopher John Dewey debated over the role of journalism in a democracy. Their differing philosophies still characterize a debate about the role of journalism in society and the nation-state.
To Lippmann, the journalist fulfilled the role of mediator, or translator, between the general public and policy-making elites. Lippmann reasoned that the public could not assess modern society's growingly complex flurry of facts; therefore, it needed an intermediary to filter its news. Journalists served as this intermediary, recording the information exchanged among elites, distilling it, and passing it on for public consumption. The public would affect the decisions of the elite with its vote; in the meantime, the elite would keep the business of power running. Effectively, Lippmann's philosophy had the public at the bottom of the power chain, inheriting its information from the elite.

Lippmann's elitism had consequences that he came to deplore. An apostle of historicism and scientism, Lippmann did not merely hold that democratic government was a problematic exercise, but regarded all political communities, of whatever stripe, as needing guidance from a transcendent partisanship for accurate information and dispassionate judgment. In "Liberty and the News" (1919) and "Public Opinion" (1921) Lippmann expressed the hope that liberty could be redefined to take account of the scientific and historical perspective and that public opinion could be managed by a system of intelligence in and out of government. Thus the liberty of the journalist was to be dedicated to gathering verifiable facts while commentators like himself would place the news in the broader perspective. Lippmann deplored the influence of powerful newspaper publishers and preferred the judgments of the "patient and fearless men of science". In so doing, he denigrated not only the opinion of the majority but also the opinion of those who had influence or power as well. In a republican form of government, the representatives are chosen by the people and share with them adherence to the fundamental principles and political institutions of the polity. Lippmann's quarrel was with those very principles and institutions, for they are the product of the pre-scientific and pre-historical viewpoint and what for him was a groundless natural-rights political philosophy.
But Lippmann turned against what he called the "collectivism" of the Progressive movement he encouraged with its de-emphasis on the foundations of American politics and government and ultimately wrote a work, "The Public Philosophy" (1955), which came very close to a return to the principles of the American founders.
Dewey, on the other hand, believed not only that the public was capable of understanding the issues created or responded to by the elite, but also that it was in the public forum that decisions should be made after discussion and debate. When issues were thoroughly vetted, then the best ideas would bubble to the surface. Dewey believed journalists should do more than simply pass on information. He believed they should weigh the consequences of the policies being enacted. Over time, his idea has been implemented in various degrees, and is more commonly known as "community journalism".

This concept of community journalism is at the centre of new developments in journalism. In this new paradigm, journalists are able to engage citizens and the experts and elites in the proposition and generation of content. While there is an assumption of equality, Dewey still celebrated expertise. Dewey believed the shared knowledge of many to be far superior to a single individual's knowledge. Experts and scholars are welcome in Dewey's framework, but there is not the hierarchical structure present in Lippmann's understanding of journalism and society. According to Dewey, conversation, debate, and dialogue lie at the heart of a democracy.
While Lippmann's journalistic philosophy might be more acceptable to government leaders, Dewey's approach is a more encompassing description of how many journalists see their role in society, and, in turn, how much of society expects journalists to function. Americans, for example, may criticize some of the excesses committed by journalists, but they tend to expect journalists to serve as watchdogs on government, businesses and actors, enabling people to make informed decisions on the issues of the time.

Bill Kovach and Tom Rosenstiel propose several guidelines for journalists in their book The Elements of Journalism. Because journalism's first loyalty is to the citizenry, journalists are obliged to tell the truth and must serve as an independent monitor of powerful individuals and institutions within society. The essence of journalism is to provide citizens with reliable information through the discipline of verification.

While various existing codes have some differences, most share common elements including the principles of — truthfulness, accuracy, objectivity, impartiality, fairness and public accountability — as these apply to the acquisition of newsworthy information and its subsequent dissemination to the public.
Some journalistic Codes of Ethics, notably the European ones, also include a concern with discriminatory references in news based on race, religion, sexual orientation, and physical or mental disabilities. The Parliamentary Assembly of the Council of Europe approved in 1993 Resolution 1003 on the Ethics of Journalism which recommends journalists to respect the presumption of innocence, in particular in cases that are still sub judice.
In the UK, all newspapers are bound by the Code of Practice of the Independent Press Standards Organisation.This includes points like respecting people's privacy and ensuring accuracy. However, the Media Standards Trust has criticized the PCC, claiming it needs to be radically changed to secure public trust of newspapers.
This is in stark contrast to the media climate prior to the 20th century, where the media market was dominated by smaller newspapers and pamphleteers who usually had an overt and often radical agenda, with no presumption of balance or objectivity.
Because of the pressure on journalists to report news promptly and before their competitors, factual errors occur more frequently than in writing produced and edited under less time pressure. Thus a typical issue of a major daily newspaper may contain several corrections of articles published the previous day. Perhaps the most famous journalistic mistake caused by time pressure was the Dewey Defeats Truman edition of the Chicago Daily Tribune, based on early election returns that failed to anticipate the actual result of the 1948 US presidential election.

Such a code of conduct can, in the real world, be difficult to uphold consistently. Reporting and editing do not occur in a vacuum but always reflect the political context in which journalists, no less than other citizens, operate.
A news organization's budget inevitably reflects decision-making about what news to cover, for what audience, and in what depth. When budgets are cut, editors may sacrifice reporters in distant news bureaus, reduce the number of staff assigned to low-income areas, or wipe entire communities from the publication's zone of interest.
Publishers, owners and other corporate executives, especially advertising sales executives, could try to use their powers over journalists to influence how news is reported and published. For this reason, journalists traditionally relied on top management to create and maintain a "firewall" between the news and other departments in a news organization to prevent undue influence on the news department.
Although some analysts point to the inherent difficulty of maintaining objectivity, and others practically deny that it is possible, still others point to the requirements of a free press in a democratic society governed by public opinion and a republican government under a limited constitution. According to this latter view, direct or implicit criticism of the government, political parties, corporations, unions, schools and colleges and even churches is both inevitable and desirable, and cannot be done well without clarity regarding fundamental political principles. Hence, objectivity consists both in truthful, accurate reporting and well-reasoned and thoughtful commentary, based upon a firm commitment to a free society's principles of equality, liberty and government by consent.

Governments have widely varying policies and practices towards journalists, which control what they can research and write, and what press organizations can publish. Some governments guarantee the freedom of the press; while other nations severely restrict what journalists can research or publish.
Journalists in many nations have some privileges that members of the general public do not, including better access to public events, crime scenes and press conferences, and to extended interviews with public officials, celebrities and others in the public eye.
Journalists who elect to cover conflicts, whether wars between nations or insurgencies within nations, often give up any expectation of protection by government, if not giving up their rights to protection by government. Journalists who are captured or detained during a conflict are expected to be treated as civilians and to be released to their national government. Many governments around the world target journalists for intimidation, harassment, and violence because of the nature of their work.

Journalists' interaction with sources sometimes involves confidentiality, an extension of freedom of the press giving journalists a legal protection to keep the identity of a confidential informant private even when demanded by police or prosecutors; withholding their sources can land journalists in contempt of court, or in jail.
In the United States, there is no right to protect sources in a federal court. However, federal courts will refuse to force journalists to reveal their sources, unless the information the court seeks is highly relevant to the case and there's no other way to get it. State courts provide varying degrees of such protection. Journalists who refuse to testify even when ordered to can be found in contempt of court and fined or jailed. On the journalistic side of keeping sources confidential, there is also a risk to the journalist's credibility because there can be no actual confirmation of whether the information is valid. As such it is highly discouraged for journalists to have confidential sources.

American Journalism Review
Columbia Journalism Review
Health News Review
Ryerson Review of Journalism
Online Journalism Review

Harcup, Tony (2009), Journalism: Principles and Practice, Thousand Oaks, California: Sage Publications, ISBN 978-1847872500, OCLC 280437077

Quick, Amanda C. ed. World Press Encyclopedia: A Survey of Press Systems Worldwide (2nd ed. 2 vol 2002); 2500 pp; highly detailed coverage of every country large and small.
de Beer Arnold S. and John C. Merrill, eds. Global Journalism: Topical Issues and Media Systems (5th ed. 2008)
Shoemaker, Pamela J. and Akiba A. Cohen, eds. News Around the World: Content, Practitioners, and the Public (2nd ed. 2005)
Sloan, W. David and Lisa Mullikin Parcell, eds. (2002). American Journalism: History, Principles, Practices. McFarland.  CS1 maint: Multiple names: authors list (link)
Sterling, Christopher H. (ed.), Encyclopedia of journalism, Thousand Oaks, California: SAGE, 2009, 6 vols.

Journalism at DMOZA newspaper is a serial publication containing news about current events, other informative articles (listed below) about politics, sports, arts, and so on, and advertising. A newspaper is usually, but not exclusively, printed on relatively inexpensive, low-grade paper such as newsprint. The journalism organizations that publish newspapers are themselves often metonymically called newspapers. As of 2017, most newspapers are now published online as well as in print. The online versions are called online newspapers or news websites. Newspapers are typically published daily or weekly. News magazines are also weekly, but they have a magazine format. General-interest newspapers typically publish news articles and feature articles on national and international news as well as local news. The news includes political events and personalities, business and finance, crime, weather, and natural disasters; health and medicine, science, and computers and technology; sports; and entertainment, society, food and cooking, clothing and home fashion, and the arts.
Typically the paper is divided into sections for each of those major groupings (labeled A, B, C, and so on, with pagination prefixes yielding page numbers A1-A20, B1-B20, C1-C20, and so on). Most traditional papers also feature an editorial page containing editorials written by an editor (or by the paper's editorial board) and expressing an opinion on a public issue, opinion articles called "op-eds" written by guest writers (which are typically in the same section as the editorial), and columns that express the personal opinions of columnists, usually offering analysis and synthesis that attempts to translate the raw data of the news into information telling the reader "what it all means" and persuading them to concur. Papers also include articles which have no byline; these articles are written by staff writers.
A wide variety of material has been published in newspapers. Besides the aforementioned news, information and opinions, they include weather forecasts; criticism and reviews of the arts (including literature, film, television, theater, fine arts, and architecture) and of local services such as restaurants; obituaries, birth notices and graduation announcements; entertainment features such as crosswords, horoscopes, editorial cartoons, gag cartoons, and comic strips; advice columns, food, and other columns; and radio and television listings (program schedules). As of 2017, newspapers may also provide information about new movies and TV shows available on streaming video services like Netflix. Newspapers have classified ad sections where people and businesses can buy small advertisements to sell goods or services; as of 2017, the huge increase in Internet websites for selling goods, such as Craigslist and eBay has led to significantly less classified ad sales for newspapers.
Most newspapers are businesses, and they pay their expenses with a mixture of subscription revenue, newsstand sales, and advertising revenue (other businesses or individuals pay to place advertisements in the pages, including display ads, classified ads, and their online equivalents). Some newspapers are government-run or at least government-funded; their reliance on advertising revenue and on profitability is less critical to their survival. The editorial independence of a newspaper is thus always subject to the interests of someone, whether owners, advertisers, or a government. Some newspapers with high editorial independence, high journalism quality, and large circulation are viewed as newspapers of record.
Many newspapers, besides employing journalists on their own payrolls, also subscribe to news agencies (wire services) (such as the Associated Press, Reuters, or Agence France-Presse), which employ journalists to find, assemble, and report the news, then sell the content to the various newspapers. This is a way to avoid duplicating the expense of reporting from around the world. Circa 2005, there were approximately 6,580 daily newspaper titles in the world selling 395 million print copies a day (in the U.S., 1,450 titles selling 55 million copies). The late 2000s–early 2010s global recession, combined with the rapid growth of free web-based alternatives, has helped cause a decline in advertising and circulation, as many papers had to retrench operations to stanch the losses. Worldwide annual revenue approached $100 billion in 2005-7, then plunged during the worldwide financial crisis of 2008-9. Revenue in 2016 fell to only $53 billion, hurting every major publisher as their efforts to gain online income fell far short of the goal.
The decline in advertising revenues affected both the print and online media as well as all other mediums; print advertising was once lucrative but has greatly declined, and the prices of online advertising are often lower than those of their print precursors. Besides remodeling advertising, the internet (especially the web) has also challenged the business models of the print-only era by crowdsourcing both publishing in general (sharing information with others) and, more specifically, journalism (the work of finding, assembling, and reporting the news). In addition, the rise of news aggregators, which bundle linked articles from many online newspapers and other sources, influences the flow of web traffic. Increasing paywalling of online newspapers may be counteracting those effects. The oldest newspaper still published is the Gazzetta di Mantova, which was established in Mantua in 1664.

Newspapers typically meet four criteria:
Public accessibility: Its contents are reasonably accessible to the public, traditionally by the paper being sold or distributed at newsstands, shops and libraries, and, since the 1990s, made available over the Internet with online newspaper websites. While online newspapers have increased access to newspapers by people with Internet access, people without Internet or computer access (e.g., homeless people, impoverished people and people living in remote or rural regions may not be able to access the Internet, and thus will not be able to read online news). Literacy is also a factor which prevents people who cannot read from being able to benefit from reading newspapers (paper or online).
Periodicity: They are published at regular intervals, typically daily or weekly. This ensures that newspapers can provide information on newly-emerging news stories or events.
Currency: Its information is as up to date as its publication schedule allows. The degree of up-to-date-ness of a print newspaper is limited by the need of time to print and distribute the newspaper. In major cities, there may be a morning edition and a later edition of the same day's paper, so that the later edition can incorporate breaking news that has occurred since the morning edition was printed. Online newspapers can be updated as frequently as new information becomes available, even a number of times per day, which means that online editions can be very up-to-date.
Universality: Newspapers covers a range of topics, from political and business news to updates on science and technology, arts, culture, and entertainment.

In Ancient Rome, Acta Diurna, or government announcement bulletins, were produced. They were carved in metal or stone and posted in public places. In China, early government-produced news-sheets, called Dibao, circulated among court officials during the late Han dynasty (second and third centuries AD). Between 713 and 734, the Kaiyuan Za Bao ("Bulletin of the Court") of the Chinese Tang Dynasty published government news; it was handwritten on silk and read by government officials. In 1582, there was the first reference to privately published newssheets in Beijing, during the late Ming Dynasty.
In Early modern Europe the increased cross-border interaction created a rising need for information which was met by concise handwritten news-sheets, called avvisi. In 1556, the government of Venice first published the monthly Notizie scritte, which cost one gazetta, a small coin. These avvisi were handwritten newsletters and used to convey political, military, and economic news quickly and efficiently to Italian cities (1500–1700)—sharing some characteristics of newspapers though usually not considered true newspapers. However, none of these publications fully met the classical criteria for proper newspapers, as they were typically not intended for the general public and restricted to a certain range of topics.

The emergence of the new media in the 17th century has to be seen in close connection with the spread of the printing press from which the publishing press derives its name. The German-language Relation aller Fürnemmen und gedenckwürdigen Historien, printed from 1605 onwards by Johann Carolus in Strasbourg, is often recognized as the first newspaper. At the time, Strasbourg was a free imperial city in the Holy Roman Empire of the German Nation; the first newspaper of modern Germany was the Avisa, published in 1609 in Wolfenbüttel.
The Dutch Courante uyt Italien, Duytslandt, &c. ('Courant from Italy, Germany, etc.') of 1618 was the first to appear in folio- rather than quarto-size. Amsterdam, a center of world trade, quickly became home to newspapers in many languages, often before they were published in their own country. The first English-language newspaper, Corrant out of Italy, Germany, etc., was published in Amsterdam in 1620. A year and a half later, Corante, or weekely newes from Italy, Germany, Hungary, Poland, Bohemia, France and the Low Countreys. was published in England by an "N.B." (generally thought to be either Nathaniel Butter or Nicholas Bourne) and Thomas Archer. The first newspaper in France was published in 1631, La Gazette (originally published as Gazette de France). The first newspaper in Portugal, A Gazeta da Restauração, was published in 1641 in Lisbon. The first Spanish newspaper, Gaceta de Madrid, was published in 1661.
Post- och Inrikes Tidningar (founded as Ordinari Post Tijdender) was first published in Sweden in 1645, and is the oldest newspaper still in existence, though it now publishes solely online. Opregte Haarlemsche Courant from Haarlem, first published in 1656, is the oldest paper still printed. It was forced to merge with the newspaper Haarlems Dagblad in 1942 when Germany occupied the Netherlands. Since then the Haarlems Dagblad has appeared with the subtitle Oprechte Haerlemse Courant 1656. Merkuriusz Polski Ordynaryjny was published in Kraków, Poland in 1661. The first successful English daily, The Daily Courant, was published from 1702 to 1735.

In Boston in 1690, Benjamin Harris published Publick Occurrences Both Forreign and Domestick. This is considered the first newspaper in the American colonies even though only one edition was published before the paper was suppressed by the government. In 1704, the governor allowed The Boston News-Letter to be published and it became the first continuously published newspaper in the colonies. Soon after, weekly papers began publishing in New York and Philadelphia. These early newspapers followed the British format and were usually four pages long. They mostly carried news from Britain and content depended on the editor's interests. In 1783, the Pennsylvania Evening Post became the first American daily.
In 1752, John Bushell published the Halifax Gazette, which claims to be "Canada's first newspaper." However, its official descendant, the Royal Gazette, is a government publication for legal notices and proclamations rather than a proper newspaper; In 1764, the Quebec Gazette was first printed 21 June 1764 and remains the oldest continuously published newspaper in North America as the Quebec Chronicle-Telegraph. It is currently published as an English-language weekly from its offices at 1040 Belvédère, suite 218, Quebec City, Quebec, Canada. In 1808, the Gazeta do Rio de Janeiro had his first edition, printed in devices brought from England, publishing news favourable for the government of the United Kingdom of Portugal, Brazil and the Algarves since it was produced by the official press service of the Portuguese crown.
In 1821, after the ending of private newspaper circulation ban, appears the first non-imperial printed publication, Diário do Rio de Janeirothough there it was already the Correio Braziliense, published by Hipólito José da Costa at the same time of the Gazeta, but from London and with a strong political and critical ideas, aiming to show the administration faults (see Portuguese Wikipedia ). The first newspaper in Peru was El Peruano, established in October 1825 and still published today, but with several name changes.

During the Tang Dynasty in China (618–906), the Kai Yuan Za Bao published the government news; it was block printed onto paper. It was the earliest newspaper to be published The first recorded attempt to found a modern-day newspaper in South Asia was by William Bolts, a Dutchman in the employ of the British East India Company in September 1768 in Calcutta. However, before he could begin his newspaper, he was deported back to Europe. A few years later, the first newsprint from this region - Hicky's Bengal Gazette- was published by an Irishman James Augustus Hicky. He used it as a means to criticize the British rule through journalism.

The history of Middle Eastern newspapers goes back to the 19th century. Many editors were not only journalists but also writers, philosophers and politicians. With unofficial journals, these intellectuals encouraged public discourse on politics in the Ottoman and Persian Empires. Literary works of all genres were serialized and published in the press as well.
The first newspapers in the Ottoman Empire were owned by foreigners living there who wanted to make propaganda about the Western world. The earliest was printed in 1795 by the Palais de France in Pera. Indigenous Middle Eastern journalism started in 1828, when Muhammad Ali, Khedive of Egypt, ordered the local establishment of the gazette Vekayi-i Misriye (Egyptian Affairs). It was first paper written in Ottoman Turkish and Arabic on opposite pages, and later in Arabic only, under the title "al-Waqa'i`a al-Masriya".
The first non-official Turkish newspaper, Ceride-i Havadis (Register of Events), was published by an Englishman, William Churchill, in 1840. The first private newspaper to be published by Turkish journalists, Tercüman-ı Ahvâl (Interpreter of Events), was founded by İbrahim Şinasi and Agah Efendi and issued in 1860. The first newspaper in Iran, Kaghaz-e Akhbar (The Newspaper), was created for the government by Mirza Saleh Shirazi in 1837. The first journals in the Arabian Peninsula appeared in Hijaz, once it had become independent of Ottoman rule, towards the end of World War I.One of the earliest women to sign her articles in the Arab press was the female medical practitioner Galila Tamarhan, who contributed articles to a medical magazine called "Ya`asub al-Tib" (Leader in Medicine) in the 1860s.

By the early 19th century, many cities in Europe, as well as North and South America, published newspaper-type publications though not all of them developed in the same way; content was vastly shaped by regional and cultural preferences. Advances in printing technology related to the Industrial Revolution enabled newspapers to become an even more widely circulated means of communication, as new printing technologies made printing less expensive and more efficient. In 1814, The Times (London) acquired a printing press capable of making 1,100 impressions per hour. Soon, this press was adapted to print on both sides of a page at once. This innovation made newspapers cheaper and thus available to a larger part of the population.
In 1830, the first inexpensive "penny press" newspaper came to the market: Lynde M. Walter's Boston Transcript. Penny press papers cost about one sixth the price of other newspapers and appealed to a wider audience, including less educated and lower-income people. In France, Émile de Girardin started "La Presse" in 1836, introducing cheap, advertising-supported dailies to France. In 1848, August Zang, an Austrian who knew Girardin in Paris, returned to Vienna to introduce the same methods with "Die Presse" (which was named for and frankly copied Girardin's publication).

While most newspapers are aimed at a broad spectrum of readers, usually geographically defined, some focus on groups of readers defined more by their interests than their location: for example, there are daily and weekly business newspapers (e.g., The Wall Street Journal) and sports newspapers. More specialist still are some weekly newspapers, usually free and distributed within limited regional areas; these may serve communities as specific as certain immigrant populations, the local gay community or indie rock enthusiasts within a city or region.

A daily newspaper is printed every day, sometimes with the exception of Sundays and occasionally Saturdays, (and some major holidays) and often of some national holidays. Saturday and, where they exist, Sunday editions of daily newspapers tend to be larger, include more specialized sections (e.g., on arts, films, entertainment) and advertising inserts, and cost more. Typically, the majority of these newspapers' staff members work Monday to Friday, so the Sunday and Monday editions largely depend on content done in advance or content that is syndicated. Most daily newspapers are sold in the morning. Afternoon or evening papers, once common but now scarce, are aimed more at commuters and office workers. In practice (though this may vary according to country), a morning newspaper is available in early editions from before midnight on the night before its cover date, further editions being printed and distributed during the night. The later editions can include breaking news which was first revealed that day, after the morning edition was already printed. Previews of tomorrow's newspapers are often a feature of late night news programs, such as Newsnight in the United Kingdom. In 1650, the first daily newspaper appeared, Einkommende Zeitung, published by Timotheus Ritzsch in Leipzig, Germany.
In the UK, unlike most other countries, "daily" newspapers do not publish on Sundays. In the past there were independent Sunday newspapers; nowadays the same publisher often produces a Sunday newspaper, distinct in many ways from the daily, usually with a related name; e.g., The Times and The Sunday Times are distinct newspapers owned by the same company, and an article published in the latter would never be credited to The Times.

Weekly newspapers are published once a week, and tend to be smaller than daily papers. Some newspapers are published two or three times a week and are known as biweekly publications. Some publications are published, for example, fortnightly (or bimonthly in American parlance).

A local newspaper serves a region such as a city, or part of a large city. Almost every market has one or two newspapers that dominate the area. Large metropolitan newspapers often have large distribution networks, and can be found outside their normal area, sometimes widely, sometimes from fewer sources.

Most nations have at least one newspaper that circulates throughout the whole country: a national newspaper. Some national newspapers, such as The Financial Times and The Wall Street Journal, are specialised (in these examples, on financial matters). There are many national newspapers in the UK, but only a few in the United States and Canada. In Canada, The Globe and Mail is sold throughout the country. In the United States, in addition to national newspapers as such, The New York Times is available throughout the country.

There is also a small group of newspapers which may be characterized as international newspapers. Some, such as The International Herald Tribune, have always had that focus, while others are repackaged national newspapers or "international editions" of national or large metropolitan newspapers. In some cases, articles that might not interest the wider range of readers are omitted from international editions; in others, of interest to expatriates, significant national news is retained. As English became the international language of business and technology, many newspapers formerly published only in non-English languages have also developed English-language editions. In places as varied as Jerusalem and Mumbai, newspapers are printed for a local and international English-speaking public, and for tourists. The advent of the Internet has also allowed non-English-language newspapers to put out a scaled-down English version to give their newspaper a global outreach.
Similarly, in many countries with a large foreign-language-speaking population or many tourists, newspapers in languages other than the national language are both published locally and imported. For example, newspapers and magazines from many countries, and locally published newspapers in many languages, are readily to be found on news-stands in central London. In the US state of Florida, so many tourists from the French-speaking Canadian province of Quebec visit for long stays during the winter ("snowbirds") that some newsstands and stores sell French-language newspapers such as Le Droit.

General newspapers cover all topics, with different emphasis. While at least mentioning all topics, some might have good coverage of international events of importance; others might concentrate more on national or local entertainment or sports. Specialised newspapers might concentrate more specifically on, for example, financial matters. There are publications covering exclusively sports, or certain sports, horse-racing, theatre, and so on, although they may no longer be called newspapers.

For centuries newspapers were printed on paper and supplied physically to readers either by local distribution, or in some cases by mail, for example for British expatriates living in India or Hong Kong who subscribed to British newspapers. Newspapers can be delivered to subscribers homes and/or businesses by a paper's own delivery people, sent via the mail, sold at newsstands, grocery stores and convenience stores, and delivered to libraries and bookstores. Newspaper organizations need a large distribution system to deliver their papers to these different distributors, which typically involves delivery trucks and delivery people. In recent years, newspapers and other media have adapted to the changing technology environment by starting to offer online editions to cater to the needs of the public. In the future, the trend towards more electronic delivery of the news will continue with more emphasis on the Internet, social media and other electronic delivery methods. However, while the method of delivery is changing, the newspaper and the industry still has a niche in the world.

As of 2007, virtually all major printed newspapers have online editions distributed over the Internet which, depending on the country may be regulated by journalism organizations such as the Press Complaints Commission in the UK. But as some publishers find their print-based models increasingly unsustainable, Web-based "newspapers" have also started to appear, such as the Southport Reporter in the UK and the Seattle Post-Intelligencer, which stopped publishing in print after 149 years in March 2009 and became an online only paper.
A new trend in newspaper publishing is the introduction of personalization through on-demand printing technologies or with online news aggregator websites like Google news. Customized newspapers allow the reader to create their individual newspaper through the selection of individual pages from multiple publications. This "Best of" approach allows to revive the print-based model and opens up a new distribution channel to increase coverage beneath the usual boundaries of distribution. Customized newspapers online have been offered by MyYahoo, I-Google, CRAYON, ICurrent.com, Kibboko.com, Twitter.times and many others. With these online newspapers, the reader can select how much of each section (politics, sports, arts, etc.) they wish to see in their news.
The newspaper has been a part of our daily life for several centuries. They have been a way for the public to be informed of important events that are occurring around the world. Newspapers have undergone dramatic changes over the course of history. Some of the earliest newspapers date back to Ancient Rome where important announcements were carved in stone tablets and placed in highly populated areas where citizens could be informed of the announcements.

In the United States, the overall manager or chief executive of the newspaper is the publisher. In small newspapers, the owner of the publication (or the largest shareholder in the corporation that owns the publication) is usually the publisher. Although he or she rarely or perhaps never writes stories, the publisher is legally responsible for the contents of the entire newspaper and also runs the business, including hiring editors, reporters, and other staff members. This title is less common outside the U.S. The equivalent position in the film industry and television news shows is the executive producer. Most newspapers have four main departments devoted to publishing the newspaper itself—editorial, production/printing, circulation, and advertising, although they are frequently referred to by a variety of other names—as well as the non-newspaper-specific departments also found in other businesses of comparable size, such as accounting, marketing, human resources, and IT.
Throughout the English-speaking world, the person who selects the content for the newspaper is usually referred to as the editor. Variations on this title such as editor-in-chief, executive editor, and so on are common. For small newspapers, a single editor may be responsible for all content areas. At large newspapers, the most senior editor is in overall charge of the publication, while less senior editors may each focus on one subject area, such as local news or sports. These divisions are called news bureaus or "desks", and each is supervised by a designated editor. Most newspaper editors copy edit the stories for their part of the newspaper, but they may share their workload with proofreaders and fact checkers.

Reporters are journalists who primarily report facts that they have gathered and those who write longer, less news-oriented articles may be called feature writers. Photographers and graphic artists provide images and illustrations to support articles. Journalists often specialize in a subject area, called a beat, such as sports, religion, or science. Columnists are journalists who write regular articles recounting their personal opinions and experiences. Printers and press operators physically print the newspaper. Printing is outsourced by many newspapers, partly because of the cost of an offset web press (the most common kind of press used to print newspapers) and also because a small newspaper's print run might require less than an hour of operation, meaning that if the newspaper had its own press it would sit idle most of the time. If the newspaper offers information online, webmasters and web designers may be employed to upload stories to the newspaper's website.
The staff of the circulation department liaise with retailers who sell the newspaper; sell subscriptions; and supervise distribution of the printed newspapers through the mail, by newspaper carriers, at retailers, and through vending machines. Free newspapers do not sell subscriptions, but they still have a circulation department responsible for distributing the newspapers. Sales staff in the advertising department not only sell ad space to clients such as local businesses, but also help clients design and plan their advertising campaigns. Other members of the advertising department may include graphic designers, who design ads according to the customers' specifications and the department's policies. In an advertising-free newspaper, there is no advertising department.

Newspapers often refine distribution of ads and news through zoning and editioning. Zoning occurs when advertising and editorial content change to reflect the location to which the product is delivered. The editorial content often may change merely to reflect changes in advertising—the quantity and layout of which affects the space available for editorial—or may contain region-specific news. In rare instances, the advertising may not change from one zone to another, but there will be different region-specific editorial content. As the content can vary widely, zoned editions are often produced in parallel. Editioning occurs in the main sections as news is updated throughout the night. The advertising is usually the same in each edition (with the exception of zoned regionals, in which it is often the 'B' section of local news that undergoes advertising changes). As each edition represents the latest news available for the next press run, these editions are produced linearly, with one completed edition being copied and updated for the next edition. The previous edition is always copied to maintain a Newspaper of Record and to fall back on if a quick correction is needed for the press. For example, both The New York Times and The Wall Street Journal offer a regional edition, printed through a local contractor, and featuring locale specific content. The Journal's global advertising rate card provides a good example of editioning.
See also Los Angeles Times suburban sections.

Most modern newspapers are in one of three sizes:
Broadsheets: 600 mm × 380 mm (23½ × 15 inches), generally associated with more intellectual newspapers, although a trend towards "compact" newspapers is changing this. Examples include The Daily Telegraph in the United Kingdom.
Tabloids: half the size of broadsheets at 380 mm × 300 mm (15 × 11¾ inches), and often perceived as sensationalist in contrast to broadsheets. Examples include The Sun, The National Enquirer, The Star Magazine, New York Post, the Chicago Sun-Times, The Princely State, The Globe.
"Microdaily" is infrequently used to refer to a tabloid-sized free daily newspaper that offers lower ad rates than its broadsheet competitors. The content of a microdaily can range from intense local news coverage to a combination of local and national stories.

Berliner or Midi: 470 mm × 315 mm (18½ × 12¼ inches) used by European papers such as Le Monde in France, La Stampa in Italy, El País in Spain and, since 2005, The Guardian in the United Kingdom.
Newspapers are usually printed on cheap, off-white paper known as newsprint. Since the 1980s, the newspaper industry has largely moved away from lower-quality letterpress printing to higher-quality, four-color process, offset printing. In addition, desktop computers, word processing software, graphics software, digital cameras and digital prepress and typesetting technologies have revolutionized the newspaper production process. These technologies have enabled newspapers to publish color photographs and graphics, as well as innovative layouts and better design. To help their titles stand out on newsstands, some newspapers are printed on coloured newsprint. For example, the Financial Times is printed on a distinctive salmon pink paper, and Sheffield's weekly sports publication derives its name, the Green 'Un, from the traditional colour of its paper. The Italian sports newspaper La Gazzetta dello Sport is also printed on pink paper while L'Équipe (formerly L'Auto) is printed on yellow paper. Both the latter promoted major cycling races and their newsprint colours were reflected in the colours of the jerseys used to denote the race leader; for example the leader in the Giro d'Italia wears a pink jersey.

The number of copies distributed, either on an average day or on particular days (typically Sunday), is called the newspaper's circulation and is one of the principal factors used to set advertising rates. Circulation is not necessarily the same as copies sold, since some copies or newspapers are distributed without cost. Readership figures may be higher than circulation figures because many copies are read by more than one person, although this is offset by the number of copies distributed but not read (especially for those distributed free). In the United States, the Alliance for Audited Media maintains historical and current data on average circulation of daily and weekly newspapers and other periodicals.
According to the Guinness Book of Records, the daily circulation of the Soviet newspaper Trud exceeded 21,500,000 in 1990, while the Soviet weekly Argumenty i Fakty boasted a circulation of 33,500,000 in 1991. According to United Nations data from 1995 Japan has three daily papers—the Yomiuri Shimbun, Asahi Shimbun, and Mainichi Shimbun—with circulations well above 5.5 million. Germany's Bild, with a circulation of 3.8 million, was the only other paper in that category. In the United Kingdom, The Sun is the top seller, with around 3.24 million copies distributed daily. In the U.S., The Wall Street Journal has a daily circulation of approximately 2.02 million, making it the most widely distributed paper in the country.
While paid readership of print newspapers has been steadily declining in the developed OECD nations, it has been rising in the chief developing nations (Brazil, India, Indonesia, China and South Africa), whose paid daily circulation exceeded those of the developed nations for the first time in 2008. In India, The Times of India is the largest-circulation English newspaper, with 3.14 million copies daily. According to the 2009 Indian Readership Survey, the Dainik Jagran is the most-read, local-language (Hindi) newspaper, with 55.7 million readers. According to Tom Standage of The Economist, India currently has daily newspaper circulation of 110 million copies.

A common measure of a newspaper's health is market penetration, expressed as a percentage of households that receive a copy of the newspaper against the total number of households in the paper's market area. In the 1920s, on a national basis in the U.S., daily newspapers achieved market penetration of 123 percent (meaning the average U.S. household received 1.23 newspapers). As other media began to compete with newspapers, and as printing became easier and less expensive giving rise to a greater diversity of publications, market penetration began to decline. It wasn't until the early 1970s, however, that market penetration dipped below 100 percent. By 2000, it was 53 percent and still falling. Many paid-for newspapers offer a variety of subscription plans. For example, someone might want only a Sunday paper, or perhaps only Sunday and Saturday, or maybe only a workweek subscription, or perhaps a daily subscription. Most newspapers provide some or all of their content on the Internet, either at no cost or for a fee. In some cases, free access is available only for a matter of days or weeks, or for a certain number of viewed articles, after which readers must register and provide personal data. In other cases, free archives are provided.

A newspaper typically generates 70–80% of its revenue from advertising, and the remainder from sales and subscriptions. The portion of the newspaper that is not advertising is called editorial content, editorial matter, or simply editorial, although the last term is also used to refer specifically to those articles in which the newspaper and its guest writers express their opinions. (This distinction, however, developed over time – early publishers like Girardin (France) and Zang (Austria) did not always distinguish paid items from editorial content.). The business model of having advertising subsidize the cost of printing and distributing newspapers (and, it is always hoped, the making of a profit) rather than having subscribers cover the full cost was first done, it seems, in 1833 by The Sun, a daily paper that was published in New York City. Rather than charging 6 cents per copy, the price of a typical New York daily at the time, they charged 1-cent, and depended on advertising to make up the difference.

Newspapers in countries with easy access to the web have been hurt by the decline of many traditional advertisers. Department stores and supermarkets could be relied upon in the past to buy pages of newspaper advertisements, but due to industry consolidation are much less likely to do so now. Additionally, newspapers are seeing traditional advertisers shift to new media platforms. The classified category is shifting to sites including Craigslist, employment websites, and auto sites. National advertisers are shifting to many types of digital content including websites, rich media platforms, and mobile.
In recent years, the advertorial emerged. Advertorials are most commonly recognized as an opposite-editorial which third parties pay a fee to have included in the paper. Advertorials commonly advertise new products or techniques, such as a new design for golf equipment, a new form of laser surgery, or weight-loss drugs. The tone is usually closer to that of a press release than of an objective news story. Such articles are often clearly distinguished from editorial content through either the design and layout of the page or with a label declaring the article as an advertisement. However, there has been growing concern over the blurring of the line between editorial and advertorial content.

Since newspapers began as a journal (record of current events), the profession involved in the making of newspapers began to be called journalism. In the yellow journalism era of the 19th century, many newspapers in the United States relied on sensational stories that were meant to anger or excite the public, rather than to inform. The restrained style of reporting that relies on fact checking and accuracy regained popularity around World War II. Criticism of journalism is varied and sometimes vehement. Credibility is questioned because of anonymous sources; errors in facts, spelling, and grammar; real or perceived bias; and scandals involving plagiarism and fabrication.
In the past, newspapers have often been owned by so-called press barons, and were used for gaining a political voice. After 1920 most major newspapers became parts of chains run by large media corporations such as Gannett, The McClatchy Company, Hearst Corporation, Cox Enterprises, Landmark Media Enterprises LLC, Morris Communications, The Tribune Company, Hollinger International, News Corporation, Swift Communications, etc. Newspapers have, in the modern world, played an important role in the exercise of freedom of expression. Whistle-blowers, and those who "leak" stories of corruption in political circles often choose to inform newspapers before other mediums of communication, relying on the perceived willingness of newspaper editors to expose the secrets and lies of those who would rather cover them. However, there have been many circumstances of the political autonomy of newspapers being curtailed. Recent research has examined the effects of a newspaper's closing on the reelection of incumbents, voter turnout, and campaign spending.
Opinions of other writers and readers are expressed in the op-ed ("opposite the editorial page") and letters to the editors sections of the paper. Some ways newspapers have tried to improve their credibility are: appointing ombudsmen, developing ethics policies and training, using more stringent corrections policies, communicating their processes and rationale with readers, and asking sources to review articles after publication.

By the late 1990s, the availability of news via 24-hour television channels and then the availability of online journalism posed an ongoing challenge to the business model of most newspapers in developed countries. Paid circulation has declined, while advertising revenue—which makes up the bulk of most newspapers' income—has been shifting from print to the new media (social media websites and news websites), resulting in a general decline in print newspapers' revenues and profits. Many newspapers around the world launched online editions in the 2000s, in an attempt to follow or stay ahead of their audience. One of the big challenges is that a number of online news websites, such as Google news, are free to access. Some online news sites are free, and rely on online advertising; other online news sites have a paywall and require paid subscription for access. However, in the non-developed countries, cheaper printing and distribution, increased literacy, the growing middle class and other factors have more than compensated for the emergence of electronic media and newspapers continue to grow.
On 10 April 1995, The American Reporter became the first daily Internet-based newspaper, with its own paid reporters around the world and all-original content. The editor-in-chief and founder is Joe Shea. The site is owned by 400 journalists. The future of newspapers in countries with high levels of Internet access has been widely debated as the industry has faced down soaring newsprint prices, slumping ad sales, the loss of much classified advertising to Craiglist, eBay and other websites, and precipitous drops in circulation. In the late 1990s the number of newspapers slated for closure, bankruptcy or severe cutbacks has risen—especially in the United States, where the industry has shed a fifth of its journalists since 2001. Revenue has plunged while competition from internet media has squeezed older print publishers.
The debate has become more urgent lately, as the 2008-2009 recession shaved newspapers' profits, and as once-explosive growth in newspaper web revenues has leveled off, forestalling what the industry hoped would become an important source of revenue. At issue is whether the newspaper industry faces a cyclical trough (or dip), or whether new technology has rendered print newspapers obsolete, at least in their traditional paper format. As of 2017, an increasing percentage of Millennials (young adults) get their news from social media websites such as Facebook. In the 2010s, many traditional newspapers have begun offering "digital editions", which can be accessed via desktop computer, laptops, and mobile devices such as tablet computers and smartphones. Online newspapers may offer new advertising opportunities to newspaper companies, as online advertising enables much more precise targeting of ads; with an online newspaper, for example, different readers, such as Baby boomers and Millennials can be sent different advertisements.

List of newspaper comic strips
Lists of newspapers

Willings Press Guide (134th ed. 3 vol. 2010), comprehensive guide to world press. Vol 1 UK, Vol 2 Europe and Vol 3 World. ISBN 1-906035-17-2
Editor and Publisher International Year Book (90th ed. 2009), comprehensive guide to American newspapers
Kevin G. Barnhurst, and John Nerone. The Form of News, A History (2001) excerpt and text search
Conley, David, and Stephen Lamble. The Daily Miracle: An Introduction to Journalism (3rd ed. 2006), 518pp; global viewpoint
Harrower, Tim. The Newspaper Designer's Handbook (6th ed. 2007) excerpt and text search
Jones, Alex. Losing the News: The Future of the News That Feeds Democracy (2009)
Sousa, Jorge Pedro Sousa (Coord.); Maria do Carmo Castelo Branco; Mário Pinto; Sandra Tuna; Gabriel Silva; Eduardo Zilles Borba; Mônica Delicato; Carlos Duarte; Nair Silva; Patrícia Teixeira. A Gazeta "da Restauração": Primeiro Periódico Português. Uma análise do discurso VOL. II — Reproduções(2011) ISBN 978-989-654-061-6
Walravens, Hartmut, ed. Newspapers in Central And Eastern Europe (2004) 251pp
Williams, Kevin. Read All About It!: A History of the British Newspaper (2009) excerpt and text search

 "Newspaper". The New Student's Reference Work. 1914.
NewsTornado – Worldwide Newspaper Circulation Map
Print Culture at A History of Central Florida Podcast
Chart – Real and Fake News (2016)/Vanessa Otero (basis) (Mark Frauenfelder)
Chart – Real and Fake News (2014) (2016)/Pew Research Center

Wikipedia List
Newspapercat – University of Florida Historical Digital Newspaper Catalog Collection
Historical newspapers from 1700s–Present : Newspapers.com
Historical newspaper database, from NewspaperARCHIVE.com
More than 8m pages of Historic European newspapers Free
Chronicling America: Historic American Newspapers from National Digital Newspaper Program.
Tairiku Nippō – A Japanese-Canadian newspaper published between 1907 and 1941, and now digitized by the UBC Library Digital Collections
All Daily Newspapers – All Daily Newspapers from around the world.
Plusnoticias – South American journals.
diarios de argentina – South America Newspapers.A programming language is a formal computer language designed to communicate instructions to a machine, particularly a computer. Programming languages can be used to create programs to control the behavior of a machine or to express algorithms.
The earliest known programmable machine preceded the invention of the digital computer and is the automatic flute player described in the 9th century by the brothers Musa in Baghdad, "during the Islamic Golden Age". From the early 1800s, "programs" were used to direct the behavior of machines such as Jacquard looms and player pianos. Thousands of different programming languages have been created, mainly in the computer field, and many more still are being created every year. Many programming languages require computation to be specified in an imperative form (i.e., as a sequence of operations to perform) while other languages use other forms of program specification such as the declarative form (i.e. the desired result is specified, not how to achieve it).
The description of a programming language is usually split into the two components of syntax (form) and semantics (meaning). Some languages are defined by a specification document (for example, the C programming language is specified by an ISO Standard) while other languages (such as Perl) have a dominant implementation that is treated as a reference. Some languages have both, with the basic language defined by a standard and extensions taken from the dominant implementation being common.

A programming language is a notation for writing programs, which are specifications of a computation or algorithm. Some, but not all, authors restrict the term "programming language" to those languages that can express all possible algorithms. Traits often considered important for what constitutes a programming language include:
Function and target
A computer programming language is a language used to write computer programs, which involve a computer performing some kind of computation or algorithm and possibly control external devices such as printers, disk drives, robots, and so on. For example, PostScript programs are frequently created by another program to control a computer printer or display. More generally, a programming language may describe computation on some, possibly abstract, machine. It is generally accepted that a complete specification for a programming language includes a description, possibly idealized, of a machine or processor for that language. In most practical contexts, a programming language involves a computer; consequently, programming languages are usually defined and studied this way. Programming languages differ from natural languages in that natural languages are only used for interaction between people, while programming languages also allow humans to communicate instructions to machines.
Abstractions
Programming languages usually contain abstractions for defining and manipulating data structures or controlling the flow of execution. The practical necessity that a programming language support adequate abstractions is expressed by the abstraction principle; this principle is sometimes formulated as a recommendation to the programmer to make proper use of such abstractions.
Expressive power
The theory of computation classifies languages by the computations they are capable of expressing. All Turing complete languages can implement the same set of algorithms. ANSI/ISO SQL-92 and Charity are examples of languages that are not Turing complete, yet often called programming languages.
Markup languages like XML, HTML, or troff, which define structured data, are not usually considered programming languages. Programming languages may, however, share the syntax with markup languages if a computational semantics is defined. XSLT, for example, is a Turing complete XML dialect. Moreover, LaTeX, which is mostly used for structuring documents, also contains a Turing complete subset.
The term computer language is sometimes used interchangeably with programming language. However, the usage of both terms varies among authors, including the exact scope of each. One usage describes programming languages as a subset of computer languages. In this vein, languages used in computing that have a different goal than expressing computer programs are generically designated computer languages. For instance, markup languages are sometimes referred to as computer languages to emphasize that they are not meant to be used for programming.
Another usage regards programming languages as theoretical constructs for programming abstract machines, and computer languages as the subset thereof that runs on physical computers, which have finite hardware resources. John C. Reynolds emphasizes that formal specification languages are just as much programming languages as are the languages intended for execution. He also argues that textual and even graphical input formats that affect the behavior of a computer are programming languages, despite the fact they are commonly not Turing-complete, and remarks that ignorance of programming language concepts is the reason for many flaws in input formats.

The earliest computers were often programmed without the help of a programming language, by writing programs in absolute machine language. The programs, in decimal or binary form, were read in from punched cards or magnetic tape or toggled in on switches on the front panel of the computer. Absolute machine languages were later termed first-generation programming languages (1GL).
The next step was development of so-called second-generation programming languages (2GL) or assembly languages, which were still closely tied to the instruction set architecture of the specific computer. These served to make the program much more human-readable and relieved the programmer of tedious and error-prone address calculations.
The first high-level programming languages, or third-generation programming languages (3GL), were written in the 1950s. An early high-level programming language to be designed for a computer was Plankalkül, developed for the German Z3 by Konrad Zuse between 1943 and 1945. However, it was not implemented until 1998 and 2000.
John Mauchly's Short Code, proposed in 1949, was one of the first high-level languages ever developed for an electronic computer. Unlike machine code, Short Code statements represented mathematical expressions in understandable form. However, the program had to be translated into machine code every time it ran, making the process much slower than running the equivalent machine code.

At the University of Manchester, Alick Glennie developed Autocode in the early 1950s. A programming language, it used a compiler to automatically convert the language into machine code. The first code and compiler was developed in 1952 for the Mark 1 computer at the University of Manchester and is considered to be the first compiled high-level programming language.
The second autocode was developed for the Mark 1 by R. A. Brooker in 1954 and was called the "Mark 1 Autocode". Brooker also developed an autocode for the Ferranti Mercury in the 1950s in conjunction with the University of Manchester. The version for the EDSAC 2 was devised by D. F. Hartley of University of Cambridge Mathematical Laboratory in 1961. Known as EDSAC 2 Autocode, it was a straight development from Mercury Autocode adapted for local circumstances and was noted for its object code optimisation and source-language diagnostics which were advanced for the time. A contemporary but separate thread of development, Atlas Autocode was developed for the University of Manchester Atlas 1 machine.
In 1954, FORTRAN was invented at IBM by John Backus. It was the first widely used high-level general purpose programming language to have a functional implementation, as opposed to just a design on paper. It is still popular language for high-performance computing and is used for programs that benchmark and rank the world's fastest supercomputers.
Another early programming language was devised by Grace Hopper in the US, called FLOW-MATIC. It was developed for the UNIVAC I at Remington Rand during the period from 1955 until 1959. Hopper found that business data processing customers were uncomfortable with mathematical notation, and in early 1955, she and her team wrote a specification for an English programming language and implemented a prototype. The FLOW-MATIC compiler became publicly available in early 1958 and was substantially complete in 1959. Flow-Matic was a major influence in the design of COBOL, since only it and its direct descendant AIMACO were in actual use at the time.

The increased use of high-level languages introduced a requirement for low-level programming languages or system programming languages. These languages, to varying degrees, provide facilities between assembly languages and high-level languages and can be used to perform tasks which require direct access to hardware facilities but still provide higher-level control structures and error-checking.
The period from the 1960s to the late 1970s brought the development of the major language paradigms now in use:
APL introduced array programming and influenced functional programming.
ALGOL refined both structured procedural programming and the discipline of language specification; the "Revised Report on the Algorithmic Language ALGOL 60" became a model for how later language specifications were written.
Lisp, implemented in 1958, was the first dynamically typed functional programming language
In the 1960s, Simula was the first language designed to support object-oriented programming; in the mid-1970s, Smalltalk followed with the first "purely" object-oriented language.
C was developed between 1969 and 1973 as a system programming language for the Unix operating system and remains popular.
Prolog, designed in 1972, was the first logic programming language.
In 1978, ML built a polymorphic type system on top of Lisp, pioneering statically typed functional programming languages.
Each of these languages spawned descendants, and most modern programming languages count at least one of them in their ancestry.
The 1960s and 1970s also saw considerable debate over the merits of structured programming, and whether programming languages should be designed to support it. Edsger Dijkstra, in a famous 1968 letter published in the Communications of the ACM, argued that GOTO statements should be eliminated from all "higher level" programming languages.

The 1980s were years of relative consolidation. C++ combined object-oriented and systems programming. The United States government standardized Ada, a systems programming language derived from Pascal and intended for use by defense contractors. In Japan and elsewhere, vast sums were spent investigating so-called "fifth generation" languages that incorporated logic programming constructs. The functional languages community moved to standardize ML and Lisp. Rather than inventing new paradigms, all of these movements elaborated upon the ideas invented in the previous decades.
One important trend in language design for programming large-scale systems during the 1980s was an increased focus on the use of modules or large-scale organizational units of code. Modula-2, Ada, and ML all developed notable module systems in the 1980s, which were often wedded to generic programming constructs.
The rapid growth of the Internet in the mid-1990s created opportunities for new languages. Perl, originally a Unix scripting tool first released in 1987, became common in dynamic websites. Java came to be used for server-side programming, and bytecode virtual machines became popular again in commercial settings with their promise of "Write once, run anywhere" (UCSD Pascal had been popular for a time in the early 1980s). These developments were not fundamentally novel, rather they were refinements of many existing languages and paradigms (although their syntax was often based on the C family of programming languages).
Programming language evolution continues, in both industry and research. Current directions include security and reliability verification, new kinds of modularity (mixins, delegates, aspects), and database integration such as Microsoft's LINQ.
Fourth-generation programming languages (4GL) are a computer programming languages which aim to provide a higher level of abstraction of the internal computer hardware details than 3GLs. Fifth generation programming languages (5GL) are programming languages based on solving problems using constraints given to the program, rather than using an algorithm written by a programmer.

All programming languages have some primitive building blocks for the description of data and the processes or transformations applied to them (like the addition of two numbers or the selection of an item from a collection). These primitives are defined by syntactic and semantic rules which describe their structure and meaning respectively.

A programming language's surface form is known as its syntax. Most programming languages are purely textual; they use sequences of text including words, numbers, and punctuation, much like written natural languages. On the other hand, there are some programming languages which are more graphical in nature, using visual relationships between symbols to specify a program.
The syntax of a language describes the possible combinations of symbols that form a syntactically correct program. The meaning given to a combination of symbols is handled by semantics (either formal or hard-coded in a reference implementation). Since most languages are textual, this article discusses textual syntax.
Programming language syntax is usually defined using a combination of regular expressions (for lexical structure) and Backus–Naur form (for grammatical structure). Below is a simple grammar, based on Lisp:

This grammar specifies the following:
an expression is either an atom or a list;
an atom is either a number or a symbol;
a number is an unbroken sequence of one or more decimal digits, optionally preceded by a plus or minus sign;
a symbol is a letter followed by zero or more of any characters (excluding whitespace); and
a list is a matched pair of parentheses, with zero or more expressions inside it.
The following are examples of well-formed token sequences in this grammar: 12345, () and (a b c232 (1)).
Not all syntactically correct programs are semantically correct. Many syntactically correct programs are nonetheless ill-formed, per the language's rules; and may (depending on the language specification and the soundness of the implementation) result in an error on translation or execution. In some cases, such programs may exhibit undefined behavior. Even when a program is well-defined within a language, it may still have a meaning that is not intended by the person who wrote it.
Using natural language as an example, it may not be possible to assign a meaning to a grammatically correct sentence or the sentence may be false:
"Colorless green ideas sleep furiously." is grammatically well-formed but has no generally accepted meaning.
"John is a married bachelor." is grammatically well-formed but expresses a meaning that cannot be true.
The following C language fragment is syntactically correct, but performs operations that are not semantically defined (the operation *p >> 4 has no meaning for a value having a complex type and p->im is not defined because the value of p is the null pointer):

If the type declaration on the first line were omitted, the program would trigger an error on compilation, as the variable "p" would not be defined. But the program would still be syntactically correct since type declarations provide only semantic information.
The grammar needed to specify a programming language can be classified by its position in the Chomsky hierarchy. The syntax of most programming languages can be specified using a Type-2 grammar, i.e., they are context-free grammars. Some languages, including Perl and Lisp, contain constructs that allow execution during the parsing phase. Languages that have constructs that allow the programmer to alter the behavior of the parser make syntax analysis an undecidable problem, and generally blur the distinction between parsing and execution. In contrast to Lisp's macro system and Perl's BEGIN blocks, which may contain general computations, C macros are merely string replacements and do not require code execution.

The term semantics refers to the meaning of languages, as opposed to their form (syntax).

The static semantics defines restrictions on the structure of valid texts that are hard or impossible to express in standard syntactic formalisms. For compiled languages, static semantics essentially include those semantic rules that can be checked at compile time. Examples include checking that every identifier is declared before it is used (in languages that require such declarations) or that the labels on the arms of a case statement are distinct. Many important restrictions of this type, like checking that identifiers are used in the appropriate context (e.g. not adding an integer to a function name), or that subroutine calls have the appropriate number and type of arguments, can be enforced by defining them as rules in a logic called a type system. Other forms of static analyses like data flow analysis may also be part of static semantics. Newer programming languages like Java and C# have definite assignment analysis, a form of data flow analysis, as part of their static semantics.

Once data has been specified, the machine must be instructed to perform operations on the data. For example, the semantics may define the strategy by which expressions are evaluated to values, or the manner in which control structures conditionally execute statements. The dynamic semantics (also known as execution semantics) of a language defines how and when the various constructs of a language should produce a program behavior. There are many ways of defining execution semantics. Natural language is often used to specify the execution semantics of languages commonly used in practice. A significant amount of academic research went into formal semantics of programming languages, which allow execution semantics to be specified in a formal manner. Results from this field of research have seen limited application to programming language design and implementation outside academia.

A type system defines how a programming language classifies values and expressions into types, how it can manipulate those types and how they interact. The goal of a type system is to verify and usually enforce a certain level of correctness in programs written in that language by detecting certain incorrect operations. Any decidable type system involves a trade-off: while it rejects many incorrect programs, it can also prohibit some correct, albeit unusual programs. In order to bypass this downside, a number of languages have type loopholes, usually unchecked casts that may be used by the programmer to explicitly allow a normally disallowed operation between different types. In most typed languages, the type system is used only to type check programs, but a number of languages, usually functional ones, infer types, relieving the programmer from the need to write type annotations. The formal design and study of type systems is known as type theory.

A language is typed if the specification of every operation defines types of data to which the operation is applicable, with the implication that it is not applicable to other types. For example, the data represented by "this text between the quotes" is a string, and in many programming languages dividing a number by a string has no meaning and will be rejected by the compilers. The invalid operation may be detected when the program is compiled ("static" type checking) and will be rejected by the compiler with a compilation error message, or it may be detected when the program is run ("dynamic" type checking), resulting in a run-time exception. Many languages allow a function called an exception handler to be written to handle this exception and, for example, always return "-1" as the result.
A special case of typed languages are the single-type languages. These are often scripting or markup languages, such as REXX or SGML, and have only one data type—most commonly character strings which are used for both symbolic and numeric data.
In contrast, an untyped language, such as most assembly languages, allows any operation to be performed on any data, which are generally considered to be sequences of bits of various lengths. High-level languages which are untyped include BCPL, Tcl, and some varieties of Forth.
In practice, while few languages are considered typed from the point of view of type theory (verifying or rejecting all operations), most modern languages offer a degree of typing. Many production languages provide means to bypass or subvert the type system, trading type-safety for finer control over the program's execution (see casting).

In static typing, all expressions have their types determined prior to when the program is executed, typically at compile-time. For example, 1 and (2+2) are integer expressions; they cannot be passed to a function that expects a string, or stored in a variable that is defined to hold dates.
Statically typed languages can be either manifestly typed or type-inferred. In the first case, the programmer must explicitly write types at certain textual positions (for example, at variable declarations). In the second case, the compiler infers the types of expressions and declarations based on context. Most mainstream statically typed languages, such as C++, C# and Java, are manifestly typed. Complete type inference has traditionally been associated with less mainstream languages, such as Haskell and ML. However, many manifestly typed languages support partial type inference; for example, Java and C# both infer types in certain limited cases. Additionally, some programming languages allow for some types to be automatically converted to other types; for example, an int can be used where the program expects a float.
Dynamic typing, also called latent typing, determines the type-safety of operations at run time; in other words, types are associated with run-time values rather than textual expressions. As with type-inferred languages, dynamically typed languages do not require the programmer to write explicit type annotations on expressions. Among other things, this may permit a single variable to refer to values of different types at different points in the program execution. However, type errors cannot be automatically detected until a piece of code is actually executed, potentially making debugging more difficult. Lisp, Smalltalk, Perl, Python, JavaScript, and Ruby are dynamically typed.

Weak typing allows a value of one type to be treated as another, for example treating a string as a number. This can occasionally be useful, but it can also allow some kinds of program faults to go undetected at compile time and even at run time.
Strong typing prevents the above. An attempt to perform an operation on the wrong type of value raises an error. Strongly typed languages are often termed type-safe or safe.
An alternative definition for "weakly typed" refers to languages, such as Perl and JavaScript, which permit a large number of implicit type conversions. In JavaScript, for example, the expression 2 * x implicitly converts x to a number, and this conversion succeeds even if x is null, undefined, an Array, or a string of letters. Such implicit conversions are often useful, but they can mask programming errors. Strong and static are now generally considered orthogonal concepts, but usage in the literature differs. Some use the term strongly typed to mean strongly, statically typed, or, even more confusingly, to mean simply statically typed. Thus C has been called both strongly typed and weakly, statically typed.
It may seem odd to some professional programmers that C could be "weakly, statically typed". However, notice that the use of the generic pointer, the void* pointer, does allow for casting of pointers to other pointers without needing to do an explicit cast. This is extremely similar to somehow casting an array of bytes to any kind of datatype in C without using an explicit cast, such as (int) or (char).

Most programming languages have an associated core library (sometimes known as the 'standard library', especially if it is included as part of the published language standard), which is conventionally made available by all implementations of the language. Core libraries typically include definitions for commonly used algorithms, data structures, and mechanisms for input and output.
The line between a language and its core library differs from language to language. In some cases, the language designers may treat the library as a separate entity from the language. However, a language's core library is often treated as part of the language by its users, and some language specifications even require that this library be made available in all implementations. Indeed, some languages are designed so that the meanings of certain syntactic constructs cannot even be described without referring to the core library. For example, in Java, a string literal is defined as an instance of the java.lang.String class; similarly, in Smalltalk, an anonymous function expression (a "block") constructs an instance of the library's BlockContext class. Conversely, Scheme contains multiple coherent subsets that suffice to construct the rest of the language as library macros, and so the language designers do not even bother to say which portions of the language must be implemented as language constructs, and which must be implemented as parts of a library.

Programming languages share properties with natural languages related to their purpose as vehicles for communication, having a syntactic form separate from its semantics, and showing language families of related languages branching one from another. But as artificial constructs, they also differ in fundamental ways from languages that have evolved through usage. A significant difference is that a programming language can be fully described and studied in its entirety, since it has a precise and finite definition. By contrast, natural languages have changing meanings given by their users in different communities. While constructed languages are also artificial languages designed from the ground up with a specific purpose, they lack the precise and complete semantic definition that a programming language has.
Many programming languages have been designed from scratch, altered to meet new needs, and combined with other languages. Many have eventually fallen into disuse. Although there have been attempts to design one "universal" programming language that serves all purposes, all of them have failed to be generally accepted as filling this role. The need for diverse programming languages arises from the diversity of contexts in which languages are used:
Programs range from tiny scripts written by individual hobbyists to huge systems written by hundreds of programmers.
Programmers range in expertise from novices who need simplicity above all else, to experts who may be comfortable with considerable complexity.
Programs must balance speed, size, and simplicity on systems ranging from microcontrollers to supercomputers.
Programs may be written once and not change for generations, or they may undergo continual modification.
Programmers may simply differ in their tastes: they may be accustomed to discussing problems and expressing them in a particular language.
One common trend in the development of programming languages has been to add more ability to solve problems using a higher level of abstraction. The earliest programming languages were tied very closely to the underlying hardware of the computer. As new programming languages have developed, features have been added that let programmers express ideas that are more remote from simple translation into underlying hardware instructions. Because programmers are less tied to the complexity of the computer, their programs can do more computing with less effort from the programmer. This lets them write more functionality per time unit.
 Natural language programming has been proposed as a way to eliminate the need for a specialized language for programming. However, this goal remains distant and its benefits are open to debate. Edsger W. Dijkstra took the position that the use of a formal language is essential to prevent the introduction of meaningless constructs, and dismissed natural language programming as "foolish". Alan Perlis was similarly dismissive of the idea. Hybrid approaches have been taken in Structured English and SQL.
A language's designers and users must construct a number of artifacts that govern and enable the practice of programming. The most important of these artifacts are the language specification and implementation.

The specification of a programming language is an artifact that the language users and the implementors can use to agree upon whether a piece of source code is a valid program in that language, and if so what its behavior shall be.
A programming language specification can take several forms, including the following:
An explicit definition of the syntax, static semantics, and execution semantics of the language. While syntax is commonly specified using a formal grammar, semantic definitions may be written in natural language (e.g., as in the C language), or a formal semantics (e.g., as in Standard ML and Scheme specifications).
A description of the behavior of a translator for the language (e.g., the C++ and Fortran specifications). The syntax and semantics of the language have to be inferred from this description, which may be written in natural or a formal language.
A reference or model implementation, sometimes written in the language being specified (e.g., Prolog or ANSI REXX). The syntax and semantics of the language are explicit in the behavior of the reference implementation.

An implementation of a programming language provides a way to write programs in that language and execute them on one or more configurations of hardware and software. There are, broadly, two approaches to programming language implementation: compilation and interpretation. It is generally possible to implement a language using either technique.
The output of a compiler may be executed by hardware or a program called an interpreter. In some implementations that make use of the interpreter approach there is no distinct boundary between compiling and interpreting. For instance, some implementations of BASIC compile and then execute the source a line at a time.
Programs that are executed directly on the hardware usually run several orders of magnitude faster than those that are interpreted in software.
One technique for improving the performance of interpreted programs is just-in-time compilation. Here the virtual machine, just before execution, translates the blocks of bytecode which are going to be used to machine code, for direct execution on the hardware.

Although most of the most commonly used programming languages have fully open specifications and implementations, many programming languages exist only as proprietary programming languages with the implementation available only from a single vendor, which may claim that such a proprietary language is their intellectual property. Proprietary programming languages are commonly domain specific languages or internal scripting languages for a single product; some proprietary languages are used only internally within a vendor, while others are available to external users.
Some programming languages exist on the border between proprietary and open; for example, Oracle Corporation asserts proprietary rights to some aspects of the Java programming language, and Microsoft's C# programming language, which has open implementations of most parts of the system, also has Common Language Runtime (CLR) as a closed environment.
Many proprietary languages are widely used, in spite of their proprietary nature; examples include MATLAB and VBScript. Some languages may make the transition from closed to open; for example, Erlang was originally an Ericsson's internal programming language.

Thousands of different programming languages have been created, mainly in the computing field. Software is commonly built with 5 programming languages or more.
Programming languages differ from most other forms of human expression in that they require a greater degree of precision and completeness. When using a natural language to communicate with other people, human authors and speakers can be ambiguous and make small errors, and still expect their intent to be understood. However, figuratively speaking, computers "do exactly what they are told to do", and cannot "understand" what code the programmer intended to write. The combination of the language definition, a program, and the program's inputs must fully specify the external behavior that occurs when the program is executed, within the domain of control of that program. On the other hand, ideas about an algorithm can be communicated to humans without the precision required for execution by using pseudocode, which interleaves natural language with code written in a programming language.
A programming language provides a structured mechanism for defining pieces of data, and the operations or transformations that may be carried out automatically on that data. A programmer uses the abstractions present in the language to represent the concepts involved in a computation. These concepts are represented as a collection of the simplest elements available (called primitives). Programming is the process by which programmers combine these primitives to compose new programs, or adapt existing ones to new uses or a changing environment.
Programs for a computer might be executed in a batch process without human interaction, or a user might type commands in an interactive session of an interpreter. In this case the "commands" are simply programs, whose execution is chained together. When a language can run its commands through an interpreter (such as a Unix shell or other command-line interface), without compiling, it is called a scripting language.

It is difficult to determine which programming languages are most widely used, and what usage means varies by context. One language may occupy the greater number of programmer hours, a different one have more lines of code, and a third may consume the most CPU time. Some languages are very popular for particular kinds of applications. For example, COBOL is still strong in the corporate data center, often on large mainframes; Fortran in scientific and engineering applications; Ada in aerospace, transportation, military, real-time and embedded applications; and C in embedded applications and operating systems. Other languages are regularly used to write many different kinds of applications.
Various methods of measuring language popularity, each subject to a different bias over what is measured, have been proposed:
counting the number of job advertisements that mention the language
the number of books sold that teach or describe the language
estimates of the number of existing lines of code written in the language –  which may underestimate languages not often found in public searches
counts of language references (i.e., to the name of the language) found using a web search engine.
Combining and averaging information from various internet sites, langpop.com claims that in 2013 the ten most popular programming languages are (in descending order by overall popularity): C, Java, PHP, JavaScript, C++, Python, Shell, Ruby, Objective-C and C#.

There is no overarching classification scheme for programming languages. A given programming language does not usually have a single ancestor language. Languages commonly arise by combining the elements of several predecessor languages with new ideas in circulation at the time. Ideas that originate in one language will diffuse throughout a family of related languages, and then leap suddenly across familial gaps to appear in an entirely different family.
The task is further complicated by the fact that languages can be classified along multiple axes. For example, Java is both an object-oriented language (because it encourages object-oriented organization) and a concurrent language (because it contains built-in constructs for running multiple threads in parallel). Python is an object-oriented scripting language.
In broad strokes, programming languages divide into programming paradigms and a classification by intended domain of use, with general-purpose programming languages distinguished from domain-specific programming languages. Traditionally, programming languages have been regarded as describing computation in terms of imperative sentences, i.e. issuing commands. These are generally called imperative programming languages. A great deal of research in programming languages has been aimed at blurring the distinction between a program as a set of instructions and a program as an assertion about the desired answer, which is the main feature of declarative programming. More refined paradigms include procedural programming, object-oriented programming, functional programming, and logic programming; some languages are hybrids of paradigms or multi-paradigmatic. An assembly language is not so much a paradigm as a direct model of an underlying machine architecture. By purpose, programming languages might be considered general purpose, system programming languages, scripting languages, domain-specific languages, or concurrent/distributed languages (or a combination of these). Some general purpose languages were designed largely with educational goals.
A programming language may also be classified by factors unrelated to programming paradigm. For instance, most programming languages use English language keywords, while a minority do not. Other languages may be classified as being deliberately esoteric or not.

In software engineering, performance testing is in general, a testing practice performed to determine how a system performs in terms of responsiveness and stability under a particular workload. It can also serve to investigate, measure, validate or verify other quality attributes of the system, such as scalability, reliability and resource usage.
Performance testing, a subset of performance engineering, is a computer science practice which strives to build performance standards into the implementation, design and architecture of a system.

Load testing is the simplest form of performance testing. A load test is usually conducted to understand the behaviour of the system under a specific expected load. This load can be the expected concurrent number of users on the application performing a specific number of transactions within the set duration. This test will give out the response times of all the important business critical transactions. The database, application server, etc. are also monitored during the test, this will assist in identifying bottlenecks in the application software and the hardware that the software is installed on.

Stress testing is normally used to understand the upper limits of capacity within the system. This kind of test is done to determine the system's robustness in terms of extreme load and helps application administrators to determine if the system will perform sufficiently if the current load goes well above the expected maximum.

Soak testing, also known as endurance testing, is usually done to determine if the system can sustain the continuous expected load. During soak tests, memory utilization is monitored to detect potential leaks. Also important, but often overlooked is performance degradation, i.e. to ensure that the throughput and/or response times after some long period of sustained activity are as good as or better than at the beginning of the test. It essentially involves applying a significant load to a system for an extended, significant period of time. The goal is to discover how the system behaves under sustained use.

Spike testing is done by suddenly increasing or decreasing the load generated by a very large number of users, and observing the behaviour of the system. The goal is to determine whether performance will suffer, the system will fail, or it will be able to handle dramatic changes in load.

Rather than testing for performance from a load perspective, tests are created to determine the effects of configuration changes to the system's components on the system's performance and behaviour. A common example would be experimenting with different methods of load-balancing.

Isolation testing is not unique to performance testing but involves repeating a test execution that resulted in a system problem. Such testing can often isolate and confirm the fault domain.

Performance testing can serve different purposes:
It can demonstrate that the system meets performance criteria.
It can compare two systems to find which performs better.
It can measure which parts of the system or workload cause the system to perform badly.
Many performance tests are undertaken without setting sufficiently realistic, goal-oriented performance goals. The first question from a business perspective should always be, "why are we performance-testing?". These considerations are part of the business case of the testing. Performance goals will differ depending on the system's technology and purpose, but should always include some of the following:

If a system identifies end-users by some form of log-in procedure then a concurrency goal is highly desirable. By definition this is the largest number of concurrent system users that the system is expected to support at any given moment. The work-flow of a scripted transaction may impact true concurrency especially if the iterative part contains the log-in and log-out activity.
If the system has no concept of end-users, then performance goal is likely to be based on a maximum throughput or transaction rate. A common example would be casual browsing of a web site such as Wikipedia.

This refers to the time taken for one system node to respond to the request of another. A simple example would be a HTTP 'GET' request from browser client to web server. In terms of response time this is what all load testing tools actually measure. It may be relevant to set server response time goals between all nodes of the system.

Load-testing tools have difficulty measuring render-response time, since they generally have no concept of what happens within a node apart from recognizing a period of time where there is no activity 'on the wire'. To measure render response time, it is generally necessary to include functional test scripts as part of the performance test scenario. Many load testing tools do not offer this feature.

It is critical to detail performance specifications (requirements) and document them in any performance test plan. Ideally, this is done during the requirements development phase of any system development project, prior to any design effort. See Performance Engineering for more details.
However, performance testing is frequently not performed against a specification; e.g., no one will have expressed what the maximum acceptable response time for a given population of users should be. Performance testing is frequently used as part of the process of performance profile tuning. The idea is to identify the “weakest link” – there is inevitably a part of the system which, if it is made to respond faster, will result in the overall system running faster. It is sometimes a difficult task to identify which part of the system represents this critical path, and some test tools include (or can have add-ons that provide) instrumentation that runs on the server (agents) and reports transaction times, database access times, network overhead, and other server monitors, which can be analyzed together with the raw performance statistics. Without such instrumentation one might have to have someone crouched over Windows Task Manager at the server to see how much CPU load the performance tests are generating (assuming a Windows system is under test).
Performance testing can be performed across the web, and even done in different parts of the country, since it is known that the response times of the internet itself vary regionally. It can also be done in-house, although routers would then need to be configured to introduce the lag that would typically occur on public networks. Loads should be introduced to the system from realistic points. For example, if 50% of a system's user base will be accessing the system via a 56K modem connection and the other half over a T1, then the load injectors (computers that simulate real users) should either inject load over the same mix of connections (ideal) or simulate the network latency of such connections, following the same user profile.
It is always helpful to have a statement of the likely peak number of users that might be expected to use the system at peak times. If there can also be a statement of what constitutes the maximum allowable 95 percentile response time, then an injector configuration could be used to test whether the proposed system met that specification.

Performance specifications should ask the following questions, at a minimum:
In detail, what is the performance test scope? What subsystems, interfaces, components, etc. are in and out of scope for this test?
For the user interfaces (UIs) involved, how many concurrent users are expected for each (specify peak vs. nominal)?
What does the target system (hardware) look like (specify all server and network appliance configurations)?
What is the Application Workload Mix of each system component? (for example: 20% log-in, 40% search, 30% item select, 10% checkout).
What is the System Workload Mix? [Multiple workloads may be simulated in a single performance test] (for example: 30% Workload A, 20% Workload B, 50% Workload C).
What are the time requirements for any/all back-end batch processes (specify peak vs. nominal)?

A stable build of the system which must resemble the production environment as closely as is possible.
To ensure consistent results, the performance testing environment should be isolated from other environments, such as user acceptance testing (UAT) or development. As a best practice it is always advisable to have a separate performance testing environment resembling the production environment as much as possible.

In performance testing, it is often crucial for the test conditions to be similar to the expected actual use. However, in practice this is hard to arrange and not wholly possible, since production systems are subjected to unpredictable workloads. Test workloads may mimic occurrences in the production environment as far as possible, but only in the simplest systems can one exactly replicate this workload variability.
Loosely-coupled architectural implementations (e.g.: SOA) have created additional complexities with performance testing. To truly replicate production-like states, enterprise services or assets that share a common infrastructure or platform require coordinated performance testing, with all consumers creating production-like transaction volumes and load on shared infrastructures or platforms. Because this activity is so complex and costly in money and time, some organizations now use tools to monitor and simulate production-like conditions (also referred as "noise") in their performance testing environments (PTE) to understand capacity and resource requirements and verify / validate quality attributes.

It is critical to the cost performance of a new system that performance test efforts begin at the inception of the development project and extend through to deployment. The later a performance defect is detected, the higher the cost of remediation. This is true in the case of functional testing, but even more so with performance testing, due to the end-to-end nature of its scope. It is crucial for a performance test team to be involved as early as possible, because it is time-consuming to acquire and prepare the testing environment and other key performance requisites.

In the diagnostic case, software engineers use tools such as profilers to measure what parts of a device or software contribute most to the poor performance, or to establish throughput levels (and thresholds) for maintained acceptable response time.

Performance testing technology employs one or more PCs or Unix servers to act as injectors, each emulating the presence of numbers of users and each running an automated sequence of interactions (recorded as a script, or as a series of scripts to emulate different types of user interaction) with the host whose performance is being tested. Usually, a separate PC acts as a test conductor, coordinating and gathering metrics from each of the injectors and collating performance data for reporting purposes. The usual sequence is to ramp up the load: to start with a few virtual users and increase the number over time to a predetermined maximum. The test result shows how the performance varies with the load, given as number of users vs. response time. Various tools are available to perform such tests. Tools in this category usually execute a suite of tests which emulate real users against the system. Sometimes the results can reveal oddities, e.g., that while the average response time might be acceptable, there are outliers of a few key transactions that take considerably longer to complete – something that might be caused by inefficient database queries, pictures, etc.
Performance testing can be combined with stress testing, in order to see what happens when an acceptable load is exceeded. Does the system crash? How long does it take to recover if a large load is reduced? Does its failure cause collateral damage?
Analytical Performance Modeling is a method to model the behaviour of a system in a spreadsheet. The model is fed with measurements of transaction resource demands (CPU, disk I/O, LAN, WAN), weighted by the transaction-mix (business transactions per hour). The weighted transaction resource demands are added up to obtain the hourly resource demands and divided by the hourly resource capacity to obtain the resource loads. Using the responsetime formula (R=S/(1-U), R=responsetime, S=servicetime, U=load), responsetimes can be calculated and calibrated with the results of the performance tests. Analytical performance modeling allows evaluation of design options and system sizing based on actual or anticipated business use. It is therefore much faster and cheaper than performance testing, though it requires thorough understanding of the hardware platforms.

Tasks to perform such a test would include:
Decide whether to use internal or external resources to perform the tests, depending on inhouse expertise (or lack of it)
Gather or elicit performance requirements (specifications) from users and/or business analysts
Develop a high-level plan (or project charter), including requirements, resources, timelines and milestones
Develop a detailed performance test plan (including detailed scenarios and test cases, workloads, environment info, etc.)
Choose test tool(s)
Specify test data needed and charter effort (often overlooked, but vital to carrying out a valid performance test)
Develop proof-of-concept scripts for each application/component under test, using chosen test tools and strategies
Develop detailed performance test project plan, including all dependencies and associated timelines
Install and configure injectors/controller
Configure the test environment (ideally identical hardware to the production platform), router configuration, quiet network (we don’t want results upset by other users), deployment of server instrumentation, database test sets developed, etc.
Dry run the tests - before actually executing the load test with predefined users, a dry run is carried out in order to check the correctness of the script
Execute tests – probably repeatedly (iteratively) in order to see whether any unaccounted-for factor might affect the results
Analyze the results - either pass/fail, or investigation of critical path and recommendation of corrective action

According to the Microsoft Developer Network the Performance Testing Methodology consists of the following activities:
Identify the Test Environment. Identify the physical test environment and the production environment as well as the tools and resources available to the test team. The physical environment includes hardware, software, and network configurations. Having a thorough understanding of the entire test environment at the outset enables more efficient test design and planning and helps you identify testing challenges early in the project. In some situations, this process must be revisited periodically throughout the project’s life cycle.
Identify Performance Acceptance Criteria. Identify the response time, throughput, and resource-use goals and constraints. In general, response time is a user concern, throughput is a business concern, and resource use is a system concern. Additionally, identify project success criteria that may not be captured by those goals and constraints; for example, using performance tests to evaluate which combination of configuration settings will result in the most desirable performance characteristics.
Plan and Design Tests. Identify key scenarios, determine variability among representative users and how to simulate that variability, define test data, and establish metrics to be collected. Consolidate this information into one or more models of system usage to be implemented, executed, and analyzed.
Configure the Test Environment. Prepare the test environment, tools, and resources necessary to execute each strategy, as features and components become available for test. Ensure that the test environment is instrumented for resource monitoring as necessary.
Implement the Test Design. Develop the performance tests in accordance with the test design.
Execute the Test. Run and monitor your tests. Validate the tests, test data, and results collection. Execute validated tests for analysis while monitoring the test and the test environment.
Analyze Results, Tune, and Retest. Analyse, Consolidate and share results data. Make a tuning change and retest. Compare the results of both tests. Each improvement made will return smaller improvement than the previous improvement. When do you stop? When you reach a CPU bottleneck, the choices then are either improve the code or add more CPU.

Stress testing (software)
Benchmark (computing)
Web server benchmarking
Application Response Measurement

The Art of Application Performance Testing - O'Reilly ISBN 978-0-596-52066-3 (Book)
Performance Testing Guidance for Web Applications (MSDN)
Performance Testing Guidance for Web Applications (Book)
Performance Testing Guidance for Web Applications (PDF)
Performance Testing Guidance (Online KB)
Enterprise IT Performance Testing (Online KB)
Performance Testing Videos (MSDN)
Open Source Performance Testing tools
"User Experience, not Metrics" and "Beyond Performance Testing"
"Performance Testing Traps / Pitfalls"A Federal Perkins Loan, or Perkins Loan, is a need-based student loan offered by the U.S. Department of Education to assist American college students in funding their post-secondary education. The program is named after Carl D. Perkins, a former member of the U.S. House of Representatives from Kentucky.
Perkins Loans carry a fixed interest rate of 5% for the duration of the ten-year repayment period. The Perkins Loan Program has a nine-month grace period, so that borrowers begin repayment in the tenth month upon graduating, falling below half-time status, or withdrawing from their college or university. Because the Perkins Loan is subsidized by the government, interest does not begin to accrue until the borrower begins to repay the loan. As of the 2009-2010 academic year, the loan limits for undergraduates are $5,500 per year with a lifetime maximum loan of $27,500. For graduate students, the limit is $8,000 per year with a lifetime limit of $60,000 (including undergraduate loans).
Perkins Loans are eligible for Federal Loan Cancellation for individuals choosing to work in a number of different public service occupations including early childhood education, elementary and secondary school teaching, speech therapy, nursing, law enforcement, librarian, public defense attorney, fire fighting and certain active duty military postings. Depending on the field of employment, further restrictions on the setting of employment may apply. For example, forgiveness for teachers may be restricted to designated low-income schools or specific teacher shortage areas such as math, science, and bilingual education and forgiveness for nurses requires employment at a non-profit medical facility. A percentage of the loan is cancelled for each year spent teaching full-time(as long as the loan remains in good standing). This cancellation also applies to Peace Corps Volunteers. Cancellation typically occurs on a graduating scale: 15% for year 1, 15% for year 2, 20% for year 3, 20% for year 4, 30% for year 5. These percentages are based on the original debt amount. Thus after 3 years of service, one would have 50% of their original debt cancelled.
The Federal Perkins Loan program is set to expire on September 30, 2017, this is an extension to an earlier program expiration of September 30, 2015. The extension was made possible by the General Education Provisions Act (GEPA), however this act also prohibits any further extensions of the Perkins Loan Program.

Stafford Loan
Student loans in the United States

U.S. Dept. of Education: Federal Perkins Loans
U.S. Dept. of Education: Federal Perkins Loan Teacher Cancellation
U.S. Office of Management and Budget: Federal Perkins Loans Program Assessment

The arts represent an outlet of expression that is usually influenced by culture and which in turn helps to change culture. As such, the arts are a physical manifestation of the internal creative impulse. Major constituents of the arts include literature – including poetry, novels and short stories, and epics; performing arts – among them music, dance, and theatre; culinary arts such as baking, chocolatiering, and winemaking; media arts like photography and cinematography, and visual arts – including drawing, painting, ceramics, and sculpting. Some art forms combine a visual element with performance (e.g. film) and the written word (e.g. comics).
From prehistoric cave paintings to modern day films, art serves as a vessel for storytelling and conveying humankind's relationship with its environment.

In its most basic abstract definition, art is a documented expression of a sentient being through or on an accessible medium so that anyone can view, hear or experience it. The act itself of producing an expression can also be referred to as a certain art, or as art in general.
If this solidified expression, or the act of producing it, is "good" or has value depends on those who access and rate it and this public rating is dependent on various subjective factors.
Merriam-Webster defines "the arts" as "painting, sculpture, music, theater, literature, etc., considered as a group of activities done by people with skill and imagination." Similarly, the United States Congress, in the National Foundation on the Arts and Humanities Act, defined "the arts" as follows:

The term 'the arts' includes, but is not limited to, music (instrumental and vocal), dance, drama, folk art, creative writing, architecture and allied fields, painting, sculpture, photography, graphic and craft arts, industrial design, costume and fashion design, motion pictures, television, radio, film, video, tape and sound recording, the arts related to the presentation, performance, execution, and exhibition of such major art forms, all those traditional arts practiced by the diverse peoples of this country. (sic) and the study and application of the arts to the human environment.

In Ancient Greece, all art and craft was referred to by the same word, Techne. Thus, there was no distinction between the arts. Ancient Greek art brought the veneration of the animal form and the development of equivalent skills to show musculature, poise, beauty, and anatomically correct proportions. Ancient Roman art depicted gods as idealized humans, shown with characteristic distinguishing features (i.e. Zeus' thunderbolt).
In Byzantine and Gothic art of the Middle Ages, the dominance of the church insisted on the expression of biblical and not material truths.
Eastern art has generally worked in a style akin to Western medieval art, namely a concentration on surface patterning and local colour (meaning the plain colour of an object, such as basic red for a red robe, rather than the modulations of that colour brought about by light, shade and reflection). A characteristic of this style is that the local colour is often defined by an outline (a contemporary equivalent is the cartoon). This is evident in, for example, the art of India, Tibet and Japan.
Religious Islamic art forbids iconography, and expresses religious ideas through geometry instead.

In the Middle Ages, the Artes Liberales (liberal arts) were taught in universities as part of the Trivium, an introductory curriculum involving grammar, rhetoric, and logic, and of the Quadrivium, a curriculum involving the "mathematical arts" of arithmetic, geometry, music, and astronomy. The Artes Mechanicae (consisting of vestiaria – tailoring and weaving; agricultura – agriculture; architectura – architecture and masonry; militia and venatoria – warfare, hunting, military education, and the martial arts; mercatura – trade; coquinaria – cooking; and metallaria – blacksmithing and metallurgy) were practised and developed in guild environments. The modern distinction between "artistic" and "non-artistic" skills did not develop until the Renaissance.
In modern academia, the arts are usually grouped with or as a subset of the humanities. Some subjects in the humanities are history, linguistics, literature, theology, philosophy, and/or logic.
The arts have also been classified as seven: Literature, painting, sculpture, and music comprise the main four arts, of which the other three are derivative; drama is literature with acting, dance is music expressed through motion, and song is music with literature and voice.

Drawing is a means of making an image, using any of a wide variety of tools and techniques. It generally involves making marks on a surface by applying pressure from a tool, or moving a tool across a surface. Common tools are graphite pencils, pen and ink, inked brushes, wax colour pencils, crayons, charcoals, pastels, and markers. Digital tools which can simulate the effects of these are also used. The main techniques used in drawing are line drawing, hatching, crosshatching, random hatching, scribbling, stippling, and blending. An artist who excels in drawing is referred to as a drafter, draftswoman, or draughtsman.
Drawing can be used to create art such as illustrations, comics, and animation.

Painting, taken literally, is the practice of applying pigment suspended in a vehicle (or medium) and a binding agent (a glue) to a surface (support) such as paper, canvas, wood panel or a wall. However, when used in an artistic sense, it means the use of this activity in combination with drawing, composition and other aesthetic considerations in order to manifest the expressive and conceptual intention of the practitioner. Painting is also used to express spiritual motifs and ideas; sites of this kind of painting range from artwork depicting mythological figures on pottery to The Sistine Chapel to the human body itself.
Colour is the essence of painting as sound is of music. Colour is highly subjective, but has observable psychological effects, although these can differ from one culture to the next. Black is associated with mourning in the West, but elsewhere white may be. Some painters, theoreticians, writers and scientists, including Goethe, Kandinsky, and Newton, have written their own colour theory. Moreover, the use of language is only an abstraction for a colour equivalent. The word "red," for example, can cover a wide range of variations on the pure red of the spectrum. There is not a formalized register of different colours in the way that there is agreement on different notes in music, such as C or C#, although the Pantone system is widely used in the printing and design industry for this purpose.
Modern artists have extended the practice of painting considerably to include, for example, collage. This began with Cubism, and is not painting in the strict sense. Some modern painters incorporate different materials such as sand, cement, straw, wood or strands of hair for their texture. Examples of this are the works of Elito Circa, Jean Dubuffet or Anselm Kiefer.
Modern and contemporary art has moved away from the historic value of craft in favour of concept; this has led some to say that painting, as a serious art form, is dead, although this has not deterred the majority of artists from continuing to practise it either as whole or part of their work. Indigenouism is also considered as Modern and contemporary Art in early 20th Century.

Ceramic art is art made from ceramic materials (including clay), which may take forms such as pottery, tile, figurines, sculpture, and tableware. While some ceramic products are considered fine art, some are considered to be decorative, industrial, or applied art objects. Ceramics may also be considered artefacts in archaeology.Ceramic art can be made by one person or by a group of people. In a pottery or ceramic factory, a group of people design, manufacture, and decorate the pottery. Products from a pottery are sometimes referred to as "art pottery." In a one-person pottery studio, ceramists or potters produce studio pottery. In modern ceramic engineering usage, "ceramics" is the art and science of making objects from inorganic, non-metallic materials by the action of heat. It excludes glass and mosaic made from glass tesserae.

Photography as an art form refers to photographs that are created in accordance with the creative vision of the photographer. Art photography stands in contrast to photojournalism, which provides a visual account for news events, and commercial photography, the primary focus of which is to advertise products or services.

Architecture is the art and science of designing buildings and structures. The word architecture comes from the Greek arkhitekton, "master builder, director of works," from αρχι- (arkhi) "chief" + τεκτων (tekton) "builder, carpenter".
A wider definition would include the design of the built environment, from the macrolevel of town planning, urban design, and landscape architecture to the microlevel of creating furniture. Architectural design usually must address both feasibility and cost for the builder, as well as function and aesthetics for the user.

In modern usage, architecture is the art and discipline of creating, or inferring an implied or apparent plan of, a complex object or system. The term can be used to connote the implied architecture of abstract things such as music or mathematics, the apparent architecture of natural things, such as geological formations or the structure of biological cells, or explicitly planned architectures of human-made things such as software, computers, enterprises, and databases, in addition to buildings. In every usage, an architecture may be seen as a subjective mapping from a human perspective (that of the user in the case of abstract or physical artefacts) to the elements or components of some kind of structure or system, which preserves the relationships among the elements or components.
Planned architecture manipulates space, volume, texture, light, shadow, or abstract elements in order to achieve pleasing aesthetics. This distinguishes it from applied science or engineering, which usually concentrate more on the functional and feasibility aspects of the design of constructions or structures.
In the field of building architecture, the skills demanded of an architect range from the more complex, such as for a hospital or a stadium, to the apparently simpler, such as planning residential houses. Many architectural works may be seen also as cultural and political symbols, and/or works of art. The role of the architect, though changing, has been central to the successful (and sometimes less than successful) design and implementation of pleasingly built environments in which people live.

Sculpture is the branch of the visual arts that operates in three dimensions. It is one of the plastic arts. Durable sculptural processes originally used carving (the removal of material) and modelling (the addition of material, as clay), in stone, metal, ceramics, wood and other materials; but since modernism, shifts in sculptural process led to an almost complete freedom of materials and process. A wide variety of materials may be worked by removal such as carving, assembled by welding or modelling, or moulded, or cast.

Conceptual art is art in which the concept(s) or idea(s) involved in the work takes precedence over traditional aesthetic and material concerns. The inception of the term in the 1960s referred to a strict and focused practice of idea-based art that often defied traditional visual criteria associated with the visual arts in its presentation as text. Through its association with the Young British Artists and the Turner Prize during the 1990s, its popular usage, particularly in the UK, developed as a synonym for all contemporary art that does not practise the traditional skills of painting and sculpture.

Literature is literally "acquaintance with letters" as in the first sense given in the Oxford English Dictionary. The noun "literature" comes from the Latin word littera meaning "an individual written character (letter)." The term has generally come to identify a collection of writings, which in Western culture are mainly prose (both fiction and non-fiction), drama and poetry. In much, if not all of the world, the artistic linguistic expression can be oral as well, and include such genres as epic, legend, myth, ballad, other forms of oral poetry, and as folktale.
Comics, the combination of drawings or other visual arts with narrating literature, are often called the "ninth art" (le neuvième art) in Francophone scholarship.

Performing arts comprise dance, music, theatre, opera, mime, and other art forms in which a human performance is the principal product. Performing arts are distinguished by this performance element in contrast with disciplines such as visual and literary arts where the product is an object that does not require a performance to be observed and experienced. Each discipline in the performing arts is temporal in nature, meaning the product is performed over a period of time. Products are broadly categorized as being either repeatable (for example, by script or score) or improvised for each performance.
Artists who participate in these arts in front of an audience are called performers, including actors, magicians, comedians, dancers, musicians, and singers. Performing arts are also supported by the services of other artists or essential workers, such as songwriting and stagecraft. Performers often adapt their appearance with tools such as costume and stage makeup.

Music is an art form whose medium is sound. Common elements of music are pitch (which governs melody and harmony), rhythm (and its associated concepts tempo, metre, and articulation), dynamics, and the sonic qualities of timbre and texture. The creation, performance, significance, and even the definition of music vary according to culture and social context. Music ranges from strictly organized compositions (and their reproduction in performance) through improvisational music to aleatoric pieces. Music can be divided into genres and subgenres, although the dividing lines and relationships between music genres are often subtle, sometimes open to individual interpretation, and occasionally controversial. Within "the arts," music may be classified as a performing art, a fine art, and auditory art.

Theatre or theater (from Greek theatron (θέατρον); from theasthai, "behold") is the branch of the performing arts concerned with acting out stories in front of an audience using combinations of speech, gesture, music, dance, sound and spectacle – indeed, any one or more elements of the other performing arts. In addition to the standard narrative dialogue style, theatre takes such forms as opera, ballet, mime, kabuki, classical Indian dance, Chinese opera and mummers' plays.

Dance (from Old French dancier, of unknown origin) generally refers to human movement either used as a form of expression or presented in a social, spiritual or performance setting. Dance is also used to describe methods of non-verbal communication (see body language) between humans or animals (bee dance, mating dance), motion in inanimate objects (the leaves danced in the wind), and certain musical forms or genres.
Choreography is the art of making dances, and the person who does this is called a choreographer. People danced to relieve stress.
Definitions of what constitutes dance are dependent on social, cultural, aesthetic, artistic and moral constraints and range from functional movement (such as Folk dance) to codified, virtuoso techniques such as ballet. In sports, gymnastics, figure skating and synchronized swimming are dance disciplines while Martial arts "kata" are often compared to dances.

Areas exist in which artistic works incorporate multiple artistic fields, such as film, opera and performance art. While opera is often categorized in the performing arts of music, the word itself is Italian for "works," because opera combines several artistic disciplines in a singular artistic experience. In a typical traditional opera, the entire work utilizes the following: the sets (visual arts), costumes (fashion), acting (dramatic performing arts), the libretto, or the words/story (literature), and singers and an orchestra (music). The composer Richard Wagner recognized the fusion of so many disciplines into a single work of opera, exemplified by his cycle Der Ring des Nibelungen ("The Ring of the Nibelung"). He did not use the term opera for his works, but instead Gesamtkunstwerk ("synthesis of the arts"), sometimes referred to as "Music Drama" in English, emphasizing the literary and theatrical components which were as important as the music. Classical ballet is another form which emerged in the 19th century in which orchestral music is combined with dance.
Other works in the late 19th, 20th and 21st centuries have fused other disciplines in unique and creative ways, such as performance art. Performance art is a performance over time which combines any number of instruments, objects, and art within a predefined or less well-defined structure, some of which can be improvised. Performance art may be scripted, unscripted, random or carefully organized; even audience participation may occur.John Cage is regarded by many as a performance artist rather than a composer, although he preferred the latter term. He did not compose for traditional ensembles. Cage's composition Living Room Music composed in 1940 is a "quartet" for unspecified instruments, really non-melodic objects, which can be found in a living room of a typical house, hence the title.

A debate exists in the fine arts and video game cultures over whether video games can be counted as an art form. Game designer Hideo Kojima professes that video games are a type of service, not an art form, because they are meant to entertain and attempt to entertain as many people as possible, rather than being a single artistic voice (despite Kojima himself being considered a gaming auteur, and the mixed opinions his games typically receive). However, he acknowledged that since video games are made up of artistic elements (for example, the visuals), game designers could be considered museum curators - not creating artistic pieces, but arranging them in a way that displays their artistry and sells tickets.
In May 2011, the National Endowment of the Arts included video games in its redefinition of what is considered a "work of art" when applying of a grant. In 2012, the Smithsonian American Art Museum presented an exhibit, The Art of the Video Game. Reviews of the exhibit were mixed, including questioning whether video games belong in an art museum.

Gastronomy is the study of the relationship between culture and food. It is often thought erroneously that the term gastronomy refers exclusively to the art of cooking (see culinary art), but this is only a small part of this discipline; it cannot always be said that a cook is also a gourmet. Gastronomy studies various cultural components with food as its central axis. Thus, it is related to the Fine Arts and Social Sciences, and even to the Natural Sciences in terms of human nutritious activity and digestive function.

Architecture criticism
Visual art criticism
Dance criticism
Film criticism
Music criticism
Television criticism
Theatre criticism

Culinary art
Fine art
Martial arts
Art in odd places
Arts education

"The Art of Video Games". SI.edu. Smithsonian American Art Museum. Retrieved 7 March 2015.
Barron, Christina (29 April 2012). "Museum exhibit asks: Is it art if you push 'start'?". The Washington Post. Retrieved 12 February 2013.
"Conceptual art". Tate Glossary. Retrieved 7 March 2015.
Feynman, Richard (1985). QED: The Strange Theory of Light and Matter. Princeton University Press. ISBN 0691024170.
"FY 2012 Arts in Media Guidelines". Endow.gov. National Endowment for the Arts. Archived from the original on 13 February 2012. Retrieved 7 March 2015.
Gibson, Ellie (24 January 2006). "Games aren't art, says Kojima". Eurogamer. Gamer Network. Retrieved 7 March 2015.
Kennicott, Philip (18 March 2012). "The Art of Video Games". The Washington Post. Retrieved 12 February 2013.
Levinson, Jerrold. "Performing Arts". The Oxford Companion to Philosophy. Oxford University Press. doi:10.1093/acref/9780199264797.001.0001. ISBN 9780199264797. Retrieved 7 March 2015.

This article displayed as a mindmap, at wikimindmap.com
Cowan, Tyler (2008). "Arts". In David R. Henderson (ed.). Concise Encyclopedia of Economics (2nd ed.). Indianapolis: Library of Economics and Liberty. ISBN 978-0865976658. OCLC 237794267.  – A look at how general economic principles govern the arts.Social history, often called the new social history, is a field of history that looks at the lived experience of the past. In its "golden age" it was a major growth field in the 1960s and 1970s among scholars, and still is well represented in history departments in Britain, Canada, France, Germany, and the United States. In the two decades from 1975 to 1995, the proportion of professors of history in American universities identifying with social history rose from 31% to 41%, while the proportion of political historians fell from 40% to 30%. In the history departments of British and Irish universities in 2014, of the 3410 faculty members reporting, 878 (26%) identified themselves with social history while political history came next with 841 (25%).

The older social history (before 1960) included numerous topics that were not part of the mainstream historiography of political, military, diplomatic and constitutional history. It was a hodgepodge without a central theme, and it often included political movements, like Populism, that were "social" in the sense of being outside the elite system. People's history was sometimes so Marxist that non-Marxists were alienated by it. Social history was contrasted with political history, intellectual history and the history of great men. English historian G. M. Trevelyan saw it as the bridging point between economic and political history, reflecting that, "Without social history, economic history is barren and political history unintelligible." While the field has often been viewed negatively as history with the politics left out, it has also been defended as "history with the people put back in."

The "new social history" exploded on the scene in the 1960s, quickly becoming one of the dominant styles of historiography in the U.S., Britain and Canada. The French version, promulgated by the Annales School, was very well organized and dominated French historiography, and influenced much of Europe and Latin America. Jürgen Kocka finds two meanings to "social history." At the simplest level, it was the subdivision of historiography that focused on social structures and processes. In this regard it stood in contrast to political or economic history. The second meaning was broader, and the Germans called it "Gesellschaftsgeschichte." It is the history of an entire society from a social-historical viewpoint.
In Germany the "Gesellschaftsgeschichte" movement introduced a vast range of topics, as Kocka, a leader of the Bielefeld School recalls:
In the 1960s and 1970s, "social history" caught the imagination of a young generation of historians. It became a central concept -- and a rallying point -- of historiographic revisionism. It meant many things at the same time. It gave priority to the study of particular kinds of phenomena, such as classes and movements, urbanization and industrialization, family and education, work and leisure, mobility, inequality, conflicts and revolutions. It stressed structures and processes over actors and events. It emphasized analytical approaches close to the social sciences rather than by the traditional methods of historical hermeneutics. Frequently social historians sympathized with the causes (as they saw them) of the little people, of the underdog, of popular movements, or of the working class. Social history was both demanded and rejected as a vigorous revisionist alternative to the more established ways of historiography, in which the reconstruction of politics and ideas, the history of events and hermeneutic methods traditionally dominated.
Americanist Paul E. Johnson recalls the heady early promise of the movement in the late 1960s:
The New Social History reached UCLA at about that time, and I was trained as a quantitative social science historian. I learned that "literary" evidence and the kinds of history that could be written from it were inherently elitist and untrustworthy. Our cousins, the Annalistes, talked of ignoring heroes and events and reconstructing the more constitutive and enduring "background" of history. Such history could be made only with quantifiable sources. The result would be a "History from the Bottom Up" that ultimately engulfed traditional history and, somehow, helped to make a Better World. Much of this was acted out with mad-scientist bravado. One well-known quantifier said that anyone who did not know statistics at least through multiple regression should not hold a job in a history department. My own advisor told us that he wanted history to become "a predictive social science." I never went that far. I was drawn to the new social history by its democratic inclusiveness as much as by its system and precision. I wanted to write the history of ordinary people—to historicize them, put them into the social structures and long-term trends that shaped their lives, and at the same time resurrect what they said and did. In the late 1960s, quantitative social history looked like the best way to do that.
The Social Science History Association was formed in 1976 to bring together scholars from numerous disciplines interested in social history. It is still active and publishes Social Science History quarterly. The field is also the specialty of the Journal of Social History, edited since 1967 by Peter Stearns It covers such topics as gender relations; race in American history; the history of personal relationships; consumerism; sexuality; the social history of politics; crime and punishment, and history of the senses. Most of the major historical journals have coverage as well.
However, after 1990 social history was increasingly challenged by cultural history, which emphasizes language and the importance of beliefs and assumptions and their causal role in group behavior.

The study of the lives of ordinary people was revolutionized in the 1960s by the introduction of sophisticated quantitative and demographic methods, often using individual data from the census and from local registers of births, marriages, deaths and taxes, as well as theoretical models from sociology such as social mobility. H-DEMOG is a daily email discussion group that covers the field broadly.
Historical demography is the study of population history and demographic processes, usually using census or similar statistical data. It became an important specialty inside social history, with strong connections with the larger field of demography, as in the study of the Demographic Transition.

Black history or African-American history studies African Americans and Africans in American history. The Association for the Study of African American Life and History was founded by Carter G. Woodson in 1915 and has 2500 members and publishes the Journal of African American History, formerly the Journal of Negro History. Since 1926 it has sponsored Black History Month every February.

Ethnic history is especially important in the U.S. and Canada, where major encyclopedias helped define the field. It covers the history of ethnic groups (usually not including blacks or Native Americans). Typical approaches include critical ethnic studies; comparative ethnic studies; critical race studies; Asian-American, and Latino/a or Chicano/a studies. In recent years Chicano/Chicana studies has become important as the Hispanic population has become the largest minority in the U.S.
The Immigration and Ethnic History Society was formed in 1976 and publishes a journal for libraries and its 829 members.
The American Conference for Irish Studies, founded in 1960, has 1,700 members and has occasional publications but no journal.
The American Italian Historical Association was founded in 1966 and has 400 members; it does not publish a journal
The American Jewish Historical Society is the oldest ethnic society, founded in 1892; it has 3,300 members and publishes American Jewish History
The Polish American Historical Association was founded in 1942, and publishes a newsletter and Polish American Studies, an interdisciplinary, refereed scholarly journal twice each year.
H-ETHNIC is a daily discussion list founded in 1993 with 1400 members; it covers topics of ethnicity and migration globally.

Labor history, deals with labor unions and the social history of workers. See for example Labor history of the United States The Study Group on International Labor and Working-Class History was established: 1971 and has a membership of 1000. It publishes International Labor and Working-Class History. H-LABOR is a daily email-based discussion group formed in 1993 that reaches over a thousand scholars and advanced students. the Labor and Working-Class History Association formed in 1988 and publishes Labor: Studies in Working-Class History of the Americas.
Kirk (2010) surveys labour historiography in Britain since the formation of the Society for the Study of Labour History in 1960. He reports that labour history has been mostly pragmatic, eclectic and empirical; it has played an important role in historiographical debates, such as those revolving around history from below, institutionalism versus the social history of labour, class, populism, gender, language, postmodernism and the turn to politics. Kirk rejects suggestions that the field is declining, and stresses its innovation, modification and renewal. Kirk also detects a move into conservative insularity and academicism. He recommends a more extensive and critical engagement with the kinds of comparative, transnational and global concerns increasingly popular among labour historians elsewhere, and calls for a revival of public and political interest in the topics. Meanwhile, Navickas, (2011) examines recent scholarship including the histories of collective action, environment and human ecology, and gender issues, with a focus on work by James Epstein, Malcolm Chase, and Peter Jones.

Women's history exploded into prominence in the 1970s, and is now well represented in every geographical topic; increasingly it includes gender history. Social history uses the approach of women's history to understand the experiences of ordinary women, as opposed to "Great Women," in the past. Feminist women's historians have critiqued early studies of social history for being too focused on the male experience.

Gender history focuses on the categories, discourses and experiences of femininity and masculinity as they develop over time. Gender history gained prominence after it was conceptualized by Joan W. Scott in her article "Gender: A Useful Category of Historical Analysis." Many social historians use Scott's concept of "perceived differences" to study how gender relations in the past have unfolded and continue to unfold. In keeping with the cultural turn, many social historians are also gender historians who study how discourses interact with everyday experiences.

The History of the family emerged as a separate field in the 1970s, with close ties to anthropology and sociology. The trend was especially pronounced in the U.S. and Canada. It emphasizes on demographic patterns, and public policy. It is quite separate from Genealogy, though often drawing on the same primary sources such as censuses and family records. An influential pioneering study was Women, Work, and Family (1978), by Louise A. Tilly and Joan W. Scott. It broke new ground with their broad interpretive framework and emphasis on the variable factors shaping women's place in the family and economy in France and England. It considered the interaction of production and reproduction in analysis of women's wage labor and thus helped to bring together labor and family history. Much work has been done on the dichotomy in women's lives between the private sphere and the public. For a recent worldwide overview covering 7000 years see Maynes and Waltner (2012). For comprehensive coverage of the American case, see Marilyn Coleman and Lawrence Ganong, eds. The Social History of the American Family: An Encyclopedia (4 vol, 2014).
The history of childhood is a growing subfield.

For much of the 20th century, the dominant American historiography, as exemplified by Ellwood Patterson Cubberley (1868-1941) at Stanford, emphasized the rise of American education as a powerful force for literacy, democracy, and equal opportunity, and a firm basis for higher education and advanced research institutions. It was a story of enlightenment and modernization triumphing over ignorance, cost-cutting, and narrow traditionalism whereby parents tried to block their children's intellectual access to the wider world. Teachers dedicated to the public interest, reformers with a wide vision, and public support from the civic-minded community were the heroes. The textbooks help inspire students to become public schools teachers and thereby fulfill their own civic mission.
The crisis came in the 1960s, when a new generation of New Left scholars and students rejected the traditional celebratory accounts, and identified the educational system as the villain for many of America's weaknesses, failures, and crimes. Michael Katz (1939-2014) states they:
tried to explain the origins of the Vietnam War; the persistence of racism and segregation; the distribution Of power among gender and classes; intractable poverty and the decay of cities; and the failure of social institutions and policies designed to deal with mental illness, crime, delinquency, and education.
The old guard fought back and bitter historiographical contests, with the younger students and scholars largely promoting the proposition that schools were not the solution To America's ills, they were In part the cause of Americans problems. The fierce battles of the 1960s died out by the 1990s, but enrollment in education history courses and never recovered.
By the 1980s, compromise had been worked out, with all sides focusing on the heavily bureaucratic nature of the American public schooling.
In recent years most histories of education deal with institutions or focus on the ideas histories of major reformers, but a new social history has recently emerged, focused on who were the students in terms of social background and social mobility. In the U.S. attention has often focused on minority and ethnic students. In Britain, Raftery et al. (2007) looks at the historiography on social change and education in Ireland, Scotland, and Wales, with particular reference to 19th-century schooling. They developed distinctive systems of schooling in the 19th century that reflected not only their relationship to England but also significant contemporaneous economic and social change. This article seeks to create a basis for comparative work by identifying research that has treated this period, offering brief analytical commentaries on some key works, discussing developments in educational historiography, and pointing to lacunae in research.
Historians have recently looked at the relationship between schooling and urban growth by studying educational institutions as agents in class formation, relating urban schooling to changes in the shape of cities, linking urbanization with social reform movements, and examining the material conditions affecting child life and the relationship between schools and other agencies that socialize the young.
The most economics-minded historians have sought to relate education to changes in the quality of labor, productivity and economic growth, and rates of return on investment in education. A major recent exemplar is Claudia Goldin and Lawrence F. Katz, The Race between Education and Technology (2009), on the social and economic history of 20th-century American schooling.

The "new urban history" emerged in the 1950s in Britain and in the 1960s in the U.S. It looked at the "city as process" and, often using quantitative methods, to learn more about the inarticulate masses in the cities, as opposed to the mayors and elites. A major early study was Stephan Thernstrom's Poverty and Progress: Social Mobility in a Nineteenth Century City (1964), which used census records to study Newburyport, Massachusetts, 1850-1880. A seminal, landmark book, it sparked interest in the 1960s and 1970s in quantitative methods, census sources, "bottom-up" history, and the measurement of upward social mobility by different ethnic groups. Other exemplars of the new urban history included Kathleen Conzen, Immigrant Milwaukee, 1836-1860 (1976); Alan Dawley, Class and Community: The Industrial Revolution in Lynn (1975; 2nd ed. 2000); Michael B. Katz, The People of Hamilton, Canada West (1976); Eric H. Monkkonen, The Dangerous Class: Crime and Poverty in Columbus Ohio 1860-1865 (1975); and Michael P. Weber, Social Change in an Industrial Town: Patterns of Progress in Warren, Pennsylvania, From Civil War to World War I. (1976).
Representative comparative studies include Leonardo Benevolo, The European City (1993); Christopher R. Friedrichs, The Early Modern City, 1450-1750 (1995), and James L. McClain, John M. Merriman, and Ugawa Kaoru. eds. Edo and Paris (1994) (Edo was the old name for Tokyo).
There were no overarching social history theories that emerged developed to explain urban development. Inspiration from urban geography and sociology, as well as a concern with workers (as opposed to labor union leaders), families, ethnic groups, racial segregation, and women's roles have proven useful. Historians now view the contending groups within the city as "agents" who shape the direction of urbanization. The subfield has flourished in Australia—where most people live in cities.

Agricultural History handles the economic and technological dimensions, while Rural history handles the social dimension. Burchardt (2007) evaluates the state of modern English rural history and identifies an "orthodox" school, focused on the economic history of agriculture. This historiography has made impressive progress in quantifying and explaining the output and productivity achievements of English farming since the "agricultural revolution." The celebratory style of the orthodox school was challenged by a dissident tradition emphasizing the social costs of agricultural progress, notably enclosure, which forced poor tenant farmers off the land. Recently, a new school, associated with the journal Rural History, has broken away from this narrative of agricultural change, elaborating a wider social history. The work of Alun Howkins has been pivotal in the recent historiography, in relation to these three traditions. Howkins, like his precursors, is constrained by an increasingly anachronistic equation of the countryside with agriculture. Geographers and sociologists have developed a concept of a "post-productivist" countryside, dominated by consumption and representation that may have something to offer historians, in conjunction with the well-established historiography of the "rural idyll." Most rural history has focused on the American South—overwhelmingly rural until the 1950s—but there is a "new rural history" of the North as well. Instead of becoming agrarian capitalists, farmers held onto preindustrial capitalist values emphasizing family and community. Rural areas maintained population stability; kinship ties determined rural immigrant settlement and community structures; and the defeminization of farm work encouraged the rural version of the "women's sphere." These findings strongly contrast with those in the old frontier history as well as those found in the new urban history.

The historiography of religion focuses mostly on theology and church organization and development. Recently the study of the social history or religious behavior and belief has become important.

Social history has dominated French historiography since the 1920s, thanks to the central role of the Annales School. Its journal Annales focuses attention on the synthesizing of historical patterns identified from social, economic, and cultural history, statistics, medical reports, family studies, and even psychoanalysis.

Social history developed within West German historiography during the 1950s-60s as the successor to the national history discredited by National Socialism. The German brand of "history of society" - Gesellschaftsgeschichte - has been known from its beginning in the 1960s for its application of sociological and political modernization theories to German history. Modernization theory was presented by Hans-Ulrich Wehler (1931-2014) and his Bielefeld School as the way to transform "traditional" German history, that is, national political history, centered on a few "great men," into an integrated and comparative history of German society encompassing societal structures outside politics. Wehler drew upon the modernization theory of Max Weber, with concepts also from Karl Marx, Otto Hintze, Gustav Schmoller, Werner Sombart and Thorstein Veblen.
In the 1970s and early 1980s German historians of society, led by Wehler and Jürgen Kocka at the "Bielefeld school" gained dominance in Germany by applying both modernization theories and social science methods. From the 1980s, however, they were increasingly criticized by proponents of the "cultural turn" for not incorporating culture in the history of society, for reducing politics to society, and for reducing individuals to structures. Historians of society inverted the traditional positions they criticized (on the model of Marx's inversion of Hegel). As a result, the problems pertaining to the positions criticized were not resolved but only turned on their heads. The traditional focus on individuals was inverted into a modern focus on structures, the traditional focus on culture was inverted into a modern focus on structures, and traditional emphatic understanding was inverted into modern causal explanation.

Before World War II, political history was in decline and an effort was made to introduce social history in the style of the French Annales School. After the war only Marxist interpretations were allowed. With the end of Communism in Hungary in 1989. Marxist historiography collapsed and social history came into its own, especially the study of the demographic patterns of the early modern period. Research priorities have shifted toward urban history and the conditions of everyday life.

When Communism ended in 1991, large parts of the Soviet archives were opened. The historians' data base leapt from a limited range of sources to a vast array of records created by modern bureaucracies. Social history flourished. The old Marxist historiography collapsed overnight.

Social history had a "golden age" in Canada in the 1970s, and continues to flourish among scholars. Its strengths include demography, women, labour, and urban studies.

While the study of elites and political institutions has produced a vast body of scholarship, the impact after 1960 of social historians has shifted emphasis onto the politics of ordinary people—especially voters and collective movements. Political historians responded with the "new political history," which has shifted attention to political cultures. Some scholars have recently applied a cultural approach to political history. Some political historians complain that social historians are likely to put too much stress on the dimensions of class, gender and race, reflecting a leftist political agenda that assumes outsiders in politics are more interesting than the actual decision makers.
Social history, with its leftist political origins, initially sought to link state power to everyday experience in the 1960s. Yet by the 1970s, social historians increasingly excluded analyses of state power from its focus. Social historians have recently engaged with political history through studies of the relationships between state formation, power and everyday life with the theoretical tools of cultural hegemony and governmentality.

List of history journals
Annales School
History of sociology
Living history and open-air museums

Marc Bloch (1886–1944). Medieval, Annales School
Asa Briggs, Baron Briggs, British
Martin Broszat (1926–1989), Germany
Merle Curti (1897-1997) American
Natalie Zemon Davis, (b. 1928) France
Herbert Gutman (1928-1985), American black and labor history
Eugene D. Genovese (1930-2012), American slavery
Oscar Handlin (1915-2011), American ethnic
Emmanuel Le Roy Ladurie (b. 1929), leader of Annales School, France
Ram Sharan Sharma (1919-2011), India
Stephan Thernstrom (b. 1934), ethnic American; social mobility
Charles Tilly (1929 – 2008), European; theory
Louise A. Tilly (born 1930, Europe; women and family
Eric Hobsbawm (1917-2012), labor history, social movements and resistances
E. P. Thompson (1924–1993), British labour
Hans-Ulrich Wehler (1931-2014), 19th-century Germany, Bielefeld School

Adas, Michael. "Social History and the Revolution in African and Asian Historiography," Journal of Social History 19 (1985): 335-378.
Anderson, Michael. Approaches to the History of the Western Family 1500-1914 (1995) 104pp excerpt and text search
Cabrera, Miguel A. Postsocial History: An Introduction. (2004). 163 pp.
Cayton, Mary Kupiec, Elliott J. Gorn, and Peter W. Williams, eds. Encyclopedia of American Social History (3 vol 1993) 2653pp; long articles pages by leading scholars; see v I: Part II, Methods and Contexts, pp 235–434
Cross, Michael S. "Social History," Canadian Encyclopedia (2008) online
Cross, Michael S. and Kealey, Gregory S., eds. Readings in Canadian Social History (5 vol 1984). 243 pp.
Dewald, Jonathan. Lost Worlds: The Emergence of French Social History, 1815-1970. (2006). 241 pp.
Eley, Geoff. A Crooked Line: From Cultural History to the History of Society. (2005). 301 pp.
Fairburn, Miles. Social History: Problems, Strategies and Methods. (1999). 325 pp.
Fass, Paula, ed. Encyclopedia of Children and Childhood: In History and Society, (3 vols. 2003).
Fletcher, Roger. "Recent Developments in West German Historiography: the Bielefeld School and its Critics." German Studies Review 1984 7(3): 451-480. ISSN 0149-7952 Fulltext: in Jstor
Hareven, Tamara K. "The History of the Family and the Complexity of Social Change," American Historical Review, Feb 1991, Vol. 96 Issue 1, pp 95–124 in JSTOR
Henretta, James. "Social History as Lived and Written," American Historical Review 84 (1979): 1293-1323 in JSTOR
Kanner, Barbara. Women in English Social History, 1800-1914: A Guide to Research (2 vol 1988-1990). 871 pp.
Lloyd, Christopher. Explanation in Social History. (1986). 375 pp.
Lorenz, Chris. "'Won't You Tell Me, Where Have All the Good Times Gone'? On the Advantages and Disadvantages of Modernization Theory for History." Rethinking History 2006 10(2): 171-200. ISSN 1364-2529 Fulltext: Ebsco
Mintz, Steven. Huck's Raft: A History of American Childhood (2006). excerpt and text search
Mintz, Steven and Susan Kellogg. Domestic Revolutions: A Social History Of American Family Life (1989) excerpt and text search
Mosley, Stephen. "Common Ground: Integrating Social and Environmental History," Journal of Social History, Volume 39, Number 3, Spring 2006, pp. 915–933, relations with Environmental History, in Project MUSE
Palmer, Bryan D., and Todd McCallum, "Working-Class History" Canadian Encyclopedia (2008)
Pomeranz, Kenneth. "Social History and World History: from Daily Life to Patterns of Change." Journal of World History 2007 18(1): 69-98. ISSN 1045-6007 Fulltext: in History Cooperative and Project Muse
Stearns, Peter N. "Social History Today ... And Tomorrow," Journal of Social History 10 (1976): 129-155.
Stearns, Peter N. "Social History Present and Future." Journal of Social History. Volume: 37. Issue: 1. (2003). pp 9+. online edition
Stearns, Peter, ed. Encyclopedia of Social History (1994) 856 pp.
Stearns, Peter, ed. Encyclopedia of European Social History from 1350 to 2000 (5 vol 2000), 209 essays by leading scholars in 3000 pp.
Sutherland, Neil. "Childhood, History of," Canadian Encyclopedia (2008)
Hobsbawm, Eric. The Age of Revolution: Europe 1789-1848.
Skocpol, Theda, and Daniel Chirot, eds. Vision and method in historical sociology (1984).
Thompson, E. P. The Essential E. P. Thompson. (2001). 512 pp. highly influential British historian of the working class
Thompson, F. M. L., ed. The Cambridge Social History of Britain, 1750-1950." Vol. 1: Regions and Communities. Vol. 2: People and Their Environment; Vol. 3: Social Agencies and Institutions. (1990). 492 pp.
Tilly, Charles. "The Old New Social History and the New Old Social History," Review 7 (3), Winter 1984: 363-406 (online)
Tilly, Charles. Big Structures, Large Processes, Huge Comparisons (1984).
Timmins, Geoffrey. "The Future of Learning and Teaching in Social History: the Research Approach and Employability." Journal of Social History 2006 39(3): 829-842. ISSN 0022-4529 Fulltext: History Cooperative and Project Muse
Wilson, Adrian, ed. Rethinking Social History: English Society, 1570-1920 and Its Interpretation. (1993). 342 pp.
Zunz, Olivier, ed. Reliving the Past: The Worlds of Social History, (1985) online edition

Binder, Frederick M. and David M. Reimers, eds. The Way We Lived: Essays and Documents in American Social History. (2000). 313 pp.

International Institute of Social History
American Social History Project
Social History Society (UK)
Amsab-Institute of Social History (Belgium)
Victorian-era social history
Society for the social history of medicine
History online
International Association of Labour History Institutions
American Social History Online (19th and 20th digital resources)
StoryCorps: National Social History Project Records Ordinary People Telling Their Stories - video by Democracy Now!Astronomy is a natural science that studies celestial objects and phenomena. It applies mathematics, physics, and chemistry, in an effort to explain the origin of those objects and phenomena and their evolution. Objects of interest include planets, moons, stars, galaxies, and comets; while the phenomena include supernovae explosions, gamma ray bursts, and cosmic microwave background radiation. More generally, all astronomical phenomena that originate outside Earth's atmosphere are within the purview of astronomy. A related but distinct subject, physical cosmology, is concerned with the study of the Universe as a whole.
Astronomy is the oldest of the natural sciences. The early civilizations in recorded history, such as the Babylonians, Greeks, Indians, Egyptians, Nubians, Iranians, Chinese, and Maya performed methodical observations of the night sky. Historically, astronomy has included disciplines as diverse as astrometry, celestial navigation, observational astronomy and the making of calendars, but professional astronomy is now often considered to be synonymous with astrophysics.
During the 20th century, the field of professional astronomy split into observational and theoretical branches. Observational astronomy is focused on acquiring data from observations of astronomical objects, which is then analyzed using basic principles of physics. Theoretical astronomy is oriented toward the development of computer or analytical models to describe astronomical objects and phenomena. The two fields complement each other, with theoretical astronomy seeking to explain the observational results and observations being used to confirm theoretical results.
Astronomy is one of the few sciences where amateurs can still play an active role, especially in the discovery and observation of transient phenomena. Amateur astronomers have made and contributed to many important astronomical discoveries, such as finding new comets.

Astronomy (from the Greek ἀστρονομία from ἄστρον astron, "star" and -νομία -nomia from νόμος nomos, "law" or "culture") means "law of the stars" (or "culture of the stars" depending on the translation). Astronomy should not be confused with astrology, the belief system which claims that human affairs are correlated with the positions of celestial objects. Although the two fields share a common origin, they are now entirely distinct.

Generally, either the term "astronomy" or "astrophysics" may be used to refer to this subject. Based on strict dictionary definitions, "astronomy" refers to "the study of objects and matter outside the Earth's atmosphere and of their physical and chemical properties" and "astrophysics" refers to the branch of astronomy dealing with "the behavior, physical properties, and dynamic processes of celestial objects and phenomena". In some cases, as in the introduction of the introductory textbook The Physical Universe by Frank Shu, "astronomy" may be used to describe the qualitative study of the subject, whereas "astrophysics" is used to describe the physics-oriented version of the subject. However, since most modern astronomical research deals with subjects related to physics, modern astronomy could actually be called astrophysics. Few fields, such as astrometry, are purely astronomy rather than also astrophysics. Various departments in which scientists carry out research on this subject may use "astronomy" and "astrophysics," partly depending on whether the department is historically affiliated with a physics department, and many professional astronomers have physics rather than astronomy degrees. One of the leading scientific journals in the field is the European journal named Astronomy and Astrophysics. The leading American journals are The Astrophysical Journal and The Astronomical Journal.

In early times, astronomy only comprised the observation and predictions of the motions of objects visible to the naked eye. In some locations, early cultures assembled massive artifacts that possibly had some astronomical purpose. In addition to their ceremonial uses, these observatories could be employed to determine the seasons, an important factor in knowing when to plant crops, as well as in understanding the length of the year.
Before tools such as the telescope were invented, early study of the stars was conducted using the naked eye. As civilizations developed, most notably in Mesopotamia, Greece, Persia, India, China, Egypt, and Central America, astronomical observatories were assembled, and ideas on the nature of the Universe began to be explored. Most of early astronomy actually consisted of mapping the positions of the stars and planets, a science now referred to as astrometry. From these observations, early ideas about the motions of the planets were formed, and the nature of the Sun, Moon and the Earth in the Universe were explored philosophically. The Earth was believed to be the center of the Universe with the Sun, the Moon and the stars rotating around it. This is known as the geocentric model of the Universe, or the Ptolemaic system, named after Ptolemy.
A particularly important early development was the beginning of mathematical and scientific astronomy, which began among the Babylonians, who laid the foundations for the later astronomical traditions that developed in many other civilizations. The Babylonians discovered that lunar eclipses recurred in a repeating cycle known as a saros.

Following the Babylonians, significant advances in astronomy were made in ancient Greece and the Hellenistic world. Greek astronomy is characterized from the start by seeking a rational, physical explanation for celestial phenomena. In the 3rd century BC, Aristarchus of Samos estimated the size and distance of the Moon and Sun, and was the first to propose a heliocentric model of the solar system. In the 2nd century BC, Hipparchus discovered precession, calculated the size and distance of the Moon and invented the earliest known astronomical devices such as the astrolabe. Hipparchus also created a comprehensive catalog of 1020 stars, and most of the constellations of the northern hemisphere derive from Greek astronomy. The Antikythera mechanism (c. 150–80 BC) was an early analog computer designed to calculate the location of the Sun, Moon, and planets for a given date. Technological artifacts of similar complexity did not reappear until the 14th century, when mechanical astronomical clocks appeared in Europe.
During the Middle Ages, astronomy was mostly stagnant in medieval Europe, at least until the 13th century. However, astronomy flourished in the Islamic world and other parts of the world. This led to the emergence of the first astronomical observatories in the Muslim world by the early 9th century. In 964, the Andromeda Galaxy, the largest galaxy in the Local Group, was discovered by the Persian astronomer Azophi and first described in his Book of Fixed Stars. The SN 1006 supernova, the brightest apparent magnitude stellar event in recorded history, was observed by the Egyptian Arabic astronomer Ali ibn Ridwan and the Chinese astronomers in 1006. Some of the prominent Islamic (mostly Persian and Arab) astronomers who made significant contributions to the science include Al-Battani, Thebit, Azophi, Albumasar, Biruni, Arzachel, Al-Birjandi, and the astronomers of the Maragheh and Samarkand observatories. Astronomers during that time introduced many Arabic names now used for individual stars. It is also believed that the ruins at Great Zimbabwe and Timbuktu may have housed an astronomical observatory. Europeans had previously believed that there had been no astronomical observation in pre-colonial Middle Ages sub-Saharan Africa but modern discoveries show otherwise.
The Roman Catholic Church gave more financial and social support to the study of astronomy for over six centuries, from the recovery of ancient learning during the late Middle Ages into the Enlightenment, than any other, and, probably, all other, institutions. Among the Church's motives was finding the date for Easter.

During the Renaissance, Nicolaus Copernicus proposed a heliocentric model of the solar system. His work was defended, expanded upon, and corrected by Galileo Galilei and Johannes Kepler. Galileo used telescopes to enhance his observations.
Kepler was the first to devise a system that described correctly the details of the motion of the planets with the Sun at the center. However, Kepler did not succeed in formulating a theory behind the laws he wrote down. It was left to Newton's invention of celestial dynamics and his law of gravitation to finally explain the motions of the planets. Newton also developed the reflecting telescope.
The English astronomer John Flamsteed catalogued over 3000 stars. Further discoveries paralleled the improvements in the size and quality of the telescope. More extensive star catalogues were produced by Lacaille. The astronomer William Herschel made a detailed catalog of nebulosity and clusters, and in 1781 discovered the planet Uranus, the first new planet found. The distance to a star was first announced in 1838 when the parallax of 61 Cygni was measured by Friedrich Bessel.
During the 18–19th centuries, the study of the three body problem by Euler, Clairaut, and D'Alembert led to more accurate predictions about the motions of the Moon and planets. This work was further refined by Lagrange and Laplace, allowing the masses of the planets and moons to be estimated from their perturbations.
Significant advances in astronomy came about with the introduction of new technology, including the spectroscope and photography. Fraunhofer discovered about 600 bands in the spectrum of the Sun in 1814–15, which, in 1859, Kirchhoff ascribed to the presence of different elements. Stars were proven to be similar to the Earth's own Sun, but with a wide range of temperatures, masses, and sizes.
The existence of the Earth's galaxy, the Milky Way, as a separate group of stars, was only proved in the 20th century, along with the existence of "external" galaxies. The observed recession of those galaxies led to the discovery of the expansion of the Universe. Theoretical astronomy led to speculations on the existence of objects such as black holes and neutron stars, which have be used to explain such observed phenomena such as quasars, pulsars, blazars, and radio galaxies. Physical cosmology made huge advances during the 20th century, with the model of the Big Bang, which is heavily supported by evidence provided by cosmic microwave background radiation, Hubble's law, and the cosmological abundances of elements. Space telescopes have enabled measurements in parts of the electromagnetic spectrum normally blocked or blurred by the atmosphere. Recently, in February 2016, it was revealed that the LIGO project had detected evidence of gravitational waves, in September 2015.

Our main source of information about celestial bodies and other objects is visible light more generally electromagnetic radiation. Observational astronomy may be divided according to the observed region of the electromagnetic spectrum. Some parts of the spectrum can be observed from the Earth's surface, while other parts are only observable from either high altitudes or outside the Earth's atmosphere. Specific information on these subfields is given below.

Radio astronomy uses radiation outside the visible range with wavelengths greater than approximately one millimeter. Radio astronomy is different from most other forms of observational astronomy in that the observed radio waves can be treated as waves rather than as discrete photons. Hence, it is relatively easier to measure both the amplitude and phase of radio waves, whereas this is not as easily done at shorter wavelengths.
Although some radio waves are emitted directly by astronomical objects, a product of thermal emission, most of the radio emission that is observed is the result of synchrotron radiation, which is produced when electrons orbit magnetic fields. Additionally, a number of spectral lines produced by interstellar gas, notably the hydrogen spectral line at 21 cm, are observable at radio wavelengths.
A wide variety of objects are observable at radio wavelengths, including supernovae, interstellar gas, pulsars, and active galactic nuclei.

Infrared astronomy is founded on the detection and analysis of infrared radiation, wavelengths longer than red light and outside the range of our vision. The infrared spectrum is useful for studying objects that are too cold to radiate visible light, such as planets, circumstellar disks or nebulae whose light is blocked by dust. The longer wavelengths of infrared can penetrate clouds of dust that block visible light, allowing the observation of young stars embedded in molecular clouds and the cores of galaxies. Observations from the Wide-field Infrared Survey Explorer (WISE) have been particularly effective at unveiling numerous Galactic protostars and their host star clusters. With the exception of infrared wavelengths close to visible light, such radiation is heavily absorbed by the atmosphere, or masked, as the atmosphere itself produces significant infrared emission. Consequently, infrared observatories have to be located in high, dry places on Earth or in space. Some molecules radiate strongly in the infrared. This allows the study of the chemistry of space; more specifically it can detect water in comets.

Historically, optical astronomy, also called visible light astronomy, is the oldest form of astronomy. Images of observations were originally drawn by hand. In the late 19th century and most of the 20th century, images were made using photographic equipment. Modern images are made using digital detectors, particularly using charge-coupled devices (CCDs) and recorded on modern medium. Although visible light itself extends from approximately 4000 Å to 7000 Å (400 nm to 700 nm), that same equipment can be used to observe some near-ultraviolet and near-infrared radiation.

Ultraviolet astronomy employs ultraviolet wavelengths between approximately 100 and 3200 Å (10 to 320 nm). Light at those wavelengths are absorbed by the Earth's atmosphere, requiring observations at these wavelengths to be performed from the upper atmosphere or from space. Ultraviolet astronomy is best suited to the study of thermal radiation and spectral emission lines from hot blue stars (OB stars) that are very bright in this wave band. This includes the blue stars in other galaxies, which have been the targets of several ultraviolet surveys. Other objects commonly observed in ultraviolet light include planetary nebulae, supernova remnants, and active galactic nuclei. However, as ultraviolet light is easily absorbed by interstellar dust, an adjustment of ultraviolet measurements is necessary.

X-ray astronomy uses X-ray wavelengths. Typically, X-ray radiation is produced by synchrotron emission (the result of electrons orbiting magnetic field lines), thermal emission from thin gases above 107 (10 million) kelvins, and thermal emission from thick gases above 107 Kelvin. Since X-rays are absorbed by the Earth's atmosphere, all X-ray observations must be performed from high-altitude balloons, rockets, or X-ray astronomy satellites. Notable X-ray sources include X-ray binaries, pulsars, supernova remnants, elliptical galaxies, clusters of galaxies, and active galactic nuclei.

Gamma ray astronomy observes astronomical objects at the shortest wavelengths of the electromagnetic spectrum. Gamma rays may be observed directly by satellites such as the Compton Gamma Ray Observatory or by specialized telescopes called atmospheric Cherenkov telescopes. The Cherenkov telescopes do not detect the gamma rays directly but instead detect the flashes of visible light produced when gamma rays are absorbed by the Earth's atmosphere.
Most gamma-ray emitting sources are actually gamma-ray bursts, objects which only produce gamma radiation for a few milliseconds to thousands of seconds before fading away. Only 10% of gamma-ray sources are non-transient sources. These steady gamma-ray emitters include pulsars, neutron stars, and black hole candidates such as active galactic nuclei.

In addition to electromagnetic radiation, a few other events originating from great distances may be observed from the Earth.
In neutrino astronomy, astronomers use heavily shielded underground facilities such as SAGE, GALLEX, and Kamioka II/III for the detection of neutrinos. The vast majority of the neutrinos streaming through the Earth originate from the Sun, but 24 neutrinos were also detected from supernova 1987A. Cosmic rays, which consist of very high energy particles (atomic nuclei) that can decay or be absorbed when they enter the Earth's atmosphere, result in a cascade of secondary particles which can be detected by current observatories. Some future neutrino detectors may also be sensitive to the particles produced when cosmic rays hit the Earth's atmosphere.
Gravitational-wave astronomy is an emerging field of astronomy that employs gravitational-wave detectors to collect observational data about distant massive objects. A few observatories have been constructed, such as the Laser Interferometer Gravitational Observatory LIGO. LIGO made its first detection on 14 September 2015, observing gravitational waves from a binary black hole. A second gravitational wave was detected on 26 December 2015 and additional observations should continue but gravitational waves require extremely sensitive instruments.
The combination of observations made using electromagnetic radiation, neutrinos or gravitational waves and other complementary information, is known as multi-messenger astronomy.

One of the oldest fields in astronomy, and in all of science, is the measurement of the positions of celestial objects. Historically, accurate knowledge of the positions of the Sun, Moon, planets and stars has been essential in celestial navigation (the use of celestial objects to guide navigation) and in the making of calendars.
Careful measurement of the positions of the planets has led to a solid understanding of gravitational perturbations, and an ability to determine past and future positions of the planets with great accuracy, a field known as celestial mechanics. More recently the tracking of near-Earth objects will allow for predictions of close encounters or potential collisions of the Earth with those objects.
The measurement of stellar parallax of nearby stars provides a fundamental baseline in the cosmic distance ladder that is used to measure the scale of the Universe. Parallax measurements of nearby stars provide an absolute baseline for the properties of more distant stars, as their properties can be compared. Measurements of the radial velocity and proper motion motion of stars allows astronomers to plot the movement of these systems through the Milky Way galaxy. Astrometric results are the basis used to calculate the distribution of speculated dark matter in the galaxy.
During the 1990s, the measurement of the stellar wobble of nearby stars was used to detect large extrasolar planets orbiting those stars.

Theoretical astronomers use several tools including analytical models and computational numerical simulations; each has its particular advantages. Analytical models of a process are generally better for giving broader insight into the heart of what is going on. Numerical models reveal the existence of phenomena and effects otherwise unobserved.
Theorists in astronomy endeavor to create theoretical models and from the results predict observational consequences of those models. The observation of a phenomenon predicted by a model allows astronomers to select between several alternate or conflicting models as the one best able to describe the phenomena.
Theorists also try to generate or modify models to take into account new data. In the case of an inconsistency between the data and model's results, the general tendency is to try to make minimal modifications to the model so that it produces results that fit the data. In some cases, a large amount of inconsistent data over time may lead to total abandonment of a model.
Phenomena modeled by theoretical astronomers include: stellar dynamics and evolution; galaxy formation; large-scale distribution of matter in the Universe; origin of cosmic rays; general relativity and physical cosmology, including string cosmology and astroparticle physics. Astrophysical relativity serves as a tool to gauge the properties of large scale structures for which gravitation plays a significant role in physical phenomena investigated and as the basis for black hole (astro)physics and the study of gravitational waves.
Some widely accepted and studied theories and models in astronomy, now included in the Lambda-CDM model are the Big Bang, Cosmic inflation, dark matter, and fundamental theories of physics.
A few examples of this process:
Dark matter and dark energy are the current leading topics in astronomy, as their discovery and controversy originated during the study of the galaxies.

At a distance of about eight light-minutes, the most frequently studied star is the Sun, a typical main-sequence dwarf star of stellar class G2 V, and about 4.6 billion years (Gyr) old. The Sun is not considered a variable star, but it does undergo periodic changes in activity known as the sunspot cycle. This is an 11-year oscillation in sunspot number. Sunspots are regions of lower-than- average temperatures that are associated with intense magnetic activity.
The Sun has steadily increased in luminosity by 40% since it first became a main-sequence star. The Sun has also undergone periodic changes in luminosity that can have a significant impact on the Earth. The Maunder minimum, for example, is believed to have caused the Little Ice Age phenomenon during the Middle Ages.
The visible outer surface of the Sun is called the photosphere. Above this layer is a thin region known as the chromosphere. This is surrounded by a transition region of rapidly increasing temperatures, and finally by the super-heated corona.
At the center of the Sun is the core region, a volume of sufficient temperature and pressure for nuclear fusion to occur. Above the core is the radiation zone, where the plasma conveys the energy flux by means of radiation. Above that is the convection zone where the gas material transports energy primarily through physical displacement of the gas known as convection. It is believed that the movement of mass within the convection zone creates the magnetic activity that generates sunspots.
A solar wind of plasma particles constantly streams outward from the Sun until, at the outermost limit of the Solar System, it reaches the heliopause. As the solar wind passes the Earth, it interacts with the Earth's magnetic field (magnetosphere) and deflects the solar wind, but traps some creating the Van Allen radiation belts that envelop the Earth . The aurora are created when solar wind particles are guided by the magnetic flux lines into the Earth's polar regions where the lines the descend into the atmosphere.

Planetary science is the study of the assemblage of planets, moons, dwarf planets, comets, asteroids, and other bodies orbiting the Sun, as well as extrasolar planets. The Solar System has been relatively well-studied, initially through telescopes and then later by spacecraft. This has provided a good overall understanding of the formation and evolution of this planetary system, although many new discoveries are still being made.

The Solar System is subdivided into the inner planets, the asteroid belt, and the outer planets. The inner terrestrial planets consist of Mercury, Venus, Earth, and Mars. The outer gas giant planets are Jupiter, Saturn, Uranus, and Neptune. Beyond Neptune lies the Kuiper Belt, and finally the Oort Cloud, which may extend as far as a light-year.
The planets were formed 4.6 billion years ago in the protoplanetary disk that surrounded the early Sun. Through a process that included gravitational attraction, collision, and accretion, the disk formed clumps of matter that, with time, became protoplanets. The radiation pressure of the solar wind then expelled most of the unaccreted matter, and only those planets with sufficient mass retained their gaseous atmosphere. The planets continued to sweep up, or eject, the remaining matter during a period of intense bombardment, evidenced by the many impact craters on the Moon. During this period, some of the protoplanets may have collided and one such collision may have formed the Moon.
Once a planet reaches sufficient mass, the materials of different densities segregate within, during planetary differentiation. This process can form a stony or metallic core, surrounded by a mantle and an outer crust. The core may include solid and liquid regions, and some planetary cores generate their own magnetic field, which can protect their atmospheres from solar wind stripping.
A planet or moon's interior heat is produced from the collisions that created the body, by the decay of radioactive materials (e.g. uranium, thorium, and 26Al), or tidal heating caused by interactions with other bodies. Some planets and moons accumulate enough heat to drive geologic processes such as volcanism and tectonics. Those that accumulate or retain an atmosphere can also undergo surface erosion from wind or water. Smaller bodies, without tidal heating, cool more quickly; and their geological activity ceases with the exception of impact cratering.

The study of stars and stellar evolution is fundamental to our understanding of the Universe. The astrophysics of stars has been determined through observation and theoretical understanding; and from computer simulations of the interior. Star formation occurs in dense regions of dust and gas, known as giant molecular clouds. When destabilized, cloud fragments can collapse under the influence of gravity, to form a protostar. A sufficiently dense, and hot, core region will trigger nuclear fusion, thus creating a main-sequence star.
Almost all elements heavier than hydrogen and helium were created inside the cores of stars.
The characteristics of the resulting star depend primarily upon its starting mass. The more massive the star, the greater its luminosity, and the more rapidly it fuses its hydrogen fuel into helium in its core. Over time, this hydrogen fuel is completely converted into helium, and the star begins to evolve. The fusion of helium requires a higher core temperature. A star with a high enough core temperature will push its outer layers outward while increasing its core density. The resulting red giant formed by the expanding outer layers enjoys a brief life span, before the helium fuel in the core is in turn consumed. Very massive stars can also undergo a series of evolutionary phases, as they fuse increasingly heavier elements.
The final fate of the star depends on its mass, with stars of mass greater than about eight times the Sun becoming core collapse supernovae; while smaller stars blow off their outer layers and leave behind the inert core in the form of a white dwarf. The ejection of the outer layers forms a planetary nebulae. The remnant of a supernova is a dense neutron star, or, if the stellar mass was at least three times that of the Sun, a black hole. Closely orbiting binary stars can follow more complex evolutionary paths, such as mass transfer onto a white dwarf companion that can potentially cause a supernova. Planetary nebulae and supernovae distribute the "metals" produced in the star by fusion to the interstellar medium; without them, all new stars (and their planetary systems) would be formed from hydrogen and helium alone.

Our solar system orbits within the Milky Way, a barred spiral galaxy that is a prominent member of the Local Group of galaxies. It is a rotating mass of gas, dust, stars and other objects, held together by mutual gravitational attraction. As the Earth is located within the dusty outer arms, there are large portions of the Milky Way that are obscured from view.
In the center of the Milky Way is the core, a bar-shaped bulge with what is believed to be a supermassive black hole at its center. This is surrounded by four primary arms that spiral from the core. This is a region of active star formation that contains many younger, population I stars. The disk is surrounded by a spheroid halo of older, population II stars, as well as relatively dense concentrations of stars known as globular clusters.
Between the stars lies the interstellar medium, a region of sparse matter. In the densest regions, molecular clouds of molecular hydrogen and other elements create star-forming regions. These begin as a compact pre-stellar core or dark nebulae, which concentrate and collapse (in volumes determined by the Jeans length) to form compact protostars.
As the more massive stars appear, they transform the cloud into an H II region (ionized atomic hydrogen) of glowing gas and plasma. The stellar wind and supernova explosions from these stars eventually cause the cloud to disperse, often leaving behind one or more young open clusters of stars. These clusters gradually disperse, and the stars join the population of the Milky Way.
Kinematic studies of matter in the Milky Way and other galaxies have demonstrated that there is more mass than can be accounted for by visible matter. A dark matter halo appears to dominate the mass, although the nature of this dark matter remains undetermined.

The study of objects outside our galaxy is a branch of astronomy concerned with the formation and evolution of Galaxies, their morphology (description) and classification, the observation of active galaxies, and at a larger scale, the groups and clusters of galaxies. Finally, the latter is important for the understanding of the large-scale structure of the cosmos.
Most galaxies are organized into distinct shapes that allow for classification schemes. They are commonly divided into spiral, elliptical and Irregular galaxies.
As the name suggests, an elliptical galaxy has the cross-sectional shape of an ellipse. The stars move along random orbits with no preferred direction. These galaxies contain little or no interstellar dust, few star-forming regions, and generally older stars. Elliptical galaxies are more commonly found at the core of galactic clusters, and may have been formed through mergers of large galaxies.
A spiral galaxy is organized into a flat, rotating disk, usually with a prominent bulge or bar at the center, and trailing bright arms that spiral outward. The arms are dusty regions of star formation within which massive young stars produce a blue tint. Spiral galaxies are typically surrounded by a halo of older stars. Both the Milky Way and one of our nearest galaxy neighbors, the Andromeda Galaxy, are spiral galaxies.
Irregular galaxies are chaotic in appearance, and are neither spiral nor elliptical. About a quarter of all galaxies are irregular, and the peculiar shapes of such galaxies may be the result of gravitational interaction.
An active galaxy is a formation that emits a significant amount of its energy from a source other than its stars, dust and gas. It is powered by a compact region at the core, thought to be a super-massive black hole that is emitting radiation from in-falling material.
A radio galaxy is an active galaxy that is very luminous in the radio portion of the spectrum, and is emitting immense plumes or lobes of gas. Active galaxies that emit shorter frequency, high-energy radiation include Seyfert galaxies, Quasars, and Blazars. Quasars are believed to be the most consistently luminous objects in the known universe.
The large-scale structure of the cosmos is represented by groups and clusters of galaxies. This structure is organized into a hierarchy of groupings, with the largest being the superclusters. The collective matter is formed into filaments and walls, leaving large voids between.

Cosmology (from the Greek κόσμος (kosmos) "world, universe" and λόγος (logos) "word, study" or literally "logic") could be considered the study of the Universe as a whole.

Observations of the large-scale structure of the Universe, a branch known as physical cosmology, have provided a deep understanding of the formation and evolution of the cosmos. Fundamental to modern cosmology is the well-accepted theory of the big bang, wherein our Universe began at a single point in time, and thereafter expanded over the course of 13.8 billion years to its present condition. The concept of the big bang can be traced back to the discovery of the microwave background radiation in 1965.
In the course of this expansion, the Universe underwent several evolutionary stages. In the very early moments, it is theorized that the Universe experienced a very rapid cosmic inflation, which homogenized the starting conditions. Thereafter, nucleosynthesis produced the elemental abundance of the early Universe. (See also nucleocosmochronology.)
When the first neutral atoms formed from a sea of primordial ions, space became transparent to radiation, releasing the energy viewed today as the microwave background radiation. The expanding Universe then underwent a Dark Age due to the lack of stellar energy sources.
A hierarchical structure of matter began to form from minute variations in the mass density of space. Matter accumulated in the densest regions, forming clouds of gas and the earliest stars, the Population III stars. These massive stars triggered the reionization process and are believed to have created many of the heavy elements in the early Universe, which, through nuclear decay, create lighter elements, allowing the cycle of nucleosynthesis to continue longer.
Gravitational aggregations clustered into filaments, leaving voids in the gaps. Gradually, organizations of gas and dust merged to form the first primitive galaxies. Over time, these pulled in more matter, and were often organized into groups and clusters of galaxies, then into larger-scale superclusters.
Fundamental to the structure of the Universe is the existence of dark matter and dark energy. These are now thought to be its dominant components, forming 96% of the mass of the Universe. For this reason, much effort is expended in trying to understand the physics of these components.

Astronomy and astrophysics have developed significant interdisciplinary links with other major scientific fields. Archaeoastronomy is the study of ancient or traditional astronomies in their cultural context, utilizing archaeological and anthropological evidence. Astrobiology is the study of the advent and evolution of biological systems in the Universe, with particular emphasis on the possibility of non-terrestrial life. Astrostatistics is the application of statistics to astrophysics to the analysis of vast amount of observational astrophysical data.
The study of chemicals found in space, including their formation, interaction and destruction, is called astrochemistry. These substances are usually found in molecular clouds, although they may also appear in low temperature stars, brown dwarfs and planets. Cosmochemistry is the study of the chemicals found within the Solar System, including the origins of the elements and variations in the isotope ratios. Both of these fields represent an overlap of the disciplines of astronomy and chemistry. As "forensic astronomy", finally, methods from astronomy have been used to solve problems of law and history.

Astronomy is one of the sciences to which amateurs can contribute the most.
Collectively, amateur astronomers observe a variety of celestial objects and phenomena sometimes with equipment that they build themselves. Common targets of amateur astronomers include the Moon, planets, stars, comets, meteor showers, and a variety of deep-sky objects such as star clusters, galaxies, and nebulae. Astronomy clubs are located throughout the world and many have programs to help their members set up and complete observational programs including those to observe all the objects in the Messier (110 objects) or Herschel 400 catalogues of points of interest in the night sky. One branch of amateur astronomy, amateur astrophotography, involves the taking of photos of the night sky. Many amateurs like to specialize in the observation of particular objects, types of objects, or types of events which interest them.
Most amateurs work at visible wavelengths, but a small minority experiment with wavelengths outside the visible spectrum. This includes the use of infrared filters on conventional telescopes, and also the use of radio telescopes. The pioneer of amateur radio astronomy was Karl Jansky, who started observing the sky at radio wavelengths in the 1930s. A number of amateur astronomers use either homemade telescopes or use radio telescopes which were originally built for astronomy research but which are now available to amateurs (e.g. the One-Mile Telescope).
Amateur astronomers continue to make scientific contributions to the field of astronomy and it is one of the few scientific disciplines where amateurs can still make significant contributions. Amateurs can make occultation measurements that are used to refine the orbits of minor planets. They can also discover comets, and perform regular observations of variable stars. Improvements in digital technology have allowed amateurs to make impressive advances in the field of astrophotography.

Although the scientific discipline of astronomy has made tremendous strides in understanding the nature of the Universe and its contents, there remain some important unanswered questions. Answers to these may require the construction of new ground- and space-based instruments, and possibly new developments in theoretical and experimental physics.
What is the origin of the stellar mass spectrum? That is, why do astronomers observe the same distribution of stellar masses – the initial mass function – apparently regardless of the initial conditions? A deeper understanding of the formation of stars and planets is needed.
Is there other life in the Universe? Especially, is there other intelligent life? If so, what is the explanation for the Fermi paradox? The existence of life elsewhere has important scientific and philosophical implications. Is the Solar System normal or atypical?
What is the nature of dark matter and dark energy? These dominate the evolution and fate of the cosmos, yet their true nature remains unknown. What will be the ultimate fate of the universe?
How did the first galaxies form? How did supermassive black holes form?
What is creating the ultra-high-energy cosmic rays?
Why is the abundance of lithium in the cosmos four times lower than predicted by the standard Big Bang model?
What really happens beyond the event horizon?

Forbes, George (1909). History of Astronomy. London: Plain Label Books. ISBN 1-60303-159-6.  Available at Project Gutenberg,Google books
Harpaz, Amos (1994). Stellar Evolution. A K Peters, Ltd. ISBN 978-1-56881-012-6.
Unsöld, A.; Baschek, B. (2001). The New Cosmos: An Introduction to Astronomy and Astrophysics. Springer. ISBN 3-540-67877-8.

NASA/IPAC Extragalactic Database (NED) (NED-Distances)
International Year of Astronomy 2009 IYA2009 Main website
Cosmic Journey: A History of Scientific Cosmology from the American Institute of Physics
Southern Hemisphere Astronomy
Celestia Motherlode Educational site for Astronomical journeys through space
Kroto, Harry, Astrophysical Chemistry Lecture Series.
Core books and Core journals in Astronomy, from the Smithsonian/NASA Astrophysics Data System
A Journey with Fred Hoyle by Wickramasinghe, Chandra.
Astronomy books from the History of Science Collection at Linda Hall LibraryIn physics, energy is a property of objects which can be transferred to other objects or converted into different forms but never created or destroyed. The amount of energy constitutes a fundamental limitation on the capacity of a system to perform work, or to provide heat. The SI unit of energy is the joule, which is the energy transferred to an object by the mechanical work of moving it a distance of 1 metre against a force of 1 newton.
Common energy forms include the kinetic energy of a moving object, the potential energy stored by an object's position in a force field (gravitational, electric or magnetic), the elastic energy stored by stretching solid objects, the chemical energy released when a fuel burns, the radiant energy carried by light, and the thermal energy due to an object's temperature. All of the many forms of energy are convertible to other kinds of energy. In Newtonian physics, there is a universal law of conservation of energy which says that energy can be neither created nor be destroyed; however, it can change from one form to another.
For "closed systems" with no external source or sink of energy, the first law of thermodynamics states that a system's energy is constant unless energy is transferred in or out by mechanical work or heat, and that no energy is lost in transfer. This means that it is impossible to create or destroy energy. While heat can always be fully converted into work in a reversible isothermal expansion of an ideal gas, for cyclic processes of practical interest in heat engines the second law of thermodynamics states that the system doing work always loses some energy as waste heat. This creates a limit to the amount of heat energy that can do work in a cyclic process, a limit called the available energy. Mechanical and other forms of energy can be transformed in the other direction into thermal energy without such limitations. The total energy of a system can be calculated by adding up all forms of energy in the system.
Examples of energy transformation include generating electric energy from heat energy via a steam turbine, or lifting an object against gravity using electrical energy driving a crane motor. Lifting against gravity performs mechanical work on the object and stores gravitational potential energy in the object. If the object falls to the ground, gravity does mechanical work on the object which transforms the potential energy in the gravitational field to the kinetic energy released as heat on impact with the ground. Our Sun transforms nuclear potential energy to other forms of energy; its total mass does not decrease due to that in itself (since it still contains the same total energy even if in different forms), but its mass does decrease when the energy escapes out to its surroundings, largely as radiant energy.
Mass and energy are closely related. Due to mass–energy equivalence, any object that has mass when stationary in a frame of reference (called rest mass) also has an equivalent amount of energy whose form is called rest energy in that frame, and any additional energy acquired by the object above that rest energy will increase an object's mass. For example, with a sensitive enough scale, one could measure an increase in mass after heating an object.
Because energy exists in many interconvertible forms, and yet can't be created or destroyed, its measurement may be equivalently "defined" and quantified via its transfer or conversions into various forms that may be found to be convenient or pedagogic or to facilitate accurate measurement; for example by energy transfer in the form of work (as measured via forces and acceleration) or heat (as measured via temperature changes of materials) or into particular forms such as kinetic (as measured via mass and speed) or by its equivalent mass.
Living organisms require available energy to stay alive, such as the energy humans get from food. Civilisation gets the energy it needs from energy resources such as fossil fuels, nuclear fuel, or renewable energy. The processes of Earth's climate and ecosystem are driven by the radiant energy Earth receives from the sun and the geothermal energy contained within the earth.
In biology, energy can be thought of as what's needed to keep entropy low.

The total energy of a system can be subdivided and classified in various ways. For example, classical mechanics distinguishes between kinetic energy, which is determined by an object's movement through space, and potential energy, which is a function of the position of an object within a field. It may also be convenient to distinguish gravitational energy, thermal energy, several types of nuclear energy (which utilize potentials from the nuclear force and the weak force), electric energy (from the electric field), and magnetic energy (from the magnetic field), among others. Many of these classifications overlap; for instance, thermal energy usually consists partly of kinetic and partly of potential energy.
Some types of energy are a varying mix of both potential and kinetic energy. An example is mechanical energy which is the sum of (usually macroscopic) kinetic and potential energy in a system. Elastic energy in materials is also dependent upon electrical potential energy (among atoms and molecules), as is chemical energy, which is stored and released from a reservoir of electrical potential energy between electrons, and the molecules or atomic nuclei that attract them..The list is also not necessarily complete. Whenever physical scientists discover that a certain phenomenon appears to violate the law of energy conservation, new forms are typically added that account for the discrepancy.
Heat and work are special cases in that they are not properties of systems, but are instead properties of processes that transfer energy. In general we cannot measure how much heat or work are present in an object, but rather only how much energy is transferred among objects in certain ways during the occurrence of a given process. Heat and work are measured as positive or negative depending on which side of the transfer we view them from.
Potential energies are often measured as positive or negative depending on whether they are greater or less than the energy of a specified base state or configuration such as two interacting bodies being infinitely far apart. Wave energies (such as radiant or sound energy), kinetic energy, and rest energy are each greater than or equal to zero because they are measured in comparison to a base state of zero energy: "no wave", "no motion", and "no inertia", respectively.
The distinctions between different kinds of energy is not always clear-cut. As Richard Feynman points out:

These notions of potential and kinetic energy depend on a notion of length scale. For example, one can speak of macroscopic potential and kinetic energy, which do not include thermal potential and kinetic energy. Also what is called chemical potential energy is a macroscopic notion, and closer examination shows that it is really the sum of the potential and kinetic energy on the atomic and subatomic scale. Similar remarks apply to nuclear "potential" energy and most other forms of energy. This dependence on length scale is non-problematic if the various length scales are decoupled, as is often the case ... but confusion can arise when different length scales are coupled, for instance when friction converts macroscopic work into microscopic thermal energy.

Some examples of different kinds of energy:

The word energy derives from the Ancient Greek: ἐνέργεια energeia "activity, operation", which possibly appears for the first time in the work of Aristotle in the 4th century BC. In contrast to the modern definition, energeia was a qualitative philosophical concept, broad enough to include ideas such as happiness and pleasure.
In the late 17th century, Gottfried Leibniz proposed the idea of the Latin: vis viva, or living force, which defined as the product of the mass of an object and its velocity squared; he believed that total vis viva was conserved. To account for slowing due to friction, Leibniz theorized that thermal energy consisted of the random motion of the constituent parts of matter, a view shared by Isaac Newton, although it would be more than a century until this was generally accepted. The modern analog of this property, kinetic energy, differs from vis viva only by a factor of two.
In 1807, Thomas Young was possibly the first to use the term "energy" instead of vis viva, in its modern sense. Gustave-Gaspard Coriolis described "kinetic energy" in 1829 in its modern sense, and in 1853, William Rankine coined the term "potential energy". The law of conservation of energy was also first postulated in the early 19th century, and applies to any isolated system. It was argued for some years whether heat was a physical substance, dubbed the caloric, or merely a physical quantity, such as momentum. In 1845 James Prescott Joule discovered the link between mechanical work and the generation of heat.
These developments led to the theory of conservation of energy, formalized largely by William Thomson (Lord Kelvin) as the field of thermodynamics. Thermodynamics aided the rapid development of explanations of chemical processes by Rudolf Clausius, Josiah Willard Gibbs, and Walther Nernst. It also led to a mathematical formulation of the concept of entropy by Clausius and to the introduction of laws of radiant energy by Jožef Stefan. According to Noether's theorem, the conservation of energy is a consequence of the fact that the laws of physics do not change over time. Thus, since 1918, theorists have understood that the law of conservation of energy is the direct mathematical consequence of the translational symmetry of the quantity conjugate to energy, namely time.

In 1843 James Prescott Joule independently discovered the mechanical equivalent in a series of experiments. The most famous of them used the "Joule apparatus": a descending weight, attached to a string, caused rotation of a paddle immersed in water, practically insulated from heat transfer. It showed that the gravitational potential energy lost by the weight in descending was equal to the internal energy gained by the water through friction with the paddle.
In the International System of Units (SI), the unit of energy is the joule, named after James Prescott Joule. It is a derived unit. It is equal to the energy expended (or work done) in applying a force of one newton through a distance of one metre. However energy is also expressed in many other units not part of the SI, such as ergs, calories, British Thermal Units, kilowatt-hours and kilocalories, which require a conversion factor when expressed in SI units.
The SI unit of energy rate (energy per unit time) is the watt, which is a joule per second. Thus, one joule is one watt-second, and 3600 joules equal one watt-hour. The CGS energy unit is the erg and the imperial and US customary unit is the foot pound. Other energy units such as the electronvolt, food calorie or thermodynamic kcal (based on the temperature change of water in a heating process), and BTU are used in specific areas of science and commerce.

In classical mechanics, energy is a conceptually and mathematically useful property, as it is a conserved quantity. Several formulations of mechanics have been developed using energy as a core concept.
Work, a form of energy, is force times distance.

is equal to the line integral of the force F along a path C; for details see the mechanical work article. Work and thus energy is frame dependent. For example, consider a ball being hit by a bat. In the center-of-mass reference frame, the bat does no work on the ball. But, in the reference frame of the person swinging the bat, considerable work is done on the ball.
The total energy of a system is sometimes called the Hamiltonian, after William Rowan Hamilton. The classical equations of motion can be written in terms of the Hamiltonian, even for highly complex or abstract systems. These classical equations have remarkably direct analogs in nonrelativistic quantum mechanics.
Another energy-related concept is called the Lagrangian, after Joseph-Louis Lagrange. This formalism is as fundamental as the Hamiltonian, and both can be used to derive the equations of motion or be derived from them. It was invented in the context of classical mechanics, but is generally useful in modern physics. The Lagrangian is defined as the kinetic energy minus the potential energy. Usually, the Lagrange formalism is mathematically more convenient than the Hamiltonian for non-conservative systems (such as systems with friction).
Noether's theorem (1918) states that any differentiable symmetry of the action of a physical system has a corresponding conservation law. Noether's theorem has become a fundamental tool of modern theoretical physics and the calculus of variations. A generalisation of the seminal formulations on constants of motion in Lagrangian and Hamiltonian mechanics (1788 and 1833, respectively), it does not apply to systems that cannot be modeled with a Lagrangian; for example, dissipative systems with continuous symmetries need not have a corresponding conservation law.

In the context of chemistry, energy is an attribute of a substance as a consequence of its atomic, molecular or aggregate structure. Since a chemical transformation is accompanied by a change in one or more of these kinds of structure, it is invariably accompanied by an increase or decrease of energy of the substances involved. Some energy is transferred between the surroundings and the reactants of the reaction in the form of heat or light; thus the products of a reaction may have more or less energy than the reactants. A reaction is said to be exergonic if the final state is lower on the energy scale than the initial state; in the case of endergonic reactions the situation is the reverse. Chemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy. The speed of a chemical reaction (at given temperature T) is related to the activation energy E, by the Boltzmann's population factor e−E/kT – that is the probability of molecule to have energy greater than or equal to E at the given temperature T. This exponential dependence of a reaction rate on temperature is known as the Arrhenius equation.The activation energy necessary for a chemical reaction can be in the form of thermal energy.

In biology, energy is an attribute of all biological systems from the biosphere to the smallest living organism. Within an organism it is responsible for growth and development of a biological cell or an organelle of a biological organism. Energy is thus often said to be stored by cells in the structures of molecules of substances such as carbohydrates (including sugars), lipids, and proteins, which release energy when reacted with oxygen in respiration. In human terms, the human equivalent (H-e) (Human energy conversion) indicates, for a given amount of energy expenditure, the relative quantity of energy needed for human metabolism, assuming an average human energy expenditure of 12,500 kJ per day and a basal metabolic rate of 80 watts. For example, if our bodies run (on average) at 80 watts, then a light bulb running at 100 watts is running at 1.25 human equivalents (100 ÷ 80) i.e. 1.25 H-e. For a difficult task of only a few seconds' duration, a person can put out thousands of watts, many times the 746 watts in one official horsepower. For tasks lasting a few minutes, a fit human can generate perhaps 1,000 watts. For an activity that must be sustained for an hour, output drops to around 300; for an activity kept up all day, 150 watts is about the maximum. The human equivalent assists understanding of energy flows in physical and biological systems by expressing energy units in human terms: it provides a "feel" for the use of a given amount of energy.
Sunlight is also captured by plants as chemical potential energy in photosynthesis, when carbon dioxide and water (two low-energy compounds) are converted into the high-energy compounds carbohydrates, lipids, and proteins. Plants also release oxygen during photosynthesis, which is utilized by living organisms as an electron acceptor, to release the energy of carbohydrates, lipids, and proteins. Release of the energy stored during photosynthesis as heat or light may be triggered suddenly by a spark, in a forest fire, or it may be made available more slowly for animal or human metabolism, when these molecules are ingested, and catabolism is triggered by enzyme action.
Any living organism relies on an external source of energy—radiation from the Sun in the case of green plants, chemical energy in some form in the case of animals—to be able to grow and reproduce. The daily 1500–2000 Calories (6–8 MJ) recommended for a human adult are taken as a combination of oxygen and food molecules, the latter mostly carbohydrates and fats, of which glucose (C6H12O6) and stearin (C57H110O6) are convenient examples. The food molecules are oxidised to carbon dioxide and water in the mitochondria

C6H12O6 + 6O2 → 6CO2 + 6H2O
C57H110O6 + 81.5O2 → 57CO2 + 55H2O

and some of the energy is used to convert ADP into ATP.

ADP + HPO42− → ATP + H2O

The rest of the chemical energy in O2 and the carbohydrate or fat is converted into heat: the ATP is used as a sort of "energy currency", and some of the chemical energy it contains is used for other metabolism when ATP reacts with OH groups and eventually splits into ADP and phosphate (at each stage of a metabolic pathway, some chemical energy is converted into heat). Only a tiny fraction of the original chemical energy is used for work:
gain in kinetic energy of a sprinter during a 100 m race: 4 kJ
gain in gravitational potential energy of a 150 kg weight lifted through 2 metres: 3 kJ
Daily food intake of a normal adult: 6–8 MJ
It would appear that living organisms are remarkably inefficient (in the physical sense) in their use of the energy they receive (chemical energy or radiation), and it is true that most real machines manage higher efficiencies. In growing organisms the energy that is converted to heat serves a vital purpose, as it allows the organism tissue to be highly ordered with regard to the molecules it is built from. The second law of thermodynamics states that energy (and matter) tends to become more evenly spread out across the universe: to concentrate energy (or matter) in one specific place, it is necessary to spread out a greater amount of energy (as heat) across the remainder of the universe ("the surroundings"). Simpler organisms can achieve higher energy efficiencies than more complex ones, but the complex organisms can occupy ecological niches that are not available to their simpler brethren. The conversion of a portion of the chemical energy to heat at each step in a metabolic pathway is the physical reason behind the pyramid of biomass observed in ecology: to take just the first step in the food chain, of the estimated 124.7 Pg/a of carbon that is fixed by photosynthesis, 64.3 Pg/a (52%) are used for the metabolism of green plants, i.e. reconverted into carbon dioxide and heat.

In geology, continental drift, mountain ranges, volcanoes, and earthquakes are phenomena that can be explained in terms of energy transformations in the Earth's interior, while meteorological phenomena like wind, rain, hail, snow, lightning, tornadoes and hurricanes are all a result of energy transformations brought about by solar energy on the atmosphere of the planet Earth.
Sunlight may be stored as gravitational potential energy after it strikes the Earth, as (for example) water evaporates from oceans and is deposited upon mountains (where, after being released at a hydroelectric dam, it can be used to drive turbines or generators to produce electricity). Sunlight also drives many weather phenomena, save those generated by volcanic events. An example of a solar-mediated weather event is a hurricane, which occurs when large unstable areas of warm ocean, heated over months, give up some of their thermal energy suddenly to power a few days of violent air movement.
In a slower process, radioactive decay of atoms in the core of the Earth releases heat. This thermal energy drives plate tectonics and may lift mountains, via orogenesis. This slow lifting represents a kind of gravitational potential energy storage of the thermal energy, which may be later released to active kinetic energy in landslides, after a triggering event. Earthquakes also release stored elastic potential energy in rocks, a store that has been produced ultimately from the same radioactive heat sources. Thus, according to present understanding, familiar events such as landslides and earthquakes release energy that has been stored as potential energy in the Earth's gravitational field or elastic strain (mechanical potential energy) in rocks. Prior to this, they represent release of energy that has been stored in heavy atoms since the collapse of long-destroyed supernova stars created these atoms.

In cosmology and astronomy the phenomena of stars, nova, supernova, quasars and gamma-ray bursts are the universe's highest-output energy transformations of matter. All stellar phenomena (including solar activity) are driven by various kinds of energy transformations. Energy in such transformations is either from gravitational collapse of matter (usually molecular hydrogen) into various classes of astronomical objects (stars, black holes, etc.), or from nuclear fusion (of lighter elements, primarily hydrogen). The nuclear fusion of hydrogen in the Sun also releases another store of potential energy which was created at the time of the Big Bang. At that time, according to theory, space expanded and the universe cooled too rapidly for hydrogen to completely fuse into heavier elements. This meant that hydrogen represents a store of potential energy that can be released by fusion. Such a fusion process is triggered by heat and pressure generated from gravitational collapse of hydrogen clouds when they produce stars, and some of the fusion energy is then transformed into sunlight.

In quantum mechanics, energy is defined in terms of the energy operator as a time derivative of the wave function. The Schrödinger equation equates the energy operator to the full energy of a particle or a system. Its results can be considered as a definition of measurement of energy in quantum mechanics. The Schrödinger equation describes the space- and time-dependence of a slowly changing (non-relativistic) wave function of quantum systems. The solution of this equation for a bound system is discrete (a set of permitted states, each characterized by an energy level) which results in the concept of quanta. In the solution of the Schrödinger equation for any oscillator (vibrator) and for electromagnetic waves in a vacuum, the resulting energy states are related to the frequency by Planck's relation:

In the case of an electromagnetic wave these energy states are called quanta of light or photons.

When calculating kinetic energy (work to accelerate a mass from zero speed to some finite speed) relativistically – using Lorentz transformations instead of Newtonian mechanics – Einstein discovered an unexpected by-product of these calculations to be an energy term which does not vanish at zero speed. He called it rest mass energy: energy which every mass must possess even when being at rest. The amount of energy is directly proportional to the mass of body:

where
m is the mass,
c is the speed of light in vacuum,
E is the rest mass energy.
For example, consider electron–positron annihilation, in which the rest mass of individual particles is destroyed, but the inertia equivalent of the system of the two particles (its invariant mass) remains (since all energy is associated with mass), and this inertia and invariant mass is carried off by photons which individually are massless, but as a system retain their mass. This is a reversible process – the inverse process is called pair creation – in which the rest mass of particles is created from energy of two (or more) annihilating photons. In this system the matter (electrons and positrons) is destroyed and changed to non-matter energy (the photons). However, the total system mass and energy do not change during this interaction.
In general relativity, the stress–energy tensor serves as the source term for the gravitational field, in rough analogy to the way mass serves as the source term in the non-relativistic Newtonian approximation.
It is not uncommon to hear that energy is "equivalent" to mass. It would be more accurate to state that every energy has an inertia and gravity equivalent, and because mass is a form of energy, then mass too has inertia and gravity associated with it.
In classical physics, energy is a scalar quantity, the canonical conjugate to time. In special relativity energy is also a scalar (although not a Lorentz scalar but a time component of the energy–momentum 4-vector). In other words, energy is invariant with respect to rotations of space, but not invariant with respect to rotations of space-time (= boosts).

Energy may be transformed between different forms at various efficiencies. Items that transform between these forms are called transducers. Examples of transducers include a battery, from chemical energy to electric energy; a dam: gravitational potential energy to kinetic energy of moving water (and the blades of a turbine) and ultimately to electric energy through an electric generator; or a heat engine, from heat to work.
There are strict limits to how efficiently heat can be converted into work in a cyclic process, e.g. in a heat engine, as described by Carnot's theorem and the second law of thermodynamics. However, some energy transformations can be quite efficient. The direction of transformations in energy (what kind of energy is transformed to what other kind) is often determined by entropy (equal energy spread among all available degrees of freedom) considerations. In practice all energy transformations are permitted on a small scale, but certain larger transformations are not permitted because it is statistically unlikely that energy or matter will randomly move into more concentrated forms or smaller spaces.
Energy transformations in the universe over time are characterized by various kinds of potential energy that has been available since the Big Bang later being "released" (transformed to more active types of energy such as kinetic or radiant energy) when a triggering mechanism is available. Familiar examples of such processes include nuclear decay, in which energy is released that was originally "stored" in heavy isotopes (such as uranium and thorium), by nucleosynthesis, a process ultimately using the gravitational potential energy released from the gravitational collapse of supernovae, to store energy in the creation of these heavy elements before they were incorporated into the solar system and the Earth. This energy is triggered and released in nuclear fission bombs or in civil nuclear power generation. Similarly, in the case of a chemical explosion, chemical potential energy is transformed to kinetic energy and thermal energy in a very short time. Yet another example is that of a pendulum. At its highest points the kinetic energy is zero and the gravitational potential energy is at maximum. At its lowest point the kinetic energy is at maximum and is equal to the decrease of potential energy. If one (unrealistically) assumes that there is no friction or other losses, the conversion of energy between these processes would be perfect, and the pendulum would continue swinging forever.
Energy is also transferred from potential energy (

Energy gives rise to weight when it is trapped in a system with zero momentum, where it can be weighed. It is also equivalent to mass, and this mass is always associated with it. Mass is also equivalent to a certain amount of energy, and likewise always appears associated with it, as described in mass-energy equivalence. The formula E = mc², derived by Albert Einstein (1905) quantifies the relationship between rest-mass and rest-energy within the concept of special relativity. In different theoretical frameworks, similar formulas were derived by J. J. Thomson (1881), Henri Poincaré (1900), Friedrich Hasenöhrl (1904) and others (see Mass-energy equivalence#History for further information).
Matter may be converted to energy (and vice versa), but mass cannot ever be destroyed; rather, mass/energy equivalence remains a constant for both the matter and the energy, during any process when they are converted into each other. However, since

joules = 21 megatons of TNT), as can be seen in nuclear reactors and nuclear weapons. Conversely, the mass equivalent of a unit of energy is minuscule, which is why a loss of energy (loss of mass) from most systems is difficult to measure by weight, unless the energy loss is very large. Examples of energy transformation into matter (i.e., kinetic energy into particles with rest mass) are found in high-energy nuclear physics.

Thermodynamics divides energy transformation into two kinds: reversible processes and irreversible processes. An irreversible process is one in which energy is dissipated (spread) into empty energy states available in a volume, from which it cannot be recovered into more concentrated forms (fewer quantum states), without degradation of even more energy. A reversible process is one in which this sort of dissipation does not happen. For example, conversion of energy from one type of potential field to another, is reversible, as in the pendulum system described above. In processes where heat is generated, quantum states of lower energy, present as possible excitations in fields between atoms, act as a reservoir for part of the energy, from which it cannot be recovered, in order to be converted with 100% efficiency into other forms of energy. In this case, the energy must partly stay as heat, and cannot be completely recovered as usable energy, except at the price of an increase in some other kind of heat-like increase in disorder in quantum states, in the universe (such as an expansion of matter, or a randomisation in a crystal).
As the universe evolves in time, more and more of its energy becomes trapped in irreversible states (i.e., as heat or other kinds of increases in disorder). This has been referred to as the inevitable thermodynamic heat death of the universe. In this heat death the energy of the universe does not change, but the fraction of energy which is available to do work through a heat engine, or be transformed to other usable forms of energy (through the use of generators attached to heat engines), grows less and less.

According to conservation of energy, energy can neither be created (produced) nor destroyed by itself. It can only be transformed. The total inflow of energy into a system must equal the total outflow of energy from the system, plus the change in the energy contained within the system. Energy is subject to a strict global conservation law; that is, whenever one measures (or calculates) the total energy of a system of particles whose interactions do not depend explicitly on time, it is found that the total energy of the system always remains constant.
Richard Feynman said during a 1961 lecture:

There is a fact, or if you wish, a law, governing all natural phenomena that are known to date. There is no known exception to this law—it is exact so far as we know. The law is called the conservation of energy. It states that there is a certain quantity, which we call energy, that does not change in manifold changes which nature undergoes. That is a most abstract idea, because it is a mathematical principle; it says that there is a numerical quantity which does not change when something happens. It is not a description of a mechanism, or anything concrete; it is just a strange fact that we can calculate some number and when we finish watching nature go through her tricks and calculate the number again, it is the same.

Most kinds of energy (with gravitational energy being a notable exception) are subject to strict local conservation laws as well. In this case, energy can only be exchanged between adjacent regions of space, and all observers agree as to the volumetric density of energy in any given space. There is also a global law of conservation of energy, stating that the total energy of the universe cannot change; this is a corollary of the local law, but not vice versa.
This law is a fundamental principle of physics. As shown rigorously by Noether's theorem, the conservation of energy is a mathematical consequence of translational symmetry of time, a property of most phenomena below the cosmic scale that makes them independent of their locations on the time coordinate. Put differently, yesterday, today, and tomorrow are physically indistinguishable. This is because energy is the quantity which is canonical conjugate to time. This mathematical entanglement of energy and time also results in the uncertainty principle - it is impossible to define the exact amount of energy during any definite time interval. The uncertainty principle should not be confused with energy conservation - rather it provides mathematical limits to which energy can in principle be defined and measured.
Each of the basic forces of nature is associated with a different type of potential energy, and all types of potential energy (like all other types of energy) appears as system mass, whenever present. For example, a compressed spring will be slightly more massive than before it was compressed. Likewise, whenever energy is transferred between systems by any mechanism, an associated mass is transferred with it.
In quantum mechanics energy is expressed using the Hamiltonian operator. On any time scales, the uncertainty in the energy is by

which is similar in form to the Heisenberg Uncertainty Principle (but not really mathematically equivalent thereto, since H and t are not dynamically conjugate variables, neither in classical nor in quantum mechanics).
In particle physics, this inequality permits a qualitative understanding of virtual particles which carry momentum, exchange by which and with real particles, is responsible for the creation of all known fundamental forces (more accurately known as fundamental interactions). Virtual photons (which are simply lowest quantum mechanical energy state of photons) are also responsible for electrostatic interaction between electric charges (which results in Coulomb law), for spontaneous radiative decay of exited atomic and nuclear states, for the Casimir force, for van der Waals bond forces and some other observable phenomena.

Energy transfer can be considered for the special case of systems which are closed to transfers of matter. The portion of the energy which is transferred by conservative forces over a distance is measured as the work the source system does on the receiving system. The portion of the energy which does not do work during the transfer is called heat. Energy can be transferred between systems in a variety of ways. Examples include the transmission of electromagnetic energy via photons, physical collisions which transfer kinetic energy, and the conductive transfer of thermal energy.
Energy is strictly conserved and is also locally conserved wherever it can be defined. In thermodynamics, for closed systems, the process of energy transfer is described by the first law:

represents the heat flow into the system. As a simplification, the heat term,

This simplified equation is the one used to define the joule, for example.

Beyond the constraints of closed systems, open systems can gain or lose energy in association with matter transfer (both of these process are illustrated by fueling an auto, a system which gains in energy thereby, without addition of either work or heat). Denoting this energy by

Internal energy is the sum of all microscopic forms of energy of a system. It is the energy needed to create the system. It is related to the potential energy, e.g., molecular structure, crystal structure, and other geometric aspects, as well as the motion of the particles, in form of kinetic energy. Thermodynamics is chiefly concerned with changes in internal energy and not its absolute value, which is impossible to determine with thermodynamics alone.

The first law of thermodynamics asserts that energy (but not necessarily thermodynamic free energy) is always conserved and that heat flow is a form of energy transfer. For homogeneous systems, with a well-defined temperature and pressure, a commonly used corollary of the first law is that, for a system subject only to pressure forces and heat transfer (e.g., a cylinder-full of gas) without chemical changes, the differential change in the internal energy of the system (with a gain in energy signified by a positive quantity) is given as

where the first term on the right is the heat transferred into the system, expressed in terms of temperature T and entropy S (in which entropy increases and the change dS is positive when the system is heated), and the last term on the right hand side is identified as work done on the system, where pressure is P and volume V (the negative sign results since compression of the system requires work to be done on it and so the volume change, dV, is negative when work is done on the system).
This equation is highly specific, ignoring all chemical, electrical, nuclear, and gravitational forces, effects such as advection of any form of energy other than heat and pV-work. The general formulation of the first law (i.e., conservation of energy) is valid even in situations in which the system is not homogeneous. For these cases the change in internal energy of a closed system is expressed in a general form by

The energy of a mechanical harmonic oscillator (a mass on a spring) is alternatively kinetic and potential. At two points in the oscillation cycle it is entirely kinetic, and alternatively at two other points it is entirely potential. Over the whole cycle, or over many cycles, net energy is thus equally split between kinetic and potential. This is called equipartition principle; total energy of a system with many degrees of freedom is equally split among all available degrees of freedom.
This principle is vitally important to understanding the behaviour of a quantity closely related to energy, called entropy. Entropy is a measure of evenness of a distribution of energy between parts of a system. When an isolated system is given more degrees of freedom (i.e., given new available energy states that are the same as existing states), then total energy spreads over all available degrees equally without distinction between "new" and "old" degrees. This mathematical result is called the second law of thermodynamics.

Combustion
Index of energy articles
Index of wave articles
Orders of magnitude (energy)
Transfer energy

Energy at DMOZ
Differences between Heat and Thermal energy - BioCabA robot is a machine—especially one programmable by a computer—capable of carrying out a complex series of actions automatically. Robots can be guided by an external control device or the control may be embedded within. Robots may be constructed to take on human form but most robots are machines designed to perform a task with no regard to how they look.
Robots can be autonomous or semi-autonomous and range from humanoids such as Honda's Advanced Step in Innovative Mobility (ASIMO) and TOSY's TOSY Ping Pong Playing Robot (TOPIO) to industrial robots, medical operating robots, patent assist robots, dog therapy robots, collectively programmed swarm robots, UAV drones such as General Atomics MQ-1 Predator, and even microscopic nano robots. By mimicking a lifelike appearance or automating movements, a robot may convey a sense of intelligence or thought of its own.
The branch of technology that deals with the design, construction, operation, and application of robots, as well as computer systems for their control, sensory feedback, and information processing is robotics. These technologies deal with automated machines that can take the place of humans in dangerous environments or manufacturing processes, or resemble humans in appearance, behavior, and/or cognition. Many of today's robots are inspired by nature contributing to the field of bio-inspired robotics. These robots have also created a newer branch of robotics: soft robotics.
From the time of ancient civilization there have been many accounts of user-configurable automated devices and even automata resembling animals and humans, designed primarily as entertainment. As mechanical techniques developed through the Industrial age, there appeared more practical applications such as automated machines, remote-control and wireless remote-control.
The word 'robot' was first used to denote a fictional humanoid in a 1920 play R.U.R. by the Czech writer, Karel Čapek but it was Karel's brother Josef Čapek who was the word's true inventor. Electronics evolved into the driving force of development with the advent of the first electronic autonomous robots created by William Grey Walter in Bristol, England in 1948. The first digital and programmable robot was invented by George Devol in 1954 and was named the Unimate. It was sold to General Motors in 1961 where it was used to lift pieces of hot metal from die casting machines at the Inland Fisher Guide Plant in the West Trenton section of Ewing Township, New Jersey.
Robots have replaced humans in performing repetitive and dangerous tasks which humans prefer not to do, or are unable to do because of size limitations, or which take place in extreme environments such as outer space or the bottom of the sea.
There are concerns about the increasing use of robots and their role in society. Robots are blamed for rising unemployment as they replace workers in increasing numbers of functions. The use of robots in military combat raises ethical concerns. The possibilities of robot autonomy and potential repercussions have been addressed in fiction and may be a realistic concern in the future.

The word robot can refer to both physical robots and virtual software agents, but the latter are usually referred to as bots. There is no consensus on which machines qualify as robots but there is general agreement among experts, and the public, that robots tend to possess some or all of the following abilities and functions: accept electronic programming, process data or physical perceptions electronically, operate autonomously to some degree, move around, operate physical parts of itself or physical processes, sense and manipulate their environment, and exhibit intelligent behavior — especially behavior which mimics humans or other animals. Closely related to the concept of a robot is the field of Synthetic Biology, which studies entities whose nature is more comparable to beings than to machines.

The idea of automata originates in the mythologies of many cultures around the world. Engineers and inventors from ancient civilizations, including Ancient China, Ancient Greece, and Ptolemaic Egypt, attempted to build self-operating machines, some resembling animals and humans. Early descriptions of automata include the artificial doves of Archytas, the artificial birds of Mozi and Lu Ban, a "speaking" automaton by Hero of Alexandria, a washstand automaton by Philo of Byzantium, and a human automaton described in the Lie Zi.

Many ancient mythologies, and most modern religions include artificial people, such as the mechanical servants built by the Greek god Hephaestus (Vulcan to the Romans), the clay golems of Jewish legend and clay giants of Norse legend, and Galatea, the mythical statue of Pygmalion that came to life. Since circa 400 BC, myths of Crete include Talos, a man of bronze who guarded the Cretan island of Europa from pirates.

In ancient Greece, the Greek engineer Ctesibius (c. 270 BC) "applied a knowledge of pneumatics and hydraulics to produce the first organ and water clocks with moving figures." In the 4th century BC, the Greek mathematician Archytas of Tarentum postulated a mechanical steam-operated bird he called "The Pigeon". Hero of Alexandria (10–70 AD), a Greek mathematician and inventor, created numerous user-configurable automated devices, and described machines powered by air pressure, steam and water.

The 11th century Lokapannatti tells of how the Buddha's relics were protected by mechanical robots (bhuta vahana yanta), from the kingdom of Roma visaya (Rome); until they were disarmed by King Ashoka.
In ancient China, the 3rd century text of the Lie Zi describes an account of humanoid automata, involving a much earlier encounter between Chinese emperor King Mu of Zhou and a mechanical engineer known as Yan Shi, an 'artificer'. Yan Shi proudly presented the king with a life-size, human-shaped figure of his mechanical 'handiwork' made of leather, wood, and artificial organs. There are also accounts of flying automata in the Han Fei Zi and other texts, which attributes the 5th century BC Mohist philosopher Mozi and his contemporary Lu Ban with the invention of artificial wooden birds (ma yuan) that could successfully fly. In 1066, the Chinese inventor Su Song built a water clock in the form of a tower which featured mechanical figurines which chimed the hours.

The beginning of automata is associated with the invention of early Su Song's astronomical clock tower featured mechanical figurines that chimed the hours. His mechanism had a programmable drum machine with pegs (cams) that bumped into little levers that operated percussion instruments. The drummer could be made to play different rhythms and different drum patterns by moving the pegs to different locations.
In Renaissance Italy, Leonardo da Vinci (1452–1519) sketched plans for a humanoid robot around 1495. Da Vinci's notebooks, rediscovered in the 1950s, contained detailed drawings of a mechanical knight now known as Leonardo's robot, able to sit up, wave its arms and move its head and jaw. The design was probably based on anatomical research recorded in his Vitruvian Man. It is not known whether he attempted to build it.
In Japan, complex animal and human automata were built between the 17th to 19th centuries, with many described in the 18th century Karakuri zui (Illustrated Machinery, 1796). One such automaton was the karakuri ningyō, a mechanized puppet. Different variations of the karakuri existed: the Butai karakuri, which were used in theatre, the Zashiki karakuri, which were small and used in homes, and the Dashi karakuri which were used in religious festivals, where the puppets were used to perform reenactments of traditional myths and legends.
In France, between 1738 and 1739, Jacques de Vaucanson exhibited several life-sized automatons: a flute player, a pipe player and a duck. The mechanical duck could flap its wings, crane its neck, and swallow food from the exhibitor's hand, and it gave the illusion of digesting its food by excreting matter stored in a hidden compartment.

Remotely operated vehicles were demonstrated in the late 19th Century in the form of several types of remotely controlled torpedoes. The early 1870s saw remotely controlled torpedoes by John Ericsson (pneumatic), John Louis Lay (electric wire guided), and Victor von Scheliha (electric wire guided).
The Brennan torpedo, invented by Louis Brennan in 1877 was powered by two contra-rotating propellors that were spun by rapidly pulling out wires from drums wound inside the torpedo. Differential speed on the wires connected to the shore station allowed the torpedo to be guided to its target, making it "the world's first practical guided missile". In 1897 the British inventor Ernest Wilson was granted a patent for a torpedo remotely controlled by "Hertzian" (radio) waves and in 1898 Nikola Tesla publicly demonstrated a wireless-controlled torpedo that he hoped to sell to the US Navy.
Archibald Low, known as the "father of radio guidance systems" for his pioneering work on guided rockets and planes during the First World War. In 1917, he demonstrated a remote controlled aircraft to the Royal Flying Corps and in the same year built the first wire-guided rocket.

'Robot' was first applied as a term for artificial automata in a 1920 play R.U.R. by the Czech writer, Karel Čapek. However, Josef Čapek was named by his brother Karel as the true inventor of the term robot. The word 'robot' itself was not new, having been in Slavic language as robota (forced laborer), a term which classified those peasants obligated to compulsory service under the feudal system widespread in 19th century Europe (see: Robot Patent). Čapek's fictional story postulated the technological creation of artificial human bodies without souls, and the old theme of the feudal robota class eloquently fit the imagination of a new class of manufactured, artificial workers.

In 1928, one of the first humanoid robots was exhibited at the annual exhibition of the Model Engineers Society in London. Invented by W. H. Richards, the robot Eric's frame consisted of an aluminium body of armour with eleven electromagnets and one motor powered by a twelve-volt power source. The robot could move its hands and head and could be controlled through remote control or voice control.
Westinghouse Electric Corporation built Televox in 1926; it was a cardboard cutout connected to various devices which users could turn on and off. In 1939, the humanoid robot known as Elektro was debuted at the 1939 New York World's Fair. Seven feet tall (2.1 m) and weighing 265 pounds (120.2 kg), it could walk by voice command, speak about 700 words (using a 78-rpm record player), smoke cigarettes, blow up balloons, and move its head and arms. The body consisted of a steel gear, cam and motor skeleton covered by an aluminum skin. In 1928, Japan's first robot, Gakutensoku, was designed and constructed by biologist Makoto Nishimura.

The first electronic autonomous robots with complex behaviour were created by William Grey Walter of the Burden Neurological Institute at Bristol, England in 1948 and 1949. He wanted to prove that rich connections between a small number of brain cells could give rise to very complex behaviors - essentially that the secret of how the brain worked lay in how it was wired up. His first robots, named Elmer and Elsie, were constructed between 1948 and 1949 and were often described as tortoises due to their shape and slow rate of movement. The three-wheeled tortoise robots were capable of phototaxis, by which they could find their way to a recharging station when they ran low on battery power.
Walter stressed the importance of using purely analogue electronics to simulate brain processes at a time when his contemporaries such as Alan Turing and John von Neumann were all turning towards a view of mental processes in terms of digital computation. His work inspired subsequent generations of robotics researchers such as Rodney Brooks, Hans Moravec and Mark Tilden. Modern incarnations of Walter's turtles may be found in the form of BEAM robotics.

The first digitally operated and programmable robot was invented by George Devol in 1954 and was ultimately called the Unimate. This ultimately laid the foundations of the modern robotics industry. Devol sold the first Unimate to General Motors in 1960, and it was installed in 1961 in a plant in Trenton, New Jersey to lift hot pieces of metal from a die casting machine and stack them. Devol’s patent for the first digitally operated programmable robotic arm represents the foundation of the modern robotics industry.
The first palletizing robot was introduced in 1963 by the Fuji Yusoki Kogyo Company. In 1973, a robot with six electromechanically driven axes was patented by KUKA robotics in Germany, and the programmable universal manipulation arm was invented by Victor Scheinman in 1976, and the design was sold to Unimation.
Commercial and industrial robots are now in widespread use performing jobs more cheaply or with greater accuracy and reliability than humans. They are also employed for jobs which are too dirty, dangerous or dull to be suitable for humans. Robots are widely used in manufacturing, assembly and packing, transport, earth and space exploration, surgery, weaponry, laboratory research, and mass production of consumer and industrial goods.

Various techniques have emerged to develop the science of robotics and robots. One method is evolutionary robotics, in which a number of differing robots are submitted to tests. Those which perform best are used as a model to create a subsequent "generation" of robots. Another method is developmental robotics, which tracks changes and development within a single robot in the areas of problem-solving and other functions. Another new type of robot is just recently introduced which acts both as a smartphone and robot and is named RoboHon.
As robots become more advanced, eventually there may be a standard computer operating system designed mainly for robots. Robot Operating System is an open-source set of programs being developed at Stanford University, the Massachusetts Institute of Technology and the Technical University of Munich, Germany, among others. ROS provides ways to program a robot's navigation and limbs regardless of the specific hardware involved. It also provides high-level commands for items like image recognition and even opening doors. When ROS boots up on a robot's computer, it would obtain data on attributes such as the length and movement of robots' limbs. It would relay this data to higher-level algorithms. Microsoft is also developing a "Windows for robots" system with its Robotics Developer Studio, which has been available since 2007.
Japan hopes to have full-scale commercialization of service robots by 2025. Much technological research in Japan is led by Japanese government agencies, particularly the Trade Ministry.
Many future applications of robotics seem obvious to people, even though they are well beyond the capabilities of robots available at the time of the prediction. As early as 1982 people were confident that someday robots would: 1. clean parts by removing molding flash 2. spray paint automobiles with absolutely no human presence 3. pack things in boxes—for example, orient and nest chocolate candies in candy boxes 4. make electrical cable harness 5. load trucks with boxes—a packing problem 6. handle soft goods, such as garments and shoes 7. shear sheep 8. prosthesis 9. cook fast food and work in other service industries 10. household robot.
Generally such predictions are overly optimistic in timescale.

In 2008, Caterpillar Inc. developed a dump truck which can drive itself without any human operator. Many analysts believe that self-driving trucks may eventually revolutionize logistics. By 2014, Caterpillar had a self-driving dump truck which is expected to greatly change the process of mining. In 2015, these Caterpillar trucks were actively used in mining operations in Australia by the mining company Rio Tinto Coal Australia. Some analysts believe that within the next few decades, most trucks will be self-driving.
A literate or 'reading robot' named Marge has intelligence that comes from software. She can read newspapers, find and correct misspelled words, learn about banks like Barclays, and understand that some restaurants are better places to eat than others.
Baxter is a new robot which is different from other industrial robots because it can learn. A worker could teach Baxter how to perform a task by moving its hands in the desired motion and having Baxter memorize them. Extra dials, buttons, and controls are available on Baxter's arm for more precision and features. Any regular worker could program Baxter and it only takes a matter of minutes, unlike usual industrial robots that take extensive programs and coding in order to be used. This means Baxter needs no programming in order to operate. No software engineers are needed. This also means Baxter can be taught to perform multiple, more complicated tasks.

The word robot was introduced to the public by the Czech interwar writer Karel Čapek in his play R.U.R. (Rossum's Universal Robots), published in 1920. The play begins in a factory that uses a chemical substitute for protoplasm to manufacture living, simplified people called robots. The play does not focus in detail on the technology behind the creation of these living creatures, but in their appearance they prefigure modern ideas of androids, creatures who can be mistaken for humans. These mass-produced workers are depicted as efficient but emotionless, incapable of original thinking and indifferent to self-preservation. At issue is whether the robots are being exploited and the consequences of human dependence upon commodified labor (especially after a number of specially-formulated robots achieve self-awareness and incite robots all around the world to rise up against the humans).
Karel Čapek himself did not coin the word. He wrote a short letter in reference to an etymology in the Oxford English Dictionary in which he named his brother, the painter and writer Josef Čapek, as its actual originator.
In an article in the Czech journal Lidové noviny in 1933, he explained that he had originally wanted to call the creatures laboři ("workers", from Latin labor). However, he did not like the word, and sought advice from his brother Josef, who suggested "roboti". The word robota means literally "corvée", "serf labor", and figuratively "drudgery" or "hard work" in Czech and also (more general) "work", "labor" in many Slavic languages (e.g.: Bulgarian, Russian, Serbian, Slovak, Polish, Macedonian, Ukrainian, archaic Czech, as well as robot in Hungarian). Traditionally the robota (Hungarian robot) was the work period a serf (corvée) had to give for his lord, typically 6 months of the year. The origin of the word is the Old Church Slavonic (Old Bulgarian) rabota "servitude" ("work" in contemporary Bulgarian and Russian), which in turn comes from the Proto-Indo-European root *orbh-. Robot is cognate with the German root Arbeit (work).
The word robotics, used to describe this field of study, was coined by the science fiction writer Isaac Asimov. Asimov created the "Three Laws of Robotics" which are a recurring theme in his books. These have since been used by many others to define laws used in fiction. (The three laws are pure fiction, and no technology yet created has the ability to understand or follow them, and in fact most robots serve military purposes, which run quite contrary to the first law and often the third law. "People think about Asimov's laws, but they were set up to point out how a simple ethical system doesn't work. If you read the short stories, every single one is about a failure, and they are totally impractical," said Dr. Joanna Bryson of the University of Bath.)

Mobile robots have the capability to move around in their environment and are not fixed to one physical location. An example of a mobile robot that is in common use today is the automated guided vehicle or automatic guided vehicle (AGV). An AGV is a mobile robot that follows markers or wires in the floor, or uses vision or lasers. AGVs are discussed later in this article.
Mobile robots are also found in industry, military and security environments. They also appear as consumer products, for entertainment or to perform certain tasks like vacuum cleaning. Mobile robots are the focus of a great deal of current research and almost every major university has one or more labs that focus on mobile robot research.
Mobile robots are usually used in tightly controlled environments such as on assembly lines because they have difficulty responding to unexpected interference. Because of this most humans rarely encounter robots. However domestic robots for cleaning and maintenance are increasingly common in and around homes in developed countries. Robots can also be found in military applications.

Industrial robots usually consist of a jointed arm (multi-linked manipulator) and an end effector that is attached to a fixed surface. One of the most common type of end effector is a gripper assembly.
The International Organization for Standardization gives a definition of a manipulating industrial robot in ISO 8373:
"an automatically controlled, reprogrammable, multipurpose, manipulator programmable in three or more axes, which may be either fixed in place or mobile for use in industrial automation applications."
This definition is used by the International Federation of Robotics, the European Robotics Research Network (EURON) and many national standards committees.

Most commonly industrial robots are fixed robotic arms and manipulators used primarily for production and distribution of goods. The term "service robot" is less well-defined. The International Federation of Robotics has proposed a tentative definition, "A service robot is a robot which operates semi- or fully autonomously to perform services useful to the well-being of humans and equipment, excluding manufacturing operations."

Robots are used as educational assistants to teachers. From the 1980s, robots such as turtles were used in schools and programmed using the Logo language.
There are robot kits like Lego Mindstorms, BIOLOID, OLLO from ROBOTIS, or BotBrain Educational Robots can help children to learn about mathematics, physics, programming, and electronics. Robotics have also been introduced into the lives of elementary and high school students in the form of robot competitions with the company FIRST (For Inspiration and Recognition of Science and Technology). The organization is the foundation for the FIRST Robotics Competition, FIRST LEGO League, Junior FIRST LEGO League, and FIRST Tech Challenge competitions.
There have also been devices shaped like robots such as the teaching computer, Leachim (1974), and 2-XL (1976), a robot shaped game / teaching toy based on an 8-track tape player, both invented Michael J. Freeman.

Modular robots are a new breed of robots that are designed to increase the utilization of robots by modularizing their architecture. The functionality and effectiveness of a modular robot is easier to increase compared to conventional robots. These robots are composed of a single type of identical, several different identical module types, or similarly shaped modules, which vary in size. Their architectural structure allows hyper-redundancy for modular robots, as they can be designed with more than 8 degrees of freedom (DOF). Creating the programming, inverse kinematics and dynamics for modular robots is more complex than with traditional robots. Modular robots may be composed of L-shaped modules, cubic modules, and U and H-shaped modules. ANAT technology, an early modular robotic technology patented by Robotics Design Inc., allows the creation of modular robots from U and H shaped modules that connect in a chain, and are used to form heterogeneous and homogenous modular robot systems. These “ANAT robots” can be designed with “n” DOF as each module is a complete motorized robotic system that folds relatively to the modules connected before and after it in its chain, and therefore a single module allows one degree of freedom. The more modules that are connected to one another, the more degrees of freedom it will have. L-shaped modules can also be designed in a chain, and must become increasingly smaller as the size of the chain increases, as payloads attached to the end of the chain place a greater strain on modules that are further from the base. ANAT H-shaped modules do not suffer from this problem, as their design allows a modular robot to distribute pressure and impacts evenly amongst other attached modules, and therefore payload-carrying capacity does not decrease as the length of the arm increases. Modular robots can be manually or self-reconfigured to form a different robot, that may perform different applications. Because modular robots of the same architecture type are composed of modules that compose different modular robots, a snake-arm robot can combine with another to form a dual or quadra-arm robot, or can split into several mobile robots, and mobile robots can split into multiple smaller ones, or combine with others into a larger or different one. This allows a single modular robot the ability to be fully specialized in a single task, as well as the capacity to be specialized to perform multiple different tasks.
Modular robotic technology is currently being applied in hybrid transportation, industrial automation, duct cleaning and handling. Many research centres and universities have also studied this technology, and have developed prototypes.

A collaborative robot or cobot is a robot that can safely and effectively interact with human workers while performing simple industrial tasks. However, end-effectors and other environmental conditions may create hazards, and as such risk assessments should be done before using any industrial motion-control application.
The collaborative robots most widely used in industries today are manufactured by Universal Robots in Denmark.
Rethink Robotics—founded by Rodney Brooks, previously with iRobot—introduced Baxter in September 2012; as an industrial robot designed to safely interact with neighboring human workers, and be programmable for performing simple tasks. Baxters stop if they detect a human in the way of their robotic arms and have prominent off switches. Intended for sale to small businesses, they are promoted as the robotic analogue of the personal computer. As of May 2014, 190 companies in the US have bought Baxters and they are being used commercially in the UK.

Roughly half of all the robots in the world are in Asia, 32% in Europe, and 16% in North America, 1% in Australasia and 1% in Africa. 40% of all the robots in the world are in Japan, making Japan the country with the highest number of robots.

As robots have become more advanced and sophisticated, experts and academics have increasingly explored the questions of what ethics might govern robots' behavior, and whether robots might be able to claim any kind of social, cultural, ethical or legal rights. One scientific team has said that it is possible that a robot brain will exist by 2019. Others predict robot intelligence breakthroughs by 2050. Recent advances have made robotic behavior more sophisticated. The social impact of intelligent robots is subject of a 2010 documentary film called Plug & Pray.
Vernor Vinge has suggested that a moment may come when computers and robots are smarter than humans. He calls this "the Singularity". He suggests that it may be somewhat or possibly very dangerous for humans. This is discussed by a philosophy called Singularitarianism.
In 2009, experts attended a conference hosted by the Association for the Advancement of Artificial Intelligence (AAAI) to discuss whether computers and robots might be able to acquire any autonomy, and how much these abilities might pose a threat or hazard. They noted that some robots have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved "cockroach intelligence." They noted that self-awareness as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls. Various media sources and scientific groups have noted separate trends in differing areas which might together result in greater robotic functionalities and autonomy, and which pose some inherent concerns. In 2015, the Nao alderen robots were shown to have a capability for a degree of self-awareness. Researchers at the Rensselaer Polytechnic Institute AI and Reasoning Lab in New York conducted an experiment where a robot became aware of itself, and corrected its answer to a question once it had realised this.

Some experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions. There are also concerns about technology which might allow some armed robots to be controlled mainly by other robots. The US Navy has funded a report which indicates that, as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions. One researcher states that autonomous robots might be more humane, as they could make decisions more effectively. However, other experts question this.
One robot in particular, the EATR, has generated public concerns over its fuel source, as it can continually refuel itself using organic substances. Although the engine for the EATR is designed to run on biomass and vegetation specifically selected by its sensors, which it can find on battlefields or other local environments, the project has stated that chicken fat can also be used.
Manuel De Landa has noted that "smart missiles" and autonomous bombs equipped with artificial perception can be considered robots, as they make some of their decisions autonomously. He believes this represents an important and dangerous trend in which humans are handing over important decisions to machines.

For centuries, experts have predicted that machines would make workers obsolete and increase unemployment.
A recent example of human replacement involves Taiwanese technology company Foxconn who, in July 2011, announced a three-year plan to replace workers with more robots. At present the company uses ten thousand robots but will increase them to a million robots over a three-year period.
Lawyers have speculated that an increased prevalence of robots in the workplace could lead to the need to revise redundancy laws.

At present, there are two main types of robots, based on their use: general-purpose autonomous robots and dedicated robots.
Robots can be classified by their specificity of purpose. A robot might be designed to perform one particular task extremely well, or a range of tasks less well. Of course, all robots by their nature can be re-programmed to behave differently, but some are limited by their physical form. For example, a factory robot arm can perform jobs such as cutting, welding, gluing, or acting as a fairground ride, while a pick-and-place robot can only populate printed circuit boards.

General-purpose autonomous robots can perform a variety of functions independently. General-purpose autonomous robots typically can navigate independently in known spaces, handle their own re-charging needs, interface with electronic doors and elevators and perform other basic tasks. Like computers, general-purpose robots can link with networks, software and accessories that increase their usefulness. They may recognize people or objects, talk, provide companionship, monitor environmental quality, respond to alarms, pick up supplies and perform other useful tasks. General-purpose robots may perform a variety of functions simultaneously or they may take on different roles at different times of day. Some such robots try to mimic human beings and may even resemble people in appearance; this type of robot is called a humanoid robot. Humanoid robots are still in a very limited stage, as no humanoid robot can, as of yet, actually navigate around a room that it has never been in. Thus, humanoid robots are really quite limited, despite their intelligent behaviors in their well-known environments.

Over the last three decades, automobile factories have become dominated by robots. A typical factory contains hundreds of industrial robots working on fully automated production lines, with one robot for every ten human workers. On an automated production line, a vehicle chassis on a conveyor is welded, glued, painted and finally assembled at a sequence of robot stations.

Industrial robots are also used extensively for palletizing and packaging of manufactured goods, for example for rapidly taking drink cartons from the end of a conveyor belt and placing them into boxes, or for loading and unloading machining centers.

Mass-produced printed circuit boards (PCBs) are almost exclusively manufactured by pick-and-place robots, typically with SCARA manipulators, which remove tiny electronic components from strips or trays, and place them on to PCBs with great accuracy. Such robots can place hundreds of thousands of components per hour, far out-performing a human in speed, accuracy, and reliability.

Mobile robots, following markers or wires in the floor, or using vision or lasers, are used to transport goods around large facilities, such as warehouses, container ports, or hospitals.

Limited to tasks that could be accurately defined and had to be performed the same way every time. Very little feedback or intelligence was required, and the robots needed only the most basic exteroceptors (sensors). The limitations of these AGVs are that their paths are not easily altered and they cannot alter their paths if obstacles block them. If one AGV breaks down, it may stop the entire operation.

Developed to deploy triangulation from beacons or bar code grids for scanning on the floor or ceiling. In most factories, triangulation systems tend to require moderate to high maintenance, such as daily cleaning of all beacons or bar codes. Also, if a tall pallet or large vehicle blocks beacons or a bar code is marred, AGVs may become lost. Often such AGVs are designed to be used in human-free environments.

Such as SmartLoader, SpeciMinder, ADAM, Tug Eskorta, and MT 400 with Motivity are designed for people-friendly workspaces. They navigate by recognizing natural features. 3D scanners or other means of sensing the environment in two or three dimensions help to eliminate cumulative errors in dead-reckoning calculations of the AGV's current position. Some AGVs can create maps of their environment using scanning lasers with simultaneous localization and mapping (SLAM) and use those maps to navigate in real time with other path planning and obstacle avoidance algorithms. They are able to operate in complex environments and perform non-repetitive and non-sequential tasks such as transporting photomasks in a semiconductor lab, specimens in hospitals and goods in warehouses. For dynamic areas, such as warehouses full of pallets, AGVs require additional strategies using three-dimensional sensors such as time-of-flight or stereovision cameras.

There are many jobs which humans would rather leave to robots. The job may be boring, such as domestic cleaning, or dangerous, such as exploring inside a volcano. Other jobs are physically inaccessible, such as exploring another planet, cleaning the inside of a long pipe, or performing laparoscopic surgery.

Almost every unmanned space probe ever launched was a robot. Some were launched in the 1960s with very limited abilities, but their ability to fly and land (in the case of Luna 9) is an indication of their status as a robot. This includes the Voyager probes and the Galileo probes, among others.

Teleoperated robots, or telerobots, are devices remotely operated from a distance by a human operator rather than following a predetermined sequence of movements, but which has semi-autonomous behaviour. They are used when a human cannot be present on site to perform a job because it is dangerous, far away, or inaccessible. The robot may be in another room or another country, or may be on a very different scale to the operator. For instance, a laparoscopic surgery robot allows the surgeon to work inside a human patient on a relatively small scale compared to open surgery, significantly shortening recovery time. They can also be used to avoid exposing workers to the hazardous and tight spaces such as in duct cleaning. When disabling a bomb, the operator sends a small robot to disable it. Several authors have been using a device called the Longpen to sign books remotely. Teleoperated robot aircraft, like the Predator Unmanned Aerial Vehicle, are increasingly being used by the military. These pilotless drones can search terrain and fire on targets. Hundreds of robots such as iRobot's Packbot and the Foster-Miller TALON are being used in Iraq and Afghanistan by the U.S. military to defuse roadside bombs or improvised explosive devices (IEDs) in an activity known as explosive ordnance disposal (EOD).

Robots are used to automate picking fruit on orchards at a cost lower than that of human pickers.

Domestic robots are simple robots dedicated to a single task work in home use. They are used in simple but unwanted jobs, such as vacuum cleaning, floor washing, and lawn mowing. An example of a domestic robot is a Roomba.

Military robots include the SWORDS robot which is currently used in ground-based combat. It can use a variety of weapons and there is some discussion of giving it some degree of autonomy in battleground situations.
Unmanned combat air vehicles (UCAVs), which are an upgraded form of UAVs, can do a wide variety of missions, including combat. UCAVs are being designed such as the BAE Systems Mantis which would have the ability to fly themselves, to pick their own course and target, and to make most decisions on their own. The BAE Taranis is a UCAV built by Great Britain which can fly across continents without a pilot and has new means to avoid detection. Flight trials are expected to begin in 2011.
The AAAI has studied this topic in depth and its president has commissioned a study to look at this issue.
Some have suggested a need to build "Friendly AI", meaning that the advances which are already occurring with AI should also include an effort to make AI intrinsically friendly and humane. Several such measures reportedly already exist, with robot-heavy countries such as Japan and South Korea having begun to pass regulations requiring robots to be equipped with safety systems, and possibly sets of 'laws' akin to Asimov's Three Laws of Robotics. An official report was issued in 2009 by the Japanese government's Robot Industry Policy Committee. Chinese officials and researchers have issued a report suggesting a set of ethical rules, and a set of new legal guidelines referred to as "Robot Legal Studies." Some concern has been expressed over a possible occurrence of robots telling apparent falsehoods.

Mining robots are designed to solve a number of problems currently facing the mining industry, including skills shortages, improving productivity from declining ore grades, and achieving environmental targets. Due to the hazardous nature of mining, in particular underground mining, the prevalence of autonomous, semi-autonomous, and tele-operated robots has greatly increased in recent times. A number of vehicle manufacturers provide autonomous trains, trucks and loaders that will load material, transport it on the mine site to its destination, and unload without requiring human intervention. One of the world's largest mining corporations, Rio Tinto, has recently expanded its autonomous truck fleet to the world's largest, consisting of 150 autonomous Komatsu trucks, operating in Western Australia. Similarly, BHP has announced the expansion of its autonomous drill fleet to the world's largest, 21 autonomous Atlas Copco drills.
Drilling, longwall and rockbreaking machines are now also available as autonomous robots. The Atlas Copco Rig Control System can autonomously execute a drilling plan on a drilling rig, moving the rig into position using GPS, set up the drill rig and drill down to specified depths. Similarly, the Transmin Rocklogic system can automatically plan a path to position a rockbreaker at a selected destination. These systems greatly enhance the safety and efficiency of mining operations.

Robots in healthcare have two main functions. Those which assist an individual, such as a sufferer of a disease like Multiple Sclerosis, and those which aid in the overall systems such as pharmacies and hospitals.

Robots used in home automation have developed over time from simple basic robotic assistants, such as the Handy 1, through to semi-autonomous robots, such as FRIEND which can assist the elderly and disabled with common tasks.
The population is aging in many countries, especially Japan, meaning that there are increasing numbers of elderly people to care for, but relatively fewer young people to care for them. Humans make the best carers, but where they are unavailable, robots are gradually being introduced.
FRIEND is a semi-autonomous robot designed to support disabled and elderly people in their daily life activities, like preparing and serving a meal. FRIEND make it possible for patients who are paraplegic, have muscle diseases or serious paralysis (due to strokes etc.), to perform tasks without help from other people like therapists or nursing staff.

Script Pro manufactures a robot designed to help pharmacies fill prescriptions that consist of oral solids or medications in pill form. The pharmacist or pharmacy technician enters the prescription information into its information system. The system, upon determining whether or not the drug is in the robot, will send the information to the robot for filling. The robot has 3 different size vials to fill determined by the size of the pill. The robot technician, user, or pharmacist determines the needed size of the vial based on the tablet when the robot is stocked. Once the vial is filled it is brought up to a conveyor belt that delivers it to a holder that spins the vial and attaches the patient label. Afterwards it is set on another conveyor that delivers the patient’s medication vial to a slot labeled with the patient's name on an LED read out. The pharmacist or technician then checks the contents of the vial to ensure it’s the correct drug for the correct patient and then seals the vials and sends it out front to be picked up. The robot is a very time efficient device that the pharmacy depends on to fill prescriptions.
McKesson's Robot RX is another healthcare robotics product that helps pharmacies dispense thousands of medications daily with little or no errors. The robot can be ten feet wide and thirty feet long and can hold hundreds of different kinds of medications and thousands of doses. The pharmacy saves many resources like staff members that are otherwise unavailable in a resource scarce industry. It uses an electromechanical head coupled with a pneumatic system to capture each dose and deliver it to its either stocked or dispensed location. The head moves along a single axis while it rotates 180 degrees to pull the medications. During this process it uses barcode technology to verify its pulling the correct drug. It then delivers the drug to a patient specific bin on a conveyor belt. Once the bin is filled with all of the drugs that a particular patient needs and that the robot stocks, the bin is then released and returned out on the conveyor belt to a technician waiting to load it into a cart for delivery to the floor.

While most robots today are installed in factories or homes, performing labour or life saving jobs, many new types of robot are being developed in laboratories around the world. Much of the research in robotics focuses not on specific industrial tasks, but on investigations into new types of robot, alternative ways to think about or design robots, and new ways to manufacture them. It is expected that these new types of robot will be able to solve real world problems when they are finally realized.

One approach to designing robots is to base them on animals. BionicKangaroo was designed and engineered by studying and applying the physiology and methods of locomotion of a kangaroo.

Nanorobotics is the emerging technology field of creating machines or robots whose components are at or close to the microscopic scale of a nanometer (10−9 meters). Also known as "nanobots" or "nanites", they would be constructed from molecular machines. So far, researchers have mostly produced only parts of these complex systems, such as bearings, sensors, and synthetic molecular motors, but functioning robots have also been made such as the entrants to the Nanobot Robocup contest. Researchers also hope to be able to create entire robots as small as viruses or bacteria, which could perform tasks on a tiny scale. Possible applications include micro surgery (on the level of individual cells), utility fog, manufacturing, weaponry and cleaning. Some people have suggested that if there were nanobots which could reproduce, the earth would turn into "grey goo", while others argue that this hypothetical outcome is nonsense.

A few researchers have investigated the possibility of creating robots which can alter their physical form to suit a particular task, like the fictional T-1000. Real robots are nowhere near that sophisticated however, and mostly consist of a small number of cube shaped units, which can move relative to their neighbours. Algorithms have been designed in case any such robots become a reality.

Robots with silicone bodies and flexible actuators (air muscles, electroactive polymers, and ferrofluids) look and feel different from robots with rigid skeletons, and can have different behaviors.

Inspired by colonies of insects such as ants and bees, researchers are modeling the behavior of swarms of thousands of tiny robots which together perform a useful task, such as finding something hidden, cleaning, or spying. Each robot is quite simple, but the emergent behavior of the swarm is more complex. The whole set of robots can be considered as one single distributed system, in the same way an ant colony can be considered a superorganism, exhibiting swarm intelligence. The largest swarms so far created include the iRobot swarm, the SRI/MobileRobots CentiBots project and the Open-source Micro-robotic Project swarm, which are being used to research collective behaviors. Swarms are also more resistant to failure. Whereas one large robot may fail and ruin a mission, a swarm can continue even if several robots fail. This could make them attractive for space exploration missions, where failure is normally extremely costly.

Robotics also has application in the design of virtual reality interfaces. Specialized robots are in widespread use in the haptic research community. These robots, called "haptic interfaces", allow touch-enabled user interaction with real and virtual environments. Robotic forces allow simulating the mechanical properties of "virtual" objects, which users can experience through their sense of touch.

Robotic characters, androids (artificial men/women) or gynoids (artificial women), and cyborgs (also "bionic men/women", or humans with significant mechanical enhancements) have become a staple of science fiction.
The first reference in Western literature to mechanical servants appears in Homer's Iliad. In Book XVIII, Hephaestus, god of fire, creates new armor for the hero Achilles, assisted by robots. According to the Rieu translation, "Golden maidservants hastened to help their master. They looked like real women and could not only speak and use their limbs but were endowed with intelligence and trained in handwork by the immortal gods." Of course, the words "robot" or "android" are not used to describe them, but they are nevertheless mechanical devices human in appearance. "The first use of the word Robot was in Karel Čapek's play R.U.R. (Rossum's Universal Robots) (written in 1920)". Writer Karel Čapek was born in Czechoslovakia (Czech Republic).
Possibly the most prolific author of the twentieth century was Isaac Asimov (1920–1992) who published over five-hundred books. Asimov is probably best remembered for his science-fiction stories and especially those about robots, where he placed robots and their interaction with society at the center of many of his works. Asimov carefully considered the problem of the ideal set of instructions robots might be given in order to lower the risk to humans, and arrived at his Three Laws of Robotics: a robot may not injure a human being or, through inaction, allow a human being to come to harm; a robot must obey orders given it by human beings, except where such orders would conflict with the First Law; and a robot must protect its own existence as long as such protection does not conflict with the First or Second Law. These were introduced in his 1942 short story "Runaround", although foreshadowed in a few earlier stories. Later, Asimov added the Zeroth Law: "A robot may not harm humanity, or, by inaction, allow humanity to come to harm"; the rest of the laws are modified sequentially to acknowledge this.
According to the Oxford English Dictionary, the first passage in Asimov's short story "Liar!" (1941) that mentions the First Law is the earliest recorded use of the word robotics. Asimov was not initially aware of this; he assumed the word already existed by analogy with mechanics, hydraulics, and other similar terms denoting branches of applied knowledge.

Robots appear in many films. Most of the robots in cinema are fictional. Two of the most famous are R2-D2 and C-3PO from the Star Wars franchise.

The concept of humanoid sex robots has elicited both public attention and concern. Opponents of the concept have stated that the development of sex robots would be morally wrong. They argue that the introduction of such devices would be socially harmful, and demeaning to women and children.

Fears and concerns about robots have been repeatedly expressed in a wide range of books and films. A common theme is the development of a master race of conscious and highly intelligent robots, motivated to take over or destroy the human race. Frankenstein (1818), often called the first science fiction novel, has become synonymous with the theme of a robot or android advancing beyond its creator.
Other works with similar themes include The Mechanical Man, The Terminator, Runaway, RoboCop, the Replicators in Stargate, the Cylons in Battlestar Galactica, the Cybermen and Daleks in Doctor Who, The Matrix, Enthiran and I, Robot. Some fictional robots are programmed to kill and destroy; others gain superhuman intelligence and abilities by upgrading their own software and hardware. Examples of popular media where the robot becomes evil are 2001: A Space Odyssey, Red Planet and Enthiran.
Another common theme is the reaction, sometimes called the "uncanny valley", of unease and even revulsion at the sight of robots that mimic humans too closely.
More recently, fictional representations of artificially intelligent robots in films such as A.I. Artificial Intelligence and Ex Machina and the 2016 TV adaptation of Westworld have engaged audience sympathy for the robots themselves.

Index of robotics articles
Outline of robotics
Artificial intelligence
William Grey Walter

Robot locomotion
Simultaneous localization and mapping
Tactile sensor
Teleoperation
von Neumann machine
Wake-up robot problem

Cognitive robotics
Domestic robot
Epigenetic robotics
Evolutionary robotics
Humanoid robot
Microbotics
Robot control

AIBO
Autonomous spaceport drone ship
Driverless car
Friendly Robotics
Lely Juno family
Liquid handling robot
PatrolBot
RoboBee
Robot App Store

Čapek, Karel (1920). R.U.R., Aventinum, Prague.
Glaser, Horst Albert and Rossbach, Sabine: The Artificial Human, Frankfurt/M., Bern, New York 2011 "The Artificial Human"
TechCast Article Series, Jason Rupinski and Richard Mix, "Public Attitudes to Androids: Robot Gender, Tasks, & Pricing"
Cheney, Margaret [1989:123] (1981). Tesla, Man Out of Time. Dorset Press. New York. ISBN 0-88029-419-1
Craig, J.J. (2005). Introduction to Robotics, Pearson Prentice Hall. Upper Saddle River, NJ.
Gutkind, L. (2006). Almost Human: Making Robots Think. New York: W. W. Norton & Company, Inc.
Needham, Joseph (1986). Science and Civilization in China: Volume 2. Taipei: Caves Books Ltd.
Sotheby's New York. The Tin Toy Robot Collection of Matt Wyse (1996)
Tsai, L. W. (1999). Robot Analysis. Wiley. New York.
DeLanda, Manuel. War in the Age of Intelligent Machines. 1991. Swerve. New York.
Journal of Field Robotics

Robotics at DMOZCommunication (from Latin commūnicāre, meaning "to share") is the act of conveying intended meanings from one entity or group to another through the use of mutually understood signs and semiotic rules.
The basic steps of communication are:
The forming of communicative intent.
Message composition.
Message encoding and decoding.
Transmission of the encoded message as a sequence of signals using a specific channel or medium.
Reception of signals.
Reconstruction of the original message.
Interpretation and making sense of the reconstructed message.
The study of communication can be divided into:
Information theory which studies the quantification, storage, and communication of information in general;
Communication studies which concerns human communication;
Biosemiotics which examines the communication of organisms in general.
The channel of communication can be visual, auditory, tactile (such as in Braille) and haptic, olfactory, Kinesics, electromagnetic, or biochemical. Human communication is unique for its extensive use of abstract language.

Nonverbal communication describes the process of conveying meaning in the form of non-word messages. Examples of nonverbal communication include haptic communication, chronemic communication, gestures, body language, facial expressions, eye contact, and how one dresses. Nonverbal communication also relates to intent of a message. Examples of intent are voluntary, intentional movements like shaking a hand or winking, as well as involuntary, such as sweating. Speech also contains nonverbal elements known as paralanguage, e.g. rhythm, intonation, tempo, and stress. There may even be a pheromone component. Research has shown that up to 55% of human communication may occur through non-verbal facial expressions, and a further 38% through para-language. It affects communication most at the subconscious level and establishes trust. Likewise, written texts include nonverbal elements such as handwriting style, spatial arrangement of words and the use of emoticons to convey emotion.
Nonverbal communication demonstrates one of Wazlawick's laws: you cannot not communicate. Once proximity has formed awareness, living creatures begin interpreting any signals received. Some of the functions of nonverbal communication in humans are to complement and illustrate, to reinforce and emphasize, to replace and substitute, to control and regulate, and to contradict the denovative message.

Verbal communication is the spoken conveying of message. Human language can be defined as a system of symbols (sometimes known as lexemes) and the grammars (rules) by which the symbols are manipulated. The word "language" also refers to common properties of languages. Language learning normally occurs most intensively during human childhood. Most of the thousands of human languages use patterns of sound or gesture for symbols which enable communication with others around them. Languages tend to share certain properties, although there are exceptions. There is no defined line between a language and a dialect. Constructed languages such as Esperanto, programming languages, and various mathematical formalism is not necessarily restricted to the properties shared by human languages.

Over time the forms of and ideas about communication have evolved through the continuing progression of technology. Advances include communications psychology and media psychology, an emerging field of study.
The progression of written communication can be divided into three "information communication revolutions":
Written communication first emerged through the use of pictographs. The pictograms were made in stone, hence written communication was not yet mobile. Pictograms began to develop standardized and simplified forms.
The next step occurred when writing began to appear on paper, papyrus, clay, wax, and other media with common shared writing systems, leading to adaptable alphabets. Communication became mobile.
The final stage is characterized by the transfer of information through controlled waves of electromagnetic radiation (i.e., radio, microwave, infrared) and other electronic signals.
Communication is thus a process by which meaning is assigned and conveyed in an attempt to create shared understanding. Gregory Bateson called it "the replication of tautologies in the universe. This process, which requires a vast repertoire of skills in interpersonal processing, listening, observing, speaking, questioning, analyzing, gestures, and evaluating enables collaboration and cooperation.

Business communication is used for a wide variety of activities including, but not limited to: strategic communications planning, media relations, public relations (which can include social media, broadcast and written communications, and more), brand management, reputation management, speech-writing, customer-client relations, and internal/employee communications.
Companies with limited resources may choose to engage in only a few of these activities, while larger organizations may employ a full spectrum of communications. Since it is difficult to develop such a broad range of skills, communications professionals often specialize in one or two of these areas but usually have at least a working knowledge of most of them. By far, the most important qualifications communications professionals can possess are excellent writing ability, good 'people' skills, and the capacity to think critically and strategically.

Communication is one of the most relevant tools in political strategies, including persuasion and propaganda. In mass media research and online media research, the effort of strategist is that of getting a precise decoding, avoiding "message reactance", that is, message refusal. The reaction to a message is referred also in terms of approach to a message, as follows:
In "radical reading" the audience rejects the meanings, values, and viewpoints built into the text by its makers. Effect: message refusal.
In "dominant reading", the audience accepts the meanings, values, and viewpoints built into the text by its makers. Effect: message acceptance.
In "subordinate reading" the audience accepts, by and large, the meanings, values, and worldview built into the text by its makers. Effect: obey to the message.
Holistic approaches are used by communication campaign leaders and communication strategists in order to examine all the options, "actors" and channels that can generate change in the semiotic landscape, that is, change in perceptions, change in credibility, change in the "memetic background", change in the image of movements, of candidates, players and managers as perceived by key influencers that can have a role in generating the desired "end-state". As the European communication researcher Daniele Trevisani highlights, the shift of political communication is moving from a "mass media" approach to a holistic and semiotic approach for each specific end-state, so that, in a "holistic communication perspective", any tool becomes a potential communication tool in shaping the "infosphere" (the information environment that surrounds us). The modern political communication field is highly influenced by the framework and practices of "information operations" doctrines that derive their nature from strategic and military studies. According to this view, what is really relevant is the concept of acting on the Information Environment. The information environment is the aggregate of individuals, organizations, and systems that collect, process, disseminate, or act on information. This environment consist s of three interrelated dimensions, which continuously interact with individuals, organizations, and systems. These dimensions are known as physical, informational, and cognitive.

Family communication is the study of the communication perspective in a broadly defined family, with intimacy and trusting relationship. The main goal of family communication is to understand the interactions of family and the pattern of behaviors of family members in different circumstances. Open and honest communication creates an atmosphere that allows family members to express their differences as well as love and admiration for one another. It also helps to understand the feelings of one another.
Family communication study looks at topics such as family rules, family roles or family dialectics and how those factors could affect the communication between family members. Researchers develop theories to understand communication behaviors. Family communication study also digs deep into certain time periods of family life such as marriage, parenthood or divorce and how communication stands in those situations. It is important for family members to understand communication as a trusted way which leads to a well constructed family.

In simple terms, interpersonal communication is the communication between one person and another (or others). It is often referred to as face-to-face communication between two (or more) people. Both verbal and nonverbal communication, or body language, play a part in how one person understands another. In verbal interpersonal communication there are two types of messages being sent: a content message and a relational message. Content messages are messages about the topic at hand and relational messages are messages about the relationship itself. This means that relational messages come across in how one says something and it demonstrates a person’s feelings, whether positive or negative, towards the individual they are talking to, indicating not only how they feel about the topic at hand, but also how they feel about their relationship with the other individual.
When texting or posting something on social media the relational message is lost and can cause people to misinterpret the message. Computer-mediated communication is a largely studied topic for this reason along with many others. In the field of Interpersonal communication research, a specific model, the Four-Distances Model of Communication, describes the four main variables that can generate "relational distance" in interpersonal communication, as opposed to a sense of "closeness" or in relational terms. The variables are Role differences, Communication Codes differences, Value and Ideological differences, and Experiential differences (personal history differences and differences in personal emotional history).
The model has been used also to study interpersonal communication problems occurred in space crews inside the International Space Station, and in other cases where interpersonal communication played a critical role in the outcome of crisis events, as in the Costa Concordia disaster.

Barriers to effective communication can retard or distort the message and intention of the message being conveyed which may result in failure of the communication process or an effect that is undesirable. These include filtering, selective perception, information overload, emotions, language, silence, communication apprehension, gender differences and political correctness
This also includes a lack of expressing "knowledge-appropriate" communication, which occurs when a person uses ambiguous or complex legal words, medical jargon, or descriptions of a situation or environment that is not understood by the recipient.
Physical barriers- Physical barriers are often due to the nature of the environment. An example of this is the natural barrier which exists if staff are located in different buildings or on different sites. Likewise, poor or outdated equipment, particularly the failure of management to introduce new technology, may also cause problems. Staff shortages are another factor which frequently causes communication difficulties for an organization.
System design- System design faults refer to problems with the structures or systems in place in an organization. Examples might include an organizational structure which is unclear and therefore makes it confusing to know whom to communicate with. Other examples could be inefficient or inappropriate information systems, a lack of supervision or training, and a lack of clarity in roles and responsibilities which can lead to staff being uncertain about what is expected of them.
Attitudinal barriers- Attitudinal barriers come about as a result of problems with staff in an organization. These may be brought about, for example, by such factors as poor management, lack of consultation with employees, personality conflicts which can result in people delaying or refusing to communicate, the personal attitudes of individual employees which may be due to lack of motivation or dissatisfaction at work, brought about by insufficient training to enable them to carry out particular tasks, or simply resistance to change due to entrenched attitudes and ideas.
Ambiguity of words/phrases- Words sounding the same but having different meaning can convey a different meaning altogether. Hence the communicator must ensure that the receiver receives the same meaning. It is better if such words are avoided by using alternatives whenever possible.
Individual linguistic ability- The use of jargon, difficult or inappropriate words in communication can prevent the recipients from understanding the message. Poorly explained or misunderstood messages can also result in confusion. However, research in communication has shown that confusion can lend legitimacy to research when persuasion fails.
Physiological barriers- These may result from individuals' personal discomfort, caused—for example—by ill health, poor eyesight or hearing difficulties.
Bypassing-These happens when the communicators (sender and the receiver) do not attach the same symbolic meanings to their words. It is when the sender is expressing a thought or a word but the receiver take it in a different meaning. For example- ASAP, Rest room
Technological multi-tasking and absorbency- With a rapid increase in technologically-driven communication in the past several decades, individuals are increasingly faced with condensed communication in the form of e-mail, text, and social updates. This has, in turn, led to a notable change in the way younger generations communicate and perceive their own self-efficacy to communicate and connect with others. With the ever-constant presence of another "world" in one's pocket, individuals are multi-tasking both physically and cognitively as constant reminders of something else happening somewhere else bombard them. Though perhaps too new of an advancement to yet see long-term effects, this is a notion currently explored by such figures as Sherry Turkle.
Fear of being criticized-This is a major factor that prevents good communication. If we exercise simple practices to improve our communication skill, we can become effective communicators. For example, read an article from the newspaper or collect some news from the television and present it in front of the mirror. This will not only boost your confidence, but also improve your language and vocabulary.
Gender barriers- Most communicators whether aware or not, often have a set agenda. This is very notable among the different genders. For example, many women are found to be more critical in addressing conflict. It's also been noted that men are more than likely to withdraw from conflict when in comparison to women. This breakdown and comparison not only shows that there are many factors to communication between two specific genders, but also room for improvement as well as established guidelines for all.

Cultural differences exist within countries (tribal/regional differences, dialects etc.), between religious groups and in organisations or at an organisational level - where companies, teams and units may have different expectations, norms and idiolects. Families and family groups may also experience the effect of cultural barriers to communication within and between different family members or groups. For example: words, colours and symbols have different meanings in different cultures. In most parts of the world, nodding your head means agreement, shaking your head means no, except in some parts of the world.
Communication to a great extent is influenced by culture and cultural variables. Understanding cultural aspects of communication refers to having knowledge of different cultures in order to communicate effectively with cross culture people. Cultural aspects of communication are of great relevance in today's world which is now a global village, thanks to globalisation. Cultural aspects of communication are the cultural differences which influences communication across borders. Impact of cultural differences on communication components are explained below:
1) Verbal communication refers to form of communication which uses spoken and written words for expressing and transferring views and ideas. Language is the most important tool of verbal communication and it is the area where cultural difference play its role. All countries have different languages and to have a better understanding of different culture it is required to have knowledge of languages of different countries.
2) Non verbal communication is a very wide concept and it includes all the other forms of communication which do not uses written or spoken words. Non verbal communication takes following forms:
Paralinguistics are the voice involved in communication other than actual language and involves tones, pitch, vocal cues etc. It also include sounds from throat and all these are greatly influenced by cultural differences across borders.
Proxemics deals with the concept of space element in communication. Proxemics explains four zones of spaces namely intimate personal, social and public. This concept differs with different culture as the permissible space vary in different countries.
Artifactics studies about the non verbal signals or communication which emerges from personal accessories such as dresses or fashion accessories worn and it varies with culture as people of different countries follow different dressing codes.
Chronemics deal with the time aspects of communication and also include importance given to the time. some issues explaining this conceptpt are pauses, silences and response lag during an interaction. This aspect of communication is also influenced by cultural differences as it is well known that there is a great difference in the value given by different cultures to time.
Kinesics mainly deals with the body languages such as postures, gestures, head nods, leg movements etc. In different countries, the same gestures and postures are used to convey different messages. Sometimes even a particular kinesic indicating something good in a country may have a negative meaning in any other culture.
So in order to have an effective communication across world it is desirable to have a knowledge of cultural variables effecting communication.
According to Michael Walsh and Ghil'ad Zuckermann, Western conversational interaction is typically "dyadic", between two particular people, where eye contact is important and the speaker controls the interaction; and "contained" in a relatively short, defined time frame. However, traditional Aboriginal conversational interaction is "communal", broadcast to many people, eye contact is not important, the listener controls the interaction; and "continuous", spread over a longer, indefinite time frame.

Every information exchange between living organisms — i.e. transmission of signals that involve a living sender and receiver can be considered a form of communication; and even primitive creatures such as corals are competent to communicate. Nonhuman communication also include cell signaling, cellular communication, and chemical transmissions between primitive organisms like bacteria and within the plant and fungal kingdoms.

The broad field of animal communication encompasses most of the issues in ethology. Animal communication can be defined as any behavior of one animal that affects the current or future behavior of another animal. The study of animal communication, called zoo semiotics (distinguishable from anthroposemiotics, the study of human communication) has played an important part in the development of ethology, sociobiology, and the study of animal cognition. Animal communication, and indeed the understanding of the animal world in general, is a rapidly growing field, and even in the 21st century so far, a great share of prior understanding related to diverse fields such as personal symbolic name use, animal emotions, animal culture and learning, and even sexual conduct, long thought to be well understood, has been revolutionized. A special field of animal communication has been investigated in more detail such as vibrational communication.

Communication is observed within the plant organism, i.e. within plant cells and between plant cells, between plants of the same or related species, and between plants and non-plant organisms, especially in the root zone. Plant roots communicate with rhizome bacteria, fungi, and insects within the soil. These interactions are governed by syntactic, pragmatic, and semantic rules, and are possible because of the decentralized "nervous system" of plants. The original meaning of the word "neuron" in Greek is "vegetable fiber" and recent research has shown that most of the microorganism plant communication processes are neuron-like. Plants also communicate via volatiles when exposed to herbivory attack behavior, thus warning neighboring plants. In parallel they produce other volatiles to attract parasites which attack these herbivores. In stress situations plants can overwrite the genomes they inherited from their parents and revert to that of their grand- or great-grandparents.
Fungi communicate to coordinate and organize their growth and development such as the formation of Marcelia and fruiting bodies. Fungi communicate with their own and related species as well as with non fungal organisms in a great variety of symbiotic interactions, especially with bacteria, unicellular eukaryote, plants and insects through biochemicals of biotic origin. The biochemicals trigger the fungal organism to react in a specific manner, while if the same chemical molecules are not part of biotic messages, they do not trigger the fungal organism to react. This implies that fungal organisms can differentiate between molecules taking part in biotic messages and similar molecules being irrelevant in the situation. So far five different primary signalling molecules are known to coordinate different behavioral patterns such as filamentation, mating, growth, and pathogenicity. Behavioral coordination and production of signaling substances is achieved through interpretation processes that enables the organism to differ between self or non-self, a biotic indicator, biotic message from similar, related, or non-related species, and even filter out "noise", i.e. similar molecules without biotic content.

Communication is not a tool used only by humans, plants and animals, but it is also used by microorganisms like bacteria. The process is called quorum sensing. Through quorum sensing, bacteria are able to sense the density of cells, and regulate gene expression accordingly. This can be seen in both gram positive and gram negative bacteria. This was first observed by Fuqua et al. in marine microorganisms like V. harveyi and V. fischeri.

The first major model for communication was introduced by Claude Shannon and Warren Weaver for Bell Laboratories in 1949 The original model was designed to mirror the functioning of radio and telephone technologies. Their initial model consisted of three primary parts: sender, channel, and receiver. The sender was the part of a telephone a person spoke into, the channel was the telephone itself, and the receiver was the part of the phone where one could hear the other person. Shannon and Weaver also recognized that often there is static that interferes with one listening to a telephone conversation, which they deemed noise.
In a simple model, often referred to as the transmission model or standard view of communication, information or content (e.g. a message in natural language) is sent in some form (as spoken language) from an emisor/ sender/ encoder to a destination/ receiver/ decoder. This common conception of communication simply views communication as a means of sending and receiving information. The strengths of this model are simplicity, generality, and quantifiability. Claude Shannon and Warren Weaver structured this model based on the following elements:
An information source, which produces a message.
A transmitter, which encodes the message into signals
A channel, to which signals are adapted for transmission
A noise source, which distorts the signal while it propagates through the channel
A receiver, which 'decodes' (reconstructs) the message from the signal.
A destination, where the message arrives.
Shannon and Weaver argued that there were three levels of problems for communication within this theory.
The technical problem: how accurately can the message be transmitted?
The semantic problem: how precisely is the meaning 'conveyed'?
The effectiveness problem: how effectively does the received meaning affect behavior?
Daniel Chandler critiques the transmission model by stating:
It assumes communicators are isolated individuals.
No allowance for differing purposes.
No allowance for differing interpretations.
No allowance for unequal power relations.
No allowance for situational contexts.
In 1960, David Berlo expanded on Shannon and Weaver's (1949) linear model of communication and created the SMCR Model of Communication. The Sender-Message-Channel-Receiver Model of communication separated the model into clear parts and has been expanded upon by other scholars.
Communication is usually described along a few major dimensions: Message (what type of things are communicated), source / emisor / sender / encoder (by whom), form (in which form), channel (through which medium), destination / receiver / target / decoder (to whom), and Receiver. Wilbur Schram (1954) also indicated that we should also examine the impact that a message has (both desired and undesired) on the target of the message. Between parties, communication includes acts that confer knowledge and experiences, give advice and commands, and ask questions. These acts may take many forms, in one of the various manners of communication. The form depends on the abilities of the group communicating. Together, communication content and form make messages that are sent towards a destination. The target can be oneself, another person or being, another entity (such as a corporation or group of beings).
Communication can be seen as processes of information transmission with three levels of semiotic rules:
Pragmatic (concerned with the relations between signs/expressions and their users)
Semantic (study of relationships between signs and symbols and what they represent) and
Syntactic (formal properties of signs and symbols).
Therefore, communication is social interaction where at least two interacting agents share a common set of signs and a common set of semiotic rules. This commonly held rule in some sense ignores autocommunication, including intrapersonal communication via diaries or self-talk, both secondary phenomena that followed the primary acquisition of communicative competences within social interactions.
In light of these weaknesses, Barnlund (2008) proposed a transactional model of communication. The basic premise of the transactional model of communication is that individuals are simultaneously engaging in the sending and receiving of messages.
In a slightly more complex form a sender and a receiver are linked reciprocally. This second attitude of communication, referred to as the constitutive model or constructionist view, focuses on how an individual communicates as the determining factor of the way the message will be interpreted. Communication is viewed as a conduit; a passage in which information travels from one individual to another and this information becomes separate from the communication itself. A particular instance of communication is called a speech act. The sender's personal filters and the receiver's personal filters may vary depending upon different regional traditions, cultures, or gender; which may alter the intended meaning of message contents. In the presence of "communication noise" on the transmission channel (air, in this case), reception and decoding of content may be faulty, and thus the speech act may not achieve the desired effect. One problem with this encode-transmit-receive-decode model is that the processes of encoding and decoding imply that the sender and receiver each possess something that functions as a codebook, and that these two code books are, at the very least, similar if not identical. Although something like code books is implied by the model, they are nowhere represented in the model, which creates many conceptual difficulties.
Theories of coregulation describe communication as a creative and dynamic continuous process, rather than a discrete exchange of information. Canadian media scholar Harold Innis had the theory that people use different types of media to communicate and which one they choose to use will offer different possibilities for the shape and durability of society (Wark, McKenzie 1997). His famous example of this is using ancient Egypt and looking at the ways they built themselves out of media with very different properties stone and papyrus. Papyrus is what he called 'Space Binding'. it made possible the transmission of written orders across space, empires and enables the waging of distant military campaigns and colonial administration. The other is stone and 'Time Binding', through the construction of temples and the pyramids can sustain their authority generation to generation, through this media they can change and shape communication in their society (Wark, McKenzie 1997).

In any communication model, noise is interference with the decoding of messages sent over a channel by an encoder. There are many examples of noise:
Environmental noise. Noise that physically disrupts communication, such as standing next to loud speakers at a party, or the noise from a construction site next to a classroom making it difficult to hear the professor.
Physiological-impairment noise. Physical maladies that prevent effective communication, such as actual deafness or blindness preventing messages from being received as they were intended.
Semantic noise. Different interpretations of the meanings of certain words. For example, the word "weed" can be interpreted as an undesirable plant in a yard, or as a euphemism for marijuana.
Syntactical noise. Mistakes in grammar can disrupt communication, such as abrupt changes in verb tense during a sentence.
Organizational noise. Poorly structured communication can prevent the receiver from accurate interpretation. For example, unclear and badly stated directions can make the receiver even more lost.
Cultural noise. Stereotypical assumptions can cause misunderstandings, such as unintentionally offending a non-Christian person by wishing them a "Merry Christmas".
Psychological noise. Certain attitudes can also make communication difficult. For instance, great anger or sadness may cause someone to lose focus on the present moment. Disorders such as autism may also severely hamper effective communication.
To face communication noise, redundancy and acknowledgement must often be used. Acknowledgements are messages from the addressee informing the originator that his/her communication has been received and is understood. Message repetition and feedback about message received are necessary in the presence of noise to reduce the probability of misunderstanding.

Advice
Augmentative and alternative communication
Communication rights
Data communication
Four Cs of 21st century learning
Human communication
Inter Mirifica
Intercultural communication
Ishin-denshin
Proactive communications
Sign system
Small talk
SPEAKING
Telecommunication
Telepathy
Understanding
21st century skills
Assertion Theory

Innis, Harold. Empire and Communications. Rev. by Mary Q. Innis; foreword by Marshall McLuhan. Toronto, Ont.: University of Toronto Press, 1972. xii, 184 p. N.B.: "Here he [i.e. Innis] develops his theory that the history of empires is determined to a large extent by their means of communication."—From the back cover of the book's pbk. ed. ISBN 0-8020-6119-2 pbkInformation is that which informs. In other words, it is the answer to a question of some kind. It is thus related to data and knowledge, as data represents values attributed to parameters, and knowledge signifies understanding of real things or abstract concepts. As it regards data, the information's existence is not necessarily coupled to an observer (it exists beyond an event horizon, for example), while in the case of knowledge, the information requires a cognitive observer.
At its most fundamental, information is any propagation of cause and effect within a system. Information is conveyed either as the content of a message or through direct or indirect observation of anything. That which is perceived can be construed as a message in its own right, and in that sense, information is always conveyed as the content of a message.
Information can be encoded into various forms for transmission and interpretation (for example, information may be encoded into a sequence of signs, or transmitted via a sequence of signals). It can also be encrypted for safe storage and communication.
Information reduces uncertainty. The uncertainty of an event is measured by its probability of occurrence and is inversely proportional to that. The more uncertain an event, the more information is required to resolve uncertainty of that event. The bit is a typical unit of information, but other units such as the nat may be used. Example: information in one "fair" coin ﬂip: log2(2/1)  2 bits.
The concept that information is the message has different meanings in different contexts. Thus the concept of information becomes closely related to notions of constraint, communication, control, data, form, education, knowledge, meaning, understanding, mental stimuli, pattern, perception, representation, and entropy.

The English word was apparently derived from the Latin stem (information-) of the nominative (informatio): this noun is derived from the verb informare (to inform) in the sense of "to give form to the mind", "to discipline", "instruct", "teach". Inform itself comes (via French informer) from the Latin verb informare, which means to give form, or to form an idea of. Furthermore, Latin itself already contained the word informatio meaning concept or idea, but the extent to which this may have influenced the development of the word information in English is not clear.
The ancient Greek word for form was μορφή (morphe; cf. morph) and also εἶδος (eidos) "kind, idea, shape, set", the latter word was famously used in a technical philosophical sense by Plato (and later Aristotle) to denote the ideal identity or essence of something (see Theory of Forms). "Eidos" can also be associated with thought, proposition, or even concept.
The ancient Greek word for information is πληροφορία, which transliterates (plērophoria) from πλήρης (plērēs) "fully" and φέρω (phorein) frequentative of (pherein) to carry-through. It literally means "fully bears" or "conveys fully". In modern Greek language the word Πληροφορία is still in daily use and has the same meaning as the word information in English. In addition to its primary meaning, the word Πληροφορία as a symbol has deep roots in Aristotle's semiotic triangle. In this regard it can be interpreted to communicate information to the one decoding that specific type of sign. This is something that occurs frequently with the etymology of many words in ancient and modern Greek language where there is a very strong denotative relationship between the signifier, e.g. the word symbol that conveys a specific encoded interpretation, and the signified, e.g. a concept whose meaning the interpreter attempts to decode.

From the stance of information theory, information is taken as an ordered sequence of symbols from an alphabet, say an input alphabet χ, and an output alphabet ϒ. Information processing consists of an input-output function that maps any input sequence from χ into an output sequence from ϒ. The mapping may be probabilistic or deterministic. It may have memory or be memoryless.

Often information can be viewed as a type of input to an organism or system. Inputs are of two kinds; some inputs are important to the function of the organism (for example, food) or system (energy) by themselves. In his book Sensory Ecology Dusenbery called these causal inputs. Other inputs (information) are important only because they are associated with causal inputs and can be used to predict the occurrence of a causal input at a later time (and perhaps another place). Some information is important because of association with other information but eventually there must be a connection to a causal input. In practice, information is usually carried by weak stimuli that must be detected by specialized sensory systems and amplified by energy inputs before they can be functional to the organism or system. For example, light is often a causal input to plants but provides information to animals. The colored light reflected from a flower is too weak to do much photosynthetic work but the visual system of the bee detects it and the bee's nervous system uses the information to guide the bee to the flower, where the bee often finds nectar or pollen, which are causal inputs, serving a nutritional function.

The cognitive scientist and applied mathematician Ronaldo Vigo argues that information is a concept that involves at least two related entities in order to make quantitative sense. These are, any dimensionally defined category of objects S, and any of its subsets R. R, in essence, is a representation of S, or, in other words, conveys representational (and hence, conceptual) information about S. Vigo then defines the amount of information that R conveys about S as the rate of change in the complexity of S whenever the objects in R are removed from S. Under "Vigo information", pattern, invariance, complexity, representation, and information—five fundamental constructs of universal science—are unified under a novel mathematical framework. Among other things, the framework aims to overcome the limitations of Shannon-Weaver information when attempting to characterize and measure subjective information.

Information is any type of pattern that influences the formation or transformation of other patterns. In this sense, there is no need for a conscious mind to perceive, much less appreciate, the pattern. Consider, for example, DNA. The sequence of nucleotides is a pattern that influences the formation and development of an organism without any need for a conscious mind.
Systems theory at times seems to refer to information in this sense, assuming information does not necessarily involve any conscious mind, and patterns circulating (due to feedback) in the system can be called information. In other words, it can be said that information in this sense is something potentially perceived as representation, though not created or presented for that purpose. For example, Gregory Bateson defines "information" as a "difference that makes a difference".
If, however, the premise of "influence" implies that information has been perceived by a conscious mind and also interpreted by it, the specific context associated with this interpretation may cause the transformation of the information into knowledge. Complex definitions of both "information" and "knowledge" make such semantic and logical analysis difficult, but the condition of "transformation" is an important point in the study of information as it relates to knowledge, especially in the business discipline of knowledge management. In this practice, tools and processes are used to assist a knowledge worker in performing research and making decisions, including steps such as:
reviewing information in order to effectively derive value and meaning
referencing metadata if any is available
establishing a relevant context, often selecting from many possible contexts
deriving new knowledge from the information
making decisions or recommendations from the resulting knowledge.
Stewart (2001) argues that the transformation of information into knowledge is a critical one, lying at the core of value creation and competitive advantage for the modern enterprise.
The Danish Dictionary of Information Terms argues that information only provides an answer to a posed question. Whether the answer provides knowledge depends on the informed person. So a generalized definition of the concept should be: "Information" = An answer to a specific question".
When Marshall McLuhan speaks of media and their effects on human cultures, he refers to the structure of artifacts that in turn shape our behaviors and mindsets. Also, pheromones are often said to be "information" in this sense.

Information has a well-defined meaning in physics. In 2003 J. D. Bekenstein claimed that a growing trend in physics was to define the physical world as being made up of information itself (and thus information is defined in this way) (see Digital physics). Examples of this include the phenomenon of quantum entanglement, where particles can interact without reference to their separation or the speed of light. Material information itself cannot travel faster than light even if that information is transmitted indirectly. This could lead to all attempts at physically observing a particle with an "entangled" relationship to another being slowed down, even though the particles are not connected in any other way other than by the information they carry.
The mathematical universe hypothesis suggests a new paradigm, in which virtually everything, from particles and fields, through biological entities and consciousness, to the multiverse itself, could be described by mathematical patterns of information. By the same token, the cosmic void can be conceived of as the absence of material information in space (setting aside the virtual particles that pop in and out of existence due to quantum fluctuations, as well as the gravitational field and the dark energy). Nothingness can be understood then as that within which no matter, energy, space, time, or any other type of information could exist, which would be possible if symmetry and structure break within the manifold of the multiverse (i.e. the manifold would have tears or holes).
Another link is demonstrated by the Maxwell's demon thought experiment. In this experiment, a direct relationship between information and another physical property, entropy, is demonstrated. A consequence is that it is impossible to destroy information without increasing the entropy of a system; in practical terms this often means generating heat. Another more philosophical outcome is that information could be thought of as interchangeable with energy. Toyabe et al. experimentally showed in nature that information can be converted into work. Thus, in the study of logic gates, the theoretical lower bound of thermal energy released by an AND gate is higher than for the NOT gate (because information is destroyed in an AND gate and simply converted in a NOT gate). Physical information is of particular importance in the theory of quantum computers.
In Thermodynamics, information is any kind of event that affects the state of a dynamic system that can interpret the information.

The information cycle (addressed as a whole or in its distinct components) is of great concern to Information Technology, Information Systems, as well as Information Science. These fields deal with those processes and techniques pertaining to information capture (through sensors) and generation (through computation, formulation or composition), processing (including encoding, encryption, compression, packaging), transmission (including all telecommunication methods), presentation (including visualization / display methods), storage (such as magnetic or optical, including holographic methods), etc. Information does not cease to exist, it may only get scrambled beyond any possibility of retrieval (within Information Theory, see lossy compression; in Physics, the black hole information paradox gets solved with the aid of the holographic principle).
Information Visualization (shortened as InfoVis) depends on the computation and digital representation of data, and assists users in pattern recognition and anomaly detection.

Information Security (shortened as InfoSec) is the ongoing process of exercising due diligence to protect information, and information systems, from unauthorized access, use, disclosure, destruction, modification, disruption or distribution, through algorithms and procedures focused on monitoring and detection, as well as incident response and repair.
Information Analysis is the process of inspecting, transforming, and modelling information, by converting raw data into actionable knowledge, in support of the decision-making process.
Information Quality (shortened as InfoQ) is the potential of a dataset to achieve a specific (scientific or practical) goal using a given empirical analysis method.
Information Communication represents the convergence of informatics, telecommunication and audio-visual media & content.

It is estimated that the world's technological capacity to store information grew from 2.6 (optimally compressed) exabytes in 1986 – which is the informational equivalent to less than one 730-MB CD-ROM per person (539 MB per person) – to 295 (optimally compressed) exabytes in 2007. This is the informational equivalent of almost 61 CD-ROM per person in 2007.
The world’s combined technological capacity to receive information through one-way broadcast networks was the informational equivalent of 174 newspapers per person per day in 2007.
The world's combined effective capacity to exchange information through two-way telecommunication networks was the informational equivalent of 6 newspapers per person per day in 2007.

Records are specialized forms of information. Essentially, records are information produced consciously or as by-products of business activities or transactions and retained because of their value. Primarily, their value is as evidence of the activities of the organization but they may also be retained for their informational value. Sound records management ensures that the integrity of records is preserved for as long as they are required.
The international standard on records management, ISO 15489, defines records as "information created, received, and maintained as evidence and information by an organization or person, in pursuance of legal obligations or in the transaction of business". The International Committee on Archives (ICA) Committee on electronic records defined a record as, "a specific piece of recorded information generated, collected or received in the initiation, conduct or completion of an activity and that comprises sufficient content, context and structure to provide proof or evidence of that activity".
Records may be maintained to retain corporate memory of the organization or to meet legal, fiscal or accountability requirements imposed on the organization. Willis (2005) expressed the view that sound management of business records and information delivered "...six key requirements for good corporate governance...transparency; accountability; due process; compliance; meeting statutory and common law requirements; and security of personal and corporate information."

Beynon-Davies explains the multi-faceted concept of information in terms of signs and signal-sign systems. Signs themselves can be considered in terms of four inter-dependent levels, layers or branches of semiotics: pragmatics, semantics, syntax, and empirics. These four layers serve to connect the social world on the one hand with the physical or technical world on the other.
Pragmatics is concerned with the purpose of communication. Pragmatics links the issue of signs with the context within which signs are used. The focus of pragmatics is on the intentions of living agents underlying communicative behaviour. In other words, pragmatics link language to action.
Semantics is concerned with the meaning of a message conveyed in a communicative act. Semantics considers the content of communication. Semantics is the study of the meaning of signs - the association between signs and behaviour. Semantics can be considered as the study of the link between symbols and their referents or concepts – particularly the way in which signs relate to human behavior.
Syntax is concerned with the formalism used to represent a message. Syntax as an area studies the form of communication in terms of the logic and grammar of sign systems. Syntax is devoted to the study of the form rather than the content of signs and sign-systems.
Nielsen (2008) discusses the relationship between semiotics and information in relation to dictionaries. The concept of lexicographic information costs is introduced and refers to the efforts users of dictionaries need to make in order to, first, find the data sought and, secondly, understand the data so that they can generate information.
Communication normally exists within the context of some social situation. The social situation sets the context for the intentions conveyed (pragmatics) and the form in which communication takes place. In a communicative situation intentions are expressed through messages which comprise collections of inter-related signs taken from a language which is mutually understood by the agents involved in the communication. Mutual understanding implies that agents involved understand the chosen language in terms of its agreed syntax (syntactics) and semantics. The sender codes the message in the language and sends the message as signals along some communication channel (empirics). The chosen communication channel will have inherent properties which determine outcomes such as the speed with which communication can take place and over what distance.

Alan Liu (2004). The Laws of Cool: Knowledge Work and the Culture of Information, University of Chicago Press
Bekenstein, Jacob D. (2003, August). Information in the holographic universe. Scientific American.
Gleick, James (2011). The Information: A History, a Theory, a Flood. Pantheon, New York, NY.
Shu-Kun Lin (2008). 'Gibbs Paradox and the Concepts of Information, Symmetry, Similarity and Their Relationship', Entropy, 10 (1), 1-5. Available online at Entropy journal website.
Luciano Floridi, (2005). 'Is Information Meaningful Data?', Philosophy and Phenomenological Research, 70 (2), pp. 351 – 370. Available online at PhilSci Archive
Luciano Floridi, (2005). 'Semantic Conceptions of Information', The Stanford Encyclopedia of Philosophy (Winter 2005 Edition), Edward N. Zalta (ed.). Available online at Stanford University
Luciano Floridi, (2010). Information: A Very Short Introduction, Oxford University Press, Oxford.
Robert K. Logan. What is Information? - Propagating Organization in the Biosphere, the Symbolosphere, the Technosphere and the Econosphere,
Toronto: DEMO Publishing.
Sandro Nielsen: 'The Effect of Lexicographical Information Costs on Dictionary Making and Use', Lexikos 18/2008, 170-189.
Stewart, Thomas, (2001). Wealth of Knowledge. Doubleday, New York, NY, 379 p.
Young, Paul. The Nature of Information (1987). Greenwood Publishing Group, Westport, Ct. ISBN 0-275-92698-2.

Semantic Conceptions of Information Review by Luciano Floridi for the Stanford Encyclopedia of Philosophy
Principia Cybernetica entry on negentropy
Fisher Information, a New Paradigm for Science: Introduction, Uncertainty principles, Wave equations, Ideas of Escher, Kant, Plato and Wheeler. This essay is continually revised in the light of ongoing research.
How Much Information? 2003 an attempt to estimate how much new information is created each year (study was produced by faculty and students at the School of Information Management and Systems at the University of California at Berkeley)
(Danish) Informationsordbogen.dk The Danish Dictionary of Information Terms / InformationsordbogenMedicine (British English /ˈmɛdsᵻn/; American English /ˈmɛdᵻsᵻn/) is the science and practice of the diagnosis, treatment, and prevention of disease. The word "medicine" is derived from Latin medicus, meaning "a physician". Medicine encompasses a variety of health care practices evolved to maintain and restore health by the prevention and treatment of illness. Contemporary medicine applies biomedical sciences, biomedical research, genetics, and medical technology to diagnose, treat, and prevent injury and disease, typically through pharmaceuticals or surgery, but also through therapies as diverse as psychotherapy, external splints and traction, medical devices, biologics, and ionizing radiation, amongst others.
Medicine has existed for thousands of years, during most of which it was an art (an area of skill and knowledge) frequently having connections to the religious and philosophical beliefs of local culture. For example, a medicine man would apply herbs and say prayers for healing, or an ancient philosopher and physician would apply bloodletting according to the theories of humorism. In recent centuries, since the advent of modern science, most medicine has become a combination of art and science (both basic and applied, under the umbrella of medical science). While stitching technique for sutures is an art learned through practice, the knowledge of what happens at the cellular and molecular level in the tissues being stitched arises through science.
Prescientific forms of medicine are now known as traditional medicine and folk medicine. They remain commonly used with or instead of scientific medicine and are thus called alternative medicine. For example, evidence on the effectiveness of acupuncture is "variable and inconsistent" for any condition, but is generally safe when done by an appropriately trained practitioner. In contrast, treatments outside the bounds of safety and efficacy are termed quackery.

Medical availability and clinical practice varies across the world due to regional differences in culture and technology. Modern scientific medicine is highly developed in the Western world, while in developing countries such as parts of Africa or Asia, the population may rely more heavily on traditional medicine with limited evidence and efficacy and no required formal training for practitioners. Even in the developed world however, evidence-based medicine is not universally used in clinical practice; for example, a 2007 survey of literature reviews found that about 49% of the interventions lacked sufficient evidence to support either benefit or harm.
In modern clinical practice, doctors personally assess patients in order to diagnose, treat, and prevent disease using clinical judgment. The doctor-patient relationship typically begins an interaction with an examination of the patient's medical history and medical record, followed by a medical interview and a physical examination. Basic diagnostic medical devices (e.g. stethoscope, tongue depressor) are typically used. After examination for signs and interviewing for symptoms, the doctor may order medical tests (e.g. blood tests), take a biopsy, or prescribe pharmaceutical drugs or other therapies. Differential diagnosis methods help to rule out conditions based on the information provided. During the encounter, properly informing the patient of all relevant facts is an important part of the relationship and the development of trust. The medical encounter is then documented in the medical record, which is a legal document in many jurisdictions. Follow-ups may be shorter but follow the same general procedure, and specialists follow a similar process. The diagnosis and treatment may take only a few minutes or a few weeks depending upon the complexity of the issue.
The components of the medical interview and encounter are:
Chief complaint (CC): the reason for the current medical visit. These are the 'symptoms.' They are in the patient's own words and are recorded along with the duration of each one. Also called 'chief concern' or 'presenting complaint'.
History of present illness (HPI): the chronological order of events of symptoms and further clarification of each symptom. Distinguishable from history of previous illness, often called past medical history (PMH). Medical history comprises HPI and PMH.
Current activity: occupation, hobbies, what the patient actually does.
Medications (Rx): what drugs the patient takes including prescribed, over-the-counter, and home remedies, as well as alternative and herbal medicines/herbal remedies. Allergies are also recorded.
Past medical history (PMH/PMHx): concurrent medical problems, past hospitalizations and operations, injuries, past infectious diseases and/or vaccinations, history of known allergies.
Social history (SH): birthplace, residences, marital history, social and economic status, habits (including diet, medications, tobacco, alcohol).
Family history (FH): listing of diseases in the family that may impact the patient. A family tree is sometimes used.
Review of systems (ROS) or systems inquiry: a set of additional questions to ask, which may be missed on HPI: a general enquiry (have you noticed any weight loss, change in sleep quality, fevers, lumps and bumps? etc.), followed by questions on the body's main organ systems (heart, lungs, digestive tract, urinary tract, etc.).
The physical examination is the examination of the patient for medical signs of disease, which are objective and observable, in contrast to symptoms which are volunteered by the patient and not necessarily objectively observable. The healthcare provider uses the senses of sight, hearing, touch, and sometimes smell (e.g., in infection, uremia, diabetic ketoacidosis). Four actions are the basis of physical examination: inspection, palpation (feel), percussion (tap to determine resonance characteristics), and auscultation (listen), generally in that order although auscultation occurs prior to percussion and palpation for abdominal assessments.
The clinical examination involves the study of:
Vital signs including height, weight, body temperature, blood pressure, pulse, respiration rate, and hemoglobin oxygen saturation
General appearance of the patient and specific indicators of disease (nutritional status, presence of jaundice, pallor or clubbing)
Skin
Head, eye, ear, nose, and throat (HEENT)
Cardiovascular (heart and blood vessels)
Respiratory (large airways and lungs)
Abdomen and rectum
Genitalia (and pregnancy if the patient is or could be pregnant)
Musculoskeletal (including spine and extremities)
Neurological (consciousness, awareness, brain, vision, cranial nerves, spinal cord and peripheral nerves)
Psychiatric (orientation, mental state, evidence of abnormal perception or thought).
It is to likely focus on areas of interest highlighted in the medical history and may not include everything listed above.
The treatment plan may include ordering additional medical laboratory tests and medical imaging studies, starting therapy, referral to a specialist, or watchful observation. Follow-up may be advised. Depending upon the health insurance plan and the managed care system, various forms of "utilization review", such as prior authorization of tests, may place barriers on accessing expensive services.
The medical decision-making (MDM) process involves analysis and synthesis of all the above data to come up with a list of possible diagnoses (the differential diagnoses), along with an idea of what needs to be done to obtain a definitive diagnosis that would explain the patient's problem.
On subsequent visits, the process may be repeated in an abbreviated manner to obtain any new history, symptoms, physical findings, and lab or imaging results or specialist consultations.

Contemporary medicine is in general conducted within health care systems. Legal, credentialing and financing frameworks are established by individual governments, augmented on occasion by international organizations, such as churches. The characteristics of any given health care system have significant impact on the way medical care is provided.
From ancient times, Christian emphasis on practical charity gave rise to the development of systematic nursing and hospitals and the Catholic Church today remains the largest non-government provider of medical services in the world. Advanced industrial countries (with the exception of the United States) and many developing countries provide medical services through a system of universal health care that aims to guarantee care for all through a single-payer health care system, or compulsory private or co-operative health insurance. This is intended to ensure that the entire population has access to medical care on the basis of need rather than ability to pay. Delivery may be via private medical practices or by state-owned hospitals and clinics, or by charities, most commonly by a combination of all three.
Most tribal societies provide no guarantee of healthcare for the population as a whole. In such societies, healthcare is available to those that can afford to pay for it or have self-insured it (either directly or as part of an employment contract) or who may be covered by care financed by the government or tribe directly.

Transparency of information is another factor defining a delivery system. Access to information on conditions, treatments, quality, and pricing greatly affects the choice by patients/consumers and, therefore, the incentives of medical professionals. While the US healthcare system has come under fire for lack of openness, new legislation may encourage greater openness. There is a perceived tension between the need for transparency on the one hand and such issues as patient confidentiality and the possible exploitation of information for commercial gain on the other.

Provision of medical care is classified into primary, secondary, and tertiary care categories.

Primary care medical services are provided by physicians, physician assistants, nurse practitioners, or other health professionals who have first contact with a patient seeking medical treatment or care. These occur in physician offices, clinics, nursing homes, schools, home visits, and other places close to patients. About 90% of medical visits can be treated by the primary care provider. These include treatment of acute and chronic illnesses, preventive care and health education for all ages and both sexes.
Secondary care medical services are provided by medical specialists in their offices or clinics or at local community hospitals for a patient referred by a primary care provider who first diagnosed or treated the patient. Referrals are made for those patients who required the expertise or procedures performed by specialists. These include both ambulatory care and inpatient services, emergency rooms, intensive care medicine, surgery services, physical therapy, labor and delivery, endoscopy units, diagnostic laboratory and medical imaging services, hospice centers, etc. Some primary care providers may also take care of hospitalized patients and deliver babies in a secondary care setting.
Tertiary care medical services are provided by specialist hospitals or regional centers equipped with diagnostic and treatment facilities not generally available at local hospitals. These include trauma centers, burn treatment centers, advanced neonatology unit services, organ transplants, high-risk pregnancy, radiation oncology, etc.
Modern medical care also depends on information – still delivered in many health care settings on paper records, but increasingly nowadays by electronic means.
In low-income countries, modern healthcare is often too expensive for the average person. International healthcare policy researchers have advocated that "user fees" be removed in these areas to ensure access, although even after removal, significant costs and barriers remain.

Working together as an interdisciplinary team, many highly trained health professionals besides medical practitioners are involved in the delivery of modern health care. Examples include: nurses, emergency medical technicians and paramedics, laboratory scientists, pharmacists, podiatrists, physiotherapists, respiratory therapists, speech therapists, occupational therapists, radiographers, dietitians, and bioengineers, surgeons, surgeon's assistant, surgical technologist.
The scope and sciences underpinning human medicine overlap many other fields. Dentistry, while considered by some a separate discipline from medicine, is a medical field.
A patient admitted to the hospital is usually under the care of a specific team based on their main presenting problem, e.g., the cardiology team, who then may interact with other specialties, e.g., surgical, radiology, to help diagnose or treat the main problem or any subsequent complications/developments.
Physicians have many specializations and subspecializations into certain branches of medicine, which are listed below. There are variations from country to country regarding which specialties certain subspecialties are in.
The main branches of medicine are:
Basic sciences of medicine; this is what every physician is educated in, and some return to in biomedical research
Medical specialties
Interdisciplinary fields, where different medical specialties are mixed to function in certain occasions.

Anatomy is the study of the physical structure of organisms. In contrast to macroscopic or gross anatomy, cytology and histology are concerned with microscopic structures.
Biochemistry is the study of the chemistry taking place in living organisms, especially the structure and function of their chemical components.
Biomechanics is the study of the structure and function of biological systems by means of the methods of Mechanics.
Biostatistics is the application of statistics to biological fields in the broadest sense. A knowledge of biostatistics is essential in the planning, evaluation, and interpretation of medical research. It is also fundamental to epidemiology and evidence-based medicine.
Biophysics is an interdisciplinary science that uses the methods of physics and physical chemistry to study biological systems.
Cytology is the microscopic study of individual cells.

Embryology is the study of the early development of organisms.
Endocrinology is the study of hormones and their effect throughout the body of animals.
Epidemiology is the study of the demographics of disease processes, and includes, but is not limited to, the study of epidemics.
Genetics is the study of genes, and their role in biological inheritance.
Histology is the study of the structures of biological tissues by light microscopy, electron microscopy and immunohistochemistry.
Immunology is the study of the immune system, which includes the innate and adaptive immune system in humans, for example.
Medical physics is the study of the applications of physics principles in medicine.
Microbiology is the study of microorganisms, including protozoa, bacteria, fungi, and viruses.
Molecular biology is the study of molecular underpinnings of the process of replication, transcription and translation of the genetic material.
Neuroscience includes those disciplines of science that are related to the study of the nervous system. A main focus of neuroscience is the biology and physiology of the human brain and spinal cord. Some related clinical specialties include neurology, neurosurgery and psychiatry.
Nutrition science (theoretical focus) and dietetics (practical focus) is the study of the relationship of food and drink to health and disease, especially in determining an optimal diet. Medical nutrition therapy is done by dietitians and is prescribed for diabetes, cardiovascular diseases, weight and eating disorders, allergies, malnutrition, and neoplastic diseases.
Pathology as a science is the study of disease—the causes, course, progression and resolution thereof.
Pharmacology is the study of drugs and their actions.
Photobiology is the study of the interactions between non-ionizing radiation and living organisms.
Physiology is the study of the normal functioning of the body and the underlying regulatory mechanisms.
Radiobiology is the study of the interactions between ionizing radiation and living organisms.
Toxicology is the study of hazardous effects of drugs and poisons.

In the broadest meaning of "medicine", there are many different specialties. In the UK, most specialities have their own body or college, which have its own entrance examination. These are collectively known as the Royal Colleges, although not all currently use the term "Royal". The development of a speciality is often driven by new technology (such as the development of effective anaesthetics) or ways of working (such as emergency departments); the new specialty leads to the formation of a unifying body of doctors and the prestige of administering their own examination.
Within medical circles, specialities usually fit into one of two broad categories: "Medicine" and "Surgery." "Medicine" refers to the practice of non-operative medicine, and most of its subspecialties require preliminary training in Internal Medicine. In the UK, this was traditionally evidenced by passing the examination for the Membership of the Royal College of Physicians (MRCP) or the equivalent college in Scotland or Ireland. "Surgery" refers to the practice of operative medicine, and most subspecialties in this area require preliminary training in General Surgery, which in the UK leads to membership of the Royal College of Surgeons of England (MRCS). At present, some specialties of medicine do not fit easily into either of these categories, such as radiology, pathology, or anesthesia. Most of these have branched from one or other of the two camps above; for example anaesthesia developed first as a faculty of the Royal College of Surgeons (for which MRCS/FRCS would have been required) before becoming the Royal College of Anaesthetists and membership of the college is attained by sitting for the examination of the Fellowship of the Royal College of Anesthetists (FRCA).

Surgery is an ancient medical specialty that uses operative manual and instrumental techniques on a patient to investigate and/or treat a pathological condition such as disease or injury, to help improve bodily function or appearance or to repair unwanted ruptured areas (for example, a perforated ear drum). Surgeons must also manage pre-operative, post-operative, and potential surgical candidates on the hospital wards. Surgery has many sub-specialties, including general surgery, ophthalmic surgery, cardiovascular surgery, colorectal surgery, neurosurgery, oral and maxillofacial surgery, oncologic surgery, orthopedic surgery, otolaryngology, plastic surgery, podiatric surgery, transplant surgery, trauma surgery, urology, vascular surgery, and pediatric surgery. In some centers, anesthesiology is part of the division of surgery (for historical and logistical reasons), although it is not a surgical discipline. Other medical specialties may employ surgical procedures, such as ophthalmology and dermatology, but are not considered surgical sub-specialties per se.
Surgical training in the U.S. requires a minimum of five years of residency after medical school. Sub-specialties of surgery often require seven or more years. In addition, fellowships can last an additional one to three years. Because post-residency fellowships can be competitive, many trainees devote two additional years to research. Thus in some cases surgical training will not finish until more than a decade after medical school. Furthermore, surgical training can be very difficult and time-consuming.

Internal medicine is the medical specialty dealing with the prevention, diagnosis, and treatment of adult diseases. According to some sources, an emphasis on internal structures is implied. In North America, specialists in internal medicine are commonly called "internists." Elsewhere, especially in Commonwealth nations, such specialists are often called physicians. These terms, internist or physician (in the narrow sense, common outside North America), generally exclude practitioners of gynecology and obstetrics, pathology, psychiatry, and especially surgery and its subspecialities.
Because their patients are often seriously ill or require complex investigations, internists do much of their work in hospitals. Formerly, many internists were not subspecialized; such general physicians would see any complex nonsurgical problem; this style of practice has become much less common. In modern urban practice, most internists are subspecialists: that is, they generally limit their medical practice to problems of one organ system or to one particular area of medical knowledge. For example, gastroenterologists and nephrologists specialize respectively in diseases of the gut and the kidneys.
In the Commonwealth of Nations and some other countries, specialist pediatricians and geriatricians are also described as specialist physicians (or internists) who have subspecialized by age of patient rather than by organ system. Elsewhere, especially in North America, general pediatrics is often a form of primary care.
There are many subspecialities (or subdisciplines) of internal medicine:

Training in internal medicine (as opposed to surgical training), varies considerably across the world: see the articles on medical education and physician for more details. In North America, it requires at least three years of residency training after medical school, which can then be followed by a one- to three-year fellowship in the subspecialties listed above. In general, resident work hours in medicine are less than those in surgery, averaging about 60 hours per week in the USA. This difference does not apply in the UK where all doctors are now required by law to work less than 48 hours per week on average.

Clinical laboratory sciences are the clinical diagnostic services that apply laboratory techniques to diagnosis and management of patients. In the United States, these services are supervised by a pathologist. The personnel that work in these medical laboratory departments are technically trained staff who do not hold medical degrees, but who usually hold an undergraduate medical technology degree, who actually perform the tests, assays, and procedures needed for providing the specific services. Subspecialties include transfusion medicine, cellular pathology, clinical chemistry, hematology, clinical microbiology and clinical immunology.
Pathology as a medical specialty is the branch of medicine that deals with the study of diseases and the morphologic, physiologic changes produced by them. As a diagnostic specialty, pathology can be considered the basis of modern scientific medical knowledge and plays a large role in evidence-based medicine. Many modern molecular tests such as flow cytometry, polymerase chain reaction (PCR), immunohistochemistry, cytogenetics, gene rearrangements studies and fluorescent in situ hybridization (FISH) fall within the territory of pathology.
Diagnostic radiology is concerned with imaging of the body, e.g. by x-rays, x-ray computed tomography, ultrasonography, and nuclear magnetic resonance tomography. Interventional radiologists can access areas in the body under imaging for an intervention or diagnostic sampling.
Nuclear medicine is concerned with studying human organ systems by administering radiolabelled substances (radiopharmaceuticals) to the body, which can then be imaged outside the body by a gamma camera or a PET scanner. Each radiopharmaceutical consists of two parts: a tracer that is specific for the function under study (e.g., neurotransmitter pathway, metabolic pathway, blood flow, or other), and a radionuclide (usually either a gamma-emitter or a positron emitter). There is a degree of overlap between nuclear medicine and radiology, as evidenced by the emergence of combined devices such as the PET/CT scanner.
Clinical neurophysiology is concerned with testing the physiology or function of the central and peripheral aspects of the nervous system. These kinds of tests can be divided into recordings of: (1) spontaneous or continuously running electrical activity, or (2) stimulus evoked responses. Subspecialties include electroencephalography, electromyography, evoked potential, nerve conduction study and polysomnography. Sometimes these tests are performed by techs without a medical degree, but the interpretation of these tests is done by a medical professional.

The followings are some major medical specialties that do not directly fit into any of the above-mentioned groups:
Anesthesiology (also known as anaesthetics): concerned with the perioperative management of the surgical patient. The anesthesiologist's role during surgery is to prevent derangement in the vital organs' (i.e. brain, heart, kidneys) functions and postoperative pain. Outside of the operating room, the anesthesiology physician also serves the same function in the labor & delivery ward, and some are specialized in critical medicine.
Dermatology is concerned with the skin and its diseases. In the UK, dermatology is a subspecialty of general medicine.
Emergency medicine is concerned with the diagnosis and treatment of acute or life-threatening conditions, including trauma, surgical, medical, pediatric, and psychiatric emergencies.
Family medicine, family practice, general practice or primary care is, in many countries, the first port-of-call for patients with non-emergency medical problems. Family physicians often provide services across a broad range of settings including office based practices, emergency room coverage, inpatient care, and nursing home care.

Obstetrics and gynecology (often abbreviated as OB/GYN (American English) or Obs & Gynae (British English)) are concerned respectively with childbirth and the female reproductive and associated organs. Reproductive medicine and fertility medicine are generally practiced by gynecological specialists.
Medical genetics is concerned with the diagnosis and management of hereditary disorders.
Neurology is concerned with diseases of the nervous system. In the UK, neurology is a subspecialty of general medicine.
Ophthalmology is exclusively concerned with the eye and ocular adnexa, combining conservative and surgical therapy.
Pediatrics (AE) or paediatrics (BE) is devoted to the care of infants, children, and adolescents. Like internal medicine, there are many pediatric subspecialties for specific age ranges, organ systems, disease classes, and sites of care delivery.
Pharmaceutical medicine is the medical scientific discipline concerned with the discovery, development, evaluation, registration, monitoring and medical aspects of marketing of medicines for the benefit of patients and public health.
Physical medicine and rehabilitation (or physiatry) is concerned with functional improvement after injury, illness, or congenital disorders.
Podiatric medicine is the study of, diagnosis, and medical & surgical treatment of disorders of the foot, ankle, lower limb, hip and lower back.
Psychiatry is the branch of medicine concerned with the bio-psycho-social study of the etiology, diagnosis, treatment and prevention of cognitive, perceptual, emotional and behavioral disorders. Related non-medical fields include psychotherapy and clinical psychology.
Preventive medicine is the branch of medicine concerned with preventing disease.
Community health or public health is an aspect of health services concerned with threats to the overall health of a community based on population health analysis.

Some interdisciplinary sub-specialties of medicine include:
Aerospace medicine deals with medical problems related to flying and space travel.
Addiction medicine deals with the treatment of addiction.
Medical ethics deals with ethical and moral principles that apply values and judgments to the practice of medicine.
Biomedical Engineering is a field dealing with the application of engineering principles to medical practice.
Clinical pharmacology is concerned with how systems of therapeutics interact with patients.
Conservation medicine studies the relationship between human and animal health, and environmental conditions. Also known as ecological medicine, environmental medicine, or medical geology.
Disaster medicine deals with medical aspects of emergency preparedness, disaster mitigation and management.
Diving medicine (or hyperbaric medicine) is the prevention and treatment of diving-related problems.
Evolutionary medicine is a perspective on medicine derived through applying evolutionary theory.
Forensic medicine deals with medical questions in legal context, such as determination of the time and cause of death, type of weapon used to inflict trauma, reconstruction of the facial features using remains of deceased (skull) thus aiding identification.
Gender-based medicine studies the biological and physiological differences between the human sexes and how that affects differences in disease.
Hospice and Palliative Medicine is a relatively modern branch of clinical medicine that deals with pain and symptom relief and emotional support in patients with terminal illnesses including cancer and heart failure.
Hospital medicine is the general medical care of hospitalized patients. Physicians whose primary professional focus is hospital medicine are called hospitalists in the USA and Canada. The term Most Responsible Physician (MRP) or attending physician is also used interchangeably to describe this role.
Laser medicine involves the use of lasers in the diagnostics and/or treatment of various conditions.
Medical humanities includes the humanities (literature, philosophy, ethics, history and religion), social science (anthropology, cultural studies, psychology, sociology), and the arts (literature, theater, film, and visual arts) and their application to medical education and practice.
Health informatics is a relatively recent field that deal with the application of computers and information technology to medicine.
Nosology is the classification of diseases for various purposes.
Nosokinetics is the science/subject of measuring and modelling the process of care in health and social care systems.
Occupational medicine's principal role is the provision of health advice to organizations and individuals to ensure that the highest standards of health and safety at work can be achieved and maintained.
Pain management (also called pain medicine, or algiatry) is the medical discipline concerned with the relief of pain.
Pharmacogenomics is a form of individualized medicine.
Podiatric medicine is the study of, diagnosis, and medical treatment of disorders of the foot, ankle, lower limb, hip and lower back.
Sexual medicine is concerned with diagnosing, assessing and treating all disorders related to sexuality.
Sports medicine deals with the treatment and prevention and rehabilitation of sports/exercise injuries such as muscle spasms, muscle tears, injuries to ligaments (ligament tears or ruptures) and their repair in athletes, amateur and professional.
Therapeutics is the field, more commonly referenced in earlier periods of history, of the various remedies that can be used to treat disease and promote health.
Travel medicine or emporiatrics deals with health problems of international travelers or travelers across highly different environments.
Tropical medicine deals with the prevention and treatment of tropical diseases. It is studied separately in temperate climates where those diseases are quite unfamiliar to medical practitioners and their local clinical needs.
Urgent care focuses on delivery of unscheduled, walk-in care outside of the hospital emergency department for injuries and illnesses that are not severe enough to require care in an emergency department. In some jurisdictions this function is combined with the emergency room.
Veterinary medicine; veterinarians apply similar techniques as physicians to the care of animals.
Wilderness medicine entails the practice of medicine in the wild, where conventional medical facilities may not be available.
Many other health science fields, e.g. dietetics

Medical education and training varies around the world. It typically involves entry level education at a university medical school, followed by a period of supervised practice or internship, and/or residency. This can be followed by postgraduate vocational training. A variety of teaching methods have been employed in medical education, still itself a focus of active research. In Canada and the United States of America, a Doctor of Medicine degree, often abbreviated M.D., or a Doctor of Osteopathic Medicine degree, often abbreviated as D.O. and unique to the United States, must be completed in and delivered from a recognized university.
Since knowledge, techniques, and medical technology continue to evolve at a rapid rate, many regulatory authorities require continuing medical education. Medical practitioners upgrade their knowledge in various ways, including medical journals, seminars, conferences, and online programs.

In most countries, it is a legal requirement for a medical doctor to be licensed or registered. In general, this entails a medical degree from a university and accreditation by a medical board or an equivalent national organization, which may ask the applicant to pass exams. This restricts the considerable legal authority of the medical profession to physicians that are trained and qualified by national standards. It is also intended as an assurance to patients and as a safeguard against charlatans that practice inadequate medicine for personal gain. While the laws generally require medical doctors to be trained in "evidence based", Western, or Hippocratic Medicine, they are not intended to discourage different paradigms of health.
In the European Union, the profession of doctor of medicine is regulated. A profession is said to be regulated when access and exercise is subject to the possession of a specific professional qualification. The regulated professions database contains a list of regulated professions for doctor of medicine in the EU member states, EEA countries and Switzerland. This list is covered by the Directive 2005/36/EC.
Doctors who are negligent or intentionally harmful in their care of patients can face charges of medical malpractice and be subject to civil, criminal, or professional sanctions.

Medical ethics is a system of moral principles that apply values and judgments to the practice of medicine. As a scholarly discipline, medical ethics encompasses its practical application in clinical settings as well as work on its history, philosophy, theology, and sociology. Six of the values that commonly apply to medical ethics discussions are:
autonomy - the patient has the right to refuse or choose their treatment. (Voluntas aegroti suprema lex.)
beneficence - a practitioner should act in the best interest of the patient. (Salus aegroti suprema lex.)
justice - concerns the distribution of scarce health resources, and the decision of who gets what treatment (fairness and equality).
non-maleficence - "first, do no harm" (primum non-nocere).
respect for persons - the patient (and the person treating the patient) have the right to be treated with dignity.
truthfulness and honesty - the concept of informed consent has increased in importance since the historical events of the Doctors' Trial of the Nuremberg trials, Tuskegee syphilis experiment, and others.
Values such as these do not give answers as to how to handle a particular situation, but provide a useful framework for understanding conflicts. When moral values are in conflict, the result may be an ethical dilemma or crisis. Sometimes, no good solution to a dilemma in medical ethics exists, and occasionally, the values of the medical community (i.e., the hospital and its staff) conflict with the values of the individual patient, family, or larger non-medical community. Conflicts can also arise between health care providers, or among family members. For example, some argue that the principles of autonomy and beneficence clash when patients refuse blood transfusions, considering them life-saving; and truth-telling was not emphasized to a large extent before the HIV era.

Prehistoric medicine incorporated plants (herbalism), animal parts, and minerals. In many cases these materials were used ritually as magical substances by priests, shamans, or medicine men. Well-known spiritual systems include animism (the notion of inanimate objects having spirits), spiritualism (an appeal to gods or communion with ancestor spirits); shamanism (the vesting of an individual with mystic powers); and divination (magically obtaining the truth). The field of medical anthropology examines the ways in which culture and society are organized around or impacted by issues of health, health care and related issues.
Early records on medicine have been discovered from ancient Egyptian medicine, Babylonian Medicine, Ayurvedic medicine (in the Indian subcontinent), classical Chinese medicine (predecessor to the modern traditional Chinese medicine), and ancient Greek medicine and Roman medicine.
In Egypt, Imhotep (3rd millennium BC) is the first physician in history known by name. The oldest Egyptian medical text is the Kahun Gynaecological Papyrus from around 2000 BCE, which describes gynaecological diseases. The Edwin Smith Papyrus dating back to 1600 BCE is an early work on surgery, while the Ebers Papyrus dating back to 1500 BCE is akin to a textbook on medicine.
In China, archaeological evidence of medicine in Chinese dates back to the Bronze Age Shang Dynasty, based on seeds for herbalism and tools presumed to have been used for surgery. The Huangdi Neijing, the progenitor of Chinese medicine, is a medical text written beginning in the 2nd century BCE and compiled in the 3rd century.
In India, the surgeon Sushruta described numerous surgical operations, including the earliest forms of plastic surgery. Earliest records of dedicated hospitals come from Mihintale in Sri Lanka where evidence of dedicated medicinal treatment facilities for patients are found.
In Greece, the Greek physician Hippocrates, the "father of western medicine", laid the foundation for a rational approach to medicine. Hippocrates introduced the Hippocratic Oath for physicians, which is still relevant and in use today, and was the first to categorize illnesses as acute, chronic, endemic and epidemic, and use terms such as, "exacerbation, relapse, resolution, crisis, paroxysm, peak, and convalescence". The Greek physician Galen was also one of the greatest surgeons of the ancient world and performed many audacious operations, including brain and eye surgeries. After the fall of the Western Roman Empire and the onset of the Early Middle Ages, the Greek tradition of medicine went into decline in Western Europe, although it continued uninterrupted in the Eastern Roman (Byzantine) Empire.
Most of our knowledge of ancient Hebrew medicine during the 1st millennium BC comes from the Torah, i.e. the Five Books of Moses, which contain various health related laws and rituals. The Hebrew contribution to the development of modern medicine started in the Byzantine Era, with the physician Asaph the Jew.

After 750 CE, the Muslim world had the works of Hippocrates, Galen and Sushruta translated into Arabic, and Islamic physicians engaged in some significant medical research. Notable Islamic medical pioneers include the Persian polymath, Avicenna, who, along with Imhotep and Hippocrates, has also been called the "father of medicine". He wrote The Canon of Medicine, considered one of the most famous books in the history of medicine. Others include Abulcasis, Avenzoar, Ibn al-Nafis, and Averroes. Rhazes was one of the first to question the Greek theory of humorism, which nevertheless remained influential in both medieval Western and medieval Islamic medicine. Al-Risalah al-Dhahabiah by Ali al-Ridha, the eighth Imam of Shia Muslims, is revered as the most precious Islamic literature in the Science of Medicine. The Persian Bimaristan hospitals were an early example of public hospitals.
In Europe, Charlemagne decreed that a hospital should be attached to each cathedral and monastery and the historian Geoffrey Blainey likened the activities of the Catholic Church in health care during the Middle Ages to an early version of a welfare state: "It conducted hospitals for the old and orphanages for the young; hospices for the sick of all ages; places for the lepers; and hostels or inns where pilgrims could buy a cheap bed and meal". It supplied food to the population during famine and distributed food to the poor. This welfare system the church funded through collecting taxes on a large scale and possessing large farmlands and estates. The Benedictine order was noted for setting up hospitals and infirmaries in their monasteries, growing medical herbs and becoming the chief medical care givers of their districts, as at the great Abbey of Cluny. The Church also established a network of cathedral schools and universities where medicine was studied. The Schola Medica Salernitana in Salerno, looking to the learning of Greek and Arab physicians, grew to be the finest medical school in Medieval Europe.

However, the fourteenth and fifteenth century Black Death devastated both the Middle East and Europe, and it has even been argued that Western Europe was generally more effective in recovering from the pandemic than the Middle East. In the early modern period, important early figures in medicine and anatomy emerged in Europe, including Gabriele Falloppio and William Harvey.
The major shift in medical thinking was the gradual rejection, especially during the Black Death in the 14th and 15th centuries, of what may be called the 'traditional authority' approach to science and medicine. This was the notion that because some prominent person in the past said something must be so, then that was the way it was, and anything one observed to the contrary was an anomaly (which was paralleled by a similar shift in European society in general – see Copernicus's rejection of Ptolemy's theories on astronomy). Physicians like Vesalius improved upon or disproved some of the theories from the past. The main tomes used both by medicine students and expert physicians were Materia Medica and Pharmacopoeia.
Andreas Vesalius was the author of De humani corporis fabrica, an important book on human anatomy. Bacteria and microorganisms were first observed with a microscope by Antonie van Leeuwenhoek in 1676, initiating the scientific field microbiology. Independently from Ibn al-Nafis, Michael Servetus rediscovered the pulmonary circulation, but this discovery did not reach the public because it was written down for the first time in the "Manuscript of Paris" in 1546, and later published in the theological work for which he paid with his life in 1553. Later this was described by Renaldus Columbus and Andrea Cesalpino. Herman Boerhaave is sometimes referred to as a "father of physiology" due to his exemplary teaching in Leiden and textbook 'Institutiones medicae' (1708). Pierre Fauchard has been called "the father of modern dentistry".

Veterinary medicine was, for the first time, truly separated from human medicine in 1761, when the French veterinarian Claude Bourgelat founded the world's first veterinary school in Lyon, France. Before this, medical doctors treated both humans and other animals.
Modern scientific biomedical research (where results are testable and reproducible) began to replace early Western traditions based on herbalism, the Greek "four humours" and other such pre-modern notions. The modern era really began with Edward Jenner's discovery of the smallpox vaccine at the end of the 18th century (inspired by the method of inoculation earlier practiced in Asia), Robert Koch's discoveries around 1880 of the transmission of disease by bacteria, and then the discovery of antibiotics around 1900.
The post-18th century modernity period brought more groundbreaking researchers from Europe. From Germany and Austria, doctors Rudolf Virchow, Wilhelm Conrad Röntgen, Karl Landsteiner and Otto Loewi made notable contributions. In the United Kingdom, Alexander Fleming, Joseph Lister, Francis Crick and Florence Nightingale are considered important. Spanish doctor Santiago Ramón y Cajal is considered the father of modern neuroscience.
From New Zealand and Australia came Maurice Wilkins, Howard Florey, and Frank Macfarlane Burnet.
In the United States, William Williams Keen, William Coley, James D. Watson, Italy (Salvador Luria), Switzerland (Alexandre Yersin), Japan (Kitasato Shibasaburō), and France (Jean-Martin Charcot, Claude Bernard, Paul Broca) and others did significant work. Russian Nikolai Korotkov also did significant work, as did Sir William Osler and Harvey Cushing.

As science and technology developed, medicine became more reliant upon medications. Throughout history and in Europe right until the late 18th century, not only animal and plant products were used as medicine, but also human body parts and fluids. Pharmacology developed in part from herbalism and some drugs are still derived from plants (atropine, ephedrine, warfarin, aspirin, digoxin, vinca alkaloids, taxol, hyoscine, etc.). Vaccines were discovered by Edward Jenner and Louis Pasteur.
The first antibiotic was arsphenamine (Salvarsan) discovered by Paul Ehrlich in 1908 after he observed that bacteria took up toxic dyes that human cells did not. The first major class of antibiotics was the sulfa drugs, derived by German chemists originally from azo dyes.
Pharmacology has become increasingly sophisticated; modern biotechnology allows drugs targeted towards specific physiological processes to be developed, sometimes designed for compatibility with the body to reduce side-effects. Genomics and knowledge of human genetics is having some influence on medicine, as the causative genes of most monogenic genetic disorders have now been identified, and the development of techniques in molecular biology and genetics are influencing medical technology, practice and decision-making.
Evidence-based medicine is a contemporary movement to establish the most effective algorithms of practice (ways of doing things) through the use of systematic reviews and meta-analysis. The movement is facilitated by modern global information science, which allows as much of the available evidence as possible to be collected and analyzed according to standard protocols that are then disseminated to healthcare providers. The Cochrane Collaboration leads this movement. A 2001 review of 160 Cochrane systematic reviews revealed that, according to two readers, 21.3% of the reviews concluded insufficient evidence, 20% concluded evidence of no effect, and 22.5% concluded positive effect.

Traditional medicine (also known as indigenous or folk medicine) comprises knowledge systems that developed over generations within various societies before the era of modern medicine. The World Health Organization (WHO) defines traditional medicine as "the sum total of the knowledge, skills, and practices based on the theories, beliefs, and experiences indigenous to different cultures, whether explicable or not, used in the maintenance of health as well as in the prevention, diagnosis, improvement or treatment of physical and mental illness."
In some Asian and African countries, up to 80% of the population relies on traditional medicine for their primary health care needs. When adopted outside of its traditional culture, traditional medicine is often called alternative medicine. Practices known as traditional medicines include Ayurveda, Siddha medicine, Unani, ancient Iranian medicine, Irani, Islamic medicine, traditional Chinese medicine, traditional Korean medicine, acupuncture, Muti, Ifá, and traditional African medicine.
The WHO notes however that "inappropriate use of traditional medicines or practices can have negative or dangerous effects" and that "further research is needed to ascertain the efficacy and safety" of several of the practices and medicinal plants used by traditional medicine systems. The line between alternative medicine and quackery is a contentious subject.
Traditional medicine may include formalized aspects of folk medicine, that is to say longstanding remedies passed on and practised by lay people. Folk medicine consists of the healing practices and ideas of body physiology and health preservation known to some in a culture, transmitted informally as general knowledge, and practiced or applied by anyone in the culture having prior experience. Folk medicine may also be referred to as traditional medicine, alternative medicine, indigenous medicine, or natural medicine. These terms are often considered interchangeable, even though some authors may prefer one or the other because of certain overtones they may be willing to highlight. In fact, out of these terms perhaps only indigenous medicine and traditional medicine have the same meaning as folk medicine, while the others should be understood rather in a modern or modernized context.

Health is the level of functional and metabolic efficiency of a living organism. In humans it is the ability of individuals or communities to adapt and self-manage when facing physical, mental or social changes. The World Health Organization (WHO) defined health in its broader sense in its 1948 constitution as "a state of complete physical, mental, and social well-being and not merely the absence of disease or infirmity." This definition has been subject to controversy, in particular as lacking operational value, the ambiguity in developing cohesive health strategies, and because of the problem created by use of the word "complete". Other definitions have been proposed, among which a recent definition that correlates health and personal satisfaction.  Classification systems such as the WHO Family of International Classifications, including the International Classification of Functioning, Disability and Health (ICF) and the International Classification of Diseases (ICD), are commonly used to define and measure the components of health.

The definition of health has evolved over time. In keeping with the biomedical perspective, early definitions of health focused on the theme of the body's ability to function; health was seen as a state of normal function that could be disrupted from time to time by disease. An example of such a definition of health is: "a state characterized by anatomic, physiologic, and psychological integrity; ability to perform personally valued family, work, and community roles; ability to deal with physical, biologic, psychological, and social stress". Then, in 1948, in a radical departure from previous definitions, the World Health Organization (WHO) proposed a definition that aimed higher, linking health to well-being, in terms of "physical, mental, and social well-being, and not merely the absence of disease and infirmity". Although this definition was welcomed by some as being innovative, it was also criticized as being vague, excessively broad, and was not construed as measurable. For a long time it was set aside as an impractical ideal and most discussions of health returned to the practicality of the biomedical model.
Just as there was a shift from viewing disease as a state to thinking of it as a process, the same shift happened in definitions of health. Again, the WHO played a leading role when it fostered the development of the health promotion movement in the 1980s. This brought in a new conception of health, not as a state, but in dynamic terms of resiliency, in other words, as "a resource for living". The 1984 WHO revised definition of health defined it as "the extent to which an individual or group is able to realize aspirations and satisfy needs, and to change or cope with the environment. Health is a resource for everyday life, not the objective of living; it is a positive concept, emphasizing social and personal resources, as well as physical capacities". Thus, health referred to the ability to maintain homeostasis and recover from insults. Mental, intellectual, emotional, and social health referred to a person's ability to handle stress, to acquire skills, to maintain relationships, all of which form resources for resiliency and independent living.
Since the late 1970s, the federal Healthy People Initiative has been a visible component of the United States’ approach to improving population health. In each decade, a new version of Healthy People is issued, featuring updated goals and identifying topic areas and quantifiable objectives for health improvement during the succeeding ten years, with assessment at that point of progress or lack thereof. Progress has been limited for many objectives, leading to concerns about the effectiveness of Healthy People in shaping outcomes in the context of a decentralized and uncoordinated US health system. Healthy People 2020 gives more prominence to health promotion and preventive approaches, and adds a substantive focus on the importance of addressing societal determinants of health. A new expanded digital interface facilitates use and dissemination rather than bulky printed books as produced in the past. The impact of these changes to Healthy People will be determined in the coming years.
Systematic activities to prevent or cure health problems and promote good health in humans are undertaken by health care providers. Applications with regard to animal health are covered by the veterinary sciences. The term "healthy" is also widely used in the context of many types of non-living organizations and their impacts for the benefit of humans, such as in the sense of healthy communities, healthy cities or healthy environments. In addition to health care interventions and a person's surroundings, a number of other factors are known to influence the health status of individuals, including their background, lifestyle, and economic, social conditions, and spirituality; these are referred to as "determinants of health." Studies have shown that high levels of stress can affect human health.

Generally, the context in which an individual lives is of great importance for both his health status and quality of their life. It is increasingly recognized that health is maintained and improved not only through the advancement and application of health science, but also through the efforts and intelligent lifestyle choices of the individual and society. According to the World Health Organization, the main determinants of health include the social and economic environment, the physical environment, and the person's individual characteristics and behaviors.
More specifically, key factors that have been found to influence whether people are healthy or unhealthy include the following:

An increasing number of studies and reports from different organizations and contexts examine the linkages between health and different factors, including lifestyles, environments, health care organization, and health policy – such as the 1974 Lalonde report from Canada; the Alameda County Study in California; and the series of World Health Reports of the World Health Organization, which focuses on global health issues including access to health care and improving public health outcomes, especially in developing countries.
The concept of the "health field," as distinct from medical care, emerged from the Lalonde report from Canada. The report identified three interdependent fields as key determinants of an individual's health. These are:
Lifestyle: the aggregation of personal decisions (i.e., over which the individual has control) that can be said to contribute to, or cause, illness or death;
Environmental: all matters related to health external to the human body and over which the individual has little or no control;
Biomedical: all aspects of health, physical and mental, developed within the human body as influenced by genetic make-up.
The maintenance and promotion of health is achieved through different combination of physical, mental, and social well-being, together sometimes referred to as the "health triangle." The WHO's 1986 Ottawa Charter for Health Promotion further stated that health is not just a state, but also "a resource for everyday life, not the objective of living. Health is a positive concept emphasizing social and personal resources, as well as physical capacities."
Focusing more on lifestyle issues and their relationships with functional health, data from the Alameda County Study suggested that people can improve their health via exercise, enough sleep, maintaining a healthy body weight, limiting alcohol use, and avoiding smoking. Health and illness can co-exist, as even people with multiple chronic diseases or terminal illnesses can consider themselves healthy.
The environment is often cited as an important factor influencing the health status of individuals. This includes characteristics of the natural environment, the built environment, and the social environment. Factors such as clean water and air, adequate housing, and safe communities and roads all have been found to contribute to good health, especially to the health of infants and children. Some studies have shown that a lack of neighborhood recreational spaces including natural environment leads to lower levels of personal satisfaction and higher levels of obesity, linked to lower overall health and well being. This suggests that the positive health benefits of natural space in urban neighborhoods should be taken into account in public policy and land use.
Genetics, or inherited traits from parents, also play a role in determining the health status of individuals and populations. This can encompass both the predisposition to certain diseases and health conditions, as well as the habits and behaviors individuals develop through the lifestyle of their families. For example, genetics may play a role in the manner in which people cope with stress, either mental, emotional or physical. For example, obesity is a very large problem in the United States that contributes to bad mental health and causes stress in a lot of people's lives. (One difficulty is the issue raised by the debate over the relative strengths of genetics and other factors; interactions between genetics and environment may be of particular importance.)

There are a lot of types of health issues common with many people across the globe. Disease is one of the most common. According to GlobalIssues.org, approximately 36 million people die each year from non-communicable (not contagious) disease including cardiovascular disease cancer, diabetes, and chronic lung disease (Shah, 2014).
As for communicable diseases, both viral and bacterial, AIDS/HIV, tuberculosis, and malaria are the most common also causing millions of deaths every year (2014).
Another health issue that causes death or contributes to other health problems is malnutrition majorly among children. One of the groups malnutrition affects most is young children. Approximately 7.5 million children under the age of 5 die from malnutrition, and it is usually brought on by not having the money to find or make food (2014).
Bodily injuries are also a common health issue worldwide. These injuries, including broken bones, fractures, and burns can reduce a person's quality of life or can cause fatalities including infections that resulted from the injury or the severity injury in general (Moffett, 2013).
Some contributing factors to poor health are lifestyle choices. These include smoking cigarettes, and also can include a poor diet, whether it is overeating or an overly constrictive diet. Inactivity can also contribute to health issues and also a lack of sleep, excessive alcohol consumption, and neglect of oral hygiene (2013). There are also genetic disorders that are inherited by the person and can vary in how much they affect the person and when they surface (2013).
The one health issue that is the most unfortunate because the majority of these health issues are preventable is that approximately 1 billion people lack access to health care systems (Shah, 2014). It is easy to say that the most common and harmful health issue is that a lot of people do not have access to quality remedies.

The World Health Organization describes mental health as "a state of well-being in which the individual realizes his or her own abilities, can cope with the normal stresses of life, can work productively and fruitfully, and is able to make a contribution to his or her community". Mental Health is not just the absence of mental illness.
Mental illness is described as 'the spectrum of cognitive, emotional, and behavioral conditions that interfere with social and emotional well-being and the lives and productivity of people. Having a mental illness can seriously impair, temporarily or permanently, the mental functioning of a person. Other terms include: 'mental health problem', 'illness', 'disorder', 'dysfunction'.
Roughly a quarter of all adults 18 and over in the US suffer from a diagnosable mental illness. Mental illnesses are the leading cause of disability in the US and Canada. Examples include, schizophrenia, ADHD, major depressive disorder, bipolar disorder, anxiety disorder, post-traumatic stress disorder and autism.
Many teens suffer from mental health issues in response to the pressures of society and social problems they encounter. Some of the key mental health issues seen in teens are: depression, eating disorders, and drug abuse. There are many ways to prevent these health issues from occurring such as communicating well with a teen suffering from mental health issues. Mental health can be treated and be attentive to teens' behavior.

Achieving and maintaining health is an ongoing process, shaped by both the evolution of health care knowledge and practices as well as personal strategies and organized interventions for staying healthy.

An important way to maintain your personal health is to have a healthy diet. A healthy diet includes a variety of plant-based and animal-based foods that provide nutrients to your body. Such nutrients give you energy and keep your body running. Nutrients help build and strengthen bones, muscles, and tendons and also regulate body processes (i.e. blood pressure). The food guide pyramid is a pyramid-shaped guide of healthy foods divided into sections. Each section shows the recommended intake for each food group (i.e. Protein, Fat, Carbohydrates, and Sugars). Making healthy food choices is important because it can lower your risk of heart disease, developing some types of cancer, and it will contribute to maintaining a healthy weight.
The Mediterranean diet is commonly associated with health-promoting effects due to the fact that it contains some bioactive compounds like phenolic compounds, isoprenoids and alkaloids.

Physical exercise enhances or maintains physical fitness and overall health and wellness. It strengthens muscles and improves the cardiovascular system.

Sleep is an essential component to maintaining health. In children, sleep is also vital for growth and development. Ongoing sleep deprivation has been linked to an increased risk for some chronic health problems. In addition, sleep deprivation has been shown to correlate with both increased susceptibility to illness and slower recovery times from illness. In one study, people with chronic insufficient sleep, set as six hours of sleep a night or less, were found to be four times more likely to catch a cold compared to those who reported sleeping for seven hours or more a night. Due to the role of sleep in regulating metabolism, insufficient sleep may also play a role in weight gain or, conversely, in impeding weight loss. Additionally, in 2007, the International Agency for Research on Cancer, which is the cancer research agency for the World Health Organization, declared that "shiftwork that involves circadian disruption is probably carcinogenic to humans," speaking to the dangers of long-term nighttime work due to its intrusion on sleep. In 2015, the National Sleep Foundation released updated recommendations for sleep duration requirements based on age and concluded that "Individuals who habitually sleep outside the normal range may be exhibiting signs or symptoms of serious health problems or, if done volitionally, may be compromising their health and well-being."

Health science is the branch of science focused on health. There are two main approaches to health science: the study and research of the body and health-related issues to understand how humans (and animals) function, and the application of that knowledge to improve health and to prevent and cure diseases and other physical and mental impairments. The science builds on many sub-fields, including biology, biochemistry, physics, epidemiology, pharmacology, medical sociology. Applied health sciences endeavor to better understand and improve human health through applications in areas such as health education, biomedical engineering, biotechnology and public health.
Organized interventions to improve health based on the principles and procedures developed through the health sciences are provided by practitioners trained in medicine, nursing, nutrition, pharmacy, social work, psychology, occupational therapy, physical therapy and other health care professions. Clinical practitioners focus mainly on the health of individuals, while public health practitioners consider the overall health of communities and populations. Workplace wellness programs are increasingly adopted by companies for their value in improving the health and well-being of their employees, as are school health services in order to improve the health and well-being of children.

Public health has been described as "the science and art of preventing disease, prolonging life and promoting health through the organized efforts and informed choices of society, organizations, public and private, communities and individuals." It is concerned with threats to the overall health of a community based on population health analysis. The population in question can be as small as a handful of people or as large as all the inhabitants of several continents (for instance, in the case of a pandemic). Public health has many sub-fields, but typically includes the interdisciplinary categories of epidemiology, biostatistics and health services. Environmental health, community health, behavioral health, and occupational health are also important areas of public health.
The focus of public health interventions is to prevent and manage diseases, injuries and other health conditions through surveillance of cases and the promotion of healthy behavior, communities, and (in aspects relevant to human health) environments. Its aim is to prevent health problems from happening or re-occurring by implementing educational programs, developing policies, administering services and conducting research. In many cases, treating a disease or controlling a pathogen can be vital to preventing it in others, such as during an outbreak. Vaccination programs and distribution of condoms to prevent the spread of communicable diseases are examples of common preventive public health measures, as are educational campaigns to promote vaccination and the use of condoms (including overcoming resistance to such).
Public health also takes various actions to limit the health disparities between different areas of the country and, in some cases, the continent or world. One issue is the access of individuals and communities to health care in terms of financial, geographical or socio-cultural constraints to accessing and using services. Applications of the public health system include the areas of maternal and child health, health services administration, emergency response, and prevention and control of infectious and chronic diseases.
The great positive impact of public health programs is widely acknowledged. Due in part to the policies and actions developed through public health, the 20th century registered a decrease in the mortality rates for infants and children and a continual increase in life expectancy in most parts of the world. For example, it is estimated that life expectancy has increased for Americans by thirty years since 1900, and worldwide by six years since 1990.

Personal health depends partially on the active, passive, and assisted cues people observe and adopt about their own health. These include personal actions for preventing or minimizing the effects of a disease, usually a chronic condition, through integrative care. They also include personal hygiene practices to prevent infection and illness, such as bathing and washing hands with soap; brushing and flossing teeth; storing, preparing and handling food safely; and many others. The information gleaned from personal observations of daily living – such as about sleep patterns, exercise behavior, nutritional intake and environmental features – may be used to inform personal decisions and actions (e.g., "I feel tired in the morning so I am going to try sleeping on a different pillow"), as well as clinical decisions and treatment plans (e.g., a patient who notices his or her shoes are tighter than usual may be having exacerbation of left-sided heart failure, and may require diuretic medication to reduce fluid overload).
Personal health also depends partially on the social structure of a person's life. The maintenance of strong social relationships, volunteering, and other social activities have been linked to positive mental health and also increased longevity. One American study among seniors over age 70, found that frequent volunteering was associated with reduced risk of dying compared with older persons who did not volunteer, regardless of physical health status. Another study from Singapore reported that volunteering retirees had significantly better cognitive performance scores, fewer depressive symptoms, and better mental well-being and life satisfaction than non-volunteering retirees.
Prolonged psychological stress may negatively impact health, and has been cited as a factor in cognitive impairment with aging, depressive illness, and expression of disease. Stress management is the application of methods to either reduce stress or increase tolerance to stress. Relaxation techniques are physical methods used to relieve stress. Psychological methods include cognitive therapy, meditation, and positive thinking, which work by reducing response to stress. Improving relevant skills, such as problem solving and time management skills, reduces uncertainty and builds confidence, which also reduces the reaction to stress-causing situations where those skills are applicable.

In addition to safety risks, many jobs also present risks of disease, illness and other long-term health problems. Among the most common occupational diseases are various forms of pneumoconiosis, including silicosis and coal worker's pneumoconiosis (black lung disease). Asthma is another respiratory illness that many workers are vulnerable to. Workers may also be vulnerable to skin diseases, including eczema, dermatitis, urticaria, sunburn, and skin cancer. Other occupational diseases of concern include carpal tunnel syndrome and lead poisoning.
As the number of service sector jobs has risen in developed countries, more and more jobs have become sedentary, presenting a different array of health problems than those associated with manufacturing and the primary sector. Contemporary problems, such as the growing rate of obesity and issues relating to stress and overwork in many countries, have further complicated the interaction between work and health.
Many governments view occupational health as a social challenge and have formed public organizations to ensure the health and safety of workers. Examples of these include the British Health and Safety Executive and in the United States, the National Institute for Occupational Safety and Health, which conducts research on occupational health and safety, and the Occupational Safety and Health Administration, which handles regulation and policy relating to worker safety and health.

Men's health
Women's health
Youth health
Population health
Public health
Global burden of disease
Health care
Health system
Medicine
Human enhancement
One Health

World Health Organization
UK National Health Service
OECD Health Statistics
Health and Medical Information from the University of Colorado