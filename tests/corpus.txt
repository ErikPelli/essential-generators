New York is a state in the northeastern United States, and is the 27th-most extensive, fourth-most populous, and seventh-most densely populated U.S. state. New York is bordered by New Jersey and Pennsylvania to the south and Connecticut, Massachusetts, and Vermont to the east. The state has a maritime border in the Atlantic Ocean with Rhode Island, east of Long Island, as well as an international border with the Canadian provinces of Quebec to the north and Ontario to the west and north. The state of New York, with an estimated 19.8 million residents in 2015, is often referred to as New York State to distinguish it from New York City, the state's most populous city and its economic hub.
With an estimated population of 8.55 million in 2015, New York City is the most populous city in the United States and the premier gateway for legal immigration to the United States. The New York City Metropolitan Area is one of the most populous urban agglomerations in the world. New York City is a global city, exerting a significant impact upon commerce, finance, media, art, fashion, research, technology, education, and entertainment, its fast pace defining the term New York minute. The home of the United Nations Headquarters, New York City is an important center for international diplomacy and has been described as the cultural and financial capital of the world, as well as the world's most economically powerful city. New York City makes up over 40% of the population of New York State. Two-thirds of the state's population lives in the New York City Metropolitan Area, and nearly 40% lives on Long Island. Both the state and New York City were named for the 17th-century Duke of York, future King James II of England. The next four most populous cities in the state are Buffalo, Rochester, Yonkers, and Syracuse, while the state capital is Albany.
New York has a diverse geography. The southern part of the state consists of Long Island and several smaller associated islands, as well as New York City and the lower Hudson River Valley, most of which lie within the wider Atlantic Coastal Plain. The large region known as Upstate New York consists of several ranges of the wider Appalachian Mountains, including the Allegheny Plateau and Catskills along New York's Southern Tier, and the Adirondack Mountains, Thousand Islands archipelago, and Saint Lawrence Seaway in the Northeastern lobe of the state. These more mountainous regions are bisected by two major river valleys—the north-south Hudson River Valley and the east-west Mohawk River Valley, which forms the core of the Erie Canal. Western New York is considered part of the Great Lakes Region and straddles Lake Ontario and Lake Erie. Between the two lakes lies Niagara Falls. The central part of the state is dominated by the Finger Lakes, a popular vacation and tourist destination.
New York had been inhabited by tribes of Algonquian and Iroquoian-speaking Native Americans for several hundred years by the time the earliest Europeans came to New York. The first Europeans to arrive were French colonists and Jesuit missionaries who arrived southward from settlements at Montreal for trade and proselytizing. In 1609, the region was claimed by Henry Hudson for the Dutch, who built Fort Nassau in 1614 at the confluence of the Hudson and Mohawk rivers, where the present-day capital of Albany later developed. The Dutch soon also settled New Amsterdam and parts of the Hudson Valley, establishing the colony of New Netherland, a multicultural community from its earliest days and a center of trade and immigration. The British annexed the colony from the Dutch in 1664. The borders of the British colony, the Province of New York, were similar to those of the present-day state.
Many landmarks in New York are well known to both international and domestic visitors, with New York State hosting four of the world's ten most-visited tourist attractions in 2013: Times Square, Central Park, Niagara Falls (shared with Ontario), and Grand Central Terminal. New York is home to the Statue of Liberty, a symbol of the United States and its ideals of freedom, democracy, and opportunity. In the 21st century, New York has emerged as a global node of creativity and entrepreneurship, social tolerance, and environmental sustainability. New York's higher education network comprises approximately 200 colleges and universities, including Columbia University, Cornell University, New York University, and Rockefeller University, which have been ranked among the top 35 in the world.






In 1524, Giovanni da Verrazzano, an Italian explorer in the service of the French crown, explored the Atlantic coast of North America between the Carolinas and Newfoundland, including New York Harbor and Narragansett Bay. On April 17, 1524 Verrazanno entered New York Bay, by way of the strait now called the Narrows into the northern bay which he named Santa Margherita, in honor of the King of France's sister. Verrazzano described it as "a vast coastline with a deep delta in which every kind of ship could pass" and he adds: "that it extends inland for a league and opens up to form a beautiful lake. This vast sheet of water swarmed with native boats". He landed on the tip of Manhattan and possibly on the furthest point of Long Island. Verrazanno's stay was interrupted by a storm which pushed him north towards Martha's Vineyard.
In 1540 French traders from New France built a chateau on Castle Island, within present-day Albany; due to flooding, it was abandoned the next year. In 1614, the Dutch under the command of Hendrick Corstiaensen, rebuilt the French chateau, which they called Fort Nassau. Fort Nassau was the first Dutch settlement in North America, and was located along the Hudson River, also within present-day Albany. The small fort served as a trading post and warehouse. Located on the Hudson River flood plain, the rudimentary "fort" was washed away by flooding in 1617, and abandoned for good after Fort Orange (New Netherland) was built nearby in 1623.




Henry Hudson's 1609 voyage marked the beginning of European involvement with the area. Sailing for the Dutch East India Company and looking for a passage to Asia, he entered the Upper New York Bay on September 11 of that year. Word of his findings encouraged Dutch merchants to explore the coast in search for profitable fur trading with local Native American tribes.
During the 17th century, Dutch trading posts established for the trade of pelts from the Lenape, Iroquois, and other tribes were founded in the colony of New Netherland. The first of these trading posts were Fort Nassau (1614, near present-day Albany); Fort Orange (1624, on the Hudson River just south of the current city of Albany and created to replace Fort Nassau), developing into settlement Beverwijck (1647), and into what became Albany; Fort Amsterdam (1625, to develop into the town New Amsterdam which is present-day New York City); and Esopus, (1653, now Kingston). The success of the patroonship of Rensselaerswyck (1630), which surrounded Albany and lasted until the mid-19th century, was also a key factor in the early success of the colony. The English captured the colony during the Second Anglo-Dutch War and governed it as the Province of New York. The city of New York was recaptured by the Dutch in 1673 during the Third Anglo-Dutch War (1672–1674) and renamed New Orange. It was returned to the English under the terms of the Treaty of Westminster a year later.




The Sons of Liberty were organized in New York City during the 1760s, largely in response to the oppressive Stamp Act passed by the British Parliament in 1765. The Stamp Act Congress met in the city on October 19 of that year, composed of representatives from across the Thirteen Colonies who set the stage for the Continental Congress to follow. The Stamp Act Congress resulted in the Declaration of Rights and Grievances, which was the first written expression by representatives of the Americans of many of the rights and complaints later expressed in the United States Declaration of Independence. This included the right to representative government. At the same time, with strong trading between Britain and the United States on both business and personal levels many New York residents were Loyalists. The Capture of Fort Ticonderoga provided the cannon and gunpowder necessary to force a British withdrawal from the Siege of Boston in 1775.
New York was the only colony to not vote for independence, as the delegates were not authorized to do so. New York then endorsed the Declaration of Independence on July 9, 1776. The New York State Constitution was framed by a convention which assembled at White Plains on July 10, 1776, and after repeated adjournments and changes of location, terminated its labors at Kingston on Sunday evening, April 20, 1777, when the new constitution drafted by John Jay was adopted with but one dissenting vote. It was not submitted to the people for ratification. On July 30, 1777, George Clinton was inaugurated as the first Governor of New York at Kingston.
About one-third of the battles of the American Revolutionary War took place in New York; the first major battle after U.S. independence was declared—and the largest battle of the entire war—was fought in New York at the Battle of Long Island (a.k.a. Battle of Brooklyn) in August 1776. After their victory, the British occupied New York City, making it their military and political base of operations in North America for the duration of the conflict, and consequently the focus of General George Washington's intelligence network. On the notorious British prison ships of Wallabout Bay, more American combatants died of intentional neglect than were killed in combat in every battle of the war, combined. Both sides of combatants lost more soldiers to disease than to outright wounds.The first of two major British armies were captured by the Continental Army at the Battle of Saratoga in 1777, a success that influenced France to ally with the revolutionaries.The state constitution was enacted in 1777. New York became the 11th state to ratify the United States Constitution, on July 26, 1788.

In an attempt to retain their sovereignty and remain an independent nation positioned between the new United States and British North America, four of the Iroquois Nations fought on the side of the British; only the Oneida and their dependents, the Tuscarora, allied themselves with the Americans. In retaliation for attacks on the frontier led by Joseph Brant and Loyalist Mohawk forces, the Sullivan Expedition of 1779 destroyed nearly 50 Iroquois villages, adjacent croplands and winter stores, forcing many refugees to British-held Niagara.
As allies of the British, the Iroquois were forced out of New York, although they had not been part of treaty negotiations. They resettled in Canada after the war and were given land grants by the Crown. In the treaty settlement, the British ceded most Indian lands to the new United States. Because New York made treaty with the Iroquois without getting Congressional approval, some of the land purchases have been subject to land claim suits since the late 20th century by the federally recognized tribes. New York put up more than 5 million acres (20,000 km2) of former Iroquois territory for sale in the years after the Revolutionary War, leading to rapid development in upstate New York. As per the Treaty of Paris, the last vestige of British authority in the former Thirteen Colonies—their troops in New York City—departed in 1783, which was long afterward celebrated as Evacuation Day.

New York City was the national capital under the Articles of Confederation and Perpetual Union, the first government. That organization was found to be insufficient, and prominent New Yorker Alexander Hamilton advocated a new government that would include an executive, national courts, and the power to tax. Hamilton led the Annapolis Convention (1786) that called for the Philadelphia Convention, which drafted the United States Constitution, in which he also took part. The new government was to be a strong federal national government to replace the relatively weaker confederation of individual states. Following heated debate, which included the publication of the now quintessential constitutional interpretation—The Federalist Papers—as a series of installments in New York City newspapers, New York was the 11th state to ratify the United States Constitution, on July 26, 1788. New York remained the national capital under the new constitution until 1790, and was the site of the inauguration of President George Washington, the drafting of United States Bill of Rights, and the first session of the United States Supreme Court. Hamilton's revival of the heavily indebted United States economy after the war and the creation of a national bank significantly contributed to New York City becoming the financial center of the new nation.
Both the Dutch and the British imported African slaves as laborers to the city and colony; New York had the second-highest population of slaves after Charleston, SC. Slavery was extensive in New York City and some agricultural areas. The state passed a law for the gradual abolition of slavery soon after the Revolutionary War, but the last slave in New York was not freed until 1827.




Transportation in western New York was by expensive wagons on muddy roads before canals opened up the rich farm lands to long-distance traffic. Governor DeWitt Clinton promoted the Erie Canal that connected New York City to the Great Lakes, by the Hudson River, the new canal, and the rivers and lakes. Work commenced in 1817, and the Erie Canal opened in 1825. Packet boats pulled by horses on tow paths traveled slowly over the canal carrying passengers and freight. Farm products came in from the Midwest, and finished manufactured moved west. It was an engineering marvel which opened up vast areas of New York to commerce and settlement. It enabled Great Lakes port cities such as Buffalo and Rochester to grow and prosper. It also connected the burgeoning agricultural production of the Midwest and shipping on the Great Lakes, with the port of New York City. Improving transportation, it enabled additional population migration to territories west of New York. After 1850, railroads largely replaced the canal.
New York City was a major ocean port and had extensive traffic importing cotton from the South and exporting manufacturing goods. Nearly half of the state's exports were related to cotton. Southern cotton factors, planters and bankers visited so often that they had favorite hotels. At the same time, activism for abolitionism was strong upstate, where some communities provided stops on the Underground Railroad. Upstate, and New York City, gave strong support for the American Civil War In terms of finances, volunteer soldiers, and supplies. The state provided more than 370,000 soldiers to the Union armies. Over 53,000 New Yorkers died in service, roughly one of every seven who served. However, Irish draft riots in 1862 were a significant embarrassment.




Since the early 19th century, New York City has been the largest port of entry for legal immigration into the United States. In the United States, the federal government did not assume direct jurisdiction for immigration until 1890. Prior to this time, the matter was delegated to the individual states, then via contract between the states and the federal government. Most immigrants to New York would disembark at the bustling docks along the Hudson and East Rivers, in the eventual Lower Manhattan. On May 4, 1847 the New York State Legislature created the Board of Commissioners of Immigration to regulate immigration.
The first permanent immigration depot in New York was established in 1855 at Castle Garden, a converted War of 1812 era fort located within what is now Battery Park, at the tip of Lower Manhattan. The first immigrants to arrive at the new depot were aboard three ships that had just been released from quarantine. Castle Garden served as New York's immigrant depot until it closed on April 18, 1890 when the federal government assumed control over immigration. During that period, more than 8 million immigrants passed through its doors (two out of every three U.S. immigrants).
When the federal government assumed control, it established the Bureau of Immigration, which chose the three-acre Ellis Island in Upper New York Harbor for an entry depot. Already federally controlled, the island had served as an ammunition depot. It was chosen due its relative isolation with proximity to New York City and the rail lines of Jersey City, New Jersey, via a short ferry ride. While the island was being developed and expanded via land reclamation, the federal government operated a temporary depot at the Barge Office at the Battery.
Ellis Island opened on January 1, 1892, and operated as a central immigration center until the National Origins Act was passed in 1924, reducing immigration. After that date, the only immigrants to pass through were displaced persons or war refugees. The island ceased all immigration processing on November 12, 1954 when the last person detained on the island, Norwegian seaman Arne Peterssen, was released. He had overstayed his shore leave and left on the 10:15 a.m. Manhattan-bound ferry to return to his ship.
More than 12 million immigrants passed through Ellis Island between 1892 and 1954. More than 100 million Americans across the United States can trace their ancestry to these immigrants.
Ellis Island was the subject of a contentious and long-running border and jurisdictional dispute between New York State and the State of New Jersey, as both claimed it. The issue was settled in 1998 by the U.S. Supreme Court which ruled that the original 3.3-acre (1.3 ha) island was New York State territory and that the balance of the 27.5 acres (11 ha) added after 1834 by landfill was in New Jersey. The island was added to the National Park Service system in May 1965 by President Lyndon B. Johnson and is still owned by the Federal government as part of the Statue of Liberty National Monument. Ellis Island was opened to the public as a museum of immigration in 1990.




On September 11, 2001, two of four hijacked planes were flown into the Twin Towers of the original World Trade Center in Lower Manhattan, and the towers collapsed. 7 World Trade Center also collapsed due to damage from fires. The other buildings of the World Trade Center complex were damaged beyond repair and demolished soon thereafter. The collapse of the Twin Towers caused extensive damage and resulted in the deaths of 2,753 victims, including 147 aboard the two planes. Since September 11, most of Lower Manhattan has been restored. In the years since, many rescue workers and residents of the area have developed several life-threatening illnesses, and some have died.
A memorial at the site, the National September 11 Memorial & Museum, was opened to the public on September 11, 2011. A permanent museum later opened at the site on March 21, 2014. Upon its completion in 2014, the new One World Trade Center became the tallest skyscraper in the Western Hemisphere, at 1,776 feet (541 m). Other skyscrapers are under construction at the site.




On October 29 and 30, 2012, Hurricane Sandy caused extensive destruction of the state's shorelines, ravaging portions of New York City and Long Island with record-high storm surge, with severe flooding and high winds causing power outages for hundreds of thousands of New Yorkers, and leading to gasoline shortages and disruption of mass transit systems. The storm and its profound effects have prompted the discussion of constructing seawalls and other coastal barriers around the shorelines of New York City and Long Island to minimize the risk from another such future event. This is considered highly probable due to global warming and rise in sea levels.




New York covers 54,555 square miles (141,300 km2) and ranks as the 27th largest state by size. The Great Appalachian Valley dominates eastern New York and contains the Lake Champlain Valley as its northern half and the Hudson Valley as its southern half within the state. The rugged Adirondack Mountains, with vast tracts of wilderness, lie west of the Lake Champlain Valley. The Hudson River begins near Lake Tear of the Clouds and flows south through the eastern part of the state without draining Lakes George or Champlain. Lake George empties at its north end into Lake Champlain, whose northern end extends into Canada, where it drains into the Richelieu River and then ultimately the Saint Lawrence River. Four of New York City's five boroughs are situated on three islands at the mouth of the Hudson River: Manhattan Island; Staten Island; and Long Island, which contains Brooklyn and Queens at its western end.
Most of the southern part of the state rests on the Allegheny Plateau, which extends from the southeastern United States to the Catskill Mountains; the section in New York State is known as the Southern Tier. The Tug Hill region arises as a cuesta east of Lake Ontario. The western section of the state is drained by the Allegheny River and rivers of the Susquehanna and Delaware River systems. The Delaware River Basin Compact, signed in 1961 by New York, New Jersey, Pennsylvania, Delaware, and the federal government, regulates the utilization of water of the Delaware system. The highest elevation in New York is Mount Marcy in the Adirondacks, at 5,344 feet (1,629 meters) above sea level; while the state's lowest point is at sea level, on the Atlantic Ocean.
Much of New York State borders water, as is true for New York City as well. Of New York State's total area, 13.5% consists of water. The state's borders touch (clockwise from the west) two Great Lakes (Lake Erie and Lake Ontario, which are connected by the Niagara River); the provinces of Ontario and Quebec in Canada, with New York and Ontario sharing the Thousand Islands archipelago within the Saint Lawrence River; Lake Champlain; three New England states (Vermont, Massachusetts, and Connecticut); the Atlantic Ocean, and two Mid-Atlantic states, New Jersey and Pennsylvania. In addition, Rhode Island shares a water border with New York. New York is the second largest of the original Thirteen Colonies and is the only state that touches both the Great Lakes and the Atlantic Ocean.
In contrast with New York City's urban landscape, the vast majority of the state's geographic area is dominated by meadows, forests, rivers, farms, mountains, and lakes. New York's Adirondack Park is the largest state park in the United States and is larger than the Yellowstone, Yosemite, Grand Canyon, Glacier, and Olympic National Parks combined. New York established the first state park in the United States at Niagara Falls in 1885. Niagara Falls is shared between New York and Ontario as it flows on the Niagara River from Lake Erie to Lake Ontario.
Upstate and downstate are often used informally to distinguish New York City or its greater metropolitan area from the rest of New York State. The placement of a boundary between the two is a matter of great contention. Unofficial and loosely defined regions of Upstate New York include the Southern Tier, which often includes the counties along the border with Pennsylvania, and the North Country, which can mean anything from the strip along the Canada–US border to everything north of the Mohawk River.




In general, New York has a humid continental climate, though under the Köppen climate classification, New York City has a humid subtropical climate. Weather in New York is heavily influenced by two continental air masses: a warm, humid one from the southwest and a cold, dry one from the northwest.
Downstate New York, comprising New York City, Long Island, and lower portions of the Hudson Valley, has rather warm summers, with some periods of high humidity, and cold, damp winters which, however, are relatively mild compared to temperatures in Upstate New York, secondary to the former region's lower elevation, proximity to the Atlantic Ocean, and relatively lower latitude compared to the latter. Upstate New York experiences warm summers, marred by only occasional, brief intervals of sultry conditions, with long and cold winters. Western New York, particularly the Tug Hill region, receives heavy lake-effect snows, especially during the earlier portions of winter, before the surface of Lake Ontario itself is covered by ice. The summer climate is cool in the Adirondacks, Catskills, and at higher elevations of the Southern Tier.
Summer daytime temperatures usually range from the upper 70s to mid-80s °F (25 to 30 °C), over much of the state. In the majority of winter seasons, a temperature of −13 °F (−25 °C) or lower can be expected in the northern highlands (Northern Plateau) and 5 °F (−15 °C) or colder in the southwestern and east-central highlands of the Southern Tier.
New York ranks 46th among the 50 states in the amount of greenhouse gases generated per person. This relative efficient energy usage is primarily due to the dense, compact settlement in the New York City metropolitan area, and the state population's high rate of mass transit use in this area and between major cities.




Due to its long history, New York has several overlapping and often conflicting definitions of regions within the state. The regions are also not fully definable due to colloquial use of regional labels. The New York State Department of Economic Development provides two distinct definitions of these regions; it divides the state into ten economic regions, which approximately correspond to terminology used by residents:

The department also groups the counties into eleven regions for tourism purposes:




New York has many state parks and two major forest preserves. Adirondack Park, roughly the size of the state of Vermont and the largest state park in the United States, was established in 1892 and given state constitutional protection to remain "forever wild" in 1894. The park is larger than Yellowstone, Everglades, Glacier, and Grand Canyon national parks combined. The thinking that led to the creation of the Park first appeared in George Perkins Marsh's Man and Nature, published in 1864.
The Catskill Park was protected in legislation passed in 1885, which declared that its land was to be conserved and never put up for sale or lease. Consisting of 700,000 acres (2,800 km2) of land, the park is a habitat for deer, minks, and fishers. There are some 400 black bears living in the region. The state operates numerous campgrounds, and there are over 300 miles (480 km) of multi-use trails in the Park.
The Montauk Point State Park boasts the 1797 Montauk Lighthouse, commissioned under President George Washington, which is a major tourist attraction on the easternmost tip of Long Island. Hither Hills park offers camping and is a popular destination with surfcasting sport fishermen.




The State of New York is well represented in the National Park System with 22 national parks, which received 16,349,381 visitors in 2011. In addition, there are 4 National Heritage Areas, 27 National Natural Landmarks, 262 National Historic Landmarks, and 5,379 listings on the National Register of Historic Places.
African Burial Ground National Monument in Lower Manhattan is the only National Monument dedicated to Americans of African ancestry. It preserves a site containing the remains of more than 400 Africans buried during the late 17th and 18th centuries in a portion of what was the largest colonial-era cemetery for people of African descent, both free and enslaved, with an estimated tens of thousands of remains interred. The site's excavation and study were called "the most important historic urban archeological project in the United States."
Fire Island National Seashore is a United States National Seashore that protects a 26-mile (42 km) section of Fire Island, an approximately 30-mile (48 km) long barrier island separated from the mainland of Long Island by the Great South Bay. The island is part of Suffolk County.
Gateway National Recreation Area is more than 26,000 acres (10,522 ha) of water, salt marsh, wetlands, islands, and shoreline at the entrance to New York Harbor, the majority of which lies within New York. Including areas on Long Island and in New Jersey, it covers more area than that of two Manhattan Islands.
General Grant National Memorial is the final resting place of President Ulysses S. Grant and is the largest mausoleum in North America.
Hamilton Grange National Memorial preserves the home of Alexander Hamilton, Caribbean immigrant and orphan who rose to be a United States founding father and associate of George Washington.
Home of Franklin D. Roosevelt National Historic Site, established in 1945, preserves the Springwood estate in Hyde Park, New York. Springwood was the birthplace, lifelong home, and burial place of the 32nd President of the United States, Franklin D. Roosevelt.
Niagara Falls National Heritage Area was designated by Congress in 2008; it stretches from the western boundary of Wheatfield, New York to the mouth of the Niagara River on Lake Ontario, including the communities of Niagara Falls, Youngstown, and Lewiston. It includes Niagara Falls State Park and Colonial Niagara Historic District. It is managed in collaboration with the state.
Saratoga National Historical Park preserves the site of the Battles of Saratoga, the first significant American military victory of the American Revolutionary War. In 1777, American forces defeated a major British Army, which led France to recognize the independence of the United States, and enter the war as a decisive military ally of the struggling Americans.
Statue of Liberty National Monument includes Ellis Island and the Statue of Liberty. The statue, designed by Frédéric Bartholdi, was a gift from France to the United States to mark the Centennial of the American Declaration of Independence; it was dedicated in New York Harbor on October 28, 1886. It has since become an icon of the United States and the concepts of democracy and freedom.
Stonewall National Monument, in the Greenwich Village neighborhood of Lower Manhattan, is the first U.S. National Monument dedicated to LGBTQ rights, designated on June 24, 2016. The monument comprises the Stonewall Inn, commonly recognized to be the cradle of the gay liberation movement as the site of the 1969 Stonewall Riots; the adjacent Christopher Park; and surrounding streets and sidewalks.
Theodore Roosevelt Birthplace National Historic Site is the birthplace and childhood home of President Theodore Roosevelt, the only US President born in New York City.




New York is divided into 62 counties. Aside from the five counties of New York City, each of these counties is subdivided into towns and cities. Towns can contain incorporated villages or unincorporated hamlets. New York City is divided into five boroughs, each coterminous with a county.
Downstate New York (New York City, Long Island, and the southern portion of the Hudson Valley) can be considered to form the central core of the Northeast megalopolis, an urbanized region stretching from New Hampshire to Virginia.
The major cities of the state developed along the key transportation and trade routes of the early 19th century, including the Erie Canal and railroads paralleling it. Today, the New York Thruway acts as a modern counterpart to commercial water routes.







The distribution of change in population growth is uneven in New York State; the New York City metropolitan area is growing considerably, along with Saratoga County and the Capital District, collectively known as Tech Valley. New York City gained more residents between April 2010 and July 2014 (316,000) than any other U.S. city. Conversely, outside of the Rochester and Ithaca areas, population growth in much of Western New York is nearly stagnant. According to immigration statistics, the state is a leading recipient of migrants from around the globe. Between 2000 and 2005, immigration failed to surpass emigration, a trend that has been reversing since 2006. New York State lost two House seats in the 2011 congressional reapportionment, secondary to relatively slow growth when compared to the rest of the United States. In 2000 and 2005, more people moved from New York to Florida than from any one state to another, contributing to New York becoming the U.S.'s fourth most populous state in 2015, behind Florida, Texas, and California. However, New York State has the second-largest international immigrant population in the country among the American states, at 4.2 million as of 2008; most reside in and around New York City, due to its size, high profile, vibrant economy, and cosmopolitan culture.
The United States Census Bureau estimates that the population of New York was 19,795,791 on July 1, 2015, a 2.16% increase since the 2010 United States Census. Despite the open land in the state, New York's population is very urban, with 92% of residents living in an urban area, predominantly in the New York City metropolitan area.
Two-thirds of New York State's population resides in New York City Metropolitan Area. New York City is the most populous city in the United States, with an estimated record high population of 8,550,405 in 2015, incorporating more immigration into the city than emigration since the 2010 United States Census. More than twice as many people live in New York City as in the second-most populous U.S. city (Los Angeles), and within a smaller area. Long Island alone accounted for a Census-estimated 7,838,722 residents in 2015, representing 39.6% of New York State's population.




These are the ten counties with the largest populations as of 2010:
Kings County (Brooklyn): 2,504,700
Queens County (Queens): 2,230,722
New York County (Manhattan): 1,585,873
Suffolk County: 1,493,350
Bronx County (the Bronx): 1,385,108
Nassau County: 1,339,532
Westchester County: 949,113
Erie County: 919,040
Monroe County: 744,344
Richmond County (Staten Island): 468,730




There are 62 cities in New York. The largest city in the state and the most populous city in the United States is New York City, which comprises five counties (each coextensive with a borough): Bronx, New York County (Manhattan), Queens, Kings County (Brooklyn), and Richmond County (Staten Island). New York City is home to more than two-fifths of the state's population. Albany, the state capital, is the sixth-largest city in New York State. The smallest city is Sherrill, New York, in Oneida County. Hempstead is the most populous town in the state; if it were a city, it would be the second largest in New York State, with over 700,000 residents.




The following are the top ten metropolitan areas in the state as of the 2010 Census:
New York City and the Hudson Valley (19,567,410 in NY/NJ/PA, 13,038,826 in NY)
Buffalo-Niagara Falls (1,135,509)
Rochester (1,079,671)
Albany and the Capital District (870,716)
Syracuse (662,577)
Utica-Rome (299,397)
Binghamton (251,725)
Kingston (182,493)
Glens Falls (128,923)
Watertown-Fort Drum (116,229)




According to the U.S. Census Bureau, the 2010 racial makeup of New York State was as follows by self-identification:
White American – 65.7%
Black or African American – 15.9%
Asian American – 7.3% (3.0% Chinese, 1.6% Indian, 0.7% Korean, 0.5% Filipino, 0.3% Pakistani, 0.3% Bangladeshi, 0.2% Japanese, 0.1% Vietnamese)
Multiracial Americans – 3.0%
Native American/American Indian – 0.6%
Some other race - 7.5%
In 2004, the major ancestry groups in New York State by self-identification were Hispanic and Latino Americans (17.6%), African American (15.8%), Italian (14.4%), Irish (12.9%), German (11.1%) and English (6%). According to a 2010 estimate, 21.7% of the population is foreign-born.

The state's most populous racial group, non-Hispanic white, has declined as a proportion of the state population from 94.6% in 1940 to 58.3% in 2010. As of 2011, 55.6% of New York's population younger than age 1 were minorities. New York's robustly increasing Jewish population, the largest outside of Israel, was the highest among states both by percentage and absolute number in 2012. It is driven by the high reproductive rate of Orthodox Jewish families, particularly in Brooklyn and communities of the Hudson Valley.
New York is home to the second-largest African American population (after Georgia) and the second largest Asian-American population (after California) in the United States. New York's uniracial Black population increased by 2.0% between 2000 and 2010, to 3,073,800. The Black population is in a state of flux, as New York is the largest recipient of immigrants from Africa, while established African Americans are migrating out of New York to the southern United States. The New York City neighborhood of Harlem has historically been a major cultural capital for African-Americans of sub-Saharan descent, and Bedford-Stuyvesant in Brooklyn has the largest such population in the United States. Meanwhile, New York's uniracial Asian population increased by a notable 36% from 2000 to 2010, to 1,420,244. Queens, in New York City, is home to the state's largest Asian-American population and is the most ethnically diverse county in the United States; it is the most ethnically diverse urban area in the world.
New York's growing uniracial Hispanic-or-Latino population numbered 3,416,922 in 2010, a 19% increase from the 2,867,583 enumerated in 2000. Queens is home to the largest Andean (Colombian, Ecuadorian, Peruvian, and Bolivian) populations in the United States. In addition, New York has the largest Puerto Rican, Dominican, and Jamaican American populations in the continental United States.
The Chinese population constitutes the fastest-growing nationality in New York State; multiple satellites of the original Manhattan Chinatown (曼哈頓華埠), in Brooklyn (布鲁克林華埠), and around Flushing, Queens (法拉盛華埠), are thriving as traditionally urban enclaves, while also expanding rapidly eastward into suburban Nassau County (拿騷縣), on Long Island (長島). New York State has become the top destination for new Chinese immigrants, and large-scale Chinese immigration continues into the state. A new China City of America is also planned in Sullivan County. Long Island, including Queens and Nassau County, is also home to several Little Indias (लघु भारत) and a large Koreatown (롱 아일랜드 코리아타운), with large and growing attendant populations of Indian Americans and Korean Americans, respectively. Brooklyn has been a destination for West Indian immigrants of African descent, as well as Asian Indian immigrants.
In the 2000 Census, New York had the largest Italian American population, composing the largest self-identified ancestral group in Staten Island and Long Island, followed by Irish Americans. Albany and the Mohawk Valley also have large communities of ethnic Italians and Irish Americans, reflecting 19th and early 20th-century immigration. In Buffalo and western New York, German Americans comprise the largest ancestry. In the North Country of New York, French Canadians represent the leading ethnicity, given the area's proximity to Quebec. Americans of English ancestry are present throughout all of upstate New York, reflecting early colonial and later immigrants.
6.5% of New York's population were under five years of age, 24.7% under 18, and 12.9% were 65 or older. Females made up 51.8% of the state's population.



In 2010, the most common American English dialects spoken in New York, besides General American English, were the New York City area dialect (including New York Latino English and North Jersey English), the Western New England accent around Albany, and Inland Northern American English in Buffalo and western New York State. As many as 800 languages are spoken in New York City, making it the most linguistically diverse city in the world.
As of 2010, 70.72% (12,788,233) of New York residents aged five and older reported speaking only English at home, while 14.44% (2,611,903) spoke Spanish, 2.61% (472,955) Chinese (which includes Cantonese and Mandarin), 1.20% (216,468) Russian, 1.18% (213,785) Italian, 0.79% (142,169) French Creole, 0.75% (135,789) French, 0.67% (121,917) Yiddish, 0.63% (114,574) Korean, and Polish was spoken by 0.53% (95,413) of the population over the age of five. In total, 29.28% (5,295,016) of New York's population aged five and older reported speaking a language other than English.



In 2010, the Association of Religion Data Archives (ARDA) reported that the largest denominations were the Catholic Church with 6,286,916; Orthodox Judaism with 588,500; Islam with 392,953; and the United Methodist Church with 328,315 adherents.




Roughly 3.8 percent of the state's adult population self-identifies as lesbian, gay, bisexual, or transgender. This constitutes a total LGBT adult population of 570,388 individuals. In 2010, the number of same-sex couple households stood at roughly 48,932. New York was the fifth state to license same-sex marriages, after New Hampshire. Michael Bloomberg, the Mayor of New York City, stated that "same-sex marriages in New York City have generated an estimated $259 million in economic impact and $16 million in City revenues" in the first year after the enactment of the Marriage Equality Act". Same-sex marriages in New York were legalized on June 24, 2011 and were authorized to take place beginning 30 days thereafter. New York City is also home to the largest transgender population in the United States, estimated at 25,000 in 2016.




New York's gross state product in 2015 was $1.44 trillion. If New York State were an independent nation, it would rank as the 12th or 13th largest economy in the world, depending upon international currency fluctuations. However, in 2013, the multi-state, New York City-centered Metropolitan Statistical Area produced a gross metropolitan product (GMP) of nearly US$1.39 trillion, while in 2012, the corresponding Combined Statistical Area generated a GMP of over US$1.55 trillion, both ranking first nationally by a wide margin and behind the GDP of only twelve nations and eleven nations, respectively.




Anchored by Wall Street in the Financial District of Lower Manhattan, New York City has been called both the most economically powerful city and the leading financial center of the world. Lower Manhattan is the third-largest central business district in the United States and is home to the New York Stock Exchange, on Wall Street, and the NASDAQ, at 165 Broadway, representing the world's largest and second largest stock exchanges, respectively, as measured both by overall average daily trading volume and by total market capitalization of their listed companies in 2013. Investment banking fees on Wall Street totaled approximately $40 billion in 2012, while in 2013, senior New York City bank officers who manage risk and compliance functions earned as much as $324,000 annually. In fiscal year 2013-14, Wall Street's securities industry generated 19% of New York State's tax revenue. New York City remains the largest global center for trading in public equity and debt capital markets, driven in part by the size and financial development of the U.S. economy. New York also leads in hedge fund management; private equity; and the monetary volume of mergers and acquisitions. Several investment banks and investment managers headquartered in Manhattan are important participants in other global financial centers. New York is also the principal commercial banking center of the United States.
Many of the world's largest media conglomerates are also based in the city. Manhattan contained approximately 520 million square feet (48.1 million m2) of office space in 2013, making it the largest office market in the United States, while Midtown Manhattan is the largest central business district in the nation.




Silicon Alley, centered in New York City, has evolved into a metonym for the sphere encompassing the New York City metropolitan region's high technology and entrepreneurship ecosystem; in 2015, Silicon Alley generated over US$7.3 billion in venture capital investment. High tech industries including digital media, biotechnology, software development, game design, and other fields in information technology are growing, bolstered by New York City's position at the terminus of several transatlantic fiber optic trunk lines, its intellectual capital, as well as its growing outdoor wireless connectivity. In December 2014, New York State announced a $50 million venture-capital fund to encourage enterprises working in biotechnology and advanced materials; according to Governor Andrew Cuomo, the seed money would facilitate entrepreneurs in bringing their research into the marketplace. On December 19, 2011, then Mayor Michael R. Bloomberg announced his choice of Cornell University and Technion-Israel Institute of Technology to build a US$2 billion graduate school of applied sciences on Roosevelt Island in Manhattan, with the goal of transforming New York City into the world's premier technology capital.




Albany, Saratoga County, Rensselaer County, and the Hudson Valley, collectively recognized as eastern New York's Tech Valley, have experienced significant growth in the computer hardware side of the high-technology industry, with great strides in the nanotechnology sector, digital electronics design, and water- and electricity-dependent integrated microchip circuit manufacturing, involving companies including IBM and its Thomas J. Watson Research Center, GlobalFoundries, Samsung, and Taiwan Semiconductor, among others. The area's high technology ecosystem is supported by technologically focused academic institutions including Rensselaer Polytechnic Institute and the SUNY Polytechnic Institute. In 2015, Tech Valley, straddling both sides of the Adirondack Northway and the New York Thruway, generated over US$163 million in venture capital investment. The Rochester area is important in the field of photographic processing and imaging as well as incubating an increasingly diverse high technology sphere encompassing STEM fields, similarly in part the result of private startup enterprises collaborating with major academic institutions, including the University of Rochester and Cornell University. Westchester County has developed a burgeoning biotechnology sector in the 21st century, with over US$1 billion in planned private investment as of 2016,




Creative industries, which are concerned with generating and distributing knowledge and information, such as new media, digital media, film and television production, advertising, fashion, design, and architecture, account for a growing share of employment, with New York City possessing a strong competitive advantage in these industries. As of 2014, New York State was offering tax incentives of up to $420 million annually for filmmaking within the state, the most generous such tax rebate among the U.S. states. New York has also attracted higher-wage visual-effects employment by further augmenting its tax credit to a maximum of 35% for performing post-film production work in Upstate New York. The filmed entertainment industry has been growing in New York, contributing nearly US$9 billion to the New York City economy alone as of 2015.




I Love New York (stylized I ❤ NY) is both a logo and a song that are the basis of an advertising campaign and have been used since 1977 to promote tourism in New York City, and later to promote New York State as well. The trademarked logo, owned by New York State Empire State Development, appears in souvenir shops and brochures throughout the state, some licensed, many not. The song is the state song of New York. The Broadway League reported that Broadway shows sold approximately US$1.27 billion worth of tickets in the 2013–2014 season, an 11.4% increase from US$1.139 billion in the 2012–2013 season. Attendance in 2013–2014 stood at 12.21 million, representing a 5.5% increase from the 2012–2013 season's 11.57 million.



New York exports a wide variety of goods such as prepared foods, computers and electronics, cut diamonds, and other commodities. In 2007, the state exported a total of $71.1 billion worth of goods, with the five largest foreign export markets being Canada (US$15 billion), the United Kingdom (US$6 billion), Switzerland (US$5.9 billion), Israel (US$4.9 billion), and Hong Kong (US$3.4 billion). New York's largest imports are oil, gold, aluminum, natural gas, electricity, rough diamonds, and lumber. The state also has a large manufacturing sector that includes printing and the production of garments, mainly in New York City; and furs, railroad equipment, automobile parts, and bus line vehicles, concentrated in Upstate regions.
New York is the nation's third-largest grape producing state, and second-largest wine producer by volume, behind California. The southern Finger Lakes hillsides, the Hudson Valley, the North Fork of Long Island, and the southern shore of Lake Erie are the primary grape- and wine-growing regions in New York, with many vineyards. In 2012, New York had 320 wineries and 37,000 grape bearing acres, generating full-time employment of nearly 25,000 and annual wages of over US$1.1 billion, and yielding US$4.8 billion in direct economic impact from New York grapes, grape juice, and wine and grape products.
New York is a major agricultural producer overall, ranking among the top five states for agricultural products including maple syrup, apples, cherries, cabbage, dairy products, onions, and potatoes. The state is the largest producer of cabbage in the U.S. The state has about a quarter of its land in farms and produced $3.4 billion in agricultural products in 2001. The south shore of Lake Ontario provides the right mix of soils and microclimate for many apple, cherry, plum, pear and peach orchards. Apples are also grown in the Hudson Valley and near Lake Champlain. A moderately sized saltwater commercial fishery is located along the Atlantic side of Long Island. The principal catches by value are clams, lobsters, squid, and flounder.




The University of the State of New York accredits and sets standards for primary, middle-level, and secondary education in the state, while the New York State Education Department oversees public schools and controls their standardized tests. The New York City Department of Education manages the New York City Public Schools system. In 1894, reflecting general racial discrimination then, the state passed a law that allowed communities to set up separate schools for children of African-American descent. In 1900, the state passed another law requiring integrated schools.
At the level of post-secondary education, the statewide public university system is the State University of New York, commonly referred to as SUNY. New York City also has its own City University of New York system, which is funded by the city. The SUNY system consists of 64 community colleges, technical colleges, undergraduate colleges, and doctoral-granting institutions, including several universities. New York's largest public university is the State University of New York at Buffalo, which was founded by U.S President and Vice President Millard Fillmore. The four SUNY University Centers, offering a wide array of academic programs, are the University at Albany, Binghamton University, Stony Brook University, and the University at Buffalo.
Notable large private universities include the Columbia University in Upper Manhattan and Cornell University in Ithaca, both Ivy League institutions, as well as New York University in Lower Manhattan, and Fordham University in the Bronx, Manhattan, and Westchester County. Smaller notable private institutions of higher education include Rockefeller University, New York Institute of Technology, Yeshiva University, and Hofstra University. There are also a multitude of postgraduate-level schools in New York State, including Medical, Law, and Engineering schools. West Point, the service academy of the U.S. Army, is located just south of Newburgh, on the west bank of the Hudson River.
During the fiscal 2013 year, New York spent more on public education per pupil than any other state, according to U.S. Census Bureau statistics.




New York has one of the most extensive and one of the oldest transportation infrastructures in the country. Engineering challenges posed by the complex terrain of the state and the unique infrastructural issues of New York City brought on by urban crowding have had to be overcome perennially. Population expansion of the state has followed the path of the early waterways, first the Hudson River and Mohawk River, then the Erie Canal. In the 19th century, railroads were constructed along the river valleys, followed by the New York State Thruway in the 20th century.
The New York State Department of Transportation (NYSDOT) is the department of the government of New York responsible for the development and operation of highways, railroads, mass transit systems, ports, waterways, and aviation facilities within New York State. The NYSDOT is headquartered at 50 Wolf Road in Colonie, Albany County. The Port Authority of New York and New Jersey (PANYNJ) is a joint venture between the States of New York and New Jersey and authorized by the US Congress, established in 1921 through an interstate compact, that oversees much of the regional transportation infrastructure, including bridges, tunnels, airports, and seaports, within the geographical jurisdiction of the Port of New York and New Jersey. This 1,500 square mile (3,900 km²) port district is generally encompassed within a 25-mile (40 km) radius of the Statue of Liberty National Monument. The Port Authority is headquartered at 4 World Trade Center in Lower Manhattan.
In addition to the well known New York City Subway system – which is confined within New York City – four suburban commuter railroad systems enter and leave the city: the Long Island Rail Road, Metro-North Railroad, Port Authority Trans-Hudson, and five of New Jersey Transit's rail lines. The New York City Department of Transportation (NYCDOT) is the agency of the government of New York City responsible for the management of much of New York City's own transportation infrastructure. Other cities and towns in New York have urban and regional public transportation. In Buffalo, the Niagara Frontier Transportation Authority runs the Buffalo Metro Rail light-rail system; in Rochester, the Rochester Subway operated from 1927 until 1956, but fell into disuse as state and federal investment went to highways.
The New York State Department of Motor Vehicles (NYSDMV or DMV) is the governmental agency responsible for registering and inspecting automobiles and other motor vehicles, as well as licensing drivers in the State of New York. As of 2008, the NYSDMV has 11,284,546 drivers licenses on file and 10,697,644 vehicle registrations in force. All gasoline-powered vehicles registered in New York State are required to have an emissions inspection every 12 months, in order to ensure that environmental quality controls are working to prevent air pollution. Diesel-powered vehicles with a gross weight rating over 8,500 lb that are registered in most Downstate New York counties must get an annual emissions inspection. All vehicles registered in New York State must get an annual safety inspection.
Portions of the transportation system are intermodal, allowing travelers to switch easily from one mode of transportation to another. One of the most notable examples is AirTrain JFK which allows rail passengers to travel directly to terminals at John F. Kennedy International Airport as well as to the underground New York City Subway system.







The Government of New York embodies the governmental structure of the State of New York as established by the New York State Constitution. It is composed of three branches: executive, legislative, and judicial.
The Governor is the State's chief executive and is assisted by the Lieutenant Governor. Both are elected on the same ticket. Additional elected officers include the Attorney General, and the Comptroller. The Secretary of State, formerly an elected officer, is currently appointed by the Governor.
The New York State Legislature is bicameral and consists of the New York State Senate and the New York State Assembly. The Assembly consists of 150 members, while the Senate varies in its number of members, currently having 63. The Legislature is empowered to make laws, subject to the Governor's power to veto a bill. However, the veto may be overridden by the Legislature if there is a two-thirds majority in favor of overriding in each House. The permanent laws of a general nature are codified in the Consolidated Laws of New York.
The highest court of appeal in the Unified Court System is the Court of Appeals whereas the primary felony trial court is the County Court (or the Supreme Court in New York City). The Supreme Court also acts as the intermediate appellate court for many cases, and the local courts handle a variety of other matters including small claims, traffic ticket cases, and local zoning matters, and are the starting point for all criminal cases. The New York City courts make up the largest local court system.
The state is divided into counties, cities, towns, and villages, all of which are municipal corporations with respect to their own governments, as well as various corporate entities that serve single purposes that are also local governments, such as school districts, fire districts, and New York state public-benefit corporations, frequently known as authorities or development corporations. Each municipal corporation is granted varying home rule powers as provided by the New York Constitution. The state also has 10 Indian reservations.




Capital punishment was reintroduced in 1995 under the Pataki administration, but the statute was declared unconstitutional in 2004, when the New York Court of Appeals ruled in People v. LaValle that it violated the state constitution. The remaining death sentence was commuted by the court to life imprisonment in 2007, in People v. John Taylor, and the death row was disestablished in 2008, under executive order from Governor Paterson. No execution has taken place in New York since 1963. Legislative efforts to amend the statute have failed, and death sentences are no longer sought at the state level, though certain crimes that fall under the jurisdiction of the federal government are subject to the federal death penalty.




The State of New York sends 27 members to the House of Representatives in addition to its two United States Senators. As of the 2000 census and the redistricting for the 2002 elections, the state had 29 members in the House, but the representation was reduced to 27 in 2013 due to the state's slower overall population growth relative to the overall national population growth. From 2016, New York will have 29 electoral votes in national presidential elections (a drop from its peak of 47 votes from 1933 to 1953).
New York is represented by Chuck Schumer and Kirsten Gillibrand in the United States Senate and has the nation's third equal highest number of congressional districts, equal with Florida and behind California's 53 and Texas's 36.
The state has a strong imbalance of payments with the federal government. According to the Office of the New York State Comptroller, New York State received 91 cents in services for every $1 it sent in taxes to the U.S. federal government in the 2013 fiscal year; New York ranked in 46th place in the federal balance of payments to the state on a per capita basis.




As of April 2016, Democrats represented a plurality of voters in New York State, constituting over twice as many registered voters as any other political party affiliation or lack thereof. Since the second half of the 20th century, New York has generally supported candidates belonging to the Democratic Party in national elections. Democratic presidential candidate Barack Obama won New York State by over 25 percentage points in both 2012 and 2008. New York City, as well as the state's other major urban locales, including Albany, Buffalo, Rochester, Yonkers, and Syracuse, are significant Democratic strongholds, with liberal politics. Rural portions of upstate New York, however, are generally more conservative than the cities and tend to favor Republicans. Heavily populated suburban areas downstate, such as Westchester County and Long Island, have swung between the major parties since the 1980s, but more often than not support Democrats.
New York City is the most important source of political fundraising in the United States for both major parties. Four of the top five zip codes in the nation for political contributions are in Manhattan. The top zip code, 10021 on the Upper East Side, generated the most money for the 2000 presidential campaigns of both George W. Bush and Al Gore.
The state of New York has the distinction of being the home state for both major-party nominees in three Presidential elections. The 1904 presidential election saw former New York Governor and incumbent President Theodore Roosevelt face Alton B. Parker, chief judge of the New York Court of Appeals. The 1944 presidential election had Franklin D. Roosevelt, following in his cousin Theodore's footsteps as former New York Governor and incumbent president running for re-election against then-current New York Governor Thomas E. Dewey. In the 2016 Presidential election, former United States Senator from New York Hillary Clinton, a resident of Chappaqua, was the Democratic Party nominee. The Republican Party nominee was businessman Donald Trump, a resident of Manhattan and a native of Queens.
New York City is an important center for international diplomacy. The United Nations Headquarters has been situated on the East Side of Midtown Manhattan since 1952.




New York State is geographically home to one National Football League team, the Buffalo Bills, based in the Buffalo suburb of Orchard Park. Although the New York Giants and New York Jets represent the New York metropolitan area and were previously located in New York City, they play in MetLife Stadium, located in East Rutherford, New Jersey. New York also has two Major League Baseball teams, the New York Yankees (based in the Bronx) and the New York Mets (based in Queens). Minor league baseball teams also play in the State of New York, including the Long Island Ducks. New York is home to three National Hockey League franchises: the New York Rangers in Manhattan, the New York Islanders in Brooklyn, and the Buffalo Sabres in Buffalo. New York has two National Basketball Association teams, the New York Knicks in Manhattan, and the Brooklyn Nets in Brooklyn. New York is the home of a Major League Soccer franchise, New York City FC, currently playing in the Bronx. Although the New York Red Bulls represent the New York metropolitan area, they play in Red Bull Arena in Harrison, New Jersey.
New York hosted the 1932 and 1980 Winter Olympics at Lake Placid. The 1980 Games are known for the USA–USSR ice hockey match dubbed the "Miracle on Ice", in which a group of American college students and amateurs defeated the heavily favored Soviet national ice hockey team 4–3 and went on to win the gold medal against Finland. Along with St. Moritz, Switzerland and Innsbruck, Austria, Lake Placid is one of the three cities to have hosted the Winter Olympic Games twice. New York City bid for the 2012 Summer Olympics but lost to London.
Several U.S. national sports halls of fame are or have been situated in New York. The National Baseball Hall of Fame and Museum is located in Cooperstown, Otsego County. The National Museum of Racing and Hall of Fame in Saratoga Springs, Saratoga County, honors achievements in the sport of thoroughbred horse racing. The physical facility of the National Soccer Hall of Fame in Oneonta, also in Otsego County, closed in 2010, although the organization itself has continued inductions. The annual United States Open Tennis Championships is one of the world's four Grand Slam tennis tournaments and is held at the National Tennis Center in Flushing Meadows-Corona Park in the New York City borough of Queens.



Index of New York-related articles
Outline of New York – organized list of topics about New York







French, John Homer (1860). Historical and statistical gazetteer of New York State. Syracuse, New York: R. Pearsall Smith. OCLC 224691273. (Full text via Google Books.)
New York State Historical Association (1940). New York: A Guide to the Empire State. New York City: Oxford University Press. ISBN 978-1-60354-031-5. OCLC 504264143. (Full text via Google Books.)



New York State Guide, from the Library of Congress
New York at DMOZ
 Geographic data related to New York at OpenStreetMapSeattle (/siˈætəl/) is a seaport city on the west coast of the United States and the seat of King County, Washington. With an estimated 684,451 residents as of 2015, Seattle is the largest city in both the state of Washington and the Pacific Northwest region of North America. In July 2013, it was the fastest-growing major city in the United States and remained in the Top 5 in May 2015 with an annual growth rate of 2.1%. The city is situated on an isthmus between Puget Sound (an inlet of the Pacific Ocean) and Lake Washington, about 100 miles (160 km) south of the Canada–United States border. A major gateway for trade with Asia, Seattle is the fourth-largest port in North America in terms of container handling as of 2016.
The Seattle area was previously inhabited by Native Americans for at least 4,000 years before the first permanent European settlers. Arthur A. Denny and his group of travelers, subsequently known as the Denny Party, arrived from Illinois via Portland, Oregon, on the schooner Exact at Alki Point on November 13, 1851. The settlement was moved to the eastern shore of Elliott Bay and named "Seattle" in 1852, after Chief Si'ahl of the local Duwamish and Suquamish tribes.
Logging was Seattle's first major industry, but by the late-19th century, the city had become a commercial and shipbuilding center as a gateway to Alaska during the Klondike Gold Rush. Growth after World War II was partially due to the local Boeing company, which established Seattle as a center for aircraft manufacturing. The Seattle area developed as a technology center beginning in the 1980s, with companies like Microsoft becoming established in the region. In 1994, Internet retailer Amazon was founded in Seattle. The stream of new software, biotechnology, and Internet companies led to an economic revival, which increased the city's population by almost 50,000 between 1990 and 2000.
Seattle has a noteworthy musical history. From 1918 to 1951, nearly two dozen jazz nightclubs existed along Jackson Street, from the current Chinatown/International District, to the Central District. The jazz scene developed the early careers of Ray Charles, Quincy Jones, Ernestine Anderson, and others. Seattle is also the birthplace of rock musician Jimi Hendrix and the alternative rock subgenre grunge.






Archaeological excavations suggest that Native Americans have inhabited the Seattle area for at least 4,000 years. By the time the first European settlers arrived, the people (subsequently called the Duwamish tribe) occupied at least seventeen villages in the areas around Elliott Bay.
The first European to visit the Seattle area was George Vancouver, in May 1792 during his 1791–95 expedition to chart the Pacific Northwest. In 1851, a large party led by Luther Collins made a location on land at the mouth of the Duwamish River; they formally claimed it on September 14, 1851. Thirteen days later, members of the Collins Party on the way to their claim passed three scouts of the Denny Party. Members of the Denny Party claimed land on Alki Point on September 28, 1851. The rest of the Denny Party set sail from Portland, Oregon and landed on Alki point during a rainstorm on November 13, 1851.




After a difficult winter, most of the Denny Party relocated across Elliott Bay and claimed land a second time at the site of present-day Pioneer Square, naming this new settlement Duwamps. Charles Terry and John Low remained at the original landing location and reestablished their old land claim and called it "New York", but renamed "New York Alki" in April 1853, from a Chinook word meaning, roughly, "by and by" or "someday". For the next few years, New York Alki and Duwamps competed for dominance, but in time Alki was abandoned and its residents moved across the bay to join the rest of the settlers.
David Swinson "Doc" Maynard, one of the founders of Duwamps, was the primary advocate to name the settlement after Chief Sealth ("Seattle") of the Duwamish and Suquamish tribes.



The name "Seattle" appears on official Washington Territory papers dated May 23, 1853, when the first plats for the village were filed. In 1855, nominal land settlements were established. On January 14, 1865, the Legislature of Territorial Washington incorporated the Town of Seattle with a board of trustees managing the city. The town of Seattle was disincorporated January 18, 1867, and remained a mere precinct of King County until late 1869, when a new petition was filed and the city was re-incorporated December 2, 1869, with a Mayor-council government. The corporate seal of the City of Seattle carries the date "1869" and a likeness of Chief Sealth in left profile.




Seattle has a history of boom-and-bust cycles, like many other cities near areas of extensive natural and mineral resources. Seattle has risen several times economically, then gone into precipitous decline, but it has typically used those periods to rebuild solid infrastructure.
The first such boom, covering the early years of the city, rode on the lumber industry. (During this period the road now known as Yesler Way won the nickname "Skid Road", supposedly after the timber skidding down the hill to Henry Yesler's sawmill. The later dereliction of the area may be a possible origin for the term which later entered the wider American lexicon as Skid Row.) Like much of the American West, Seattle saw numerous conflicts between labor and management, as well as ethnic tensions that culminated in the anti-Chinese riots of 1885–1886. This violence originated with unemployed whites who were determined to drive the Chinese from Seattle (anti-Chinese riots also occurred in Tacoma). In 1900, Asians were 4.2% of the population. Authorities declared martial law and federal troops arrived to put down the disorder.
Seattle achieved sufficient economic success that when the Great Seattle Fire of 1889 destroyed the central business district, a far grander city-center rapidly emerged in its place. Finance company Washington Mutual, for example, was founded in the immediate wake of the fire. However, the Panic of 1893 hit Seattle hard.




The second and most dramatic boom and bust resulted from the Klondike Gold Rush, which ended the depression that had begun with the Panic of 1893; in a short time, Seattle became a major transportation center. On July 14, 1897, the S.S. Portland docked with its famed "ton of gold", and Seattle became the main transport and supply point for the miners in Alaska and the Yukon. Few of those working men found lasting wealth, however; it was Seattle's business of clothing the miners and feeding them salmon that panned out in the long run. Along with Seattle, other cities like Everett, Tacoma, Port Townsend, Bremerton, and Olympia, all in the Puget Sound region, became competitors for exchange, rather than mother lodes for extraction, of precious metals. The boom lasted well into the early part of the 20th century and funded many new Seattle companies and products. In 1907, 19-year-old James E. Casey borrowed $100 from a friend and founded the American Messenger Company (later UPS). Other Seattle companies founded during this period include Nordstrom and Eddie Bauer. Seattle brought in the Olmsted Brothers landscape architecture firm to design a system of parks and boulevards.

The Gold Rush era culminated in the Alaska-Yukon-Pacific Exposition of 1909, which is largely responsible for the layout of today's University of Washington campus.
A shipbuilding boom in the early part of the 20th century became massive during World War I, making Seattle somewhat of a company town; the subsequent retrenchment led to the Seattle General Strike of 1919, the first general strike in the country. A 1912 city development plan by Virgil Bogue went largely unused. Seattle was mildly prosperous in the 1920s but was particularly hard hit in the Great Depression, experiencing some of the country's harshest labor strife in that era. Violence during the Maritime Strike of 1934 cost Seattle much of its maritime traffic, which was rerouted to the Port of Los Angeles.
Seattle was also the home base of impresario Alexander Pantages who, starting in 1902, opened a number of theaters in the city exhibiting vaudeville acts and silent movies. His activities soon expanded, and the thrifty Greek went on and became one of America's greatest theater and movie tycoons. Between Pantages and his rival John Considine, Seattle was for a while the western United States' vaudeville mecca. B. Marcus Priteca, the Scottish-born and Seattle-based architect, built several theaters for Pantages, including some in Seattle. The theaters he built for Pantages in Seattle have been either demolished or converted to other uses, but many other theaters survive in other cities of the U.S., often retaining the Pantages name; Seattle's surviving Paramount Theatre, on which he collaborated, was not a Pantages theater.




War work again brought local prosperity during World War II, this time centered on Boeing aircraft. The war dispersed the city's numerous Japanese-American businessmen due to the Japanese American internment. After the war, the local economy dipped. It rose again with Boeing's growing dominance in the commercial airliner market. Seattle celebrated its restored prosperity and made a bid for world recognition with the Century 21 Exposition, the 1962 World's Fair. Another major local economic downturn was in the late 1960s and early 1970s, at a time when Boeing was heavily affected by the oil crises, loss of Government contracts, and costs and delays associated with the Boeing 747. Many people left the area to look for work elsewhere, and two local real estate agents put up a billboard reading "Will the last person leaving Seattle – Turn out the lights."
Seattle remained the corporate headquarters of Boeing until 2001, when the company separated its headquarters from its major production facilities; the headquarters were moved to Chicago. The Seattle area is still home to Boeing's Renton narrow-body plant (where the 707, 720, 727, and 757 were assembled, and the 737 is assembled today) and Everett wide-body plant (assembly plant for the 747, 767, 777, and 787). The company's credit union for employees, BECU, remains based in the Seattle area, though it is now open to all residents of Washington.
As prosperity began to return in the 1980s, the city was stunned by the Wah Mee massacre in 1983, when 13 people were killed in an illegal gambling club in the International District, Seattle's Chinatown. Beginning with Microsoft's 1979 move from Albuquerque, New Mexico, to nearby Bellevue, Washington, Seattle and its suburbs became home to a number of technology companies including Amazon.com, RealNetworks, Nintendo of America, McCaw Cellular (now part of AT&T Mobility), VoiceStream (now T-Mobile), and biomedical corporations such as HeartStream (later purchased by Philips), Heart Technologies (later purchased by Boston Scientific), Physio-Control (later purchased by Medtronic), ZymoGenetics, ICOS (later purchased by Eli Lilly and Company) and Immunex (later purchased by Amgen). This success brought an influx of new residents with a population increase within city limits of almost 50,000 between 1990 and 2000, and saw Seattle's real estate become some of the most expensive in the country. In 1993, the movie Sleepless in Seattle brought the city further national attention. Many of the Seattle area's tech companies remained relatively strong, but the frenzied dot-com boom years ended in early 2001.
Seattle in this period attracted widespread attention as home to these many companies, but also by hosting the 1990 Goodwill Games and the APEC leaders conference in 1993, as well as through the worldwide popularity of grunge, a sound that had developed in Seattle's independent music scene. Another bid for worldwide attention—hosting the World Trade Organization Ministerial Conference of 1999—garnered visibility, but not in the way its sponsors desired, as related protest activity and police reactions to those protests overshadowed the conference itself. The city was further shaken by the Mardi Gras Riots in 2001, and then literally shaken the following day by the Nisqually earthquake.
Yet another boom began as the city emerged from the Great Recession. Amazon.com moved its headquarters from North Beacon Hill to South Lake Union and began a rapid expansion. For the five years beginning in 2010, Seattle gained an average of 14,511 residents per year, with the growth strongly skewed toward the center of the city, as unemployment dropped from roughly 9 percent to 3.6 percent. The city has found itself "bursting at the seams", with over 45,000 households spending more than half their income on housing and at least 2,800 people homeless, and with the country's sixth-worst rush hour traffic.



With a land area of 83.9 square miles (217.3 km²), Seattle is the northernmost city with at least 500,000 people in the United States, farther north than Canadian cities such as Toronto, Ottawa, and Montreal, at about the same latitude as Salzburg, Austria.
The topography of Seattle is hilly. The city lies on several hills, including Capitol Hill, First Hill, West Seattle, Beacon Hill, Magnolia, Denny Hill, and Queen Anne. The Kitsap and the Olympic peninsulas along with the Olympic mountains lie to the west of Puget Sound, while the Cascade Range and Lake Sammamish lie to the east of Lake Washington. The city has over 5,540 acres (2,242 ha) of parkland.







Seattle is located between the saltwater Puget Sound (an arm of the Pacific Ocean) to the west and Lake Washington to the east. The city's chief harbor, Elliott Bay, is part of Puget Sound, which makes the city an oceanic port. To the west, beyond Puget Sound, are the Kitsap Peninsula and Olympic Mountains on the Olympic Peninsula; to the east, beyond Lake Washington and the Eastside suburbs, are Lake Sammamish and the Cascade Range. Lake Washington's waters flow to Puget Sound through the Lake Washington Ship Canal (consisting of two man-made canals, Lake Union, and the Hiram M. Chittenden Locks at Salmon Bay, ending in Shilshole Bay on Puget Sound).

The sea, rivers, forests, lakes, and fields surrounding Seattle were once rich enough to support one of the world's few sedentary hunter-gatherer societies. The surrounding area lends itself well to sailing, skiing, bicycling, camping, and hiking year-round.
The city itself is hilly, though not uniformly so. Like Rome, the city is said to lie on seven hills; the lists vary but typically include Capitol Hill, First Hill, West Seattle, Beacon Hill, Queen Anne, Magnolia, and the former Denny Hill. The Wallingford, Mount Baker, and Crown Hill neighborhoods are technically located on hills as well. Many of the hilliest areas are near the city center, with Capitol Hill, First Hill, and Beacon Hill collectively constituting something of a ridge along an isthmus between Elliott Bay and Lake Washington. The break in the ridge between First Hill and Beacon Hill is man-made, the result of two of the many regrading projects that reshaped the topography of the city center. The topography of the city center was also changed by the construction of a seawall and the artificial Harbor Island (completed 1909) at the mouth of the city's industrial Duwamish Waterway, the terminus of the Green River. The highest point within city limits is at High Point in West Seattle, which is roughly located near 35th Ave SW and SW Myrtle St. Other notable hills include Crown Hill, View Ridge/Wedgwood/Bryant, Maple Leaf, Phinney Ridge, Mt. Baker Ridge, and Highlands/Carkeek/Bitterlake.

North of the city center, Lake Washington Ship Canal connects Puget Sound to Lake Washington. It incorporates four natural bodies of water: Lake Union, Salmon Bay, Portage Bay, and Union Bay.
Due to its location in the Pacific Ring of Fire, Seattle is in a major earthquake zone. On February 28, 2001, the magnitude 6.8 Nisqually earthquake did significant architectural damage, especially in the Pioneer Square area (built on reclaimed land, as are the Industrial District and part of the city center), but caused only one fatality. Other strong quakes occurred on January 26, 1700 (estimated at 9 magnitude), December 14, 1872 (7.3 or 7.4), April 13, 1949 (7.1), and April 29, 1965 (6.5). The 1965 quake caused three deaths in Seattle directly and one more by heart failure. Although the Seattle Fault passes just south of the city center, neither it nor the Cascadia subduction zone has caused an earthquake since the city's founding. The Cascadia subduction zone poses the threat of an earthquake of magnitude 9.0 or greater, capable of seriously damaging the city and collapsing many buildings, especially in zones built on fill.
According to the United States Census Bureau, the city has a total area of 142.5 square miles (369 km2), 83.9 square miles (217 km2) of which is land and 58.7 square miles (152 km2), water (41.16% of the total area).




Seattle's climate is classified as oceanic or temperate marine, with cool, wet winters and mild, relatively dry summers.  The city and environs are part of USDA hardiness zone 8b, with isolated coastal pockets falling under 9a.
Temperature extremes are moderated by the adjacent Puget Sound, greater Pacific Ocean, and Lake Washington. Thus extreme heat waves are rare in the Seattle area, as are very cold temperatures (below about 15 F). The Seattle area is the cloudiest region of the United States, due in part to frequent storms and lows moving in from the adjacent Pacific Ocean. Despite having a reputation for frequent rain, Seattle receives less precipitation than many other US cities like Chicago or New York City. However, unlike many other US cities, Seattle has many more "rain days", when a very light drizzle falls from the sky for many days. In an average year, at least 0.01 inches (0.25 mm) of precipitation falls on 150 days, more than nearly all U.S. cities east of the Rocky Mountains. It is cloudy 201 days out of the year and partly cloudy 93 days. Official weather and climatic data is collected at Seattle–Tacoma International Airport, located about 19 km (12 mi) south of downtown in the city of SeaTac, which is at a higher elevation, and records more cloudy days and fewer partly cloudy days per year.
Hot temperature extremes are enhanced by dry, compressed wind from the west slopes of the Cascades, while cold temperatures are generated mainly from the Fraser Valley in British Columbia.
From 1981 to 2010, the average annual precipitation measured at Seattle–Tacoma International Airport was 37.49 inches (952 mm). Annual precipitation has ranged from 23.78 in (604 mm) in 1952 to 55.14 in (1,401 mm) in 1950; for water year (October 1 – September 30) precipitation, the range is 23.16 in (588 mm) in 1976–77 to 51.82 in (1,316 mm) in 1996–97. Due to local variations in microclimate, Seattle also receives significantly lower precipitation than some other locations west of the Cascades. Around 80 mi (129 km) to the west, the Hoh Rain Forest in Olympic National Park on the western flank of the Olympic Mountains receives an annual average precipitation of 142 in (3.61 m). Sixty miles (95 km) to the south of Seattle, the state capital Olympia, which is out of the Olympic Mountains' rain shadow, receives an annual average precipitation of 50 in (1,270 mm). The city of Bremerton, about 15 mi (24 km) west of downtown Seattle on the other side of the Puget Sound, receives 56.4 in (1,430 mm) of precipitation annually.
Conversely, the northeastern portion of the Olympic Peninsula, which lies east of the Olympic Mountains is located within the Olympic rain shadow and receives significantly less precipitation than its surrounding areas. Prevailing airflow from the west is forced to cool and compress when colliding with the mountain range, resulting in high levels of precipitation within the mountains and its western slopes. Once the airflow reaches the leeward side of the mountains it then lowers and expands resulting in warmer, and significantly dryer air. Sequim, Washington, nicknamed "Sunny Sequim", is located approximately 40 miles northwest of downtown Seattle and receives just 16.51" of annual precipitation, more comparable to that of Los Angeles. Oftentimes an area devoid of cloud cover can be seen extending out over the Puget Sound to the north and east of Sequim. On average Sequim observes 127 sunny days per year in addition to 127 days with partial cloud cover. Other areas influenced by the Olympic rain shadow include Port Angeles, Port Townsend, extending as far north as Victoria, British Columbia.
In November, Seattle averages more rainfall than any other U.S. city of more than 250,000 people; it also ranks highly in winter precipitation. Conversely, the city receives some of the lowest precipitation amounts of any large city from June to September. Seattle is one of the five rainiest major U.S. cities as measured by the number of days with precipitation, and it receives some of the lowest amounts of annual sunshine among major cities in the lower 48 states, along with some cities in the Northeast, Ohio and Michigan. Thunderstorms are rare, as the city reports thunder on just seven days per year. By comparison, Fort Myers, Florida, reports thunder on 93 days per year, Kansas City on 52, and New York City on 25.
Seattle experiences its heaviest rainfall during the months of November, December and January, receiving roughly half of its annual rainfall (by volume) during this period. In late fall and early winter, atmospheric rivers (also known as "Pineapple Express" systems), strong frontal systems, and Pacific low pressure systems are common. Light rain & drizzle are the predominant forms of precipitation during the remainder of the year; for instance, on average, less than 1.6 in (41 mm) of rain falls in July and August combined when rain is rare. On occasion, Seattle experiences somewhat more significant weather events. One such event occurred on December 2–4, 2007, when sustained hurricane-force winds and widespread heavy rainfall associated with a strong Pineapple Express event occurred in the greater Puget Sound area and the western parts of Washington and Oregon. Precipitation totals exceeded 13.8 in (350 mm) in some areas with winds topping out at 209 km/h (130 mph) along coastal Oregon. It became the second wettest event in Seattle history when a little over 130 mm (5.1 in) of rain fell on Seattle in a 24-hour period. Lack of adaptation to the heavy rain contributed to five deaths and widespread flooding and damage.
Autumn, winter, and early spring are frequently characterized by rain. Winters are cool and wet with December, the coolest month, averaging 40.6 °F (4.8 °C), with 28 annual days with lows that reach the freezing mark, and 2.0 days where the temperature stays at or below freezing all day; the temperature rarely lowers to 20 °F (−7 °C). Summers are sunny, dry and warm, with August, the warmest month, with high temperatures averaging 76.1 °F (24.5 °C), and reaching 90 °F (32 °C) on 3.1 days per year. In 2015 the city recorded 13 days over 90 °F. The hottest officially recorded temperature was 103 °F (39 °C) on July 29, 2009; the coldest recorded temperature was 0 °F (−18 °C) on January 31, 1950; the record cold daily maximum is 16 °F (−9 °C) on January 14, 1950, while, conversely, the record warm daily minimum is 71 °F (22 °C) the day the official record high was set. The average window for freezing temperatures is November 16 through March 10, allowing a growing season of 250 days.
Seattle typically receives some snowfall on an annual basis but heavy snow is rare. Average annual snowfall, as measured at Sea-Tac Airport, is 6.8 inches (17.3 cm). Single calendar-day snowfall of six inches (15 cm) or greater has occurred on only 15 days since 1948, and only once since February 17, 1990, when 6.8 in (17.3 cm) of snow officially fell at Sea-Tac airport on January 18, 2012. This moderate snow event was officially the 12th snowiest calendar day at the airport since 1948 and snowiest since November 1985. Much of the city of Seattle proper received somewhat lesser snowfall accumulations. Locations to the south of Seattle received more, with Olympia and Chehalis receiving 14 to 18 in (36 to 46 cm). Another moderate snow event occurred from December 12–25, 2008, when over one foot (30 cm) of snow fell and stuck on much of the roads over those two weeks, when temperatures remained below 32 °F (0 °C), causing widespread difficulties in a city not equipped for clearing snow. The largest documented snowstorm occurred from January 5–9, 1880, with snow drifting to 6 feet (1.8 m) in places at the end of the snow event. From January 31 to February 2, 1916, another heavy snow event occurred with 29 in (74 cm) of snow on the ground by the time the event was over. With official records dating to 1948, the largest single-day snowfall is 20.0 in (51 cm) on January 13, 1950. Seasonal snowfall has ranged from zero in 1991–92 to 67.5 in (171 cm) in 1968–69, with trace amounts having occurred as recently as 2009–10. The month of January 1950 was particularly severe, bringing 57.2 in (145 cm) of snow, the most of any month along with the aforementioned record cold.
The Puget Sound Convergence Zone is an important feature of Seattle's weather. In the convergence zone, air arriving from the north meets air flowing in from the south. Both streams of air originate over the Pacific Ocean; airflow is split by the Olympic Mountains to Seattle's west, then reunited to the east. When the air currents meet, they are forced upward, resulting in convection. Thunderstorms caused by this activity are usually weak and can occur north and south of town, but Seattle itself rarely receives more than occasional thunder and small hail showers. The Hanukkah Eve Wind Storm in December 2006 is an exception that brought heavy rain and winds gusting up to 69 mph (111 km/h), an event that was not caused by the Puget Sound Convergence Zone and was widespread across the Pacific Northwest.
One of many exceptions to Seattle's reputation as a damp location occurs in El Niño years, when marine weather systems track as far south as California and little precipitation falls in the Puget Sound area. Since the region's water comes from mountain snow packs during the dry summer months, El Niño winters can not only produce substandard skiing but can result in water rationing and a shortage of hydroelectric power the following summer.




According to the 2010 United States Census, Seattle had a population of 608,660 with a racial and ethnic composition as follows:
White: 69.5% (Non-Hispanic Whites: 66.3%)
Asian: 13.8% (4.1% Chinese, 2.6% Filipino, 2.2% Vietnamese, 1.3% Japanese, 1.1% Korean, 0.8% Indian, 0.3% Cambodian, 0.3% Laotian, 0.2% Pakistanis, 0.2% Indonesian, 0.2% Thai)
Black or African American: 7.9%
Hispanic or Latino (of any race): 6.6% (4.1% Mexican, 0.3% Puerto Rican, 0.2% Guatemalan, 0.2% Salvadoran, 0.2% Cuban)
American Indian and Alaska Native: 0.8%
Native Hawaiian and Other Pacific Islander: 0.4%
Other race: 2.4%
Two or more races: 5.1%
Seattle's population historically has been predominantly white. The 2010 census showed that Seattle was one of the whitest big cities in the country, although its proportion of white residents has been gradually declining. In 1960, whites comprised 91.6% of the city's population, while in 2010 they comprised 69.5%. According to the 2006–2008 American Community Survey, approximately 78.9% of residents over the age of five spoke only English at home. Those who spoke Asian languages other than Indo-European languages made up 10.2% of the population, Spanish was spoken by 4.5% of the population, speakers of other Indo-European languages made up 3.9%, and speakers of other languages made up 2.5%.
Seattle's foreign-born population grew 40% between the 1990 and 2000 censuses. The Chinese population in the Seattle area has origins in mainland China, Hong Kong, Southeast Asia, and Taiwan. The earliest Chinese-Americans that came in the late 19th and early 20th centuries were almost entirely from Guangdong Province. The Seattle area is also home to a large Vietnamese population of more than 55,000 residents, as well as over 30,000 Somali immigrants. The Seattle-Tacoma area is also home to one of the largest Cambodian communities in the United States, numbering about 19,000 Cambodian Americans, and one of the largest Samoan communities in the mainland U.S., with over 15,000 people having Samoan ancestry. Additionally, the Seattle area had the highest percentage of self-identified mixed-race people of any large metropolitan area in the United States, according to the 2000 United States Census Bureau. According to a 2012 HistoryLink study, Seattle's 98118 ZIP code (in the Columbia City neighborhood) was one of the most diverse ZIP Code Tabulation Areas in the United States.
In 1999, the median income of a city household was $45,736, and the median income for a family was $62,195. Males had a median income of $40,929 versus $35,134 for females. The per capita income for the city was $30,306. 11.8% of the population and 6.9% of families are below the poverty line. Of people living in poverty, 13.8% are under the age of 18 and 10.2% are 65 or older.
It is estimated that King County has 8,000 homeless people on any given night, and many of those live in Seattle. In September 2005, King County adopted a "Ten-Year Plan to End Homelessness", one of the near-term results of which is a shift of funding from homeless shelter beds to permanent housing.
In recent years, the city has experienced steady population growth, and has been faced with the issue of accommodating more residents. In 2006, after growing by 4,000 citizens per year for the previous 16 years, regional planners expected the population of Seattle to grow by 200,000 people by 2040. However, former mayor Greg Nickels supported plans that would increase the population by 60%, or 350,000 people, by 2040 and worked on ways to accommodate this growth while keeping Seattle's single-family housing zoning laws. The Seattle City Council later voted to relax height limits on buildings in the greater part of Downtown, partly with the aim to increase residential density in the city center. As a sign of increasing inner-city growth, the downtown population crested to over 60,000 in 2009, up 77% since 1990.
Seattle also has large lesbian, gay, bisexual, and transgender populations. According to a 2006 study by UCLA, 12.9% of city residents polled identified as gay, lesbian, or bisexual. This was the second-highest proportion of any major U.S. city, behind San Francisco Greater Seattle also ranked second among major U.S. metropolitan areas, with 6.5% of the population identifying as gay, lesbian, or bisexual. According to 2012 estimates from the United States Census Bureau, Seattle has the highest percentage of same-sex households in the United States, at 2.6 per cent, surpassing San Francisco.
In addition, Seattle has a relatively high number of people living alone. According to the 2000 U.S. Census interim measurements of 2004, Seattle has the fifth highest proportion of single-person households nationwide among cities of 100,000 or more residents, at 40.8%.




Seattle's economy is driven by a mix of older industrial companies, and "new economy" Internet and technology companies, service, design and clean technology companies. The city's gross metropolitan product was $231 billion in 2010, making it the 11th largest metropolitan economy in the United States. The Port of Seattle, which also operates Seattle–Tacoma International Airport, is a major gateway for trade with Asia and cruises to Alaska, and is the 8th largest port in the United States in terms of container capacity. Though it was affected by the Great Recession, Seattle has retained a comparatively strong economy, and remains a hotbed for start-up businesses, especially in green building and clean technologies: it was ranked as America's No. 1 "smarter city" based on its government policies and green economy. In February 2010, the city government committed Seattle to becoming North America's first "climate neutral" city, with a goal of reaching zero net per capita greenhouse gas emissions by 2030.

Still, very large companies dominate the business landscape. Four companies on the 2013 Fortune 500 list of the United States' largest companies, based on total revenue, are headquartered in Seattle: Internet retailer Amazon.com (#49), coffee chain Starbucks (#208), department store Nordstrom (#227), and freight forwarder Expeditors International of Washington (#428). Other Fortune 500 companies popularly associated with Seattle are based in nearby Puget Sound cities. Warehouse club chain Costco (#22), the largest retail company in Washington, is based in Issaquah. Microsoft (#35) is located in Redmond. Weyerhaeuser, the forest products company (#363), is based in Federal Way. Finally, Bellevue is home to truck manufacturer Paccar (#168). Other major companies in the area include Nintendo of America in Redmond, T-Mobile US in Bellevue, Expedia Inc. in Bellevue and Providence Health & Services — the state's largest health care system and fifth largest employer — in Renton. The city has a reputation for heavy coffee consumption; coffee companies founded or based in Seattle include Starbucks, Seattle's Best Coffee, and Tully's. There are also many successful independent artisanal espresso roasters and cafés.
Prior to moving its headquarters to Chicago, aerospace manufacturer Boeing (#30) was the largest company based in Seattle. Its largest division is still headquartered in nearby Renton, and the company has large aircraft manufacturing plants in Everett and Renton, so it remains the largest private employer in the Seattle metropolitan area. Former Seattle Mayor Greg Nickels announced a desire to spark a new economic boom driven by the biotechnology industry in 2006. Major redevelopment of the South Lake Union neighborhood is underway, in an effort to attract new and established biotech companies to the city, joining biotech companies Corixa (acquired by GlaxoSmithKline), Immunex (now part of Amgen), Trubion, and ZymoGenetics. Vulcan Inc., the holding company of billionaire Paul Allen, is behind most of the development projects in the region. While some see the new development as an economic boon, others have criticized Nickels and the Seattle City Council for pandering to Allen's interests at taxpayers' expense. Also in 2006, Expansion Magazine ranked Seattle among the top 10 metropolitan areas in the nation for climates favorable to business expansion. In 2005, Forbes ranked Seattle as the most expensive American city for buying a house based on the local income levels. In 2013, however, the magazine ranked Seattle No. 9 on its list of the Best Places for Business and Careers.
Alaska Airlines, operating a hub at Seattle–Tacoma International Airport, maintains its headquarters in the city of SeaTac, next to the airport.
Seattle is a hub for global health with the headquarters of the Bill & Melinda Gates Foundation, PATH, Infectious Disease Research Institute, Fred Hutchinson Cancer Research Center and the Institute for Health Metrics and Evaluation. In 2015, the Washington Global Health Alliance counted 168 global health organizations in Washington state, many are headquartered in Seattle.






From 1869 until 1982, Seattle was known as the "Queen City". Seattle's current official nickname is the "Emerald City", the result of a contest held in 1981; the reference is to the lush evergreen forests of the area. Seattle is also referred to informally as the "Gateway to Alaska" for being the nearest major city in the contiguous US to Alaska, "Rain City" for its frequent cloudy and rainy weather, and "Jet City" from the local influence of Boeing. The city has two official slogans or mottos: "The City of Flowers", meant to encourage the planting of flowers to beautify the city, and "The City of Goodwill", adopted prior to the 1990 Goodwill Games. Seattle residents are known as Seattleites.




Seattle has been a regional center for the performing arts for many years. The century-old Seattle Symphony Orchestra is among the world's most recorded and performs primarily at Benaroya Hall. The Seattle Opera and Pacific Northwest Ballet, which perform at McCaw Hall (opened 2003 on the site of the former Seattle Opera House at Seattle Center), are comparably distinguished, with the Opera being particularly known for its performances of the works of Richard Wagner and the PNB School (founded in 1974) ranking as one of the top three ballet training institutions in the United States. The Seattle Youth Symphony Orchestras (SYSO) is the largest symphonic youth organization in the United States. The city also boasts lauded summer and winter chamber music festivals organized by the Seattle Chamber Music Society.
The 5th Avenue Theatre, built in 1926, stages Broadway-style musical shows featuring both local talent and international stars. Seattle has "around 100" theatrical production companies and over two dozen live theatre venues, many of them associated with fringe theatre; Seattle is probably second only to New York for number of equity theaters (28 Seattle theater companies have some sort of Actors' Equity contract). In addition, the 900-seat Romanesque Revival Town Hall on First Hill hosts numerous cultural events, especially lectures and recitals.

Between 1918 and 1951, there were nearly two dozen jazz nightclubs along Jackson Street, running from the current Chinatown/International District to the Central District. The jazz scene developed the early careers of Ray Charles, Quincy Jones, Bumps Blackwell, Ernestine Anderson, and others.
Early popular musical acts from the Seattle/Puget Sound area include the collegiate folk group The Brothers Four, vocal group The Fleetwoods, 1960s garage rockers The Wailers and The Sonics, and instrumental surf group The Ventures, some of whom are still active.
Seattle is considered the home of grunge music, having produced artists such as Nirvana, Soundgarden, Alice in Chains, Pearl Jam, and Mudhoney, all of whom reached international audiences in the early 1990s. The city is also home to such varied artists as avant-garde jazz musicians Bill Frisell and Wayne Horvitz, hot jazz musician Glenn Crytzer, hip hop artists Sir Mix-a-Lot, Macklemore, Blue Scholars, and Shabazz Palaces, smooth jazz saxophonist Kenny G, classic rock staples Heart and Queensrÿche, and alternative rock bands such as Foo Fighters, Harvey Danger, The Presidents of the United States of America, The Posies, Modest Mouse, Band of Horses, Death Cab for Cutie, and Fleet Foxes. Rock musicians such as Jimi Hendrix, Duff McKagan, and Nikki Sixx spent their formative years in Seattle.
The Seattle-based Sub Pop record company continues to be one of the world's best-known independent/alternative music labels.
Over the years, a number of songs have been written about Seattle.
Seattle annually sends a team of spoken word slammers to the National Poetry Slam and considers itself home to such performance poets as Buddy Wakefield, two-time Individual World Poetry Slam Champ; Anis Mojgani, two-time National Poetry Slam Champ; and Danny Sherrard, 2007 National Poetry Slam Champ and 2008 Individual World Poetry Slam Champ. Seattle also hosted the 2001 national Poetry Slam Tournament. The Seattle Poetry Festival is a biennial poetry festival that (launched first as the Poetry Circus in 1997) has featured local, regional, national, and international names in poetry.
The city also has movie houses showing both Hollywood productions and works by independent filmmakers. Among these, the Seattle Cinerama stands out as one of only three movie theaters in the world still capable of showing three-panel Cinerama films.




Among Seattle's prominent annual fairs and festivals are the 24-day Seattle International Film Festival, Northwest Folklife over the Memorial Day weekend, numerous Seafair events throughout July and August (ranging from a Bon Odori celebration to the Seafair Cup hydroplane races), the Bite of Seattle, one of the largest Gay Pride festivals in the United States, and the art and music festival Bumbershoot, which programs music as well as other art and entertainment over the Labor Day weekend. All are typically attended by 100,000 people annually, as are the Seattle Hempfest and two separate Independence Day celebrations.
Other significant events include numerous Native American pow-wows, a Greek Festival hosted by St. Demetrios Greek Orthodox Church in Montlake, and numerous ethnic festivals (many associated with Festál at Seattle Center).
There are other annual events, ranging from the Seattle Antiquarian Book Fair & Book Arts Show; an anime convention, Sakura-Con; Penny Arcade Expo, a gaming convention; a two-day, 9,000-rider Seattle to Portland Bicycle Classic; and specialized film festivals, such as the Maelstrom International Fantastic Film Festival, the Seattle Asian American Film Festival (formerly known as the Northwest Asian American Film Festival), Children's Film Festival Seattle, Translation: the Seattle Transgender Film Festival, the Seattle Gay and Lesbian Film Festival, Seattle Latino Film Festival, and the Seattle Polish Film Festival.
The Henry Art Gallery opened in 1927, the first public art museum in Washington. The Seattle Art Museum (SAM) opened in 1933; SAM opened a museum downtown in 1991 (expanded and reopened 2007); since 1991, the 1933 building has been SAM's Seattle Asian Art Museum (SAAM). SAM also operates the Olympic Sculpture Park (opened 2007) on the waterfront north of the downtown piers. The Frye Art Museum is a free museum on First Hill.
Regional history collections are at the Loghouse Museum in Alki, Klondike Gold Rush National Historical Park, the Museum of History and Industry, and the Burke Museum of Natural History and Culture. Industry collections are at the Center for Wooden Boats and the adjacent Northwest Seaport, the Seattle Metropolitan Police Museum, and the Museum of Flight. Regional ethnic collections include the Nordic Heritage Museum, the Wing Luke Asian Museum, and the Northwest African American Museum. Seattle has artist-run galleries, including ten-year veteran Soil Art Gallery, and the newer Crawl Space Gallery.

The Seattle Great Wheel, one of the largest Ferris wheels in the US, opened in June 2012 as a new, permanent attraction on the city's waterfront, at Pier 57, next to Downtown Seattle. The city also has many community centers for recreation, including Rainier Beach, Van Asselt, Rainier, and Jefferson south of the Ship Canal and Green Lake, Laurelhurst, Loyal Heights north of the Canal, and Meadowbrook.
Woodland Park Zoo opened as a private menagerie in 1889 but was sold to the city in 1899. The Seattle Aquarium has been open on the downtown waterfront since 1977 (undergoing a renovation 2006). The Seattle Underground Tour is an exhibit of places that existed before the Great Fire.
Since the middle 1990s, Seattle has experienced significant growth in the cruise industry, especially as a departure point for Alaska cruises. In 2008, a record total of 886,039 cruise passengers passed through the city, surpassing the number for Vancouver, BC, the other major departure point for Alaska cruises.




Seattle has three major men's professional sports teams: the National Football League (NFL)'s Seattle Seahawks, Major League Baseball (MLB)'s Seattle Mariners, and Major League Soccer (MLS)'s Seattle Sounders FC. Other professional sports teams include the Women's National Basketball Association (WNBA)'s Seattle Storm, who won the WNBA championship in 2004 and 2010, and the Seattle Reign of the National Women's Soccer League.
The Seahawks' CenturyLink Field has hosted NFL playoff games in 2006, 2008, 2011, 2014, 2015, and 2017. The Seahawks have advanced to the Super Bowl three times: 2005, 2013 and 2014. They defeated the Denver Broncos 43–8 to win their first Super Bowl championship in Super Bowl XLVIII, but lost 24–28 against the New England Patriots in Super Bowl XLIX. The Seahawks also held the NFL playoffs at the Kingdome in 1983, 1984 and 2000. The 2000 playoff game was the last game of football of any type and of any sport at The Kingdome.
Seattle Sounders FC has played in Major League Soccer since 2009, sharing CenturyLink Field with the Seahawks, as a continuation of earlier teams in the lower divisions of American soccer. The Sounders have won the MLS Supporters' Shield in 2014 and the Lamar Hunt U.S. Open Cup on four occasions: 2009, 2010, 2011, and 2014. The Sounders won their first MLS Cup after defeating Toronto FC, 5-4 in penalty kicks, in MLS Cup 2016. With the Sounders' first MLS Cup championship in franchise history, the Mariners are currently the only men's professional sports team in the city without a championship, let alone a championship series appearance.

Seattle's professional sports history began at the start of the 20th century with the PCHA's Seattle Metropolitans, which in 1917 became the first American hockey team to win the Stanley Cup. Seattle was also home to a previous Major League Baseball franchise in 1969: the Seattle Pilots. The Pilots relocated to Milwaukee, Wisconsin and became the Milwaukee Brewers for the 1970 season. From 1967 to 2008 Seattle was also home to an National Basketball Association (NBA) franchise: the Seattle SuperSonics, who were the 1978–79 NBA champions. The SuperSonics relocated to Oklahoma City and became the Oklahoma City Thunder for the 2008–09 season.
The Major League Baseball All-Star Game was held in Seattle twice, first at the Kingdome in 1979 and again at Safeco Field in 2001. That same year, the Seattle Mariners tied the all-time single regular season wins record with 116 wins. The NBA All-Star Game was also held in Seattle twice: the first in 1974 at the Seattle Center Coliseum and the second in 1987 at the Kingdome.
The Seattle Thunderbirds hockey team plays in the Canadian major-junior Western Hockey League and are based in the Seattle suburb of Kent. Seattle also boasts a strong history in collegiate sports. The University of Washington and Seattle University are NCAA Division I schools. The University of Washington's athletic program, nicknamed the Huskies, competes in the Pac-12 Conference, and Seattle University's athletic program, nicknamed the Redhawks, competes in the Western Athletic Conference.




Seattle's mild, temperate, marine climate allows year-round outdoor recreation, including walking, cycling, hiking, skiing, snowboarding, kayaking, rock climbing, motor boating, sailing, team sports, and swimming.
In town, many people walk around Green Lake, through the forests and along the bluffs and beaches of 535-acre (2.2 km2) Discovery Park (the largest park in the city) in Magnolia, along the shores of Myrtle Edwards Park on the Downtown waterfront, along the shoreline of Lake Washington at Seward Park, along Alki Beach in West Seattle, or along the Burke-Gilman Trail.

Gas Works Park features the majestic preserved superstructure of a coal gasification plant closed in 1956. Located across Lake Union from downtown, the park provides panoramic views of the Seattle skyline.
Also popular are hikes and skiing in the nearby Cascade or Olympic Mountains and kayaking and sailing in the waters of Puget Sound, the Strait of Juan de Fuca, and the Strait of Georgia. In 2005, Men's Fitness magazine named Seattle the fittest city in the United States.
In its 2013 ParkScore ranking, the Trust for Public Land reported that Seattle had the tenth best park system among the 50 most populous US cities. ParkScore ranks city park systems by a formula that analyzes acreage, access, and service and investment.




Seattle is a charter city, with a mayor–council form of government. From 1911 to 2013, Seattle's nine city councillors were elected at large, rather than by geographic subdivisions. For the 2015 election, this changed to a hybrid system of seven district members and two at-large members as a result of a ballot measure passed on November 5, 2013. The only other elected offices are the city attorney and Municipal Court judges. All city offices are officially non-partisan.
Like some other parts of the United States, government and laws are also run by a series of ballot initiatives (allowing citizens to pass or reject laws), referenda (allowing citizens to approve or reject legislation already passed), and propositions (allowing specific government agencies to propose new laws or tax increases directly to the people). Federally, Seattle is part of Washington's 7th congressional district, represented by Democrat Jim McDermott, elected in 1988 and one of Congress's liberal members. Ed Murray is currently serving as mayor.
Seattle's political culture is very liberal and progressive for the United States, with over 80% of the population voting for the Democratic Party. All precincts in Seattle voted for Democratic Party candidate Barack Obama in the 2012 presidential election. In partisan elections for the Washington State Legislature and United States Congress, nearly all elections are won by Democrats. Seattle is considered the first major American city to elect a female mayor, Bertha Knight Landes. It has also elected an openly gay mayor, Ed Murray, and a socialist councillor, Kshama Sawant. For the first time in United States history, an openly gay black woman was elected to public office when Sherry Harris was elected as a Seattle city councillor in 1991. The majority of the current city council is female, while white men comprise a minority.
Seattle is widely considered one of the most liberal cities in the United States, even surpassing its neighbor, Portland, Oregon. Support for issues such as same-sex marriage and reproductive rights are largely taken for granted in local politics. In the 2012 U.S. general election, an overwhelming majority of Seattleites voted to approve Referendum 74 and legalize gay marriage in Washington state. In the same election, an overwhelming majority of Seattleites also voted to approve the legalization of the recreational use of cannabis in the state. Like much of the Pacific Northwest (which has the lowest rate of church attendance in the United States and consistently reports the highest percentage of atheism), church attendance, religious belief, and political influence of religious leaders are much lower than in other parts of America.
Seattle also has a thriving alternative press, with the Web-based daily Seattle Post-Intelligencer, several other online dailies (including Publicola and Crosscut), The Stranger (an alternative, left-leaning weekly), Seattle Weekly, and a number of issue-focused publications, including the nation's two largest online environmental magazines, Worldchanging and Grist.org.
In July 2012, Seattle banned plastic shopping bags. In June 2014 the city passed a local ordinance to increase the minimum wage to $15 an hour on a staged basis from 2015 to 2021. When fully implemented the $15 hourly rate will be the highest minimum wage in the nation.
On October 6, 2014, Seattle officially replaced Columbus Day with Indigenous Peoples' Day, honoring Seattle's Native American community and controversies surrounding the legacy of Christopher Columbus.




Of the city's population over the age of 25, 53.8% (vs. a national average of 27.4%) hold a bachelor's degree or higher, and 91.9% (vs. 84.5% nationally) have a high school diploma or equivalent. A 2008 United States Census Bureau survey showed that Seattle had the highest percentage of college and university graduates of any major U.S. city. The city was listed as the most literate of the country's 69 largest cities in 2005 and 2006, the second most literate in 2007 and the most literate in 2008 in studies conducted by Central Connecticut State University.

Seattle Public Schools desegregated without a court order but continue to struggle to achieve racial balance in a somewhat ethnically divided city (the south part of town having more ethnic minorities than the north). In 2007, Seattle's racial tie-breaking system was struck down by the United States Supreme Court, but the ruling left the door open for desegregation formulae based on other indicators (e.g., income or socioeconomic class).
The public school system is supplemented by a moderate number of private schools: five of the private high schools are Catholic, one is Lutheran, and six are secular.
Seattle is home to the University of Washington, as well as the institution's professional and continuing education unit, the University of Washington Educational Outreach. A study by Newsweek International in 2006 cited the University of Washington as the twenty-second best university in the world. Seattle also has a number of smaller private universities including Seattle University and Seattle Pacific University, the former a Jesuit Catholic institution, the latter Free Methodist; universities aimed at the working adult, like City University and Antioch University; colleges within the Seattle Colleges District system, comprising North, Central, and South; seminaries, including Western Seminary and a number of arts colleges, such as Cornish College of the Arts, Pratt Fine Arts Center, and The Art Institute of Seattle. In 2001, Time magazine selected Seattle Central Community College as community college of the year, stating the school "pushes diverse students to work together in small teams".




As of 2010, Seattle has one major daily newspaper, The Seattle Times. The Seattle Post-Intelligencer, known as the P-I, published a daily newspaper from 1863 to March 17, 2009, before switching to a strictly on-line publication. There is also the Seattle Daily Journal of Commerce, and the University of Washington publishes The Daily, a student-run publication, when school is in session. The most prominent weeklies are the Seattle Weekly and The Stranger; both consider themselves "alternative" papers. The weekly LGBT newspaper is the Seattle Gay News. Real Change is a weekly street newspaper that is sold mainly by homeless persons as an alternative to panhandling. There are also several ethnic newspapers, including The Facts, Northwest Asian Weekly and the International Examiner, and numerous neighborhood newspapers.
Seattle is also well served by television and radio, with all major U.S. networks represented, along with at least five other English-language stations and two Spanish-language stations. Seattle cable viewers also receive CBUT 2 (CBC) from Vancouver, British Columbia.
Non-commercial radio stations include NPR affiliates KUOW-FM 94.9 and KNKX 88.5 (Tacoma), as well as classical music station KING-FM 98.1. Other non-commercial stations include KEXP-FM 90.3 (affiliated with the UW), community radio KBCS-FM 91.3 (affiliated with Bellevue College), and high school radio KNHC-FM 89.5, which broadcasts an electronic dance music radio format and is owned by the public school system and operated by students of Nathan Hale High School. Many Seattle radio stations are also available through Internet radio, with KEXP in particular being a pioneer of Internet radio. Seattle also has numerous commercial radio stations. In a March 2012 report by the consumer research firm Arbitron, the top FM stations were KRWM (adult contemporary format), KIRO-FM (news/talk), and KISW (active rock) while the top AM stations were KOMO (AM) (all news), KJR (AM) (all sports), KIRO (AM) (all sports).
Seattle-based online magazines Worldchanging and Grist.org were two of the "Top Green Websites" in 2007 according to TIME.
Seattle also has many online news media websites. The two largest are The Seattle Times and the Seattle Post-Intelligencer.







The University of Washington is consistently ranked among the country's top leading institutions in medical research, earning special merits for programs in neurology and neurosurgery. Seattle has seen local developments of modern paramedic services with the establishment of Medic One in 1970. In 1974, a 60 Minutes story on the success of the then four-year-old Medic One paramedic system called Seattle "the best place in the world to have a heart attack".
Three of Seattle's largest medical centers are located on First Hill. Harborview Medical Center, the public county hospital, is the only Level I trauma hospital in a region that includes Washington, Alaska, Montana, and Idaho. Virginia Mason Medical Center and Swedish Medical Center's two largest campuses are also located in this part of Seattle, including the Virginia Mason Hospital. This concentration of hospitals resulted in the neighborhood's nickname "Pill Hill".
Located in the Laurelhurst neighborhood, Seattle Children's, formerly Children's Hospital and Regional Medical Center, is the pediatric referral center for Washington, Alaska, Montana, and Idaho. The Fred Hutchinson Cancer Research Center has a campus in the Eastlake neighborhood. The University District is home to the University of Washington Medical Center which, along with Harborview, is operated by the University of Washington. Seattle is also served by a Veterans Affairs hospital on Beacon Hill, a third campus of Swedish in Ballard, and Northwest Hospital and Medical Center near Northgate Mall.




The first streetcars appeared in 1889 and were instrumental in the creation of a relatively well-defined downtown and strong neighborhoods at the end of their lines. The advent of the automobile sounded the death knell for rail in Seattle. Tacoma–Seattle railway service ended in 1929 and the Everett–Seattle service came to an end in 1939, replaced by inexpensive automobiles running on the recently developed highway system. Rails on city streets were paved over or removed, and the opening of the Seattle trolleybus system brought the end of streetcars in Seattle in 1941. This left an extensive network of privately owned buses (later public) as the only mass transit within the city and throughout the region.

King County Metro provides frequent stop bus service within the city and surrounding county, as well as a South Lake Union Streetcar line between the South Lake Union neighborhood and Westlake Center in downtown. Seattle is one of the few cities in North America whose bus fleet includes electric trolleybuses. Sound Transit currently provides an express bus service within the metropolitan area, two Sounder commuter rail lines between the suburbs and downtown, and its Central Link light rail line between the University of Washington and Sea-Tac Airport. Washington State Ferries, which manages the largest network of ferries in the United States and third largest in the world, connects Seattle to Bainbridge and Vashon Islands in Puget Sound and to Bremerton and Southworth on the Kitsap Peninsula.

According to the 2007 American Community Survey, 18.6% of Seattle residents used one of the three public transit systems that serve the city, giving it the highest transit ridership of all major cities without heavy or light rail prior to the completion of Sound Transit's Central Link line. The city has also been described by Bert Sperling as the fourth most walkable U.S. city and by Walk Score as the sixth most walkable of the fifty largest U.S. cities.
Seattle–Tacoma International Airport, locally known as Sea-Tac Airport and located just south in the neighboring city of SeaTac, is operated by the Port of Seattle and provides commercial air service to destinations throughout the world. Closer to downtown, Boeing Field is used for general aviation, cargo flights, and testing/delivery of Boeing airliners.

The main mode of transportation, however, relies on Seattle's streets, which are laid out in a cardinal directions grid pattern, except in the central business district where early city leaders Arthur Denny and Carson Boren insisted on orienting their plats relative to the shoreline rather than to true North. Only two roads, Interstate 5 and State Route 99 (both limited-access highways), run uninterrupted through the city from north to south. State Route 99 runs through downtown Seattle on the Alaskan Way Viaduct, which was built in 1953. However, due to damage sustained during the 2001 Nisqually earthquake the viaduct will be replaced by a tunnel. The 2-mile (3.2 km) Alaskan Way Viaduct replacement tunnel was originally scheduled to be completed in December 2015 at a cost of US$4.25 billion. Unfortunately, due to issues with the worlds largest tunnel boring machine (TBM), which is nicknamed "Bertha" and is 57 feet (17 m) in diameter, the projected date of completion has been pushed back to 2017. Seattle has the 8th worst traffic congestion of all American cities, and is 10th among all North American cities.
The city has started moving away from the automobile and towards mass transit. From 2004 to 2009, the annual number of unlinked public transportation trips increased by approximately 21%. In 2006, voters in King County passed proposition 2 (Transit Now) which increased bus service hours on high ridership routes and paid for five bus rapid transit lines called RapidRide. After rejecting a roads and transit measure in 2007, Seattle-area voters passed a transit only measure in 2008 to increase ST Express bus service, extend the Link light rail system, and expand and improve Sounder commuter rail service. A light rail line from downtown heading south to Sea-Tac Airport began service on December 19, 2009, giving the city its first rapid transit line with intermediate stations within the city limits. An extension north to the University of Washington opened on March 19, 2016; and further extensions are planned to reach Lynnwood to the north, Des Moines to the south, and Bellevue and Redmond to the east by 2023. Voters in the Puget Sound region approved an additional tax increase in November, 2016 to expand light rail to West Seattle and Ballard as well as Tacoma, Everett, and Issaquah.




Water and electric power are municipal services, provided by Seattle Public Utilities and Seattle City Light respectively. Other utility companies serving Seattle include Puget Sound Energy (natural gas, electricity); Seattle Steam Company (steam); Waste Management, Inc and CleanScapes, Inc. (curbside recycling and solid waste removal); CenturyLink, Frontier Communications, Wave Broadband, and Comcast (telecommunications and television).
About 90% of Seattle's electricity is produced using hydropower. Less than 2% of electricity is produced using fossil fuels.







Seattle is partnered with:




National Register of Historic Places listings in Seattle, Washington
Seattle Freeze
Seattle process
Seattle tugboats
Tillicum Village












Jones, Nard (1972). Seattle. New York: Doubleday. ISBN 978-0-385-01875-3. 
Morgan, Murray (1982) [1951]. Skid Road: an Informal Portrait of Seattle (revised and updated, first illustrated ed.). Seattle and London: University of Washington Press. ISBN 978-0-295-95846-0. 
Ochsner, Jeffrey Karl, ed. (1998) [1994]. Shaping Seattle Architecture: A Historical Guide to the Architects. Seattle and London: University of Washington Press. ISBN 978-0-295-97366-1. 
Sale, Roger (1976). Seattle: Past to Present. Seattle and London: University of Washington Press. ISBN 978-0-295-95615-2. 
Speidel, William C. (1978). Doc Maynard: The Man Who Invented Seattle. Seattle: Nettle Creek Publishing Company. pp. 196–197, 200. ISBN 978-0-914890-02-7. 
Speidel, William C. (1967). Sons of the profits; or, There's no business like grow business: the Seattle story, 1851–1901. Seattle: Nettle Creek Publishing Company. pp. 196–197, 200. ISBN 0-914890-00-X. 



Klingle, Matthew (2007). Emerald City: An Environmental History of Seattle. New Haven: Yale University Press. ISBN 978-0-300-11641-0. 
MacGibbon, Elma (1904). "Seattle, the city of destiny". Leaves of knowledge (DJVU). Washington State Library's Classics in Washington History collection. Shaw & Borden. OCLC 61326250. 
Pierce, J. Kingston (2003). Eccentric Seattle: Pillars and Pariahs Who Made the City Not Such a Boring Place After All. Pullman, Washington: Washington State University Press. ISBN 978-0-87422-269-2. 
Sanders, Jeffrey Craig. Seattle and the Roots of Urban Sustainability: Inventing Ecotopia (University of Pittsburgh Press; 2010) 288 pages; the rise of environmental activism




Official website of the City of Seattle
Historylink.org, history of Seattle and Washington
Seattle Photographs from the University of Washington Digital Collections
Seattle Historic Photograph Collection from the Seattle Public Library
Seattle Civil Rights and Labor History Project
Seattle, a National Park Service Discover Our Shared Heritage Travel ItineraryChicago (/ʃᵻˈkɑːɡoʊ/ or /ʃᵻˈkɔːɡoʊ/), officially the City of Chicago, is the third-most populous city in the United States, and the fifth-most populous city in North America. With over 2.7 million residents, it is the most populous city in the state of Illinois and the Midwestern United States, and the county seat of Cook County. The Chicago metropolitan area, often referred to as Chicagoland, has nearly 10 million people and is the third-largest in the U.S.
Chicago was incorporated as a city in 1837, near a portage between the Great Lakes and the Mississippi River watershed, and grew rapidly in the mid-nineteenth century. The city is an international hub for finance, commerce, industry, technology, telecommunications, and transportation: O'Hare International Airport is the second-busiest airport in the world when measured by aircraft traffic; the region also has the largest number of U.S. highways and rail road freight. In 2012, Chicago was listed as an alpha global city by the Globalization and World Cities Research Network, and ranked seventh in the world in the 2014 Global Cities Index. Chicago has the third-largest gross metropolitan product in the United States—about $640 billion according to 2015 estimates. The city has one of the world's largest and most diversified economies with no single industry employing more than 14% of the workforce.
In 2016, Chicago hosted over 54 million international and domestic visitors, a new record for the city making it one of the top visited cities in the nation. Chicago's culture includes the visual arts, novels, film, theater, especially improvisational comedy, and music, particularly jazz, blues, soul, gospel and house music. There are many colleges and universities in the Chicago area; among these, Northwestern University, the University of Chicago, and the University of Illinois at Chicago are classified as "highest research" doctoral universities. Chicago also has professional sports teams in each of the major professional leagues. The city has many nicknames, the best-known being the Windy City.







The name "Chicago" is derived from a French rendering of the Native American word shikaakwa, known to botanists as Allium tricoccum, from the Miami-Illinois language. The first known reference to the site of the current city of Chicago as "Checagou" was by Robert de LaSalle around 1679 in a memoir. Henri Joutel, in his journal of 1688, noted that the wild garlic, called "chicagoua", grew abundantly in the area. According to his diary of late September 1687:

when we arrived at the said place called Chicagou which, according to what we were able to learn of it, has taken this name because of the quantity of garlic which grows in the forests in this region.

In the mid-18th century, the area was inhabited by a Native American tribe known as the Potawatomi, who had taken the place of the Miami and Sauk and Fox peoples. The first known non-indigenous permanent settler in Chicago was Jean Baptiste Point du Sable. Du Sable was of African and French descent and arrived in the 1780s. He is commonly known as the "Founder of Chicago".
In 1795, following the Northwest Indian War, an area that was to be part of Chicago was turned over to the United States for a military post by native tribes in accordance with the Treaty of Greenville. In 1803, the United States Army built Fort Dearborn, which was destroyed in 1812 in the Battle of Fort Dearborn and later rebuilt. The Ottawa, Ojibwe, and Potawatomi tribes had ceded additional land to the United States in the 1816 Treaty of St. Louis. The Potawatomi were forcibly removed from their land after the Treaty of Chicago in 1833.




On August 12, 1833, the Town of Chicago was organized with a population of about 200. Within seven years it grew to more than 4,000 people. On June 15, 1835, the first public land sales began with Edmund Dick Taylor as U.S. receiver of public moneys. The City of Chicago was incorporated on Saturday, March 4, 1837 and for several decades was the world's fastest growing city.
As the site of the Chicago Portage, the city became an important transportation hub between the eastern and western United States. Chicago's first railway, Galena and Chicago Union Railroad, and the Illinois and Michigan Canal opened in 1848. The canal allowed steamboats and sailing ships on the Great Lakes to connect to the Mississippi River.
A flourishing economy brought residents from rural communities and immigrants from abroad. Manufacturing and retail and finance sectors became dominant, influencing the American economy. The Chicago Board of Trade (established 1848) listed the first ever standardized 'exchange traded' forward contracts, which were called futures contracts.

In the 1850s, Chicago gained national political prominence as the home of Senator Stephen Douglas, the champion of the Kansas–Nebraska Act and the "popular sovereignty" approach to the issue of the spread of slavery. These issues also helped propel another Illinoisan, Abraham Lincoln, to the national stage. Lincoln was nominated in Chicago for US President at the 1860 Republican National Convention. He defeated Douglas in the general election, and this set the stage for the American Civil War.
To accommodate rapid population growth and demand for better sanitation, the city improved its infrastructure. In February 1856, Chicago's Common Council approved Chesbrough's plan to build the United States' first comprehensive sewerage system. The project raised much of central Chicago to a new grade. While elevating Chicago, and at first improving the city's health, the untreated sewage and industrial waste now flowed into the Chicago River, then into Lake Michigan, polluting the city's primary freshwater source.
The city responded by tunneling two miles (3 km) out into Lake Michigan to newly-built water cribs. In 1900, the problem of sewage contamination was largely resolved when the city completed a major engineering feat. It reversed the flow of the Chicago River so the water flowed away from Lake Michigan rather than into it. This project began with the construction and improvement of the Illinois and Michigan Canal, and was completed with the Chicago Sanitary and Ship Canal that connects to the Illinois River, which flows into the Mississippi River.
In 1871, the Great Chicago Fire destroyed an area of about 4 miles long and 1 mile wide, a large section of the city at the time. Much of the city, including railroads and stockyards, survived intact, and from the ruins of the previous wooden structures arose more modern constructions of steel and stone. These set a precedent for worldwide construction. During its rebuilding period, Chicago constructed the world's first skyscraper in 1885, using steel-skeleton construction.
The city grew significantly in size and population by incorporating many neighboring townships between 1851 and 1920, with the largest annexation happening in 1889, with five townships joining the city, including the Hyde Park Township, which now comprises most of the South Side of Chicago and the far southeast of Chicago, and the Jefferson Township, which now makes up most of Chicago's Northwest Side. The desire to join the city was driven by municipal services the city could provide its residents.

Chicago's flourishing economy attracted huge numbers of new immigrants from Europe and migrants from the Eastern United States. Of the total population in 1900, more than 77% were either foreign-born or born in the United States of foreign parentage. Germans, Irish, Poles, Swedes and Czechs made up nearly two-thirds of the foreign-born population (by 1900, whites were 98.1% of the city's population).
Labor conflicts followed the industrial boom and the rapid expansion of the labor pool, including the Haymarket affair on May 4, 1886. Concern for social problems among Chicago's immigrant poor led Jane Addams and Ellen Gates Starr to found Hull House in 1889. Programs developed there became a model for the new field of social work.
During the 1870s and 1880s, Chicago attained national stature as the leader in the movement to improve public health. City, and later state laws, that upgraded standards for the medical profession and fought urban epidemics of cholera, smallpox, and yellow fever were both passed and enforced. These laws became templates for public health reform in other cities and states.
The city established many large, well-landscaped municipal parks, which also included public sanitation facilities. The chief advocate for improving public health in Chicago was Dr. John H. Rauch, M.D.. Rauch established a plan for Chicago's park system in 1866. He created Lincoln Park by closing a cemetery filled with shallow graves, and in 1867, in response to an outbreak of cholera he helped establish a new Chicago Board of Health. Ten years later, he became the secretary and then the president of the first Illinois State Board of Health, which carried out most of its activities in Chicago.
In the 19th century, Chicago became the nation's railroad center, and by 1910 over 20 railroads operated passenger service out of six different downtown terminals. In 1883, Chicago's railway managers needed a general time convention, so they developed the standardized system of North American time zones. This system for telling time spread throughout the continent.
In 1893, Chicago hosted the World's Columbian Exposition on former marshland at the present location of Jackson Park. The Exposition drew 27.5 million visitors, and is considered the most influential world's fair in history. The University of Chicago, formerly at another location, moved to the same South Side location in 1892. The term "midway" for a fair or carnival referred originally to the Midway Plaisance, a strip of park land that still runs through the University of Chicago campus and connects the Washington and Jackson Parks.






During World War I and the 1920s there was a major expansion in industry. The availability of jobs attracted African-Americans from the Southern United States. Between 1910 and 1930, the African-American population of Chicago increased dramatically, from 44,103 to 233,903. This Great Migration had an immense cultural impact, called the Chicago Black Renaissance, part of the New Negro Movement, in art, literature, and music. Continuing racial tensions and violence, such as the Chicago Race Riot of 1919, also occurred.
The ratification of the 18th amendment to the Constitution in 1919 made the production and sale (including exportation) of alcoholic beverages illegal in the United States. This ushered in the beginning of what is known as the Gangster Era, a time that roughly spans from 1919 until 1933 when Prohibition was repealed. The 1920s saw gangsters, including Al Capone, Dion O'Banion, Bugs Moran and Tony Accardo battle law enforcement and each other on the streets of Chicago during the Prohibition era. Chicago was the location of the infamous St. Valentine's Day Massacre in 1929, where Al Capone sent men to gun down members of his rival gang, North Side, led by Bugs Moran.
In 1924, Chicago was the first American city to have a homosexual-rights organization, the Society for Human Rights. This organization produced the first American publication for homosexuals, Friendship and Freedom. Police and political pressure caused the organization to disband.
In 1933, Chicago Mayor Anton Cermak was fatally wounded in Miami, Florida during a failed assassination attempt on President-elect Franklin D. Roosevelt. In 1933 and 1934, the city celebrated its centennial by hosting the Century of Progress International Exposition Worlds Fair. The theme of the fair was technological innovation over the century since Chicago's founding.
In March 1937, there was a violent strike by about 3,500 drivers for Checker and Yellow Cab Companies which included rioting that went on for weeks. The cab companies hired "strike breakers", and the cab drivers union hired "sluggers" who ragged through the downtown Chicago area looking for cabs and drivers not participating in the strike.



On December 2, 1942, physicist Enrico Fermi conducted the world's first controlled nuclear reaction at the University of Chicago as part of the top-secret Manhattan Project. This led to the creation of the atomic bomb by the United States, which it used in World War II in 1945.
Mayor Richard J. Daley, a Democrat, was elected in 1955, in the era of machine politics.
By the early 1960s, white residents in several neighborhoods left the city for the suburban areas – in many Northern American cities, a process known as White flight – as African Americans continued to move beyond the Black Belt. While home loan discriminatory redlining against blacks continued, the real estate industry, to promote turnover practiced what became known as blockbusting. Whole neighborhoods were completely changed based on race. Structural changes in industry, such as globalization and job outsourcing, caused heavy losses of jobs for lower skilled workers. In 1966, Martin Luther King, Jr. and Albert Raby led the Chicago Freedom Movement, which culminated in agreements between Mayor Richard J. Daley and the movement leaders.

Two years later, the city hosted the tumultuous 1968 Democratic National Convention, which featured physical confrontations both inside and outside the convention hall, with anti-war protesters, journalists and bystanders being beaten by police. Major construction projects, including the Sears Tower (now known as the Willis Tower, which in 1974 became the world's tallest building), University of Illinois at Chicago, McCormick Place, and O'Hare International Airport, were undertaken during Richard J. Daley's tenure. In 1979, Jane Byrne, the city's first female mayor, was elected. She helped reduce crime in the Cabrini-Green housing project and led Chicago's school system out of a financial crisis.



In 1983, Harold Washington became the first black mayor of the city of Chicago. Washington's first term in office directed attention to poor and previously neglected minority neighborhoods. He was re‑elected in 1987 but died of a heart attack soon after. Washington was succeeded by 6th ward Alderman Eugene Sawyer who was elected by the Chicago City Council and served until a special election.
Richard M. Daley, son of Richard J. Daley, was elected in 1989. His accomplishments included improvements to parks and creating incentives for sustainable development. After successfully standing for re-election five times, and becoming Chicago's longest serving mayor, Richard M. Daley declined to run for a seventh term.
On February 23, 2011, former Illinois Congressman and White House Chief of Staff, Rahm Emanuel, won the mayoral election, beating five rivals with 55 percent of the vote, and was sworn in as Mayor on May 16, 2011.










Chicago is located in northeastern Illinois on the southwestern shores of Lake Michigan. It is the principal city in the Chicago Metropolitan Area, situated in the Midwestern United States and the Great Lakes region. Chicago rests on a continental divide at the site of the Chicago Portage, connecting the Mississippi River and the Great Lakes watersheds. The city lies beside huge freshwater Lake Michigan, and two rivers—the Chicago River in downtown and the Calumet River in the industrial far South Side—flow entirely or partially through Chicago. Chicago's history and economy are closely tied to its proximity to Lake Michigan. While the Chicago River historically handled much of the region's waterborne cargo, today's huge lake freighters use the city's Lake Calumet Harbor on the South Side. The lake also provides another positive effect, moderating Chicago's climate; making waterfront neighborhoods slightly warmer in winter and cooler in summer.
When Chicago was founded in 1833, most of the early building was around the mouth of the Chicago River, as can be seen on a map of the city's original 58 blocks. The overall grade of the city's central, built-up areas, is relatively consistent with the natural flatness of its overall natural geography, generally exhibiting only slight differentiation otherwise. The average land elevation is 579 ft (176.5 m) above sea level. The lowest points are along the lake shore at 578 ft (176.2 m), while the highest point, at 672 ft (205 m), is the morainal ridge of Blue Island in the city's far south side.
The Chicago Loop is the central business district, but Chicago is also a city of neighborhoods. Lake Shore Drive runs adjacent to a large portion of Chicago's lakefront. Some of the parks along the waterfront include Lincoln Park, Grant Park, Burnham Park and Jackson Park. There are twenty-four public beaches across 26 miles (42 km) of the waterfront. Landfill extends into portions of the lake providing space for Navy Pier, Northerly Island, the Museum Campus, and large portions of the McCormick Place Convention Center. Most of the city's high-rise commercial and residential buildings are close to the waterfront.
An informal name for the entire Chicago metropolitan area is "Chicagoland". There is no precise definition for the term "Chicagoland", but it generally means the entire conurbation. The Chicago Tribune, which coined the term, includes the city of Chicago, the rest of Cook County, eight nearby Illinois counties: Lake, McHenry, DuPage, Kane, Kendall, Grundy, Will and Kankakee, and three counties in Indiana: Lake, Porter and LaPorte. The Illinois Department of Tourism defines Chicagoland as Cook County without the city of Chicago, and only Lake, DuPage, Kane and Will counties. The Chicagoland Chamber of Commerce defines it as all of Cook and DuPage, Kane, Lake, McHenry and Will counties.




Major sections of the city include the central business district, called The Loop, and the North, the South, and West Sides. The three sides of the city are represented on the Flag of Chicago by three horizontal white stripes. The North Side is the most densely populated residential section of the city, and many high-rises are located on this side of the city along the lakefront. The South Side is the largest section of the city, encompassing roughly 60% of the city's land area. The South Side contains the University of Chicago and most of the facilities of the Port of Chicago.
In the late 1920s, sociologists at the University of Chicago subdivided the city into 77 distinct community areas, which can further be subdivided into over 200 informally defined neighborhoods.




Chicago's streets were laid out in a street grid that grew from the city's original townsite plat, which was bounded by Lake Michigan on the east, North Avenue on the north, Wood Street on the west, and 22nd Street on the south. Streets following the Public Land Survey System section lines later became arterial streets in outlying sections. As new additions to the city were platted, city ordinance required them to be laid out with eight streets to the mile in one direction and sixteen in the other direction (about one street per 201 meters by two in the other direction). The grid's regularity provided an efficient means of developing new real estate property. A scattering of diagonal streets, many of them originally Native American trails, also cross the city (Elston, Milwaukee, Ogden, Lincoln, etc.). Many additional diagonal streets were recommended in the Plan of Chicago, but only the extension of Ogden Avenue was ever constructed.
In 2016, Chicago was ranked the sixth-most walkable large city in the United States. Many of the city's residential streets have a wide patch of grass and/or trees between the street and the sidewalk itself. This helps to keep pedestrians on the sidewalk further away from the street traffic. Chicago's Western Avenue is the longest continuous urban street in the world. Other famous streets include Michigan Avenue, State Street, Clark Street, and Belmont Avenue. The City Beautiful movement inspired Chicago's boulevards and parkways.




The destruction caused by the Great Chicago Fire led to the largest building boom in the history of the nation. In 1885, the first steel-framed high-rise building, the Home Insurance Building, rose in the city as Chicago ushered in the skyscraper era, which would then be followed by many other cities around the world. Today, Chicago's skyline is among the world's tallest and most dense.
Some of the United States' tallest towers are located in Chicago; Willis Tower (formerly Sears Tower) is the second tallest building in the Western Hemisphere after One World Trade Center, and Trump International Hotel and Tower is the third tallest in the country. The Loop's historic buildings include the Chicago Board of Trade Building, the Fine Arts Building, 35 East Wacker, and the Chicago Building, 860-880 Lake Shore Drive Apartments by Mies van der Rohe. Many other architects have left their impression on the Chicago skyline such as Daniel Burnham, Louis Sullivan, Charles B. Atwood, John Root, and Helmut Jahn.
The Merchandise Mart, once first on the list of largest buildings in the world, currently listed as 44th-largest (as of September 9, 2013), had its own zip code until 2008, and stands near the junction of the North and South branches of the Chicago River. Presently, the four tallest buildings in the city are Willis Tower (formerly the Sears Tower, also a building with its own zip code), Trump International Hotel and Tower, the Aon Center (previously the Standard Oil Building), and the John Hancock Center. Industrial districts, such as some areas on the South Side, the areas along the Chicago Sanitary and Ship Canal, and the Northwest Indiana area are clustered.
Chicago gave its name to the Chicago School and was home to the Prairie School, two movements in architecture. Multiple kinds and scales of houses, townhouses, condominiums, and apartment buildings can be found throughout Chicago. Large swaths of the city's residential areas away from the lake are characterized by brick bungalows built from the early 20th century through the end of World War II. Chicago is also a prominent center of the Polish Cathedral style of church architecture. The Chicago suburb of Oak Park was home to famous architect Frank Lloyd Wright, who had designed The Robie House located near the University of Chicago.




Chicago is famous for its outdoor public art with donors establishing funding for such art as far back as Benjamin Ferguson's 1905 trust. A number of Chicago's public art works are by modern figurative artists. Among these are Chagall's Four Seasons; the Chicago Picasso; Miro's Chicago; Calder's Flamingo; Oldenburg's Batcolumn; Moore's Large Interior Form, 1953-54, Man Enters the Cosmos and Nuclear Energy; Dubuffet's Monument with Standing Beast, Abakanowicz's Agora; and, Anish Kapoor's Cloud Gate which has become an icon of the city. Some events which shaped the city's history have also been memorialized by art works, including the Great Northern Migration (Saar) and the centennial of statehood for Illinois. Finally, two fountains near the Loop also function as monumental works of art: Plensa's Crown Fountain as well as Burnham and Bennett's Buckingham Fountain.
More representational and portrait statuary includes a number of works by Lorado Taft (Fountain of Time, The Crusader, Eternal Silence, and the Heald Square Monument completed by Crunelle), French's Statue of the Republic, Edward Kemys's Lions, Saint-Gaudens's Abraham Lincoln: The Man (a.k.a. Standing Lincoln) and Abraham Lincoln: The Head of State (a.k.a. Seated Lincoln), Brioschi's Christopher Columbus, Meštrović's The Bowman and The Spearman, Dallin's Signal of Peace, Fairbanks's The Chicago Lincoln, Boyle's The Alarm, Polasek's memorial to Masaryk, memorials along Solidarity Promenade to Kościuszko, Havliček and Copernicus by Chodzinski, Strachovský, and Thorvaldsen, a memorial to General Logan by Saint-Gaudens, and Kearney's Moose (W-02-03). A number of statues also honor recent local heroes such as Michael Jordan (by Amrany and Rotblatt-Amrany), Stan Mikita, and Bobby Hull outside of the United Center; Harry Caray (by Amrany and Cella) outside Wrigley field, Jack Brickhouse (by McKenna) next to the WGN studios, and Irv Kupcinet at the Wabash Avenue Bridge.
There are preliminary plans to erect a 1:1‑scale replica of Wacław Szymanowski's Art Nouveau statue of Frédéric Chopin found in Warsaw's Royal Baths along Chicago's lakefront in addition to a different sculpture commemorating the artist in Chopin Park for the 200th anniversary of Frédéric Chopin's birth.




The city lies within the humid continental climate zone (Köppen: Dfa), and experiences four distinct seasons. Summers are warm to hot and often humid, with a July daily average of 75.8 °F (24.3 °C). In a normal summer, temperatures can exceed 90 °F (32 °C) as many as 21 days. Winters are cold and snowy with few sunny days, and the normal January high is just below freezing. Spring and autumn are mild seasons with low humidity. Dewpoint temperatures in the summer range from 55.7 °F (13.2 °C) in June to 61.7 °F (16.5 °C) in July. The city is part of the USDA Plant Hardiness zone 6a, transitioning to 5b in the suburbs.
According to the National Weather Service, Chicago's highest official temperature reading of 105 °F (41 °C) was recorded on July 24, 1934, although Midway Airport reached 109 °F (43 °C) one day prior and recorded a heat index of 125 °F (52 °C) during the 1995 heatwave. The lowest official temperature of −27 °F (−33 °C) was recorded on January 20, 1985, at O'Hare Airport. The city can experience extreme winter cold waves and summer heat waves that may last for several consecutive days. Thunderstorms are common during the spring and summer months which may sometimes produce hail, high winds, and tornadoes. Like other major cities, Chicago also experiences urban heat island, making the city and its suburbs milder than surrounding rural areas, especially at night and in winter. Also, the proximity to Lake Michigan keeps lakefront Chicago cooler in early summer and milder in winter than areas to the west.




During its first hundred years, Chicago was one of the fastest-growing cities in the world. When founded in 1833, fewer than 200 people had settled on what was then the American frontier. By the time of its first census, seven years later, the population had reached over 4,000. In the forty years from 1850 to 1890, the city's population grew from slightly under 30,000 to over 1 million. At the end of the 19th century, Chicago was the fifth-largest city in the world, and the largest of the cities that did not exist at the dawn of the century. Within sixty years of the Great Chicago Fire of 1871, the population went from about 300,000 to over 3 million, and reached its highest ever recorded population of 3.6 million for the 1950 census.
From the last two decades of the 19th century, Chicago was the destination of waves of immigrants from Ireland, Southern, Central and Eastern Europe, including Italians, Jews, Poles, Lithuanians, Serbs and Czechs. To these ethnic groups, the basis of the city's industrial working class, were added an additional influx of African-Americans from the American South — with Chicago's black population doubling between 1910 and 1920 and doubling again between 1920 and 1930.
In the 1920s and 1930s, the great majority of African Americans moving to Chicago were clustered in a so‑called "Black Belt" on the city's South Side. By 1930, two-thirds of Chicago's African-American population lived in sections of the city which were 90% black in racial composition. Chicago's South Side emerged as America's second-largest urban black concentration, following New York's Harlem.
Since 2010, Chicago's population has rebounded adding nearly 25,000 people in the most recent 2015 population estimates.
As of the 2010 census, there were 2,695,598 people with 1,045,560 households living in Chicago. More than half the population of the state of Illinois lives in the Chicago metropolitan area. Chicago is one of the United States' most densely populated major cities, and the largest city in the Great Lakes Megalopolis. The racial composition of the city was:
45.0% White (31.7% non-Hispanic whites);
32.9% Black or African American;
28.9% Hispanic or Latino (of any race);
13.4% from some other race;
5.5% Asian (1.6% Chinese, 1.1% Indian, 1.1% Filipino, 0.4% Korean, 0.3% Pakistani, 0.3% Vietnamese, 0.2% Japanese, 0.1% Thai);
2.7% from two or more races;
0.5% American Indian.
Chicago has a Hispanic or Latino population of 28.9%. (Its members may belong to any race; 21.4% Mexican, 3.8% Puerto Rican, 0.7% Guatemalan, 0.6% Ecuadorian, 0.3% Cuban, 0.3% Colombian, 0.2% Honduran, 0.2% Salvadoran, 0.2% Peruvian)
The city's previous largest ethnic group, non-Hispanic white, declined from 59% in 1970 to 31.7% in 2010.
Chicago has the third-largest LGBT population in the United States. In 2015, roughly 4% of the population identified as LGBT. Since the legalization of same-sex marriage in the State of Illinois in 2013, over 10,000 same-sex couples have wed in Cook County, a majority in Chicago.
According to the U.S. Census Bureau's American Community Survey data estimates for 2008–2012, the median income for a household in the city was $47,408, and the median income for a family was $54,188. Male full-time workers had a median income of $47,074 versus $42,063 for females. About 18.3% of families and 22.1% of the population lived below the poverty line.
According to the 2008–2012 American Community Survey, the ancestral groups having 10,000 or more persons in Chicago were:

Persons identifying themselves as "Other groups" were classified at 1.72 million, and unclassified or not reported were approximately 153,000.




71% of Chicagoans identify as Christians, 7% identity with other faiths, and 22% have no religious affiliation. Chicago also has many Jews, Muslims, Buddhists, Hindus, and others. Chicago is the headquarters of several religious denominations, including the Evangelical Covenant Church and the Evangelical Lutheran Church in America. The Fourth Presbyterian Church is one of the largest Presbyterian congregations in the United States based on memberships.
The first two Parliament of the World's Religions in 1893 and 1993 were held in Chicago. Many international religious leaders have visited Chicago, including Mother Teresa, the Dalai Lama, and Pope John Paul II in 1979.




Chicago has the third-largest gross metropolitan product in the United States—about $630.3 billion according to 2014–2016 estimates. The city has also been rated as having the most balanced economy in the United States, due to its high level of diversification. In 2007, Chicago was named the fourth-most important business center in the world in the MasterCard Worldwide Centers of Commerce Index. Additionally, the Chicago metropolitan area recorded the greatest number of new or expanded corporate facilities in the United States for calendar year 2014. The Chicago metropolitan area has the third-largest science and engineering work force of any metropolitan area in the nation. In 2009 Chicago placed 9th on the UBS list of the world's richest cities. Chicago was the base of commercial operations for industrialists John Crerar, John Whitfield Bunn, Richard Teller Crane, Marshall Field, John Farwell, Julius Rosenwald and many other commercial visionaries who laid the foundation for Midwestern and global industry.
Chicago is a major world financial center, with the second-largest central business district in the United States. The city is the headquarters of the Federal Reserve Bank of Chicago (the Seventh District of the Federal Reserve). The city has major financial and futures exchanges, including the Chicago Stock Exchange, the Chicago Board Options Exchange (CBOE), and the Chicago Mercantile Exchange (the "Merc"), which is owned, along with the Chicago Board of Trade (CBOT) by Chicago's CME Group. The CME Group, in addition, owns the New York Mercantile Exchange (NYMEX), the Commodities Exchange Inc. (COMEX) and the Dow Jones Indexes. Perhaps due to the influence of the Chicago school of economics, the city also has markets trading unusual contracts such as emissions (on the Chicago Climate Exchange) and equity style indices (on the U.S. Futures Exchange). Chase Bank has its commercial and retail banking headquarters in Chicago's Chase Tower.
The city and its surrounding metropolitan area contain the third-largest labor pool in the United States with about 4.48 million workers, as of 2014. In addition, the state of Illinois is home to 66 Fortune 1000 companies, including those in Chicago. The city of Chicago also hosts 12 Fortune Global 500 companies and 17 Financial Times 500 companies. The city claims one Dow 30 company: aerospace giant Boeing, which moved its headquarters from Seattle to the Chicago Loop in 2001. Two more Dow 30 companies, Kraft Foods and McDonald's are in the Chicago suburbs, as are Sears Holdings Corporation and the technology spin-offs of Motorola. The headquarters of United Continental Holdings, are in the United Building and its operations center and its United Airlines subsidiary are in the Willis Tower in Chicago. In June 2016, McDonald's confirmed plans to move its global headquarters to Chicago's West Loop neighborhood by early 2018.

Manufacturing, printing, publishing and food processing also play major roles in the city's economy. Several medical products and services companies are headquartered in the Chicago area, including Baxter International, Boeing, Abbott Laboratories, and the Healthcare division of General Electric. In addition to Boeing, which located its headquarters in Chicago in 2001, and United Airlines in 2011, GE Transportation moved its offices to the city in 2013 and GE Healthcare moved its HQ to the city in 2016, as did ThyssenKrupp North America, and agriculture giant Archer Daniels Midland. Moreover, the construction of the Illinois and Michigan Canal, which helped move goods from the Great Lakes south on the Mississippi River, and of the railroads in the 19th century made the city a major transportation center in the United States. In the 1840s, Chicago became a major grain port, and in the 1850s and 1860s Chicago's pork and beef industry expanded. As the major meat companies grew in Chicago many, such as Armour and Company, created global enterprises. Though the meatpacking industry currently plays a lesser role in the city's economy, Chicago continues to be a major transportation and distribution center. Lured by a combination of large business customers, federal research dollars, and a large hiring pool fed by the area's universities, Chicago is also the site of a growing number of web startup companies like CareerBuilder, Orbitz, 37signals, Groupon, Feedburner, and NowSecure.
Chicago has been a hub of the Retail sector since its early development, with Montgomery Ward, Sears, and Marshall Field's. Today the Chicago metropolitan area is the headquarters of several retailers, including Walgreens, Sears, Ace Hardware, Claire's, ULTA Beauty and Crate & Barrel.
Late in the 19th century, Chicago was part of the bicycle craze, with the Western Wheel Company, which introduced stamping to the production process and significantly reduced costs, while early in the 20th century, the city was part of the automobile revolution, hosting the Brass Era car builder Bugmobile, which was founded there in 1907. Chicago was also the site of the Schwinn Bicycle Company.
Chicago is a major world convention destination. The city's main convention center is McCormick Place. With its four interconnected buildings, it is the largest convention center in the nation and third-largest in the world. Chicago also ranks third in the U.S. (behind Las Vegas and Orlando) in number of conventions hosted annually.
Chicago's minimum wage for non-tipped employees is one of the highest in the nation and will incrementally reach $13 per hour by 2019.




The city's waterfront location and nightlife has attracted residents and tourists alike. Over a third of the city population is concentrated in the lakefront neighborhoods from Rogers Park in the north to South Shore in the south. The city has many upscale dining establishments as well as many ethnic restaurant districts. These districts include the Mexican American neighborhoods, such as Pilsen along 18th street, and La Villita along 26th Street; the Puerto Rican enclave of Paseo Boricua in the Humboldt Park neighborhood; Greektown, along South Halsted Street, immediately west of downtown; Little Italy, along Taylor Street; Chinatown in Armour Square; Polish Patches in West Town; Little Seoul in Albany Park around Lawrence Avenue; Little Vietnam near Broadway in Uptown; and the Desi area, along Devon Avenue in West Ridge.
Downtown is the center of Chicago's financial, cultural, governmental and commercial institutions and the site of Grant Park and many of the city's skyscrapers. Many of the city's financial institutions, such as the CBOT and the Federal Reserve Bank of Chicago, are located within a section of downtown called "The Loop", which is an eight-block by five-block area of city streets that is encircled by elevated rail tracks. The term "The Loop" is largely used by locals to refer to the entire downtown area as well. The central area includes the Near North Side, the Near South Side, and the Near West Side, as well as the Loop. These areas contribute famous skyscrapers, abundant restaurants, shopping, museums, a stadium for the Chicago Bears, convention facilities, parkland, and beaches.
Lincoln Park contains the Lincoln Park Zoo and the Lincoln Park Conservatory. The River North Gallery District features the nation's largest concentration of contemporary art galleries outside of New York City.
Lakeview is home to Boystown (pronounced boys town), which, along with Andersonville, are some of the best-known LGBT neighborhoods in the nation. Each year in June, Boystown hosts the Chicago Pride Parade, one of the world's largest with over 1,000,000 people in attendance.

The South Side neighborhood of Hyde Park is the home of former US President Barack Obama. It also contains the University of Chicago (U of C), ranked one of the world's top ten universities; and the Museum of Science and Industry. The 6-mile (9.7 km) long Burnham Park stretches along the waterfront of the South Side. Two of the city's largest parks are also located on this side of the city: Jackson Park, bordering the waterfront, hosted the World's Columbian Exposition in 1893, and is the site of the aforementioned museum; and slightly west sits Washington Park. The two parks themselves are connected by a wide strip of parkland called the Midway Plaisance, running adjacent to the University of Chicago. The South Side hosts one of the city's largest parades, the annual African American Bud Billiken Parade and Picnic, which travels through Bronzeville to Washington Park. Ford Motor Company has an automobile assembly plant on the South Side in Hegewisch, and most of the facilities of the Port of Chicago are also on the South Side.
The West Side holds the Garfield Park Conservatory, one of the largest collections of tropical plants in any U.S. city. Prominent Latino cultural attractions found here include Humboldt Park's Institute of Puerto Rican Arts and Culture and the annual Puerto Rican People's Parade, as well as the National Museum of Mexican Art and St. Adalbert's Church in Pilsen. The Near West Side holds the University of Illinois at Chicago and was once home to Oprah Winfrey's Harpo Studios.
The city's distinctive accent, made famous by its use in classic films like The Blues Brothers and television programs like the Saturday Night Live skit "Bill Swerski's Superfans", is an advanced form of Inland Northern American English. This dialect can also be found in other cities bordering the Great Lakes such as Cleveland, Milwaukee, Detroit, and Rochester, New York, and most prominently features a rearrangement of certain vowel sounds, such as the short 'a' sound as in "cat", which can sound more like "kyet" to outsiders. The accent remains well associated with the city.




Renowned Chicago theater companies include the Goodman Theatre in the Loop; the Steppenwolf Theatre Company and Victory Gardens Theater in Lincoln Park; and the Chicago Shakespeare Theater at Navy Pier. Broadway In Chicago offers Broadway-style entertainment at five theaters: the Ford Center for the Performing Arts Oriental Theatre, Bank of America Theatre, Cadillac Palace Theatre, Auditorium Building of Roosevelt University, and Broadway Playhouse at Water Tower Place. Polish language productions for Chicago's large Polish speaking population can be seen at the historic Gateway Theatre in Jefferson Park. Since 1968, the Joseph Jefferson Awards are given annually to acknowledge excellence in theater in the Chicago area. Chicago's theater community spawned modern improvisational theater, and includes the prominent groups The Second City and I.O. (formerly ImprovOlympic).
The Chicago Symphony Orchestra (CSO) performs at Symphony Center, and is recognized as one of the best orchestras in the world. Also performing regularly at Symphony Center is the Chicago Sinfonietta, a more diverse and multicultural counterpart to the CSO. In the summer, many outdoor concerts are given in Grant Park and Millennium Park. Ravinia Festival, located 25 miles (40 km) north of Chicago, is the summer home of the CSO, and is a favorite destination for many Chicagoans. The Civic Opera House is home to the Lyric Opera of Chicago. The Lithuanian Opera Company of Chicago was founded by Lithuanian Chicagoans in 1956, and presents operas in Lithuanian.
The Joffrey Ballet and Chicago Festival Ballet perform in various venues, including the Harris Theater in Millennium Park. Chicago has several other contemporary and jazz dance troupes, such as the Hubbard Street Dance Chicago and Chicago Dance Crash.
Other live-music genre which are part of the city's cultural heritage include Chicago blues, Chicago soul, jazz, and gospel. The city is the birthplace of house music, a very popular form of Electronic Dance Music, and industrial music and is the site of an influential hip-hop scene. In the 1980s and 90s, the city was the global center for house and industrial music, two forms of music created in Chicago, as well as being popular for alternative rock, punk, and new wave. The city has been an epicenter for rave culture, since the 1980s. A flourishing independent rock music culture brought forth Chicago indie. Annual festivals feature various acts, such as Lollapalooza and the Pitchfork Music Festival. A 2007 report on the Chicago music industry by the University of Chicago Cultural Policy Center ranked Chicago third among metropolitan U.S. areas in "size of music industry" and fourth among all U.S. cities in "number of concerts and performances".
Chicago has a distinctive fine art tradition. For much of the twentieth century, it nurtured a strong style of figurative surrealism, as in the works of Ivan Albright and Ed Paschke. In 1968 and 1969, members of the Chicago Imagists, such as Roger Brown, Leon Golub, Robert Lostutter, Jim Nutt, and Barbara Rossi produced bizarre representational paintings.
Chicago contains a number of large, outdoor works by well-known artists. These include the Chicago Picasso, Miró's Chicago, Flamingo and Flying Dragon by Alexander Calder, Agora by Magdalena Abakanowicz, Monument with Standing Beast by Jean Dubuffet, Batcolumn by Claes Oldenburg, Cloud Gate by Anish Kapoor, Crown Fountain by Jaume Plensa, and the Four Seasons mosaic by Marc Chagall.
Chicago also has a nationally televised Thanksgiving parade that occurs annually. The McDonald's Thanksgiving Parade is seen across the nation on WGN-TV and WGN America, featuring a variety of diverse acts from the community, marching bands from across the country, and is the only parade in the city to feature inflatable balloons every year.




In 2014, Chicago attracted 50.17 million domestic leisure travelers, 11.09 million domestic business travelers and 1.308 million overseas visitors. These visitors contributed more than US$13.7 billion to Chicago's economy. Upscale shopping along the Magnificent Mile and State Street, thousands of restaurants, as well as Chicago's eminent architecture, continue to draw tourists. The city is the United States' third-largest convention destination. A 2011 study by Walk Score ranked Chicago the fourth-most walkable of fifty largest cities in the United States. Most conventions are held at McCormick Place, just south of Soldier Field. The historic Chicago Cultural Center (1897), originally serving as the Chicago Public Library, now houses the city's Visitor Information Center, galleries and exhibit halls. The ceiling of its Preston Bradley Hall includes a 38-foot (12 m) Tiffany glass dome. Grant Park holds Millennium Park, Buckingham Fountain (1927), and the Art Institute of Chicago. The park also hosts the annual Taste of Chicago festival. In Millennium Park, there is the reflective Cloud Gate sculpture. Cloud Gate, a public sculpture by Indian-born British artist Anish Kapoor, is the centerpiece of the AT&T Plaza in Millennium Park. Also, an outdoor restaurant transforms into an ice rink in the winter season. Two tall glass sculptures make up the Crown Fountain. The fountain's two towers display visual effects from LED images of Chicagoans' faces, along with water spouting from their lips. Frank Gehry's detailed, stainless steel band shell, the Jay Pritzker Pavilion, hosts the classical Grant Park Music Festival concert series. Behind the pavilion's stage is the Harris Theater for Music and Dance, an indoor venue for mid-sized performing arts companies, including the Chicago Opera Theater and Music of the Baroque.
Navy Pier, located just east of Streeterville, is 3,000 ft (910 m) long and houses retail stores, restaurants, museums, exhibition halls and auditoriums. In the summer of 2016, Navy Pier will have constructed their new DW60 Ferris wheel. Dutch Wheels a world renowned company that manufactures ferris wheels was selected to design the new wheel. It will feature 42 navy blue gondolas that can hold up to eight adults and two kids. It will also have entertainment systems inside the gondolas as well as a climate controlled environment. The DW60 will stand at approximately 196 ft (60 m), which is 46 ft taller than the previous wheel. The new DW60 will be the first in the United States and will be the sixth tallest in the U.S. Chicago was the first city in the world to ever erect a ferris wheel.
On June 4, 1998, the city officially opened the Museum Campus, a 10-acre (4.0 ha) lakefront park, surrounding three of the city's main museums, each of which is of national importance: the Adler Planetarium & Astronomy Museum, the Field Museum of Natural History, and the Shedd Aquarium. The Museum Campus joins the southern section of Grant Park, which includes the renowned Art Institute of Chicago. Buckingham Fountain anchors the downtown park along the lakefront. The University of Chicago Oriental Institute has an extensive collection of ancient Egyptian and Near Eastern archaeological artifacts. Other museums and galleries in Chicago include the Chicago History Museum, the Driehaus Museum, the DuSable Museum of African American History, the Museum of Contemporary Art, the Peggy Notebaert Nature Museum, the Polish Museum of America, the Museum of Broadcast Communications, the Pritzker Military Library, the Chicago Architecture Foundation, and the Museum of Science and Industry.
With an estimated completion date of 2020, the Barack Obama Presidential Center will be housed at the University of Chicago in Hyde Park and include both the Obama presidential library and offices of the Obama Foundation.
The Willis Tower (formerly named Sears Tower) is a popular destination for tourists. The Willis Tower has an observation deck open to tourists year round with high up views overlooking Chicago and Lake Michigan. The observation deck includes an enclosed glass balcony that extends 10 feet out on the side of the building. Tourists are able to look straight down.
In 2013, Chicago was chosen as one of the "Top Ten Cities in the United States" to visit for its restaurants, skyscrapers, museums, and waterfront, by the readers of Condé Nast Traveler.




Chicago lays claim to a large number of regional specialties that reflect the city's ethnic and working-class roots. Included among these are its nationally renowned deep-dish pizza; this style is said to have originated at Pizzeria Uno. The Chicago-style thin crust is also popular in the city.
The Chicago-style hot dog, typically an all-beef hot dog, is loaded with an array of toppings that often includes pickle relish, yellow mustard, pickled sport peppers, tomato wedges, dill pickle spear and topped off with celery salt on a poppy seed bun. Enthusiasts of the Chicago-style dog frown upon the use of ketchup as a garnish, but may prefer to add giardiniera.
There are several distinctly Chicago sandwiches, among them the Italian beef sandwich, which is thinly sliced beef simmered in au jus and served on an Italian roll with sweet peppers or spicy giardiniera. A popular modification is the Combo—an Italian beef sandwich with the addition of an Italian sausage. Another is the Maxwell Street Polish, a grilled or deep-fried kielbasa — on a hot dog roll, topped with grilled onions, yellow mustard, and hot sport peppers.
Ethnically originated creations include chicken Vesuvio, with roasted bone-in chicken cooked in oil and garlic next to garlicky oven-roasted potato wedges and a sprinkling of green peas. Another is the Puerto Rican-influenced jibarito, a sandwich made with flattened, fried green plantains instead of bread. There is also the mother-in-law, a tamale topped with chili and served on a hot dog bun. The tradition of serving the Greek dish, saganaki while aflame, has its origins in Chicago's Greek community. The appetizer, which consists of a square of fried cheese, is doused with Metaxa and flambéed table-side.
Two of the world's most decorated restaurants and also receiving the Michelin Guide 3 Star Award, Alinea and Grace are both located in Chicago. In addition, a number of well-known chefs have had restaurants in Chicago, including Charlie Trotter, Rick Tramonto, Grant Achatz, and Rick Bayless. In 2003, Robb Report named Chicago the country's "most exceptional dining destination".




Chicago literature finds its roots in the city's tradition of lucid, direct journalism, lending to a strong tradition of social realism. In the Encyclopedia of Chicago, Northwestern University Professor Bill Savage describes Chicago fiction as prose which tries to "capture the essence of the city, its spaces and its people". The challenge for early writers was that Chicago was a frontier outpost that transformed into a global metropolis in the span of two generations. Narrative fiction of that time, much of it in the style of "high-flown romance" and "genteel realism", needed a new approach to describe the urban social, political, and economic conditions of Chicago. Nonetheless, Chicagoans worked hard to create a literary tradition that would stand the test of time, and create a "city of feeling" out of concrete, steel, vast lake, and open prairie. Much notable Chicago fiction focuses on the city itself, with social criticism keeping exultation in check.
At least, three short periods in the history of Chicago have had a lasting influence on American Literature. These include from the time of the Great Chicago Fire to about 1900, what became known as the Chicago Literary Renaissance in the 1910s and early 1920s, and the period of the Great Depression through the 1940s.
What would become the influential Poetry magazine was founded in 1912 by Harriet Monroe, who was working as an art critic for the Chicago Tribune. The magazine discovered such poets as Gwendolyn Brooks, James Merrill, and John Ashbery. T. S. Eliot's first professionally published poem, "The Love Song of J. Alfred Prufrock", was first published by Poetry. Contributors have included Ezra Pound, William Butler Yeats, William Carlos Williams, Langston Hughes, and Carl Sandburg, among others. The magazine was instrumental in launching the Imagist and Objectivist poetic movements.




Sporting News named Chicago the "Best Sports City" in the United States in 1993, 2006, and 2010. Along with Boston, Chicago is the only city to continuously host major professional sports since 1871, having only taken 1872 and 1873 off due to the Great Chicago Fire. Additionally, Chicago is one of the six cities in the United States to have won championships in the four major professional leagues and, along with New York and Los Angeles, is one of three cities to have won soccer championships as well. Several major franchises have won championships within recent years – the Bears (1985), the Bulls (91, '92, '93, '96, '97, and '98), the White Sox (2005), the Cubs (2016), the Blackhawks (2010, 2013, 2015), and the Fire (1998).
The city has two Major League Baseball (MLB) teams: the Chicago Cubs of the National League play in Wrigley Field on the North Side; and the Chicago White Sox of the American League play in Guaranteed Rate Field on the South Side. Chicago is the only city that has had more than one MLB franchise every year since the AL began in 1901 (New York hosted only one between 1958 and early 1962). The Cubs are the oldest Major League Baseball team to have never changed their city; they have played in Chicago since 1871, and continuously so since 1874 due to the Great Chicago Fire. They have played more games and have more wins than any other team in Major League baseball since 1876. They have won three World Series titles, but had the dubious honor of having the two longest droughts in American professional sports: They had not won their sport's title since 1908, and had not participated in a World Series since 1945, both records, until they beat the Cleveland Indians in the 2016 World Series.
The White Sox have played on the South Side continuously since 1901, with all three of their home fields throughout the years being within blocks of one another. They have won three World Series titles (1906, 1917, 2005) and six American League pennants, including the first in 1901. The Sox are fifth in the American League in all-time wins, and sixth in pennants.
The Chicago Bears, one of the last two remaining charter members of the National Football League (NFL), have won nine NFL Championships, including the 1985 Super Bowl XX. The other remaining charter franchise, the Chicago Cardinals, also started out in the city, but is now known as the Arizona Cardinals. The Bears have won more games in the history of the NFL than any other team, and only the Green Bay Packers, their longtime rivals, have won more championships. The Bears play their home games at Soldier Field. Soldier Field re-opened in 2003 after an extensive renovation.

The Chicago Bulls of the National Basketball Association (NBA) is one of the most recognized basketball teams in the world. During the 1990s, with Michael Jordan leading them, the Bulls won six NBA championships in eight seasons. They also boast the youngest player to win the NBA Most Valuable Player Award, Derrick Rose, who won it for the 2010–11 season.
The Chicago Blackhawks of the National Hockey League (NHL) began play in 1926, and are one of the "Original Six" teams of the NHL. The Blackhawks have won six Stanley Cups, including in 2010, 2013, and 2015. Both the Bulls and the Blackhawks play at the United Center.
The Chicago Fire Soccer Club is a member of Major League Soccer (MLS) and plays at Toyota Park in suburban Bridgeview, after playing its first eight seasons at Soldier Field. The Fire have won one league title and four U.S. Open Cups, since their founding in 1997. In 1994, the United States hosted a successful FIFA World Cup with games played at Soldier Field. The Chicago Sky is a professional basketball team based in Rosemont, Illinois, playing in the Women's National Basketball Association (WNBA). They play home games at the Allstate Arena. The team was founded before the 2006 WNBA season began.
The Chicago Marathon has been held each year since 1977 except for 1987, when a half marathon was run in its place. The Chicago Marathon is one of six World Marathon Majors.
Five area colleges play in Division I conferences: two from major conferences — the DePaul Blue Demons (Big East Conference) and the Northwestern Wildcats (Big Ten Conference) — and three from other D1 conferences — the Chicago State Cougars (Western Athletic Conference); the Loyola Ramblers (Missouri Valley Conference); and the UIC Flames (Horizon League).




When Chicago was incorporated in 1837, it chose the motto Urbs in Horto, a Latin phrase which means "City in a Garden". Today, the Chicago Park District consists of more than 570 parks with over 8,000 acres (3,200 ha) of municipal parkland. There are 31 sand beaches, a plethora of museums, two world-class conservatories, and 50 nature areas. Lincoln Park, the largest of the city's parks, covers 1,200 acres (490 ha) and has over 20 million visitors each year, making it third in the number of visitors after Central Park in New York City, and the National Mall and Memorial Parks in Washington, D.C.
There is an historic boulevard system, a network of wide, tree-lined boulevards which connect a number of Chicago parks. The boulevards and the parks were authorized by the Illinois legislature in 1869. A number of Chicago neighborhoods emerged along these roadways in the 19th century. The building of the boulevard system continued intermittently until 1942. It includes nineteen boulevards, eight parks, and six squares, along twenty-six miles of interconnected streets. Part of the system in the Logan Square Boulevards Historic District was listed in the National Register of Historic Places in 1985.
With berths for more than 6,000 boats, the Chicago Park District operates the nation's largest municipal harbor system. In addition to ongoing beautification and renewal projects for the existing parks, a number of new parks have been added in recent years, such as the Ping Tom Memorial Park in Chinatown, DuSable Park on the Near North Side, and most notably, Millennium Park, which is in the northwestern corner of one of Chicago's oldest parks, Grant Park in the Chicago Loop.
The wealth of greenspace afforded by Chicago's parks is further augmented by the Cook County Forest Preserves, a network of open spaces containing forest, prairie, wetland, streams, and lakes that are set aside as natural areas which lie along the city's outskirts, including both the Chicago Botanic Garden in Glencoe and the Brookfield Zoo in Brookfield. Washington Park is also one of the city's biggest parks; covering nearly 400 acres (160 ha). The park is listed on the National Register of Historic Places listings in South Side Chicago.







The government of the City of Chicago is divided into executive and legislative branches. The Mayor of Chicago is the chief executive, elected by general election for a term of four years, with no term limits. The current mayor is Rahm Emanuel. The mayor appoints commissioners and other officials who oversee the various departments. As well as the mayor, Chicago's clerk and treasurer are also elected citywide. The City Council is the legislative branch and is made up of 50 aldermen, one elected from each ward in the city. The council takes official action through the passage of ordinances and resolutions and approves the city budget.
The Chicago Police Department provides law enforcement and the Chicago Fire Department provides fire suppression and emergency medical services for the city and its residents. Civil and criminal law cases are heard in the Cook County Circuit Court of the State of Illinois court system, or in the Northern District of Illinois, in the federal system. In the state court, the public prosecutor is the Illinois State's Attorney; in the Federal court it is the United States Attorney.




During much of the last half of the 19th century, Chicago's politics were dominated by a growing Democratic Party organization. During the 1880s and 1890s, Chicago had a powerful radical tradition with large and highly organized socialist, anarchist and labor organizations. For much of the 20th century, Chicago has been among the largest and most reliable Democratic strongholds in the United States; with Chicago's Democratic vote the state of Illinois has been "solid blue" in presidential elections since 1992. Even before then, it was not unheard of for Republican presidential candidates to win handily in downstate Illinois, only to lose statewide due to large Democratic margins in Chicago. The citizens of Chicago have not elected a Republican mayor since 1927, when William Thompson was voted into office. The strength of the party in the city is partly a consequence of Illinois state politics, where the Republicans have come to represent rural and farm concerns while the Democrats support urban issues such as Chicago's public school funding. Chicago contains less than 25% of the state's population, but 8 of Illinois' 19 U.S. Representatives have part of Chicago in their districts.
Machine politics persisted in Chicago after the decline of similar machines in other large U.S. cities. During much of that time, the city administration found opposition mainly from a liberal "independent" faction of the Democratic Party. The independents finally gained control of city government in 1983 with the election of Harold Washington (in office 1983–1987). From 1989 until May 16, 2011, Chicago was under the leadership of its longest serving mayor, Richard M. Daley, the son of Richard J. Daley. On May 16, 2011, Rahm Emanuel was sworn in as the 55th mayor of Chicago. Because of the dominance of the Democratic Party in Chicago, the Democratic primary vote held in the spring is generally more significant than the general elections in November for U.S. House and Illinois State seats. The aldermanic, mayoral, and other city offices are filled through nonpartisan elections with runoffs as needed.
Formerly a state legislator representing Chicago and later a US Senator, the city is home of United States President Barack Obama and First Lady Michelle Obama. The Obama's residence is located near the University of Chicago in Kenwood on the city's south side.




Chicago had a murder rate of 18.5 per 100,000 residents in 2012, ranking 16th among cities with 100,000 people or more. This was higher than in New York City and Los Angeles, the two largest cities in the United States, which have lower murder rates and lower total homicides. However, it was less than in many smaller American cities, including New Orleans, Newark, and Detroit, which had 53 murders per 100,000 residents in 2012. The 2015 year-end crime statistics showed there were 468 murders in Chicago in 2015 compared with 416 the year before, a 12.5% increase, as well as 2,900 shootings—13% more than the year prior, and up 29% since 2013. Chicago had more homicides than any other city in 2015, according to the Chicago Tribune. In its annual crime statistics for 2016, the Chicago Police Department reported that the city experienced a dramatic rise in gun violence, with 4,331 shooting victims. The department also reported 762 murders in Chicago for the year 2016, a total that marked a 62.8% increase in homicides from 2015.
According to reports in 2013, "most of Chicago's violent crime comes from gangs trying to maintain control of drug-selling territories", and is specifically related to the activities of the Sinaloa Cartel, which by 2006 had decided to seek to control illicit drug distribution, against local street gangs. Violent crime rates vary significantly by area of the city, with more economically developed areas having low rates, but other sections have much higher rates of crime. In 2013, the violent crime rate was 910 per 100,000 people; the murder rate was 10.4 – while high crime districts saw 38.9, low crime districts saw 2.5 murders per 100,000.
The number of murders in Chicago peaked at 970 in 1974, when the city's population was over 3 million people (a murder rate of about 29 per 100,000), and it reached 943 murders in 1992, (a murder rate of 34 per 100,000). However, Chicago and other major U.S. cities, experienced a significant reduction in violent crime rates through the 1990s, falling to 448 homicides in 2004, its lowest total since 1965 and only 15.65 murders per 100,000). Chicago's homicide tally remained low during 2005 (449), 2006 (452), and 2007 (435) but rose to 510 in 2008, breaking 500 for the first time since 2003. In 2009, the murder count fell to 458 (10% down). and in 2010 Chicago's murder rate fell to 435 (16.14 per 100,000), a 5% decrease from 2009 and lowest levels since 1965. In 2011, Chicago's murders fell another 1.2% to 431 (a rate of 15.94 per 100,000). but shot up to 506 in 2012.
In 2012, Chicago ranked 21st in the United States in numbers of homicides per person, but in the first half of 2013 there was a significant drop per-person, in all categories of violent crime, including homicide (down 26%). Chicago ended 2013 with 415 murders, the lowest number of murders since 1965, and overall crime rates dropped by 16 percent. (In 1965, there were 397 murders.)
Jens Ludwig, director of the University of Chicago Crime Lab, estimated that shootings cost the city of Chicago $2.5 billion in 2012.
In 2014, the Chicago police department reported a total murder count of 390 through December 20, 2014, according to the Chicago Sun-Times. That means that Chicago was able to record their lowest number of murder totals in close to five years for the second continuous calendar year, despite an overall increase in shootings. The Cook County medical examiner's office had reported a total of 410 homicides with 16 of those including fatal police shootings, all within the same time period.






Chicago Public Schools (CPS) is the governing body of the school district that contains over 600 public elementary and high schools citywide, including several selective-admission magnet schools. There are eleven selective enrollment high schools in the Chicago Public Schools, designed to meet the needs of Chicago's most academically advanced students. These schools offer a rigorous curriculum with mainly honors and Advanced Placement (AP) courses. Northside College Preparatory High School is ranked number one in the city of Chicago and the state of Illinois. Walter Payton College Prep High School is ranked second, Jones College Prep is third, and the oldest magnet school in the city, Whitney M. Young Magnet High School, which was opened in 1975, is ranked fourth. The magnet school with the largest enrollment is Lane Technical College Prep High School. Lane is one of the oldest schools in Chicago and in 2012 was designated a National Blue Ribbon School by the U.S. Department of Education.
Chicago high school rankings are determined by the average test scores on state achievement tests. The district, with an enrollment exceeding 400,545 students (2013–2014 20th Day Enrollment), is the third-largest in the U.S. On September 10, 2012, teachers for the Chicago Teachers Union went on strike for the first time since 1987 over pay, resources and other issues. According to data complied in 2014, Chicago's "choice system", where students who test or apply and may attend one of a number of public high schools (there are about 130), sorts students of different achievement levels into different schools (high performing, middle performing, and low performing schools).
Chicago has a network of Lutheran schools, and several private schools are run by other denominations and faiths, such as the Ida Crown Jewish Academy in West Ridge. Several private schools are completely secular, such as the Latin School of Chicago in the Near North Side neighborhood, the University of Chicago Laboratory Schools in Hyde Park, the British School of Chicago and the Francis W. Parker School in Lincoln Park, the Lycée Français de Chicago in Uptown, the Feltre School in River North and the Morgan Park Academy. There are also the private Chicago Academy for the Arts, a high school focused on six different categories of the arts and the public Chicago High School for the Arts, a high school focused on five categories (visual arts, theatre, musical theatre, dance, and music) of the arts.
The Roman Catholic Archdiocese of Chicago operates Catholic schools, that include Jesuit preparatory schools and others including St. Rita of Cascia High School, De La Salle Institute, Josephinum Academy, DePaul College Prep, Cristo Rey Jesuit High School, Brother Rice High School, St. Ignatius College Preparatory School, Mount Carmel High School, Queen of Peace High School, Mother McAuley Liberal Arts High School, Marist High School, St. Patrick High School and Resurrection High School.
The Chicago Public Library system operates 79 public libraries, including the central library, two regional libraries, and numerous branches distributed throughout the city.




Since the 1850s, Chicago has been a world center of higher education and research with several universities that are in the city proper or in the immediate environs. These institutions consistently rank among the top "National Universities" in the United States, as determined by U.S. News & World Report. Top universities in Chicago are: the University of Chicago; Illinois Institute of Technology; Northwestern University; Loyola University Chicago; DePaul University and University of Illinois at Chicago. Other notable schools include: Chicago State University; the School of the Art Institute of Chicago, the Illinois Institute of Art – Chicago; East–West University; National Louis University; North Park University; Northeastern Illinois University; Columbia College Chicago; Robert Morris University Illinois; Roosevelt University; Saint Xavier University; Rush University; and Shimer College.
William Rainey Harper, the first president of the University of Chicago, was instrumental in the creation of the junior college concept, establishing nearby Joliet Junior College as the first in the nation in 1901. His legacy continues with the multiple community colleges in the Chicago proper, including the seven City Colleges of Chicago: Richard J. Daley College, Kennedy–King College, Malcolm X College, Olive–Harvey College, Truman College, Harold Washington College and Wilbur Wright College, in addition to the privately held MacCormac College.
Chicago also has a high concentration of post-baccalaureate institutions, graduate schools, seminaries, and theological schools, such as the Adler School of Professional Psychology, The Chicago School of Professional Psychology, the Erikson Institute, The Institute for Clinical Social Work, the Lutheran School of Theology at Chicago, the Catholic Theological Union, the Moody Bible Institute, the John Marshall Law School and the University of Chicago Divinity School.




The Chicago metropolitan area is the third-largest media market in North America, after New York City and Los Angeles. Each of the big four U.S. television networks, CBS, ABC, NBC and Fox, directly owns and operates a high-definition television station in Chicago (WBBM 2, WLS 7, WMAQ 5 and WFLD 32, respectively). Former CW affiliate WGN-TV 9, which is owned by the Tribune Media, is carried with some programming differences, as "WGN America" on cable and satellite TV nationwide and in parts of the Caribbean. The city has also been the base of several talk shows, including, formerly, The Oprah Winfrey Show. Chicago Public Radio produces programs such as PRI's This American Life and NPR's Wait Wait...Don't Tell Me! The city also has two PBS member stations: WTTW 11, producer of shows such as Sneak Previews, The Frugal Gourmet, Lamb Chop's Play-Along and The McLaughlin Group, just to name a few, and WYCC 20.
Two major daily newspapers are published in Chicago: the Chicago Tribune and the Chicago Sun-Times, with the Tribune having the larger circulation. There are also several regional and special-interest newspapers and magazines, such as Chicago, the Dziennik Związkowy (Polish Daily News), Draugas (the Lithuanian daily newspaper), the Chicago Reader, the SouthtownStar, the Chicago Defender, the Daily Herald, Newcity, StreetWise and the Windy City Times. The entertainment and cultural magazine Time Out Chicago and GRAB magazine are also published in the city, as well as local music magazine Chicago Innerview. In addition, Chicago is the recent home of satirical national news outlet, The Onion, as well as its sister pop-culture publication, The A.V. Club.
Since the 1980s, many motion pictures have been filmed and/or set in the city such as The Blues Brothers, Brewster's Millions, Ferris Bueller's Day Off, Sixteen Candles, Home Alone, The Fugitive, I, Robot, Mean Girls, Wanted, Batman Begins, The Dark Knight, Transformers: Dark of the Moon, Transformers: Age of Extinction, Divergent, Insurgent, Batman v Superman: Dawn of Justice, Sinister 2 and Suicide Squad.
Chicago has also been the setting for many popular television shows, including the situation comedies Perfect Strangers and its spinoff Family Matters, Punky Brewster, Married... with Children, Kenan & Kel, Still Standing, The League, The Bob Newhart Show, and Shake It Up. The city served as the venue for the medical dramas ER and Chicago Hope, as well as the fantasy drama series Early Edition and the 2005–2009 drama Prison Break. Discovery Channel films two shows in Chicago: Cook County Jail and the Chicago version of Cash Cab. Chicago is currently the setting for CBS's The Good Wife and Mike and Molly, Showtime's Shameless, and NBC's Chicago Fire, Chicago P.D. and Chicago Med.
Chicago has five 50,000 watt AM radio stations: the CBS Radio-owned WBBM and WSCR; the Tribune Broadcasting-owned WGN; the Cumulus Media-owned WLS; and the ESPN Radio-owned WMVP. Chicago is also home to a number of national radio shows, including Beyond the Beltway with Bruce DuMont on Sunday evenings.
Chicago is also featured in a few video games, including Watch Dogs and Midtown Madness, a real-life, car-driving simulation game. In 2005, indie rock artist Sufjan Stevens created a concept album about Illinois titled Illinois; many of its songs were about Chicago and its history.







Chicago is a major transportation hub in the United States. It is an important component in global distribution, as it is the third-largest inter-modal port in the world after Hong Kong and Singapore.




Seven mainline and four auxiliary interstate highways (55, 57, 65 (only in Indiana), 80 (also in Indiana), 88, 90 (also in Indiana), 94 (also in Indiana), 190, 290, 294, and 355) run through Chicago and its suburbs. Segments that link to the city center are named after influential politicians, with three of them named after former U.S. Presidents (Eisenhower, Kennedy, and Reagan) and one named after two-time Democratic candidate Adlai Stevenson.
The Kennedy and Dan Ryan Expressways are the busiest state maintained routes in the entire state of Illinois.




The Regional Transportation Authority (RTA) coordinates the operation of the three service boards: CTA, Metra, and Pace.
The Chicago Transit Authority (CTA) handles public transportation in the City of Chicago and a few adjacent suburbs outside of the Chicago city limits. The CTA operates an extensive network of buses and a rapid transit elevated and subway system known as the 'L' (for "elevated"), with lines designated by colors. These rapid transit lines also serve both Midway and O'Hare Airports. The CTA's rail lines consist of the Red, Blue, Green, Orange, Brown, Purple, Pink, and Yellow lines. Both the Red and Blue lines offer 24‑hour service which makes Chicago one of a handful of cities around the world (and one of two in the United States, the other being New York City) to offer rail service 24 hours a day, every day of the year, within the city's limits.
Metra, the nation's second-most used passenger regional rail network, operates an 11-line commuter rail service in Chicago and throughout the Chicago suburbs. The Metra Electric Line shares its trackage with Northern Indiana Commuter Transportation District's South Shore Line, which provides commuter service between South Bend and Chicago.
Pace provides bus and paratransit service in over 200 surrounding suburbs with some extensions into the city as well. A 2005 study found that one quarter of commuters used public transit.
Greyhound Lines provides inter-city bus service to and from the city, and Chicago is also the hub for the Midwest network of Megabus (North America).




Amtrak long distance and commuter rail services originate from Union Station. Chicago is one of the largest hubs of passenger rail service in the nation. The services terminate in San Francisco, Washington, D.C., New York City, Indianapolis, New Orleans, Portland, Seattle, Milwaukee, Quincy, St. Louis, Carbondale, Boston, Grand Rapids, Port Huron, Pontiac, Los Angeles, and San Antonio. An attempt was made in the early 20th century to link Chicago with New York City via the Chicago – New York Electric Air Line Railroad. Parts of this were built, but it was never completed.



Chicago's bike share program, Divvy bikes, was launched in 2013. In 2016, there are 5,837 bikes and 576 rental stations across the city. PBSC Urban Solutions Inc., provides bikes and docking stations.



Chicago is the largest hub in the railroad industry. Six of the seven Class I railroads meet in Chicago, with the exception being the Kansas City Southern Railway. As of 2002, severe freight train congestion caused trains to take as long to get through the Chicago region as it took to get there from the West Coast of the country (about 2 days). According to U.S. Department of Transportation, the volume of imported and exported goods transported via rail to, from, or through Chicago is forecast to increase nearly 150 percent between 2010 and 2040. CREATE, the Chicago Region Environmental and Transport Efficiency program, comprises about 70 programs, including crossovers, overpasses and underpasses, that intend to significantly improve the speed of freight movements in the Chicago area.




Chicago is served by O'Hare International Airport, the world's second-busiest airport measured by airline operations, on the far Northwest Side, and Midway International Airport on the Southwest Side. In 2005, O'Hare was the world's busiest airport by aircraft movements and the second-busiest by total passenger traffic (due to government enforced flight caps). Both O'Hare and Midway are owned and operated by the City of Chicago. Gary/Chicago International Airport and Chicago Rockford International Airport, located in Gary, Indiana and Rockford, Illinois, respectively, can serve as alternate Chicago area airports, however they do not offer as many commercial flights as O'Hare and Midway. In recent years the state of Illinois has been leaning towards building an entirely new airport in the Illinois suburbs of Chicago. The City of Chicago is the world headquarters for United Airlines, the world's third-largest airline.




The Port of Chicago consists of several major port facilities within the city of Chicago operated by the Illinois International Port District (formerly known as the Chicago Regional Port District). The central element of the Port District, Calumet Harbor, is maintained by the U.S. Army Corps of Engineers.
Iroquois Landing Lakefront Terminal: at the mouth of the Calumet River, it includes 100 acres (0.40 km2) of warehouses and facilities on Lake Michigan with over 780,000 square meters (8,390,000 square feet) of storage.
Lake Calumet terminal: located at the union of the Grand Calumet River and Little Calumet River 6 miles (9.7 km) inland from Lake Michigan. Includes three transit sheds totaling over 29,000 square meters (315,000 square feet) adjacent to over 900 linear meters (3,000 linear feet) of ship and barge berthing.
Grain (14 million bushels) and bulk liquid (800,000 barrels) storage facilities along Lake Calumet.
The Illinois International Port district also operates Foreign trade zone No. 22, which extends 60 miles (97 km) from Chicago's city limits.



Electricity for most of northern Illinois is provided by Commonwealth Edison, also known as ComEd. Their service territory borders Iroquois County to the south, the Wisconsin border to the north, the Iowa border to the west and the Indiana border to the east. In northern Illinois, ComEd (a division of Exelon) operates the greatest number of nuclear generating plants in any US state. Because of this, ComEd reports indicate that Chicago receives about 75% of its electricity from nuclear power. Recently, the city began installing wind turbines on government buildings to promote renewable energy.
Natural gas is provided by Peoples Gas, a subsidiary of Integrys Energy Group, which is headquartered in Chicago.
Domestic and industrial waste was once incinerated but it is now landfilled, mainly in the Calumet area. From 1995 to 2008, the city had a blue bag program to divert recyclable refuse from landfills. Because of low participation in the blue bag programs, the city began a pilot program for blue bin recycling like other cities. This proved successful and blue bins were rolled out across the city.



The Illinois Medical District is on the Near West Side. It includes Rush University Medical Center, ranked as the second best hospital in the Chicago metropolitan area by U.S. News & World Report for 2014–15, the University of Illinois Medical Center at Chicago, Jesse Brown VA Hospital, and John H. Stroger, Jr. Hospital of Cook County, one of the busiest trauma centers in the nation.
Two of the country's premier academic medical centers reside in Chicago, including Northwestern Memorial Hospital and the University of Chicago Medical Center. The Chicago campus of Northwestern University includes the Feinberg School of Medicine; Northwestern Memorial Hospital, which is ranked as the best hospital in the Chicago metropolitan area by U.S. News & World Report for 2010–11; the Rehabilitation Institute of Chicago, which is ranked the best U.S. rehabilitation hospital by U.S. News & World Report; the new Prentice Women's Hospital; and Ann & Robert H. Lurie Children's Hospital of Chicago.
The University of Illinois College of Medicine at UIC is the second largest medical school in the United States (2,600 students including those at campuses in Peoria, Rockford and Urbana–Champaign).
In addition, the Chicago Medical School and Loyola University Chicago's Stritch School of Medicine are located in the suburbs of North Chicago and Maywood, respectively. The Midwestern University Chicago College of Osteopathic Medicine is in Downers Grove.
The American Medical Association, Accreditation Council for Graduate Medical Education, Accreditation Council for Continuing Medical Education, American Osteopathic Association, American Dental Association, Academy of General Dentistry, Academy of Nutrition and Dietetics, American Association of Nurse Anesthetists, American College of Surgeons, American Society for Clinical Pathology, American College of Healthcare Executives, the American Hospital Association and Blue Cross and Blue Shield Association are all based in Chicago.







Chicago has 28 sister cities around the world. Like Chicago, many of them are or were the second-most populous city or second-most influential city of their country, or they are the main city of a country that has had large amounts of immigrants settle in Chicago. These relationships have sought to promote economic, cultural, educational, and other ties.
To celebrate the sister cities, Chicago hosts a yearly festival in Daley Plaza, which features cultural acts and food tastings from the other cities. In addition, the Chicago Sister Cities program hosts a number of delegation and formal exchanges. In some cases, these exchanges have led to further informal collaborations, such as the academic relationship between the Buehler Center on Aging, Health & Society at the Feinberg School of Medicine of Northwestern University and the Institute of Gerontology of Ukraine (originally of the Soviet Union), that was originally established as part of the Chicago-Kiev sister cities program.
Sister cities




Chicago Wilderness
List of cities with the most skyscrapers
List of fiction set in Chicago
National Register of Historic Places listings in Central Chicago
National Register of Historic Places listings in North Side Chicago
National Register of Historic Places listings in South Side Chicago
National Register of Historic Places listings in West Side Chicago



Notes

References







Official website
Choose Chicago official tourism website
Chicago at DMOZ
Maps of Chicago from the American Geographical Society Library
Historic American Landscapes Survey (HALS) No. IL-10, "Chicago Cityscape, Chicago, Cook County, IL", 45 photos, 4 photo caption pages
Chicago – LocalWiki Local Chicago WikiTampa (/ˈtæmpə/) is a major city in, and the county seat of, Hillsborough County, Florida. It is located on the west coast of Florida on Tampa Bay, near the Gulf of Mexico, and is part of the Tampa Bay Metropolitan Area. The city had a population of 346,037 in 2011.
The current location of Tampa was once inhabited by indigenous peoples of the Safety Harbor culture (most notably the Tocobaga and the Pohoy, who lived along the shores of Tampa Bay). The area was explored by Spanish explorers in the 16th century, resulting in violent conflicts and the introduction of European diseases, which wiped out the original native cultures. Although Spain claimed Florida as part of New Spain, it did not found a colony in the Tampa area, and there were no permanent American or European settlements within today's city limits until after the United States acquired Florida from Spain in 1819.
In 1824, the United States Army established a frontier outpost called Fort Brooke at the mouth of the Hillsborough River, near the site of today's Tampa Convention Center. The first civilian residents were pioneers who settled near the fort for protection from the nearby Seminole population, and the small village was first incorporated as "Tampa" in 1849. The town grew slowly until the 1880s, when railroad links, the discovery of phosphate, and the arrival of the cigar industry jump-started its development, helping it to grow from a quiet village of less than 800 residents in 1880 to a bustling city of over 30,000 by the early 1900s.
Today, Tampa is part of the metropolitan area most commonly referred to as the "Tampa Bay Area". For U.S. Census purposes, Tampa is part of the Tampa-St. Petersburg-Clearwater, Florida Metropolitan Statistical Area. The four-county area is composed of roughly 2.9 million residents, making it the second largest metropolitan statistical area (MSA) in the state, and the fourth largest in the Southeastern United States, behind Miami, Washington, D.C. and Atlanta. The Greater Tampa Bay area has over 4 million residents and generally includes the Tampa and Sarasota metro areas. The Tampa Bay Partnership and U.S. Census data showed an average annual growth of 2.47 percent, or a gain of approximately 97,000 residents per year. Between 2000 and 2006, the Greater Tampa Bay Market experienced a combined growth rate of 14.8 percent, growing from 3.4 million to 3.9 million and hitting the 4 million population mark on April 1, 2007. A 2012 estimate shows the Tampa Bay area population to have 4,310,524 people and a 2017 projection of 4,536,854 people.
Tampa was ranked as the 5th best outdoor city by Forbes in 2008. Tampa also ranks as the fifth most popular American city, based on where people want to live, according to a 2009 Pew Research Center study. A 2004 survey by the NYU newspaper Washington Square News ranked Tampa as a top city for "twenty-somethings." Tampa is ranked as a "Gamma+" world city by Loughborough University, ranked alongside other world cities such as Phoenix, Charlotte, Rotterdam, and Santo Domingo.






The word "Tampa" may mean "sticks of fire" in the language of the Calusa, a Native American tribe that once lived south of today's Tampa Bay. This might be a reference to the many lightning strikes that the area receives during the summer months. Other historians claim the name means "the place to gather sticks". Toponymist George R. Stewart writes that the name was the result of a miscommunication between the Spanish and the Indians, the Indian word being "itimpi", meaning simply "near it". The name first appears in the "Memoir" of Hernando de Escalante Fontaneda (1575), who had spent 17 years as a Calusa captive. He calls it "Tanpa" and describes it as an important Calusa town. While "Tanpa" may be the basis for the modern name "Tampa", archaeologist Jerald Milanich places the Calusa village of Tanpa at the mouth of Charlotte Harbor, the original "Bay of Tanpa". A later Spanish expedition did not notice Charlotte Harbor while sailing north along the west coast of Florida and assumed that the current Tampa Bay was the bay they sought. The name was accidentally transferred north. Map makers were using the term Bay or Bahia Tampa as early as 1695.
People from Tampa are known as "Tampans" or "Tampanians". Local authorities consulted by Michael Kruse of the Tampa Bay Times suggest that "Tampan" was historically more common, while "Tampanian" became popular when the former term came to be seen as a potential insult. Latin Americans from Tampa are known as "tampeños", or "tampeñas" for females. These terms of Spanish origin emerged after 1900 for the immigrant communities in West Tampa and Ybor City. The tampeño, or "Tampa Latin", community is a mix of Cuban, Italian, Spanish, and American influences, with Cuban influence being dominant.




Not much is known about the cultures who called the Tampa Bay area home before European contact. When Spanish explorers arrived in the 1520s, they found Tocobaga villages around the northern half of Tampa Bay and Calusa villages along the southern portion of the bay.

Expeditions led by Pánfilo de Narváez and Hernando de Soto landed near Tampa, but neither conquistador stayed long. The native inhabitants repulsed any Spanish attempt to establish a permanent settlement or convert them to Catholicism. The newcomers brought with them infectious disease, resulting in a total collapse of the native cultures of Florida. The Tampa area was depopulated and ignored for more than 200 years.
In the mid-18th century, events in American colonies drove the Seminole Indians into northern Florida. During this period, the Tampa area had only a handful of residents: Cuban and Native American fishermen. They lived in a small village at the mouth of Spanishtown Creek on Tampa Bay, in today's Hyde Park neighborhood along Bayshore Boulevard.



After purchasing Florida from Spain in 1821, the United States built forts and trading posts in the new territory. Fort Brooke was established in January 1824 at the mouth of the Hillsborough River on Tampa Bay, in Downtown Tampa. Tampa was initially an isolated frontier outpost. The sparse civilian population practically abandoned the area during the Second Seminole War from 1835 to 1842, after which the Seminoles were forced out and many settlers returned.

Florida became the 27th state in 1845. On January 18, 1849, Tampa was officially incorporated as the "Village of Tampa". Tampa was home to 185 civilians, or 974 total residents including military personnel, in 1850. Tampa was reincorporated as a town on December 15, 1855.



During the Civil War, Florida seceded along with most of the southern states to form the Confederate States of America, and Fort Brooke was manned by Confederate troops. Martial law was declared in Tampa in January 1862, and Tampa's city government ceased to operate for the duration of the war.
In 1861, the Union Navy set up a blockade around many southern ports to cut off the Confederacy, and several ships were stationed near the mouth of Tampa Bay. The Battle of Fort Brooke on October 16 and the Battle of Ballast Point on October 18, 1863 damaged the Confederates, with Union troops destroying Confederate blockade runners. The Civil War ended in April 1865 with a Confederate defeat.
In May 1865, federal troops arrived in Tampa to occupy the fort and the town as part of Reconstruction. They remained until August 1869.
Tampa was a fishing village with very few people and little industry, and limited prospects for development. Tampa's chronic yellow fever epidemics, borne by mosquitoes from the swampland, were widespread during the late 1860s and 1870s, and many residents left.
In 1869, residents voted to abolish the city of Tampa government. The population of "Tampa Town" was below 800 by 1870, and had fallen further by 1880. Fort Brooke was decommissioned in 1883, and except for two cannons displayed on the University of Tampa campus, all traces of the fort are gone.




In the mid-1880s, Tampa's fortunes took several sudden turns for the better. First, phosphate was discovered in the Bone Valley region southeast of Tampa in 1883. The mineral, vital for the production of fertilizers and other products, was soon being shipped out from the Port of Tampa in great volume. Tampa is still a major phosphate exporter.
The discovery of phosphate, the arrival of Plant's railroad, and the founding of Ybor City and West Tampa—all in the mid-1880s—were crucial to Tampa's development. The once-struggling village of Tampa became a bustling boomtown almost overnight, and had grown into one of the largest cities in Florida by 1900.



Henry B. Plant's narrow-gauge South Florida Railroad reached Tampa and its port in late 1883, finally connecting the small town to the nation's railroad system after years of efforts by local leaders. Previously, Tampa's overland transportation links had consisted of sandy roads stretching across the Florida countryside. Plant's railroad made it much easier to get goods in and out of the Tampa Bay area. Phosphate and commercial fishing exports could be sent north by rail and many new products were brought into the Tampa market, along with the first tourists.




The new railroad link enabled another important industry to come to Tampa. In 1885, the Tampa Board of Trade enticed Vicente Martinez Ybor to move his cigar manufacturing operations to Tampa from Key West. Proximity to Cuba made importation of "clear Havana tobacco" easy by sea, and Plant's railroad made shipment of finished cigars to the rest of the US market easy by land.
Since Tampa was still a small town at the time (population less than 5000), Ybor built hundreds of small houses around his factory to accommodate the immediate influx of mainly Cuban and Spanish cigar workers. Ybor City's factories rolled their first cigars in 1886, and many different cigar manufacturers moved their operations to town in ensuing years. Many Italian and a few eastern European Jewish immigrants arrived starting in the late 1880s, opening businesses and shops that catered to cigar workers. By 1900, over 10,000 immigrants had moved to the neighborhood. Several thousand more Cuban immigrants built West Tampa, another cigar-centric suburb founded a few years later by Hugh MacFarlane. Between them, two "Latin" communities combined to exponentially expand Tampa's population, economic base, and tax revenues, as Tampa became the "Cigar Capital of the World".



During the first few decades of the 20th century, the cigar-making industry was the backbone of Tampa's economy. The factories in Ybor City and West Tampa made an enormous number of cigars—in the peak year of 1929, over 500,000,000 cigars were hand rolled in the city.
In 1904, a local civic association of local businessmen dubbed themselves Ye Mystic Krewe of Gasparilla (named after local mythical pirate Jose Gaspar), and staged an "invasion" of the city followed by a parade. With a few exceptions, the Gasparilla Pirate Festival has been held every year since.



Beginning in the late 19th century, illegal bolita lotteries were very popular among the Tampa working classes, especially in Ybor City. In the early 1920s, this small-time operation was taken over by Charlie Wall, the rebellious son of a prominent Tampa family, and went big-time. Bolita was able to openly thrive only because of kick-backs and bribes to key local politicians and law enforcement officials, and many were on the take.
Profits from the bolita lotteries and Prohibition-era bootlegging led to the development of several organized crime factions in the city. Charlie Wall was the first major boss, but various power struggles culminated in consolidation of control by Sicilian mafioso Santo Trafficante, Sr., and his faction in the 1950s. After his death in 1954 from cancer, control passed to his son Santo Trafficante, Jr., who established alliances with families in New York City and extended his power throughout Florida and into Batista-era Cuba.
The era of rampant and open corruption ended in the 1950s, when the Estes Kefauver's traveling organized crime hearings came to town and were followed by the sensational misconduct trials of several local officials. Although many of the worst offenders in government and the mob were not charged, the trials helped to end the sense of lawlessness which had prevailed in Tampa for decades.




Tampa grew considerably as a result of World War II. Prior to the United States' involvement in the conflict, construction began on MacDill Field, the predecessor of present-day MacDill Air Force Base. MacDill Field served as a main base for Army Air Corps and later Army Air Forces operations just before and during World War II, with multiple auxiliary airfields around the Tampa Bay area and surrounding counties. At the end of the war, MacDill remained as an active military installation while the auxiliary fields reverted to civilian control. Two of these auxiliary fields would later become the present day Tampa International Airport and St. Petersburg-Clearwater International Airport. With the establishment of an independent U.S. Air Force in 1947, MacDill Field became MacDill AFB.
During the 1950s and 1960s Tampa saw record-setting population growth that has not been seen since. This amazing growth spurred major expansion of the city's highways and bridges bringing thousands into the city and creating endless possibilities for Tampa business owners who welcomed tourists and new citizens alike into their neighborhoods. It was during this time period in the city's history that two of the most popular tourist attractions in the area were developed – Busch Gardens and Lowry Park. Many of the well-known institutions that play an important role in the economic development of the city were established during this time period.
The University of South Florida was established in North Tampa in 1956 and opened for students in September 1960. The school spurred the construction of several residential and commercial development in the previously agriculture-dominated area around the new campus. Overall, Tampa continued to expand away from the city center during the 1960s as new hospitals, schools, churches and subdivisions all began appearing to accommodate the growth. Many business offices began moving away from the traditional downtown office building into more convenient neighborhood office plazas.
In 1970, the Census Bureau reported city's population as 80.0% white and 19.7% black.
Four attempts have been made to consolidate the municipal government of the city of Tampa with the county government of Hillsborough County (1967, 1970, 1971, and 1972), all of which failed at the ballot box; the greatest loss was also the most recent attempt in 1972, with the final tally being 33,160 (31%) in favor and 73,568 (69%) against the proposed charter.
The biggest recent growth in the city was the development of New Tampa, which started in 1988 when the city annexed a mostly rural area of 24 square miles (62 km2) between I-275 and I-75.
East Tampa, historically a mostly black community, was the scene of several race riots during and for some time after the period of racial segregation, mainly due to problems between residents and the Tampa Police Department.







According to the United States Census Bureau, the city has a total area of 170.6 square miles (442 km2) including 112.1 square miles (290 km2) of land and 58.5 square miles (151.5 km2) (34.31%) of water. The highest point in the city is only 48 feet (15 m). Tampa is bordered by two bodies of water, Old Tampa Bay and Hillsborough Bay, both of which flow together to form Tampa Bay, which in turn flows into the Gulf of Mexico. The Hillsborough River flows out into Hillsborough Bay, passing directly in front of Downtown Tampa and supplying Tampa's main source of fresh water. Palm River is a smaller river flowing from just east of the city into McKay Bay, which is a smaller inlet, sited at the northeast end of Hillsborough Bay Tampa's geography is marked by the Interbay Peninsula which divides Hillsborough Bay (the eastern) from Old Tampa Bay (the western).




Tampa's climate displays characteristics of a tropical climate, but is situated on the southern fringe of the humid subtropical climate (Köppen Cfa) zone. Tampa's climate generally features hot summer days with frequent thunderstorms in the summer (rain is less frequent in the fall and winter), and a threat of a light winter freeze from November 15 through March 5 caused by occasional cold fronts from the north. Average highs range from 70 to 90 °F (21 to 32 °C) year round, and lows 52 to 76 °F (11 to 24 °C). Tampa's official recorded high has never hit 100 °F (37.8 °C) – the all-time record high temperature is 99 °F (37 °C), recorded on June 5, 1985.

Because of Tampa Bay, Tampa is split between two USDA climate zones. According to the 2012 USDA Plant Hardiness Zone Map, Tampa is listed as USDA zone 9b north of Kennedy Boulevard away from the bay and 10a south of Kennedy Boulevard and along the bay, Zone 10a is about the northern limit of where coconut palms and royal palms can be grown, although some specimens do grow in northern Tampa. Southern Tampa has much more tropical foliage than the northern parts of the city.



Temperatures are warm to hot from around mid-May through mid-October, which roughly coincides with the rainy season. Summertime weather is very consistent from June through September, with daytime highs near 90 °F (32 °C), lows in the mid-70s °F (23–24 °C), and high humidity.
Afternoon thunderstorms, usually generated by the interaction of the Gulf and Atlantic sea breezes, are such a regular occurrence during the summer that the Tampa Bay area is recognized as the "Lightning Capital of North America". Every year, Florida averages 10 deaths and 30 injuries from lightning strikes, with several of these usually occurring in or around Tampa. Because of the frequent summer thunderstorms, Tampa has a pronounced wet season, receiving an average of 26.1 inches (663 mm) of rain from June to September but only about 18.6 inches (472 mm) during the remaining eight months of the year. The historical averages during the late summer, especially September, are augmented by passing tropical systems, which can easily dump many inches of rain in one day. Tropical Storm Debby in 2012 dropped 8.57 inches (218 mm) of rain at Tampa International Airport on June 24, 2012 and amounts up to 10.36 inches (263 mm) was reported by a CoCoRaHS observer in NW Tampa. Outside of the summer rainy season, most of the area's precipitation is delivered by the occasional passage of a weather front.
The regular summertime afternoon thundershowers occasionally intensify into a severe thunderstorm, bringing heavy downpours, frequent lightning, strong straight-line winds, and sometimes hail.



Though it is affected by tropical storms every few years and threatened by tropical systems almost annually, Tampa has not taken a direct hit from a hurricane since 1921. That seemed about to change in 2004, when Hurricane Charley was forecast to make landfall near downtown Tampa, with potentially devastating effects for the entire region. The danger prompted the largest evacuation order for Pinellas County history and the largest evacuation request in Florida since Hurricane Floyd five years before. But Charley never reached Tampa Bay. After paralleling Florida's southwest coastline, the storm swerved to the east and slammed into Punta Gorda instead.



In the winter, average temperatures range from the low to mid 70s °F (21–24 °C) during the day to the low to mid 50s °F (10–13 °C) at night. However, sustained colder air pushes into the area on several occasions every winter, dropping the highs and lows to 15 °F (8 °C) or more below the seasonal averages for several days at a time. The temperature can fall below freezing an average of 2 to 3 times per year, though this does not occur every season.
Since the Tampa area is home to a diverse range of freeze-sensitive agriculture and aquaculture, hard freezes, although very infrequent, are a major concern. Since Tampa has some characteristics of a tropical climate, hard freezes (defined as below 28 °F (−2.2 °C)) happen rarely (every 5 to 20 years depending on location). The last officially recorded freeze at Tampa International Airport took place on the morning of January 13, 2011, when the temperature dropped to 31 °F (−1 °C).
The lowest temperature ever recorded in Tampa was 18 °F (−8 °C) on December 13, 1962. The last measurable snow in Tampa fell on January 19, 1977, with a total accumulation of 0.2 inches (0.5 cm). Three major freezes occurred in the 1980s: in January 1982, January 1985, and December 1989. The losses suffered by farmers forced many to sell off their citrus groves, which helped fuel a boom in subdivision development in the 1990s and 2000s.
In January 2010, a prolonged cold snap was the longest stretch of cold weather in the history of Tampa. Temperatures did not get above 49 °F (9.4 °C) for 5 days and there were freezes every night in northern Tampa for a week straight, causing significant damage to tropical foliage.







The city is divided into many neighborhoods, many of which were towns and unincorporated communities annexed by the growing city. Generally, the city is divided into the following areas: Downtown Tampa, New Tampa, West Tampa, East Tampa, North Tampa, and South Tampa. Well-known neighborhoods include Ybor City, Forest Hills, Ballast Point, Sulphur Springs, Seminole Heights, Tampa Heights, Palma Ceia, Hyde Park, Davis Islands, Tampa Palms, College Hill, and non-residential areas of Gary and the Westshore Business District.



Tampa displays a wide variety of architectural designs and styles. Most of Tampa's high rises demonstrate Post-modern architecture. The design for the renovated Tampa Museum of Art, displays Post-modern architecture, while the city hall and the Tampa Theatre belong to Art Deco architecture. The Tampa mayor Pam Iorio made the redevelopment of Tampa's downtown, especially residential development, a priority. Several residential and mixed-development high-rises have been constructed. Another of Mayor Iorio's initiatives was the Tampa Riverwalk, a mixed use path along the Hillsborough River in downtown and Channelside (Channelside was recently approved to undergo major renovations by Tampa Bay Lightning owner Jeff Vinik along with other investors). Several museums are part of the plan, including new homes for the Tampa Bay History Center, the Tampa Children's Museum, and the Tampa Museum of Art. Mayor Bob Buckhorn has continued these developments.
Tampa is the site of several skyscrapers. Overall, there are 18 completed buildings that rise over 250 feet (76 m) high. The city also has 69 high-rises, second only to Miami in the state of Florida. The tallest building in the city is 100 North Tampa, formerly the AmSouth Building, which rises 42 floors and 579 feet (176 m) in Downtown Tampa. The structure was completed in 1992, and is the tallest building in Florida outside of Miami and Jacksonville.




The Sulphur Springs Water Tower, a landmark in Sulphur Springs section of the city, dates back to the late 1920s. This boom period for Florida also saw the construction of an ornate movie palace, the Tampa Theatre, a Mediterranean revival on Davis Islands, and Bayshore Boulevard, which borders Hillsborough Bay from downtown Tampa to areas in South Tampa. The road has a 6-mile (10 km) continuous sidewalk on the eastern end, the longest in the world.
The Ybor City District is home to several buildings on the National Register of Historic Places and has been declared a National Historic Landmark. Notable structures include El Centro Español de Tampa, Centro Asturiano de Tampa and other social clubs built in the early 1900s.

Babe Zaharias Golf Course in the Forest Hills area of Tampa has been designated a Historical Landmark by the National Register of Historic Places. It was bought in 1949 by the famous 'Babe', who had a residence nearby, and closed upon her death. In 1974, the city of Tampa opened the golf course to the public. The Story of Tampa, a public painting by Lynn Ash, is a 4-by-8-foot (1.2 m × 2.4 m) oil on masonite mural that weaves together many of the notable aspects of Tampa's unique character and identity. It was commissioned in 2003 by the city's Public Art Program and can be found in the lobby of the Tampa Municipal Office Building. Park Tower (originally the First Financial Bank of Florida) is the first substantial skyscraper in downtown Tampa. Completed in 1973, it was the tallest skyscraper in Tampa until the completion of One Tampa City Center in 1981. The Rivergate building, a cylindrical building known as the "Beer Can building", was featured in the movie "The Punisher".
Spanning the southern part of Tampa Bay, is the massive steel-span Sunshine Skyway Bridge.
Tampa is home to the Bro Bowl, one of the last remaining skateparks built during skateboarding's "Golden Era" in the 1970s. It opened in 1979 and was constructed by Tampa Parks and Recreation. It was the first public skatepark to be constructed in Florida and the third on the East Coast.



As of 2000, the largest European ancestries in the city were German (9.2%), Irish (8.4%), English (7.7%), Italian (5.6%), and French (2.4%).
As of 2010, there were 157,130 households out of which 13.5% were vacant. In 2000, 27.6% households had children under the age of 18 living with them, 36.4% were married couples living together, 16.1% had a female householder with no husband present, and 42.9% were non-families. 33.7% of all households were made up of individuals and 10.2% had someone living alone who was 65 years of age or older. The average household size was 2.36 and the average family size was 3.07.
In 2000, the city's population was spread out with 24.6% under the age of 18, 10.0% from 18 to 24, 32.3% from 25 to 44, 20.5% from 45 to 64, and 12.5% who were 65 years of age or older. The median age was 34.7 years old. For every 100 females there were 95.3 males. For every 100 females age 18 and over, there were 92.1 males.
In 2006, the median income for a household in the city was $39,602, and the median income for a family was $45,823. Males had a median income of $40,461 versus $29,868 for females. The per capita income for the city was $26,522. 20.1% of the population and 16.4% of families were below the poverty line. 31.0% of those under the age of 18 and 13.6% of those 65 and older are living below the poverty level.
As of 2000, those who spoke only English at home accounted for 77.4% of all residents, while 22.6% spoke other languages in their homes. The most significant was Spanish speakers who made up 17.8% of the population, while French came up as the third most spoken language, which made up 0.6%, and Italian was at fourth, with 0.6% of the population.
There is a large gay population and a gay cultural center known as the GaYbor District.




Communities of faith have organized in Tampa from 1846, when a Methodist congregation established the city's first church, to 1939, when a 21-year-old Billy Graham began his career as a spiritual evangelist and preacher on downtown's Franklin Street, and through to today. Among Tampa's noteworthy religious structures are Sacred Heart Catholic Church, a 1905 downtown landmark noted for its soaring, Romanesque revival construction in granite and marble with German-crafted stained glass windows, the distinctive rock and mortar St. James Episcopal House of Prayer, listed with the U.S. historic registry, and the St. Paul AME church, which has seen the likes of Dr. Martin Luther King, Jr., and President Bill Clinton speak from its pulpit. The later two have been designated by the city government as Local Landmark Structures.
Tampa's religious community includes a broad representation of Christian denominations, including those above, and Presbyterian, Lutheran, Christian Science, Church of God, United Church of Christ, Philippine Independent Church, Unitarian Universalist, Metropolitan Community Church, Seventh-day Adventist, Eastern Orthodox (Greek, Coptic, Syrian, and OCA), various Pentecostal movements, Anglicans, the Quakers, Jehovah's Witnesses, and The Church of Jesus Christ of Latter-day Saints. There is also at least one congregation of Messianic Jews in Tampa. In addition there is a Korean Baptist church., as well as a Mennonite Church, several Haitian Churches, and a Vietnamese Baptist Church. Tampa also has several Jewish synagogues practicing Orthodox, Conservative, and Reform. In addition, there is a small Zoroastrian community present in Tampa.
Around the city are located a handful of mosques for followers of Islam, as well as a Tibetan-style Buddhist temple, a Thai Buddhist Wat, and local worship centers for the Sikh, Hindu and Bahá'í faiths. The Church of Scientology, based in nearby Clearwater, maintains a location for its members in Tampa.
Overall, Tampa is 50th out of the largest 51 metropolitan area in the percentage of the populace that attends religious services of any kind.




Finance, retail, healthcare, insurance, shipping by air and sea, national defense, professional sports, tourism, and real estate all play a vital role in the area's economy. Hillsborough County alone has an estimated 740,000 employees, a figure which is projected to increase to 922,000 by 2015. Several large corporations, such as banks and telecommunications companies, maintain regional offices in Tampa.

Several Fortune 1000 companies are headquartered in the metropolitan area, including OSI Restaurant Partners, WellCare, TECO Energy, and Raymond James Financial.
Downtown Tampa is undergoing significant development and redevelopment in line with a general national trend toward urban residential development. As of April 2007, the Tampa Downtown Partnership noted development proceeding on 20 residential, hotel, and mixed-use projects. Many of the new downtown developments are nearing completion in the midst of a housing market slump, which has caused numerous projects to be delayed or revamped, and some of the 20 projects TDP lists have not broken ground and are being refinanced. Nonetheless several developments are nearing completion, which city leaders hope will make downtown into a 24-hour neighborhood instead of 9 to 5 business district. As of 2010, Tampa residents faced a decline in rent of 2%. Nationally rent had decreased 4%. The Tampa Business Journal found Tampa to be the number two city for real estate investment in 2014.
Tampa's port is now the seventh largest in the nation and Florida's largest tonnage port, handling nearly half of all seaborne commerce that passes through the state. Tampa currently ranks second in the state behind Miami in terms of cruise ship travel. Besides smaller regional cruise ships such as Yacht Starship and SunCruz Casino, Tampa also serves as a port of call for three cruise lines: Holland America's MS Ryndam, Royal Caribbean's Grandeur of the Seas and Radiance of the Seas, and Carnival's Inspiration and Legend.
The main server farm for Wikipedia and other Wikimedia Foundation projects is located in Tampa.

MacDill Air Force Base remains a major employer as the parent installation for over 15,000 active uniformed military, Department of Defense (DoD) civil service and DoD contractor personnel in the Tampa Bay area. A significant majority of the civil service and contractor personnel are, in fact, themselves retired career military personnel. In addition to the 6th Air Mobility Wing, which is "host wing" for the base, MacDill is also home to Headquarters, United States Central Command (USCENTCOM), Headquarters, United States Special Operations Command (USSOCOM), the 927th Air Refueling Wing, Headquarters, United States Marine Forces Central Command (USMARCENT), Headquarters, United States Special Operations Command Central (USSOCCENT), and numerous other military activities of the active and reserve components of the armed forces.
Since the year 2000, Tampa has seen a notable upsurge in high-market demand from consumers, signaling more wealth concentrated in the area.







Tampa is home to a variety of stage and performing arts venues and theaters, including the David A. Straz Jr. Center for the Performing Arts, Tampa Theatre, Gorilla Theatre, and the MidFlorida Credit Union Amphitheatre next to the Florida State Fairgrounds.

Performing arts companies and organizations which call Tampa home include the Florida Orchestra, Opera Tampa, Jobsite Theater, the Master Chorale of Tampa Bay, Stageworks Theatre, Spanish Lyric Theater, Tampa Bay Opera, and the Tampa Bay Symphony.
Current popular nightlife districts include Channelside, Ybor City, SoHo, International Plaza and Bay Street, and Seminole Hard Rock. Downtown Tampa also contains some nightlife, and there are more clubs/bars to be found in other areas of the city. Tampa is rated sixth on Maxim magazine's list of top party cities.
The area has become a "de facto" headquarters of professional wrestling, with many pros living in the area. WWE's developmental territory, Florida Championship Wrestling, is also based in Tampa.
Tampa is home to several death metal bands, an extreme form of heavy metal music that evolved from thrash metal. Many of the genre's pioneers and foremost figures are based in and around the city. Chief among these are Deicide, Six Feet Under, Obituary, Cannibal Corpse, Death and Morbid Angel. The Tampa scene grew with the birth of Morrisound Recording, which established itself as an international recording destination for metal bands.
The underground rock band, the Baskervils, got their start in Tampa. They played the Tampa Bay area between 1994 and 1997 and then moved to New York City. Underground hip-hop group Equilibrium is based out of Tampa, as well as the Christian metalcore band, Underoath.
In 2009, the new Frank Wildhorn musical Wonderland: Alice's New Musical Adventure hosted its world premiere at the Straz Center.




The Tampa area is home to a number of museums that cover a wide array of subjects and studies. These include the Museum of Science & Industry (MOSI), which has several floors of science-related exhibits plus the only domed IMAX theater in Florida and a planetarium; the Tampa Museum of Art; the USF Contemporary Art Museum; the Tampa Bay History Center; the Tampa Firefighters Museum; the Henry B. Plant Museum; and Ybor City Museum State Park. Permanently docked in downtown's Channel District is the SS American Victory, a former World War II Victory Ship which is now used as a museum ship.




Tampa has a diverse culinary scene from small cafes and bakeries to bistros and farm-to-table restaurants. The food of Tampa has a history of Cuban, Spanish, Floribbean and Italian cuisines. There are also many Colombian cuisine, Puerto Rican cuisine, Vietnamese cuisine and Barbecue restaurants. Seafood is also very popular in Tampa, and Greek cuisine is prominent in the area, including around Tarpon Springs. Food trucks in Tampa, Florida are popular and the area holds the record for the world's largest food truck rally. In addition to Ybor, the areas of Seminole Heights and South Tampa are known for their restaurants.
Tampa is the birthplace of the Florida version of the deviled crab and the Cuban sandwich, which has been officially designated as the "signature sandwich of the city of Tampa" by the city council. A Tampa Cuban sandwich is distinct from other regional versions, as Genoa salami is layered in with the other ingredients. likely due to the influence of Italian immigrants living next to Cubans and Spaniards in Ybor City.
Tampa is also where several restaurant chains were founded or headquartered, including Outback Steakhouse, Melting Pot, Front Burner Brands, Carrabba's, Fleming's Prime Steakhouse & Wine Bar, Bonefish Grill, Columbia Restaurant, Checkers and Rally's, Taco Bus, and PDQ.




The city of Tampa operates over 165 parks and beaches covering 2,286 acres (9.25 km2) within city limits; 42 more in surrounding suburbs covering 70,000 acres (280 km2), are maintained by Hillsborough County. These areas include the Hillsborough River State Park, just northeast of the city. Tampa is also home to a number of attractions and theme parks, including Busch Gardens Tampa Bay, Adventure Island, Lowry Park Zoo, and Florida Aquarium.
Lowry Park Zoo features over 2,000 animals, interactive exhibits, rides, educational shows and more. The zoo serves as an economic, cultural, environmental and educational anchor in Tampa.
Big Cat Rescue is one of the largest accredited sanctuaries in the world dedicated entirely to abused and abandoned big cats. It is home to about 80 lions, tigers, bobcats, cougars and other species, most of whom have been abandoned, abused, orphaned, saved from being turned into fur coats, or retired from performing acts. They have a variety of different tours available.
Busch Gardens Tampa Bay is a 335-acre (1.36 km2) Africa-themed park located near the University of South Florida. It features many thrilling roller coasters, for which it is known, including Sheikra, Montu, Gwazi and Kumba. Visitors can also view and interact with a number of African wildlife. Adventure Island is a 30-acre (12 ha) water park adjacent to Busch Gardens.
The Florida Aquarium is a 250,000 sq ft (23,000 m2) aquarium located in the Channel District. It hosts over 20,000 species of aquatic plants and animals. It is known for its unique glass architecture. Adjacent to the Aquarium is the SS American Victory, a World War II Victory ship preserved as a museum ship.
The Tampa Bay History Center is a museum located in the Channel District. It boasts over 60,000 sq ft (5,600 m2) of exhibits through 12,000 years. Theaters, map gallery, research center and museum store.
Well-known shopping areas include International Plaza and Bay Street, WestShore Plaza, SoHo district, and Hyde Park Village. Palma Ceia is also home to a shopping district, called Palma Ceia Design District. Previously, Tampa had also been home to the Floriland Mall (now an office park), Tampa Bay Center (demolished and replaced with the new Tampa Bay Buccaneers training facility, known as "One Buc Place"), and East Lake Square Mall (now an office park).
The Tampa Port Authority currently operates three cruise ship terminals in Tampa's Channel District. The Port of Tampa is the year-round home port for Carnival Cruise Lines' MS Carnival Inspiration and MS Carnival Legend. In 2010 Tampa will also be a seasonal port for Holland America Line's MS Ryndam, as well as Royal Caribbean International's MS Grandeur of the Seas and MS Radiance of the Seas. A fourth company, Norwegian Cruise Line, has announced plans to sail out of Tampa for the first time. The 2,240 passenger MS Norwegian Star will be Tampa's largest cruise ship when it debuts a seasonal schedule in 2011. Cruise itineraries from Tampa include stops in the Eastern and Western Caribbean islands, Honduras, Belize, and Mexico.




Perhaps the most well known and anticipated events are those from Tampa's annual celebration of "Gasparilla", particularly the Gasparilla Pirate Festival, a mock pirate invasion held since 1904 in late January or early February. Often referred to as Tampa's "Mardi Gras", the invasion flotilla led by the pirate ship, Jose Gasparilla, and subsequent parade draw over 400,000 attendees, contributing tens of millions of dollars to the city's economy. Beyond the initial invasion, numerous Gasparilla festivities take place each year between January and March, including the Gasparilla Children's Parade, the more adult-oriented Sant'Yago Knight Parade, the Gasparilla Distance Classic, Gasparilla Festival of the Arts, and the Gasparilla International Film Festival, among other pirate themed events.
Other notable events include the Outback Bowl, which is held New Year's Day at Raymond James Stadium. Each February, The Florida State Fair brings crowds from across the state, while "Fiesta Day" celebrates Tampa's Cuban, Spanish, German, Italian, English, Irish, Jewish, and African-Cuban immigrant heritage. The India International Film Festival (IIFF) of Tampa Bay also takes place in February. In April the MacDill Air Fest entertains as one of the largest military air shows in the U.S. Guavaween, a nighttime street celebration infuses Halloween with the Latin flavor of Ybor City. Downtown Tampa hosts the largest anime convention in Florida, Metrocon, a three-day event held in either June or July at the Tampa Convention Center. Ybor also hosts "GaYbor Days", an annual street party in the GLBT-friendly GaYbor district. The Tampa International Gay and Lesbian Film Festival, held annually since 1989, is the city's largest film festival event, and one of the largest independent gay film festivals in the country.
Tampa hosted the 2012 Republican National Convention and the 15th International Indian Film Academy Awards in April 2014.




Tampa is represented by teams in three major professional sports leagues: the National Football League, the National Hockey League, and Major League Baseball. The NFL's Tampa Bay Buccaneers and the NHL's Tampa Bay Lightning call Tampa home, while the Tampa Bay Rays of the MLB play across the bay in St. Petersburg. As indicated by their names, these teams, plus several other sports teams, represent the entire Tampa metropolitan area.
The Tampa Bay area has long been a site for Major League Baseball spring training facilities and minor league baseball teams. The New York Yankees conduct spring training in Tampa, and the Tampa Yankees play there in the summer. On the collegiate level, the University of South Florida Bulls and the University of Tampa Spartans participate in many different sports.




The Tampa Bay Buccaneers began in 1976 as an expansion team of the NFL. They struggled at first, losing their first 26 games in a row to set a league record for futility. After a brief taste of success in the late 1970s, the Bucs again returned to their losing ways, and at one point lost 10+ games for 12 seasons in a row. The hiring of Tony Dungy in 1996 started an improving trend that eventually led to the team's victory in Super Bowl XXXVII in 2003 under coach Jon Gruden.
Tampa has hosted four Super Bowls: Super Bowl XVIII (1984), Super Bowl XXV (1991), Super Bowl XXXV (2001), and Super Bowl XLIII (2009). The first two events were held at Tampa Stadium, and the other two at Raymond James Stadium.
Originally the Pittsburgh Gladiators and a charter member of the Arena Football League, the Tampa Bay Storm relocated from Pittsburgh in 1991 and won ArenaBowl V that year. They have won 4 more ArenaBowls since then: ArenaBowl VII, IX, X, and XVII, and also appeared in ArenaBowl I, III, XII and XXIII. They have the most Arena Bowl titles.
Tampa was also home to the Tampa Bay Bandits of the United States Football League. The Bandits made the playoffs twice in their three seasons under head coach Steve Spurrier and drew league-leading crowds to Tampa Stadium, but the team folded along with the rest of the USFL after the 1985 season. They played at Tampa Stadium, which hosted the 1984 USFL Championship Game.




The Tampa Bay area has long been home to spring training, minor league, and excellent amateur baseball. The Tampa Bay Rays (originally "Devil Rays") began playing in 1998 at Tropicana Field in St. Petersburg. After a decade of futility, the Rays won the 2008 American League Pennant and made it to the World Series. They also won American League East titles in 2008 and 2010.
In 2007, the Rays began the process of searching for a stadium site closer to the center of the area's population, possibly in Tampa.
Several Major League baseball teams conduct spring training in the area, and most also operate minor league teams in the Class-A Florida State League. The New York Yankees and the affiliated Tampa Yankees use George M. Steinbrenner Field across Dale Mabry Highway from Raymond James Stadium.



The NHL's Tampa Bay Lightning was established in 1992, and currently play their home games in the Amalie Arena, located in downtown Tampa. In 2004, the team won their first Stanley Cup. The Lightning made the Eastern Conference Final in 2011 and were Eastern Conference Champions in 2015. They returned to the Eastern Conference Final in 2016.



The Tampa Bay Rowdies compete in the United Soccer League (2nd Division) after spending their first 6 seasons in the North American Soccer League. The team began play at Tampa's George M. Steinbrenner Field in 2010, then moved to St. Petersburg's Al Lang Field in 2011. The Rowdies won their first league championship in Soccer Bowl 2012.
Previously, Tampa had hosted two top-level soccer teams. The Tampa Bay Rowdies of the original North American Soccer League was the area's first major sports franchise, beginning play in 1975 at Tampa Stadium. The Rowdies were an immediate success, drawing good crowds and winning Soccer Bowl '75 in their first season to bring Tampa its first professional sports championship. Though the NASL ceased operations in 1984, the Rowdies continued to compete in various soccer leagues until finally folding in 1993.
The success of the Rowdies prompted Major League Soccer (MLS) to award Tampa a charter member of the new league in 1996. The Tampa Bay Mutiny were the first MLS Supporters' Shield winner and had much early success beginning in 1996. However, the club folded in 2001 when local ownership could not be secured mainly due to a financially poor lease agreement for Raymond James Stadium. The city has no current representation in MLS, however the Rowdies are currently seeking to join the league.



The University of South Florida is the only NCAA Division I sports program in Tampa. USF began playing intercollegiate sports in 1965. The South Florida Bulls established a basketball team in 1971 and a football team in 1997. The Bulls joined the Big East in 2005, and the football team rose to as high as #2 in the BCS rankings in 2007. They are now part of the American Athletic Conference.
The University of Tampa Spartans compete at the NCAA Division II level in the Sunshine State Conference (SSC).



Tampa is governed under the strong mayor form of government. The Mayor of Tampa is the chief executive officer of city government and is elected in four-year terms, with a limit of two consecutive terms. The current mayor is Bob Buckhorn, who took office on April 1, 2011. The City Council is a legislative body served by seven members. Four members are elected from specific numbered areas designated City Districts, and the other three are "at-large" members (serving citywide).




The City of Tampa is served by Tampa Fire Rescue. With 22 fire stations, the department provides fire and medical protection for Tampa and New Tampa, and provides support to other departments such as Tampa International Airport and Hillsborough County Fire Rescue.




The city of Tampa has a large police department that provides law enforcement services. The Tampa Police Department has over 1000 sworn officers and many civilian service support personnel.







Public primary and secondary education is operated by Hillsborough County Public Schools, officially known as the School District of Hillsborough County (SDHC). It is ranked the eighth largest school district in the United States, with around 189,469 enrolled students. SDHC runs 208 schools, 133 being elementary, 42 middle, 27 high schools, two K-8s, and four career centers. There are 73 additional schools in the district that are charter, ESE, alternative, etc. Twelve out of 27 high schools in the SDHC are included in Newsweek's list of America's Best High Schools.



Tampa's library system is operated by the Tampa-Hillsborough County Public Library System. THPLS operates 25 libraries throughout Tampa and Hillsborough County, including the John F. Germany Public Library in Downtown Tampa. The Tampa library system first started in the early 20th century, with the West Tampa Library, which was made possible with funds donated by Andrew Carnegie. Tampa's libraries are also a part of a larger library network, The Hillsborough County Public Library Cooperative, which includes the libraries of the neighboring municipalities of Temple Terrace and Plant City.




There are a number of institutions of higher education in Tampa.
The city is home to the main campus of the University of South Florida (USF), a member of the State University System of Florida founded in 1956. In 2010, it was the eleventh highest individual campus enrollment in the U.S. with over 46,000 students. The University of Tampa (UT) is a private, four-year liberal arts institution. It was founded in 1931, and in 1933, it moved into the former Tampa Bay Hotel across the Hillsborough River from downtown Tampa. "UT" has undergone several expansions in recent years, and had an enrollment of over 8000 students in 2015.
Hillsborough Community College is a two-year community college in the Florida College System with campuses in Tampa and Hillsborough County. Southern Technical College is a private two-year college that operates a campus in Tampa. Hillsborough Technical Education Centers (HiTEC) is the postsecondary extension of the local areas Public Schools district. The schools provide for a variety of technical training certification courses as well as job placement skills.
The Stetson University College of Law is located in Gulfport and has a second campus, the Tampa Law Center, in downtown Tampa. The Law Center houses the Tampa branch of Florida's Second District Court of Appeal.
Other colleges and universities in the wider Tampa Bay Area include Jersey College, Eckerd College, and St. Petersburg College in St. Petersburg.




The major daily newspaper serving the city is the Tampa Bay Times, which purchased its longtime competition, the The Tampa Tribune, in 2016. Print news coverage is also provided by a variety of smaller regional newspapers, alternative weeklies, and magazines, including the Florida Sentinel Bulletin, Creative Loafing, Reax Music Magazine, The Oracle, Tampa Bay Business Journal, MacDill Thunderbolt, and La Gaceta, which notable for being the nation's only trilingual newspaper - English, Spanish, and Italian, owing to its roots in the cigar-making immigrant neighborhood of Ybor City.
Major television stations include WFTS 28 (ABC), WTSP 10 (CBS), WFLA 8 (NBC), WTVT 13 (Fox), WTOG 44 (The CW), WTTA 38 (MyNetworkTV), WEDU 3 (PBS), WUSF-TV 16 (PBS), WMOR 32 (Independent), WXPX 66 (ION), WCLF 22 (CTN), WFTT 50 (UniMás) and WVEA 62 (Univision).
The area is served by dozens of FM and AM radio stations including WDAE, which was the first radio station in Florida when it went on the air in 1922.










Three motor vehicle bridges cross Tampa Bay to Pinellas County from Tampa city limits: the Howard Frankland Bridge (I-275), the Courtney Campbell Causeway (SR 60), and the Gandy Bridge (U.S. 92). The old Gandy Bridge was completely replaced by new spans during the 1990s, but a span of the old bridge was saved and converted into a pedestrian and biking bridge renamed The Friendship Trail. It is the longest overwater recreation trail in the world. However, the bridge was closed in 2008 due to structural problems.

There are two major expressways (toll) bringing traffic in and out of Tampa. The Lee Roy Selmon Expressway (SR 618) (formerly known as the Crosstown Expressway), runs from suburban Brandon at its eastern terminus, through Downtown Tampa, to the neighborhoods in South Tampa (near MacDill Air Force Base) at its western terminus. The Veterans Expressway (SR 589), meanwhile connects Tampa International Airport and the bay bridges to the northwestern suburbs of Carrollwood, Northdale, Westchase, Citrus Park, Cheval, and Lutz, before continuing north as the Suncoast Parkway into Pasco and Hernando counties.
Three interstate highways run through the city. Interstate 4 and Interstate 275 cut across the city and intersect near downtown. Interstate 75 runs along the east side of town for much of its route through Hillsborough County until veering to the west to bisect New Tampa.
Along with highways, major surface roads serve as main arteries of the city. These roads are Hillsborough Avenue (U.S. 92 and U.S. 41), Dale Mabry Highway (U.S. 92), Nebraska Avenue (U.S. 41/SR 45), Florida Avenue (U.S. 41 Business), Bruce B. Downs Boulevard, Fowler Avenue, Busch Boulevard, Kennedy Boulevard (SR 60), Adamo Drive, and Dr. Martin Luther King Jr. Boulevard.




Tampa is served by three airports (one in Tampa, two in the metro area) that provide significant scheduled passenger air service:
Tampa International Airport (IATA: TPA) is Tampa's main airport and the primary location for commercial passenger airline service into the Tampa Bay area. It is also a consistent favorite in surveys of the industry and the traveling public. The readers of Condé Nast Traveler have frequently placed Tampa International in their list of Best Airports, ranking it #1 in 2003, and #2 in 2008 A survey by Zagat in 2007 ranked Tampa International first among U.S. airports in overall quality. During 2008, it was the 26th-busiest airport in North America.
St. Petersburg-Clearwater International Airport (IATA: PIE) lies just across the bay from Tampa International Airport in neighboring Pinellas County. The airport has become a popular destination for discount carriers, with over 90% of its flights are on low-cost carrier Allegiant Air. A joint civil-military aviation facility, it is also home to Coast Guard Air Station Clearwater, the largest air station in the U.S. Coast Guard.
Sarasota–Bradenton International Airport (IATA: SRQ) is located in nearby Sarasota. Sarasota airport has more flights to Delta's Atlanta hub than any other city, but also serves several other large U.S. cities.



Tampa's intercity passenger rail service is based at Tampa Union Station, a historic facility, adjacent to downtown between the Channel District and Ybor City. The station is served by Amtrak's Silver Star, which calls on Tampa twice daily: southbound to Miami and northbound for New York City. Union Station also serves as the transfer hub for Amtrak Thruway Motorcoach service, offering bus connections to several cities in Southwest Florida and to Orlando.
Uceta Rail Yard on Tampa's east side services CSX as a storage and intermodal freight transport facility. Freight and container cargo operations at the city's seaports also depend upon dockside rail facilities.




The Port of Tampa is the largest port in Florida in throughput tonnage, making it one of the busiest commercial ports in North America. Petroleum and phosphate are the lead commodities, accounting for two-thirds of the 37 million tons of total bulk and general cargo handled by the port in 2009. The port is also home to Foreign Trade Zone #79, which assists companies in Tampa Bay and along the I-4 Corridor in importing, exporting, manufacturing, and distribution activities as part of the United States foreign trade zone program.
Weekly containerized cargo service is available in the Port of Tampa. Cargo service is offered by Ports America, Zim American Integrated Shipping Company, and MSC which has recently partnered with Zim. Currently 3,000 to 4,250 TEU containerships regularly call the Port of Tampa.
The bay bottom is very sandy, with the U.S. Army Corps of Engineers constantly dredging the ship channels to keep them navigable to large cargo ships.




Public mass transit in Tampa is operated by the Hillsborough Area Regional Transit Authority (HART), and includes public bus as well as a streetcar line. The HART bus system's main hub is the Marion Transit Center in Downtown Tampa, serving nearly 30 local and express routes. HART is also currently making a bus rapid transit system called MetroRapid that will run between Downtown and the University of South Florida.
The TECO Line Streetcar System runs electric streetcar service along eleven stations on a 2.7-mile (4.3 km) route, connecting Ybor City, the Channel District, the Tampa Convention Center, and downtown Tampa. The TECO Line fleet features varnished wood interiors reminiscent of late 19th and mid-20th century streetcars.
Limited transportation by privately operated "Neighborhood Electric Vehicles" (NEV) is available, primarily in Downtown Tampa and Ybor City. Water taxis are available on a charter basis for tours along the downtown waterfront and the Hillsborough River.
The Tampa Bay Area Regional Transportation Authority (TBARTA) develops bus, light rail, and other transportation options for the seven-county Tampa Bay area.




Tampa and its surrounding suburbs are host to over 20 hospitals, four trauma centers, and multiple Cancer treatment centers. Three of the area's hospitals were ranked among "America's best hospitals" by US News and World Report. Tampa is also home to many health research institutions. The major hospitals in Tampa include Tampa General Hospital, St. Joseph's Children's & Women's Hospital, James A. Haley Veterans Hospital, H. Lee Moffitt Cancer Center & Research Institute, and The Pepin Heart Institute. Shriners Hospitals for Children is based in Tampa. USF's Byrd Alzheimer's Institute is both a prominent research facility and Alzheimer's patient care center in Tampa. Along with human health care, there are hundreds of animal medical centers including a Humane Society of America.



Water in the area is managed by the Southwest Florida Water Management District. The water is mainly supplied by the Hillsborough River, which in turn arises from the Green Swamp, but several other rivers and desalination plants in the area contribute to the supply. Power is mainly generated by TECO Energy.







Tampa has formalized sister city agreements with the following cities:



Baldomero Lopez
Largest metropolitan areas in the Americas
List of public art in Tampa, Florida
United States cities by population
Seal of Tampa













Official website
Tampa Bay Convention and Visitors Bureau
Tampa Chamber of Commerce
Tampa website dedicated to historic Tampa photographs
Tampa Bay at DMOZ
Tampa Changing – Historical and modern photographs of Tampa
National Park Service Battle Description of the Battle of Fort Brooke and Ballast PointAtlanta is the capital of and the most populous city in the U.S. state of Georgia, with an estimated 2015 population of 463,878. Atlanta is the cultural and economic center of the Atlanta metropolitan area, home to 5,710,795 people and the ninth largest metropolitan area in the United States. Atlanta is the county seat of Fulton County, and a small portion of the city extends eastward into DeKalb County.
Atlanta was established in 1837 at the intersection of two railroad lines, and the city rose from the ashes of the American Civil War to become a national center of commerce. In the decades following the Civil Rights Movement, during which the city earned a reputation as "too busy to hate" for the relatively progressive views of some of its citizens and leaders compared to other cities in the Deep South Atlanta attained international prominence. Atlanta is the primary transportation hub of the Southeastern United States, via highway, railroad, and air, with Hartsfield–Jackson Atlanta International Airport being the world's busiest airport since 1998.
Atlanta is an "alpha-" or "world city", exerting a significant impact upon commerce, finance, research, technology, education, media, art, and entertainment. It ranks 36th among world cities and 8th in the nation with a gross domestic product of $270 billion. Atlanta's economy is considered diverse, with dominant sectors including logistics, professional and business services, media operations, and information technology. Topographically, Atlanta is marked by rolling hills and dense tree coverage. Revitalization of Atlanta's neighborhoods, initially spurred by the 1996 Olympics in Atlanta, has intensified in the 21st century, altering the city's demographics, politics, and culture.




Prior to the arrival of European settlers in north Georgia, Creek Indians inhabited the area. Standing Peachtree, a Creek village located where Peachtree Creek flows into the Chattahoochee River, was the closest Indian settlement to what is now Atlanta. As part of the systematic removal of Native Americans from northern Georgia from 1802 to 1825, the Creek ceded the area in 1821, and white settlers arrived the following year.

In 1836, the Georgia General Assembly voted to build the Western and Atlantic Railroad in order to provide a link between the port of Savannah and the Midwest. The initial route was to run southward from Chattanooga to a terminus east of the Chattahoochee River, which would then be linked to Savannah. After engineers surveyed various possible locations for the terminus, the "zero milepost" was driven into the ground in what is now Five Points. A year later, the area around the milepost had developed into a settlement, first known as "Terminus", and later as "Thrasherville" after a local merchant who built homes and a general store in the area. By 1842, the town had six buildings and 30 residents, and was renamed "Marthasville" to honor the Governor's daughter. J. Edgar Thomson, Chief Engineer of the Georgia Railroad, suggested the town be renamed "Atlantica-Pacifica," which was shortened to "Atlanta". The residents approved, and the town was incorporated as Atlanta on December 29, 1847.
By 1860, Atlanta's population had grown to 9,554. During the Civil War, the nexus of multiple railroads in Atlanta made the city a hub for the distribution of military supplies. In 1864, following the capture of Chattanooga, the Union Army moved southward and began its invasion of north Georgia. The region surrounding Atlanta was the location of several major army battles, culminating with the Battle of Atlanta and a four-month-long siege of the city by the Union Army under the command of General William Tecumseh Sherman. On September 1, 1864, Confederate General John Bell Hood made the decision to retreat from Atlanta, ordering all public buildings and possible assets to the Union Army destroyed. On the next day, Mayor James Calhoun surrendered Atlanta to the Union Army, and on September 7, General Sherman ordered the city's civilian population to evacuate. On November 11, 1864, in preparation of the Union Army's march to Savannah, Sherman ordered Atlanta to be burned to the ground, sparing only the city's churches and hospitals.

After the Civil War ended in 1865, Atlanta was gradually rebuilt. Due to the city's superior rail transportation network, the state capital was moved to Atlanta from Milledgeville in 1868. In the 1880 Census, Atlanta surpassed Savannah as Georgia's largest city. Beginning in the 1880s, Henry W. Grady, the editor of the Atlanta Constitution newspaper, promoted Atlanta to potential investors as a city of the "New South" that would be based upon a modern economy and less reliant on agriculture. By 1885, the founding of the Georgia School of Technology (now Georgia Tech) and the city's black colleges had established Atlanta as a center for higher education. In 1895, Atlanta hosted the Cotton States and International Exposition, which attracted nearly 800,000 attendees and successfully promoted the New South's development to the world.
During the first decades of the 20th century, Atlanta experienced a period of unprecedented growth. In three decades' time, Atlanta's population tripled as the city limits expanded to include nearby streetcar suburbs; the city's skyline emerged with the construction of the Equitable, Flatiron, Empire, and Candler buildings; and Sweet Auburn emerged as a center of black commerce. However, the period was also marked by strife and tragedy. Increased racial tensions led to the Atlanta Race Riot of 1906, which left at least 27 people dead and over 70 injured. In 1915, Leo Frank, a Jewish-American factory superintendent, convicted of murder, was hanged in Marietta by a lynch mob, drawing attention to antisemitism in the United States. On May 21, 1917, the Great Atlanta Fire destroyed 1,938 buildings in what is now the Old Fourth Ward, resulting in one fatality and the displacement of 10,000 people.

On December 15, 1939, Atlanta hosted the film premiere of Gone with the Wind, the epic film based on the best-selling novel by Atlanta's Margaret Mitchell. The film's legendary producer, David O. Selznick, as well as the film's stars Clark Gable, Vivien Leigh, and Olivia de Havilland attended the gala event at Loew's Grand Theatre, but Oscar winner Hattie McDaniel, an African American, was barred from the event due to racial segregation laws and policies.
Atlanta played a vital role in the Allied effort during World War II due the city's war-related manufacturing companies, railroad network, and military bases, leading to rapid growth in the city's population and economy. In the 1950s, the city's newly constructed freeway system allowed middle class Atlantans the ability to relocate to the suburbs. As a result, the city began to make up an ever-smaller proportion of the metropolitan area's population.
During the 1960s, Atlanta was a major organizing center of the Civil Rights Movement, with Dr. Martin Luther King, Jr., Ralph David Abernathy, and students from Atlanta's historically black colleges and universities playing major roles in the movement's leadership. While minimal compared to other cities, Atlanta was not completely free of racial strife. In 1961, the city attempted to thwart blockbusting by erecting road barriers in Cascade Heights, countering the efforts of civic and business leaders to foster Atlanta as the "city too busy to hate". Desegregation of the public sphere came in stages, with public transportation desegregated by 1959, the restaurant at Rich's department store by 1961, movie theaters by 1963, and public schools by 1973.

In 1960, whites comprised 61.7% of the city's population. By 1970, African Americans were a majority of the city's population and exercised new-found political influence by electing Atlanta's first black mayor, Maynard Jackson, in 1973. Under Mayor Jackson's tenure, Atlanta's airport was modernized, solidifying the city's role as a transportation center. The opening of the Georgia World Congress Center in 1976 heralded Atlanta's rise as a convention city. Construction of the city's subway system began in 1975, with rail service commencing in 1979. However, despite these improvements, Atlanta lost over 100,000 residents between 1970 and 1990, over 20% of its population.
In 1990, Atlanta was selected as the site for the 1996 Summer Olympic Games. Following the announcement, the city government undertook several major construction projects to improve Atlanta's parks, sporting venues, and transportation infrastructure. While the games themselves were marred by numerous organizational inefficiencies, as well as the Centennial Olympic Park bombing, they were a watershed event in Atlanta's history, initiating a fundamental transformation of the city in the decade that followed.
During the 2000s, Atlanta underwent a profound transformation demographically, physically, and culturally. Suburbanization, a booming economy, and new migrants decreased the city's black percentage from a high of 67% in 1990 to 54% in 2010. From 2000 to 2010, Atlanta gained 22,763 white residents, 5,142 Asian residents, and 3,095 Hispanic residents, while the city's black population decreased by 31,678. Much of the city's demographic change during the decade was driven by young, college-educated professionals: from 2000 to 2009, the three-mile radius surrounding Downtown Atlanta gained 9,722 residents aged 25 to 34 holding at least a four-year degree, an increase of 61%. Between the mid-1990s and 2010, stimulated by funding from the HOPE VI program, Atlanta demolished nearly all of its public housing, a total of 17,000 units and about 10% of all housing units in the city. In 2005, the $2.8 billion BeltLine project was adopted, with the stated goals of converting a disused 22-mile freight railroad loop that surrounds the central city into an art-filled multi-use trail and increasing the city's park space by 40%. Lastly, Atlanta's cultural offerings expanded during the 2000s: the High Museum of Art doubled in size; the Alliance Theatre won a Tony Award; and numerous art galleries were established on the once-industrial Westside.




Atlanta encompasses 134.0 square miles (347.1 km2), of which 133.2 square miles (344.9 km2) is land and 0.85 square miles (2.2 km2) is water. The city is situated among the foothills of the Appalachian Mountains, and at 1,050 feet (320 m) above mean sea level, Atlanta has the highest elevation of major cities east of the Mississippi River. Atlanta straddles the Eastern Continental Divide, such that rainwater that falls on the south and east side of the divide flows into the Atlantic Ocean, while rainwater on the north and west side of the divide flows into the Gulf of Mexico. Atlanta sits atop a ridge south of the Chattahoochee River, which is part of the ACF River Basin. Located at the far northwestern edge of the city, much of the river's natural habitat is preserved, in part by the Chattahoochee River National Recreation Area.




Most of Atlanta was burned during the Civil War, depleting the city of a large stock of its historic architecture. Yet architecturally, the city had never been particularly "southern"—because Atlanta originated as a railroad town, rather than a patrician southern seaport like Savannah or Charleston, many of the city's landmarks could have easily been erected in the Northeast or Midwest.

During the Cold War era, Atlanta embraced global modernist trends, especially regarding commercial and institutional architecture. Examples of modernist architecture include the 1,196,240sq.ft Westin Peachtree Plaza (1976), Georgia-Pacific Tower (1982), the State of Georgia Building (1966), and the Atlanta Marriott Marquis (1985). In the latter half of the 1980s, Atlanta became one of the early adopters of postmodern designs that reintroduced classical elements to the cityscape. Many of Atlanta's tallest skyscrapers were built in the late 1980s and early 1990s, with most displaying tapering spires or otherwise ornamented crowns, such as the 1,187,676 sq.ftOne Atlantic Center (1987), 191 Peachtree Tower (1991), and the Four Seasons Hotel Atlanta (1992). Also completed during the era is Atlanta's tallest skyscraper, the Bank of America Plaza (1992), which, at 1,023 feet (312 m), is the 61st-tallest building in the world and the 9th-tallest building in the United States. The Bank of America Plaza is the tallest building outside of New York City and Chicago, and was the last building built in the United States to be in the top 10 tallest buildings in the world until One World Trade Center was completed externally in May 2013. The city's embrace of modern architecture, however, translated into an ambivalent approach toward historic preservation, leading to the destruction of notable architectural landmarks, including the Equitable Building (1892–1971), Terminal Station (1905–1972), and the Carnegie Library (1902–1977). The Fox Theatre (1929)—Atlanta's cultural icon—would have met the same fate had it not been for a grassroots effort to save it in the mid-1970s.
Atlanta is divided into 242 officially defined neighborhoods. The city contains three major high-rise districts, which form a north-south axis along Peachtree: Downtown, Midtown, and Buckhead. Surrounding these high-density districts are leafy, low-density neighborhoods, most of which are dominated by single-family homes.
Downtown Atlanta contains the most office space in the metro area, much of it occupied by government entities. Downtown is also home to the city's sporting venues and many of its tourist attractions. Midtown Atlanta is the city's second-largest business district, containing the offices of many of the region's law firms. Midtown is also known for its art institutions, cultural attractions, institutions of higher education, and dense form. Buckhead, the city's uptown district, is eight miles (13 km) north of Downtown and the city's third-largest business district. The district is marked by an urbanized core along Peachtree Road, surrounded by suburban single-family neighborhoods situated among dense forests and rolling hills.

Surrounding Atlanta's three high-rise districts are the city's low- and medium-density neighborhoods, where the craftsman bungalow single-family home is dominant. The eastside is marked by historic streetcar suburbs built from the 1890s-1930s as havens for the upper middle class. These neighborhoods, many of which contain their own villages encircled by shaded, architecturally-distinct residential streets, include the Victorian Inman Park, Bohemian East Atlanta, and eclectic Old Fourth Ward. On the westside, former warehouses and factories have been converted into housing, retail space, and art galleries, transforming the once-industrial West Midtown into a model neighborhood for smart growth, historic rehabilitation, and infill construction. In southwest Atlanta, neighborhoods closer to downtown originated as streetcar suburbs, including the historic West End, while those farther from downtown retain a postwar suburban layout, including Collier Heights and Cascade Heights, home to much of the city's affluent African American population. Northwest Atlanta contains the areas of the city to west of Marietta Boulevard and to the north of Martin Luther King, Jr. Drive, including those neighborhoods remote to downtown, such as Riverside, Bolton and Whittier Mill, which is one of Atlanta's designated Landmark Historical Neighborhoods. Vine City, though technically Northwest, adjoins the city's Downtown area and has recently been the target of community outreach programs and economic development initiatives.
Gentrification of the city's neighborhoods is one of the more controversial and transformative forces shaping contemporary Atlanta. The gentrification of Atlanta has its origins in the 1970s, after many of Atlanta's neighborhoods had undergone the urban decay that affected other major American cities in the mid-20th century. When neighborhood opposition successfully prevented two freeways from being built through city's the east side in 1975, the area became the starting point for Atlanta's gentrification. After Atlanta was awarded the Olympic games in 1990, gentrification expanded into other parts of the city, stimulated by infrastructure improvements undertaken in preparation for the games. Gentrification was also aided by the Atlanta Housing Authority's eradication of the city's public housing.




Under the Köppen classification, Atlanta has a humid subtropical climate (Cfa) with four distinct seasons and generous precipitation year-round, typical for the inland South. Summers are hot and humid, with temperatures somewhat moderated by the city's elevation. Winters are cool but variable, with an average of 48 freezing days per year and temperatures dropping to 0 °F (−17.8 °C) on rare occasions. Warm air from the Gulf of Mexico can bring spring-like highs while strong Arctic air masses can push lows into the teens (≤ −7 °C).
July averages 80.2 °F (26.8 °C), with high temperatures reaching 90 °F (32 °C) on an average 44 days per year, though 100 °F (38 °C) readings are not seen most years. January averages 43.5 °F (6.4 °C), with temperatures in the suburbs slightly cooler due largely to the urban heat island effect. Lows at or below freezing can be expected 40 nights annually, but extended stretches with daily high temperatures below 40 °F (4 °C) are very rare, with a recent exception in January 2014. Extremes range from −9 °F (−23 °C) on February 13, 1899 to 106 °F (41 °C) on June 30, 2012. Dewpoints in the summer range from 63.6 °F (18 °C) in June to 67.8 °F (20 °C) in July.
Typical of the southeastern U.S., Atlanta receives abundant rainfall that is relatively evenly distributed throughout the year, though spring and early fall are markedly drier. The average annual rainfall is 50.2 inches (1,280 mm), while snowfall is typically light at around 2.1 inches (5.3 cm) per year. The heaviest single snowfall occurred on January 23, 1940, with around 10 inches (25 cm) of snow. However, ice storms usually cause more problems than snowfall does, the most severe occurring on January 7, 1973. Tornadoes are rare in the city itself, but the March 15, 2008 EF2 tornado damaged prominent structures in downtown Atlanta.




The 2010 United States Census reported that Atlanta had a population of 420,003. The population density was 3,154 per square mile (1232/km2). The racial makeup and population of Atlanta was 54.0% Black or African American, 38.4% White, 3.1% Asian and 0.2% Native American. Those from some other race made up 2.2% of the city's population, while those from two or more races made up 2.0%. Hispanics of any race made up 5.2% of the city's population. The median income for a household in the city was $45,171. The per capita income for the city was $35,453. 22.6% percent of the population was living below the poverty line. However, compared to the rest of the country, Atlanta's cost of living is 6.00% lower than the U.S. average. Atlanta has one of the highest LGBT populations per capita, ranking third among major American cities, behind San Francisco and slightly behind Seattle, with 12.8% of the city's total population identifying as gay, lesbian, or bisexual. 7.3% of Atlantans were born abroad.
In the 2010 Census, Atlanta was recorded as the nation's fourth-largest majority-black city. It has long been known as a center of African-American political power, education, and culture, often called a black mecca. African-American residents of Atlanta have followed whites to newer housing in the suburbs in the early 21st century. From 2000 to 2010, the city's black population decreased by 31,678 people, shrinking from 61.4% of the city's population in 2000 to 54.0% in 2010.
At the same time, the white population of Atlanta has increased. Between 2000 and 2010, the proportion of whites in the city's population grew faster than that of any other U.S. city. In that decade, Atlanta's white population grew from 31% to 38% of the city's population, an absolute increase of 22,753 people, more than triple the increase that occurred between 1990 and 2000.
Out of the total population five years and older, 83.3% spoke only English at home, while 8.8% spoke Spanish, 3.9% another Indo-European language, and 2.8% an Asian language. Atlanta's dialect has traditionally been a variation of Southern American English. The Chattahoochee River long formed a border between the Coastal Southern and Southern Appalachian dialects. Because of the development of corporate headquarters in the region, attracting migrants from other areas of the country, by 2003, Atlanta magazine concluded that Atlanta had become significantly "de-Southernized." A Southern accent was considered a handicap in some circumstances. In general, Southern accents are less prevalent among residents of the city and inner suburbs and among younger people; they are more common in the outer suburbs and among older people. At the same time, residents of the city express Southern variations of African American Vernacular English.
Religion in Atlanta, while historically centered on Protestant Christianity, now involves many faiths as a result of the city and metro area's increasingly international population. Protestant Christianity still maintains a strong presence in the city (63%), but in recent decades the Catholic Church has increased in numbers and influence because of new migrants in the region. Metro Atlanta also has numerous ethnic or national Christian congregations, including Korean and Indian churches. The larger non-Christian faiths are Judaism, Islam and Hinduism. Overall, there are over 1,000 places of worship within Atlanta.




Encompassing $304 billion, the Atlanta metropolitan area is the eighth-largest economy in the country and 17th-largest in the world. Corporate operations comprise a large portion of the Atlanta's economy, with the city serving as the regional, national, or global headquarters for many corporations. Atlanta contains the country's third largest concentration of Fortune 500 companies, and the city is the global headquarters of corporations such as The Coca-Cola Company, The Home Depot, Delta Air Lines, AT&T Mobility, Chick-fil-A, UPS, and Newell-Rubbermaid. Over 75 percent of Fortune 1000 companies conduct business operations in the Atlanta metropolitan area, and the region hosts offices of about 1,250 multinational corporations. Many corporations are drawn to Atlanta on account of the city's educated workforce; as of 2014, 45% of adults 25 or older in the city of Atlanta have at least 4-year college degrees, compared to 28% in the nation as a whole.

Atlanta began as a railroad town and logistics has remained a major component of the city's economy to this day. Atlanta is an important rail junction and contains major classification yards for Norfolk Southern and CSX. Since its construction in the 1950s, Hartsfield-Jackson Atlanta International Airport has served as a key engine of Atlanta's economic growth. Delta Air Lines, the city's largest employer and the metro area's third largest, operates the world's largest airline hub at Hartsfield-Jackson and has helped make it the world's busiest airport, both in terms of passenger traffic and aircraft operations. Partly due to the airport, Atlanta has become a hub for diplomatic missions; as of 2012, the city contains 25 general consulates, the seventh-highest concentration of diplomatic missions in the United States.
Media is also an important aspect of Atlanta's economy. The city is a major cable television programming center. Ted Turner established the headquarters of both the Cable News Network (CNN) and the Turner Broadcasting System (TBS) in Atlanta. Cox Enterprises, the country's third-largest cable television service and the publisher of over a dozen major American newspapers, is headquartered in the city. The Weather Channel, owned by NBCUniversal, Bain Capital, and The Blackstone Group, is headquartered just outside Atlanta in Cobb County.
Information technology, an economic sector that includes publishing, software development, entertainment and data processing has garnered a larger percentage of Atlanta's economic output. Indeed, Atlanta has been nicknamed the Silicon peach due to its burgeoning technology sector. As of 2013, Atlanta contains the fourth-largest concentration of information technology jobs in the United States, numbering 85,000. Atlanta also ranks as the sixth fastest-growing city for information technology jobs, with an employment growth of 4.8% in 2012 and a three-year growth near 9%, or 16,000 jobs. Information technology companies are drawn to Atlanta's lower costs and educated workforce.
Largely due to a statewide tax incentive enacted in 2005, the Georgia Entertainment Industry Investment Act, which awards qualified productions a transferable income tax credit of 20% of all in-state costs for film and television investments of $500,000 or more, Atlanta has become a center for film and television production. Film and television production facilities in Atlanta include Turner Studios, Pinewood Studios (Pinewood Atlanta), Tyler Perry Studios, Williams Street Productions, and the EUE/Screen Gems soundstages. Film and television production injected $6 billion into Georgia's economy in 2015, with Atlanta garnering most of the projects. Atlanta has gained recognition as a center of production of horror and zombie-related productions, with Atlanta magazine dubbing the city the "Zombie Capital of the World".

Compared to other American cities, Atlanta's economy has been disproportionately affected by the 2008 financial crisis and subsequent recession, with the city's economy earning a ranking of 68 among 100 American cities in a September 2014 report due to an elevated unemployment rate, declining real income levels, and a depressed housing market. From 2010 to 2011, Atlanta saw a 0.9% contraction in employment and only a 0.4% rise in income. Though unemployment had dropped to 7% by late 2014, this was still higher than the national unemployment rate of 5.8% Atlanta's housing market has also struggled, with home prices falling by 2.1% in January 2012, reaching levels not seen since 1996. Compared with a year earlier, the average home price in Atlanta fell 17.3% in February 2012, the largest annual drop in the history of the index for any city. The collapse in home prices has led some economists to deem Atlanta the worst housing market in the country. Nevertheless, in August 2013, Atlanta appeared on Forbes magazine's list of the Best Places for Business and Careers.




Atlanta, while located in the South, has a culture that is no longer strictly Southern. This is due to a large population of migrants from other parts of the U.S., in addition to many recent immigrants to the U.S. who have made the metropolitan area their home, establishing Atlanta as the cultural and economic hub of an increasingly multi-cultural metropolitan area. Thus, although traditional Southern culture is part of Atlanta's cultural fabric, it is mostly the backdrop to one of the nation's most cosmopolitan cities. This unique cultural combination reveals itself in the arts district of Midtown, the quirky neighborhoods on the city's eastside, and the multi-ethnic enclaves found along Buford Highway.




Atlanta is one of few United States cities with permanent, professional, resident companies in all major performing arts disciplines: opera (Atlanta Opera), ballet (Atlanta Ballet), orchestral music (Atlanta Symphony Orchestra), and theater (the Alliance Theatre). Atlanta also attracts many touring Broadway acts, concerts, shows, and exhibitions catering to a variety of interests. Atlanta's performing arts district is concentrated in Midtown Atlanta at the Woodruff Arts Center, which is home to the Atlanta Symphony Orchestra and the Alliance Theatre. The city also frequently hosts touring Broadway acts, especially at The Fox Theatre, a historic landmark that is among the highest grossing theatres of its size.
As a national center for the arts, Atlanta is home to significant art museums and institutions. The renowned High Museum of Art is arguably the South's leading art museum and among the most-visited art museums in the world. The Museum of Design Atlanta (MODA), a design museum, is the only such museum in the Southeast. Contemporary art museums include the Atlanta Contemporary Art Center and the Museum of Contemporary Art of Georgia. Institutions of higher education also contribute to Atlanta's art scene, with the Savannah College of Art and Design's Atlanta campus providing the city's arts community with a steady stream of curators, and Emory University's Michael C. Carlos Museum containing the largest collection of ancient art in the Southeast.




Atlanta has played a major or contributing role in the development of various genres of American music at different points in the city's history. Beginning as early as the 1920s, Atlanta emerged as a center for country music, which was brought to the city by migrants from Appalachia. During the countercultural 1960s, Atlanta hosted the Atlanta International Pop Festival, with the 1969 festival taking place more than a month before Woodstock and featuring many of the same bands. The city was also a center for Southern rock during its 1970s heyday: the Allman Brothers Band's hit instrumental "Hot 'Lanta" is an ode to the city, while Lynyrd Skynyrd's famous live rendition of "Free Bird" was recorded at the Fox Theatre in 1976, with lead singer Ronnie Van Zant directing the band to "play it pretty for Atlanta". During the 1980s, Atlanta had an active Punk rock scene that was centered on two of the city's music venues, 688 Club and the Metroplex, and Atlanta famously played host to the Sex Pistols first U.S. show, which was performed at the Great Southeastern Music Hall. The 1990s saw the birth of Atlanta hip hop, a subgenre that gained relevance following the success of home-grown duo OutKast; however, it was not until the 2000s that Atlanta moved "from the margins to becoming hip-hop's center of gravity, part of a larger shift in hip-hop innovation to the South". Also in the 2000s, Atlanta was recognized by the Brooklyn-based Vice magazine for its impressive yet under-appreciated Indie rock scene, which revolves around the various live music venues found on the city's alternative eastside.




As of 2010, Atlanta is the seventh-most visited city in the United States, with over 35 million visitors per year. Although the most popular attraction among visitors to Atlanta is the Georgia Aquarium, the world's largest indoor aquarium, Atlanta's tourism industry is mostly driven by the city's history museums and outdoor attractions. Atlanta contains a notable amount of historical museums and sites, including the Martin Luther King, Jr. National Historic Site, which includes the preserved childhood home of Dr. Martin Luther King, Jr., as well as his final resting place; the Atlanta Cyclorama & Civil War Museum, which houses a massive painting and diorama in-the-round, with a rotating central audience platform, depicting the Battle of Atlanta in the Civil War; the World of Coca-Cola, featuring the history of the world-famous soft drink brand and its well-known advertising; the College Football Hall of Fame which honors college football and its athletes; the National Center for Civil and Human Rights, which explores the Civil Rights Movement and its connection to contemporary human rights movements throughout the world; the Carter Center and Presidential Library, housing U.S. President Jimmy Carter's papers and other material relating to the Carter administration and the Carter family's life; and the Margaret Mitchell House and Museum, where Mitchell wrote the best-selling novel Gone with the Wind.
Atlanta also contains various outdoor attractions. The Atlanta Botanical Garden, adjacent to Piedmont Park, is home to the 600-foot-long (180 m) Kendeda Canopy Walk, a skywalk that allows visitors to tour one of the city's last remaining urban forests from 40-foot-high (12 m). The Canopy Walk is considered the only canopy-level pathway of its kind in the United States. Zoo Atlanta, located in Grant Park, accommodates over 1,300 animals representing more than 220 species. Home to the nation's largest collections of gorillas and orangutans, the Zoo is also one of only four zoos in the U.S. to house giant pandas. Festivals showcasing arts and crafts, film, and music, including the Atlanta Dogwood Festival, the Atlanta Film Festival, and Music Midtown, respectively, are also popular with tourists.

Tourists are also drawn to the city's culinary scene, which comprises a mix of urban establishments garnering national attention, ethnic restaurants serving cuisine from every corner of the world, and traditional eateries specializing in Southern dining. Since the turn of the 21st century, Atlanta has emerged as a sophisticated restaurant town. Many restaurants opened in the city's gentrifying neighborhoods have received praise at the national level, including Bocado, Bacchanalia, and Miller Union in West Midtown, Empire State South in Midtown, and Two Urban Licks and Rathbun's on the east side. In 2011, the New York Times characterized Empire State South and Miller Union as reflecting "a new kind of sophisticated Southern sensibility centered on the farm but experienced in the city." Visitors seeking to sample international Atlanta are directed to Buford Highway, the city's international corridor. There, the million-plus immigrants that make Atlanta home have established various authentic ethnic restaurants representing virtually every nationality on the globe. For traditional Southern fare, one of the city's most famous establishments is The Varsity, a long-lived fast food chain and the world's largest drive-in restaurant. Mary Mac's Tea Room and Paschal's are more formal destinations for Southern food.




Atlanta is home to professional franchises for three major team sports: the Atlanta Braves of Major League Baseball, the Atlanta Hawks of the National Basketball Association, and the Atlanta Falcons of the National Football League. The Braves, who moved to Atlanta in 1966, were established as the Boston Red Stockings in 1871 and are the oldest continually operating professional sports franchise in the United States. The Braves won the World Series in 1995, and had an unprecedented run of 14 straight divisional championships from 1991 to 2005.
The Atlanta Falcons have played in Atlanta since their inception in 1966. The Falcons have won the division title five times (1980, 1998, 2004, 2010, 2012) and the conference championship once, when they finished as the runner-up to the Denver Broncos in Super Bowl XXXIII in 1999. The Atlanta Hawks began in 1946 as the Tri-Cities Blackhawks, playing in Moline, Illinois. The team moved to Atlanta in 1968, and they currently play their games in Philips Arena. The Atlanta Dream is the city's Women's National Basketball Association franchise.
Atlanta has also had its own professional ice hockey and soccer franchises. The National Hockey League (NHL) has had two Atlanta franchises: the Atlanta Flames began play in 1972 before moving to Calgary in 1980, while the Atlanta Thrashers began play in 1999 before moving to Winnipeg in 2011. The Atlanta Chiefs was the city's professional soccer team from 1967 to 1972, and the team won a national championship in 1968. In 1998 another professional soccer team was formed, the Atlanta Silverbacks of the North American Soccer League. In April 2014, a Major League Soccer team, Atlanta United FC, was formed as an expansion team to begin play in 2017.
Atlanta has been the host city for various international, professional and collegiate sporting events. Most famously, Atlanta hosted the Centennial 1996 Summer Olympics. Atlanta has also hosted Super Bowl XXVIII in 1994 and Super Bowl XXXIV in 2000. In professional golf, The Tour Championship, the final PGA Tour event of the season, is played annually at East Lake Golf Club. In 2001 and 2011, Atlanta hosted the PGA Championship, one of the four major championships in men's professional golf, at the Atlanta Athletic Club. In professional ice hockey, the city hosted the 56th NHL All-Star Game in 2008, three years before the Thrashers moved. In 2011, Atlanta hosted professional wrestling's annual WrestleMania. The city has hosted the NCAA Final Four Men's Basketball Championship four times, most recently in 2013. In college football, Atlanta hosts the Chick-fil-A Kickoff Game, the SEC Championship Game, and the Chick-fil-A Peach Bowl.




Atlanta's 343 parks, nature preserves, and gardens cover 3,622 acres (14.66 km2), which amounts to only 5.6% of the city's total acreage, compared to the national average of just over 10%. However, 64% of Atlantans live within a 10-minute walk of a park, a percentage equal to the national average. Furthermore, in its 2013 ParkScore ranking, The Trust for Public Land, a national land conservation organization, reported that among the park systems of the 50 most populous U.S. cities, Atlanta's park system received a ranking of 31. Piedmont Park, located in Midtown is Atlanta's most iconic green space. The park, which has undergone a major renovation and expansion in recent years, attracts visitors from across the region and hosts cultural events throughout the year. Other notable city parks include Centennial Olympic Park, a legacy of the 1996 Summer Olympics that forms the centerpiece of the city's tourist district; Woodruff Park, which anchors the campus of Georgia State University; Grant Park, home to Zoo Atlanta; and Chastain Park, which houses an amphitheater used for live music concerts. The Chattahoochee River National Recreation Area, located in the northwestern corner of the city, preserves a 48 mi (77 km) stretch of the river for public recreation opportunities. The Atlanta Botanical Garden, adjacent to Piedmont Park, contains formal gardens, including a Japanese garden and a rose garden, woodland areas, and a conservatory that includes indoor exhibits of plants from tropical rainforests and deserts. The BeltLine, a former rail corridor that forms a 22 mi (35 km) loop around Atlanta's core, will eventually be transformed into a series of parks, connected by a multi-use trail, increasing Atlanta's park space by 40%.
Atlanta offers resources and opportunities for amateur and participatory sports and recreation. Jogging is a particularly popular local sport. The Peachtree Road Race, the world's largest 10 km race, is held annually on Independence Day. The Georgia Marathon, which begins and ends at Centennial Olympic Park, routes through the city's historic east side neighborhoods. Golf and tennis are also popular in Atlanta, and the city contains six public golf courses and 182 tennis courts. Facilities located along the Chattahoochee River cater to watersports enthusiasts, providing the opportunity for kayaking, canoeing, fishing, boating, or tubing. The city's only skate park, a 15,000 square feet (1,400 m2) facility that offers bowls, curbs, and smooth-rolling concrete mounds, is located at Historic Fourth Ward Park.




Atlanta is governed by a mayor and the Atlanta City Council. The city council consists of 15 representatives—one from each of the city's 12 districts and three at-large positions. The mayor may veto a bill passed by the council, but the council can override the veto with a two-thirds majority. The mayor of Atlanta is Kasim Reed, a Democrat elected on a nonpartisan ballot whose first term in office expired at the end of 2013. Reed was elected to a second term on November 5, 2013. Every mayor elected since 1973 has been black. In 2001, Shirley Franklin became the first woman to be elected Mayor of Atlanta, and the first African-American woman to serve as mayor of a major southern city. Atlanta city politics suffered from a notorious reputation for corruption during the 1990s administration of Bill Campbell, who was convicted by a federal jury in 2006 on three counts of tax evasion in connection with gambling income he received while Mayor during trips he took with city contractors.
As the state capital, Atlanta is the site of most of Georgia's state government. The Georgia State Capitol building, located downtown, houses the offices of the governor, lieutenant governor and secretary of state, as well as the General Assembly. The Governor's Mansion is located in a residential section of Buckhead. Atlanta serves as the regional hub for many arms of the federal bureaucracy, including the Federal Reserve Bank of Atlanta and the Centers for Disease Control and Prevention. Atlanta also plays an important role in federal judiciary system, containing the United States Court of Appeals for the Eleventh Circuit and of the United States District Court for the Northern District of Georgia.
Historically, Atlanta has been a stronghold for the Democratic Party. Although municipal elections are officially nonpartisan, nearly all of the city's elected officials are registered Democrats. The city is split among 14 state house districts and four state senate districts, all held by Democrats. At the federal level, Atlanta is split between two congressional districts. The northern three-fourths of the city is located in the 5th district, represented by Democrat John Lewis. The southern fourth is in the 13th district, represented by Democrat David Scott.
The city is served by the Atlanta Police Department, which numbers 2,000 officers and oversaw a 40% decrease in the city's crime rate between 2001 and 2009. Specifically, homicide decreased by 57%, rape by 72%, and violent crime overall by 55%. Crime is down across the country, but Atlanta's improvement has occurred at more than twice the national rate. Nevertheless, Forbes ranked Atlanta as the sixth most dangerous city in the United States in 2012.




Due to the more than 30 colleges and universities located in the city, Atlanta is considered a center for higher education. Among the most prominent public universities in Atlanta is the Georgia Institute of Technology, a research university located in Midtown that has been consistently ranked among the nation's top ten public universities for its degree programs in engineering, computing, management, the sciences, architecture, and liberal arts. Georgia State University, a public research university located in Downtown Atlanta, is the largest of the 29 public colleges and universities in the University System of Georgia and a major contributor to the revitalization of the city's central business district. Atlanta is also home to nationally renowned private colleges and universities, most notably Emory University, a leading liberal arts and research institution that ranks among the top 20 schools in the United States and operates Emory Healthcare, the largest health care system in Georgia.  Also located in the city is the Atlanta University Center, the largest contiguous consortium of historically black colleges, comprising Spelman College, Clark Atlanta University, Morehouse College, Morehouse School of Medicine, and Interdenominational Theological Center. Atlanta also contains a campus of the Savannah College of Art and Design, a private art and design university that has proven to be a major factor in the recent growth of Atlanta's visual art community.
Atlanta Public Schools enrolls 55,000 students in 106 schools, some of which are operated as charter schools. The district has been plagued by a widely publicized cheating scandal exposed in 2009. Atlanta is also served by many private schools, including parochial Roman Catholic schools operated by the Archdiocese of Atlanta.




The primary network-affiliated television stations in Atlanta are WXIA-TV (NBC), WGCL-TV (CBS), WSB-TV (ABC), and WAGA-TV (FOX). The Atlanta metropolitan area is served by two public television stations and one public radio station. WGTV is the flagship station of the statewide Georgia Public Television network and is a PBS member station, while WPBA is owned by Atlanta Public Schools. Georgia Public Radio is listener-funded and comprises one NPR member station, WABE, a classical music station operated by Atlanta Public Schools.
Atlanta is served by the Atlanta Journal-Constitution, its only major daily newspaper with wide distribution. The Atlanta Journal-Constitution is the result of a 1950 merger between The Atlanta Journal and The Atlanta Constitution, with staff consolidation occurring in 1982 and separate publication of the morning Constitution and afternoon Journal ceasing in 2001. Alternative weekly newspapers include Creative Loafing, which has a weekly print circulation of 80,000. Atlanta magazine is an award-winning, monthly general-interest magazine based in and covering Atlanta.




Atlanta's transportation infrastructure comprises a complex network that includes a heavy rail rapid transit system, a light rail streetcar loop, a multi-county bus system, Amtrak service via the Crescent, multiple freight train lines, an Interstate Highway System, several airports, including the world's busiest, and over 45 miles (72 kilometres) of bike paths.
With a network of freeways that radiate out from the city, automobiles are the dominant mode of transportation in the region. Three major interstate highways converge in Atlanta: I-20 (east-west), I-75 (northwest-southeast), and I-85 (northeast-southwest). The latter two combine in the middle of the city to form the Downtown Connector (I-75/85), which carries more than 340,000 vehicles per day and is one of the most congested segments of interstate highway in the United States. Atlanta is mostly encircled by Interstate 285, a beltway locally known as "the Perimeter" that has come to mark the boundary between "Inside the Perimeter" (ITP), the city and close-in suburbs, and "Outside the Perimeter" (OTP), the outer suburbs and exurbs. The heavy reliance on automobiles for transportation in Atlanta has resulted in traffic, commute, and air pollution rates that rank among the worst in the country.
The Metropolitan Atlanta Rapid Transit Authority (MARTA) provides public transportation in the form of buses and heavy rail. Notwithstanding heavy automotive usage in Atlanta, the city's subway system is the eighth busiest in the country. MARTA rail lines connect many key destinations, such as the airport, Downtown, Midtown, Buckhead, and Perimeter Center. However, significant destinations, such as Emory University and Cumberland, remain unserved. As a result, a 2012 Brookings Institution study placed Atlanta 87th of 100 metro areas for transit accessibility. Emory University operates its Cliff shuttle buses with 200,000 boardings per month, while private minibuses supply Buford Highway. Amtrak, the national rail passenger system, provides service to Atlanta via the Crescent train (New York–New Orleans), which stops at Peachtree Station. In 2014, the Atlanta Streetcar opened to the public. The streetcar's line, which is also known as the Downtown Loop, runs 2.7 miles around the downtown tourist areas of Peachtree Center, Centennial Olympic Park, the Martin Luther King, Jr. National Historic Site, and Sweet Auburn. The Atlanta Streetcar line is also being expanded on in the coming years to include a wider range of Atlanta's neighborhoods and important places of interest, with a total of over 50 miles of track in the plan.
Hartsfield-Jackson Atlanta International Airport, the world's busiest airport as measured by passenger traffic and aircraft traffic, offers air service to over 150 U.S. destinations and more than 80 international destinations in 52 countries, with over 2,700 arrivals and departures daily. Delta Air Lines maintains its largest hub at the airport. Situated 10 miles (16 km) south of downtown, the airport covers most of the land inside a wedge formed by Interstate 75, Interstate 85, and Interstate 285.
Cycling is a growing mode of transportation in Atlanta, more than doubling since 2009, when it comprised 1.1% of all commutes (up from 0.3% in 2000). Although Atlanta's lack of bike lanes and hilly topography may deter many residents from cycling, the city's transportation plan calls for the construction of 226 miles (364 kilometres) of bike lanes by 2020, with the BeltLine helping to achieve this goal. In 2012, Atlanta's first "bike track" was constructed on 10th Street in Midtown. The two lane bike track runs from Monroe Drive west to Charles Allen Drive, with connections to the Beltline and Piedmont Park. Starting in June 2016, Atlanta received a bike sharing program with 100 bikes in Downtown, with 500 more being expected by the end of the year.




Atlanta has a reputation as a "city in a forest" due to an abundance of trees that is rare among major cities. The city's main street is named after a tree, and beyond the Downtown, Midtown, and Buckhead business districts, the skyline gives way to a dense canopy of woods that spreads into the suburbs. The city is home to the Atlanta Dogwood Festival, an annual arts and crafts festival held one weekend during early April, when the native dogwoods are in bloom. However, the nickname is also factually accurate, as the city's tree coverage percentage is at 36%, the highest out of all major American cities, and above the national average of 27%. Atlanta's tree coverage does not go unnoticed—it was the main reason cited by National Geographic in naming Atlanta a "Place of a Lifetime".
The city's lush tree canopy, which filters out pollutants and cools sidewalks and buildings, has increasingly been under assault from man and nature due to heavy rains, drought, aged forests, new pests, and urban construction. A 2001 study found that Atlanta's heavy tree cover declined from 48% in 1974 to 38% in 1996. However, the problem is being addressed by community organizations and city government: Trees Atlanta, a non-profit organization founded in 1985, has planted and distributed over 75,000 shade trees in the city, while Atlanta's government has awarded $130,000 in grants to neighborhood groups to plant trees.




Atlanta has 19 sister cities, as designated by Sister Cities International, Inc. (SCI):



List of people from Atlanta
Urban forest
















Official website
Atlanta Department of Watershed Management
Atlanta Police Department
Atlanta Convention and Visitors Bureau
Entry in the New Georgia Encyclopedia
Atlanta Historic Newspapers Archive Digital Library of Georgia
Atlanta History Photograph Collection from the Atlanta History Center
Atlanta Time Machine
Atlanta, Georgia, a National Park Service Discover Our Shared Heritage Travel Itinerary
Atlanta City Online Travel GuideCalifornia (/ˌkælᵻˈfɔːrnjə, -ni.ə/ KAL-ə-FORN-yə, KAL-ə-FORN-ee-ə) is the most populous state in the United States and the third most extensive by area. Located on the western (Pacific Ocean) coast of the U.S., California is bordered by the other U.S. states of Oregon, Nevada, and Arizona and shares an international border with the Mexican state of Baja California. The state capital is Sacramento. Los Angeles is California's most populous city, and the country's second largest after New York City. The state also has the nation's most populous county, Los Angeles County, and its largest county by area, San Bernardino County.
California's diverse geography ranges from the Pacific Coast in the west to the Sierra Nevada mountain range in the east; and from the redwood–Douglas fir forests in the northwest to the Mojave Desert in the southeast. The Central Valley, a major agricultural area, dominates the state's center. Though California is well-known for its warm Mediterranean climate, the large size of the state means it can vary from moist temperate rainforest in the north, to arid desert in the interior, as well as snowy alpine in the mountains.
What is now California was first settled by various Native American tribes before being explored by a number of European expeditions during the 16th and 17th centuries. The Spanish Empire then claimed it as part of Alta California in their New Spain colony. The area became a part of Mexico in 1821 following its successful war for independence, but was ceded to the United States in 1848 after the Mexican–American War. The western portion of Alta California then was organized as the State of California, and admitted as the 31st state on September 9, 1850. The California Gold Rush starting in 1848 led to dramatic social and demographic changes, with large-scale emigration from the east and abroad with an accompanying economic boom.
If it were a country, California would be the 6th largest economy in the world and the 35th most populous. It is also regarded as a global trendsetter in both popular culture and politics, and is the birthplace of the film industry, the hippie counterculture, the Internet, and the personal computer, among others. Fifty-eight percent of the state's economy is centered on finance, government, real estate services, technology, and professional, scientific and technical business services. The San Francisco Bay Area has the nation's highest median household income by metropolitan area, and is the headquarters of three of the world's largest 20 firms by revenue, Chevron, Apple, and McKesson. Although it accounts for only 1.5 percent of the state's economy, California's agriculture industry has the highest output of any U.S. state.




The word California originally referred to the Baja California Peninsula of Mexico; it was later extended to the entire region composed of the current United States states of California, Nevada, and Utah, and parts of Arizona, New Mexico, Texas and Wyoming.
The name California is surmised by some writers to have derived from a fictional paradise peopled by Black Amazons and ruled by Queen Calafia, who fought alongside Muslims and whose name was chosen to echo the title of a Muslim leader, the Caliph, fictionally implying that California was the Caliphate. The story of Calafia is recorded in a 1510 work The Adventures of Esplandián, written as a sequel to Amadis de Gaula by Spanish adventure writer Garci Rodríguez de Montalvo. The kingdom of Queen Calafia, according to Montalvo, was said to be a remote land inhabited by griffins and other strange beasts, and rich in gold.

Know ye that at the right hand of the Indies there is an island called California, very close to that part of the Terrestrial Paradise, which was inhabited by black women without a single man among them, and they lived in the manner of Amazons. They were robust of body with strong passionate hearts and great virtue. The island itself is one of the wildest in the world on account of the bold and craggy rocks.

When Spanish explorer Francisco de Ulloa was exploring the western coast of North America, his initial surveys of the Baja California Peninsula led him to believe that it was an island rather than part of the larger continent, so he dubbed the "island" after the mythical island in Montalvo's writing. This conventional wisdom that California was an island, with maps drawn to reflect this belief, lasted as late as the 1700's.
Shortened forms of the state's name include CA, Cal., Calif. and US-CA.






Settled by successive waves of arrivals during the last 10,000 years, California was one of the most culturally and linguistically diverse areas in pre-Columbian North America. Various estimates of the native population range from 100,000 to 300,000. The Indigenous peoples of California included more than 70 distinct groups of Native Americans, ranging from large, settled populations living on the coast to groups in the interior. California groups also were diverse in their political organization with bands, tribes, villages, and on the resource-rich coasts, large chiefdoms, such as the Chumash, Pomo and Salinan. Trade, intermarriage and military alliances fostered many social and economic relationships among the diverse groups.




The first European effort to explore the coast as far north as the Russian River was a Spanish sailing expedition, led by Portuguese captain Juan Rodríguez Cabrillo, in 1542. Some 37 years later English explorer Francis Drake also explored and claimed an undefined portion of the California coast in 1579. Spanish traders made unintended visits with the Manila galleons on their return trips from the Philippines beginning in 1565. The first Asians to set foot on what would be the United States occurred in 1587, when Filipino sailors arrived in Spanish ships at Morro Bay. Sebastián Vizcaíno explored and mapped the coast of California in 1602 for New Spain.
Despite the on-the-ground explorations of California in the 16th century, Rodríguez's idea of California as an island persisted. That depiction appeared on many European maps well into the 18th century.
After the Portolà expedition of 1769–70, Spanish missionaries began setting up 21 California Missions on or near the coast of Alta (Upper) California, beginning in San Diego. During the same period, Spanish military forces built several forts (presidios) and three small towns (pueblos). Two of the pueblos grew into the cities of Los Angeles and San Jose. The Spanish colonization brought the genocide of the indigenous Californian peoples.




Imperial Russia explored the California coast and established a trading post at Fort Ross. Its early 19th-century coastal settlements north of San Francisco Bay constituted the southernmost Russian colony in North America and were spread over an area stretching from Point Arena to Tomales Bay.
In 1821, the Mexican War of Independence gave Mexico (including California) independence from Spain; for the next 25 years, Alta California remained a remote northern province of the nation of Mexico.
Cattle ranches, or ranchos, emerged as the dominant institutions of Mexican California. After Mexican independence from Spain, the chain of missions became the property of the Mexican government and were secularized by 1834. The ranchos developed under ownership by Californios (Spanish-speaking Californians) who had received land grants, and traded cowhides and tallow with Boston merchants.
From the 1820s, trappers and settlers from the United States and Canada arrived in Northern California. These new arrivals used the Siskiyou Trail, California Trail, Oregon Trail and Old Spanish Trail to cross the rugged mountains and harsh deserts in and surrounding California.
Between 1831 and 1836, California experienced a series of revolts against Mexico; this culminated in the 1836 California revolt led by Juan Bautista Alvarado, which ended after Mexico appointed him governor of the department. The revolt, which had momentarily declared California an independent state, was successful with the assistance of American and British residents of California, including Isaac Graham; after 1840, 100 of those residents who did not have passports were arrested, leading to the Graham affair in 1840.
One of the largest ranchers in California was John Marsh. After failing to obtain justice against squatters on his land from the Mexican courts, he determined that California should become part of the United States. Marsh conducted a letter-writing campaign espousing the California climate, soil and other reasons to settle there, as well as the best route to follow, which became known as "Marsh's route." His letters were read, reread, passed around, and printed in newspapers throughout the country, and started the first wagon trains rolling to California. He invited immigrants to stay on his ranch until they could get settled, and assisted in their obtaining passports.
After ushering in the period of organized emigration to California, Marsh helped end the rule of the last Mexican governor of California, thereby paving the way to California's ultimate acquisition by the United States.
In 1846, settlers rebelled against Mexican rule during the Bear Flag Revolt. Afterwards, rebels raised the Bear Flag (featuring a bear, a star, a red stripe and the words "California Republic") at Sonoma. The Republic's only president was William B. Ide, who played a pivotal role during the Bear Flag Revolt.
The California Republic was short lived; the same year marked the outbreak of the Mexican–American War (1846–48). When Commodore John D. Sloat of the United States Navy sailed into Monterey Bay and began the military occupation of California by the United States, Northern California capitulated in less than a month to the United States forces. After a series of defensive battles in Southern California, the Treaty of Cahuenga was signed by the Californios on January 13, 1847, securing American control in California.
Following the Treaty of Guadalupe Hidalgo that ended the war, the western territory of Alta California, became the United States state of California, and Arizona, Nevada, Colorado and Utah became United States Territories. The lightly populated lower region of California, the Baja Peninsula, remained in the possession of Mexico.
In 1846, the non-native population of California was estimated to be no more than 8,000, plus about 100,000 Native Americans down from about 300,000 before Hispanic settlement in 1769. After gold was discovered in 1848, the population burgeoned with United States citizens, Europeans, Chinese and other immigrants during the great California Gold Rush. By 1854 over 300,000 settlers had come. Between 1847 and 1870, the population of San Francisco increased from 500 to 150,000. On September 9, 1850, as part of the Compromise of 1850, California was admitted to the United States undivided as a free state, denying the expansion of slavery to the Pacific Coast.
California's native population precipitously declined, above all, from Eurasian diseases to which they had no natural immunity. As in other states, the native inhabitants were forcibly removed from their lands by incoming miners, ranchers, and farmers. And although California entered the union as a free state, the "loitering or orphaned Indians" were de facto enslaved by Mexican and Anglo-American masters under the 1853 Act for the Government and Protection of Indians. There were massacres in which hundreds of indigenous people were killed. Between 1850 and 1860, California paid around 1.5 million dollars (some 250,000 of which was reimbursed by the federal government) to hire militias whose purpose was to protect settlers from the indigenous populations. In later decades, the native population was placed in reservations and rancherias, which were often small and isolated and without enough natural resources or funding from the government to sustain the populations living on them. As a result, the rise of California was a calamity for the native inhabitants. Several scholars and Native American activists, including Benjamin Madley and Ed Castillo, have described the actions of the California government as a genocide.
The seat of government for California under Spanish and later Mexican rule was located at Monterey from 1777 until 1845. Pio Pico, last Mexican governor of Alta California, moved the capital to Los Angeles in 1845. The United States consulate was also located in Monterey, under consul Thomas O. Larkin.
In 1849, the Constitutional Convention was first held in Monterey. Among the tasks was a decision on a location for the new state capital. The first legislative sessions were held in San Jose (1850–1851). Subsequent locations included Vallejo (1852–1853), and nearby Benicia (1853–1854); these locations eventually proved to be inadequate as well. The capital has been located in Sacramento since 1854 with only a short break in 1862 when legislative sessions were held in San Francisco due to flooding in Sacramento.
Initially, travel between California and the rest of the continental United States was time consuming and dangerous. A more direct connection came in 1869 with the completion of the First Transcontinental Railroad through Donner Pass in the Sierra Nevada mountains. Once completed, hundreds of thousands of United States citizens came west, where new Californians were discovering that land in the state, if irrigated during the dry summer months, was extremely well suited to fruit cultivation and agriculture in general. Vast expanses of wheat, other cereal crops, vegetable crops, cotton, and nut and fruit trees were grown (including oranges in Southern California), and the foundation was laid for the state's prodigious agricultural production in the Central Valley and elsewhere.




Migration to California accelerated during the early 20th century with the completion of major transcontinental highways like the Lincoln Highway and Route 66. In the period from 1900 to 1965, the population grew from fewer than one million to become the most populous state in the Union. In 1940, the Census Bureau reported California's population as 6.0% Hispanic, 2.4% Asian, and 89.5% non-Hispanic white.
To meet the population's needs, major engineering feats like the California and Los Angeles Aqueducts; the Oroville and Shasta Dams; and the Bay and Golden Gate Bridges were built across the state. The state government also adopted the California Master Plan for Higher Education in 1960 to develop a highly efficient system of public education.
Meanwhile, attracted to the mild Mediterranean climate, cheap land, and the state's wide variety of geography, filmmakers established the studio system in Hollywood in the 1920s. California manufactured 8.7 percent of total United States military armaments produced during World War II, ranking third (behind New York and Michigan) among the 48 states. After World War II, California's economy greatly expanded due to strong aerospace and defense industries, whose size decreased following the end of the Cold War. Stanford University and its Dean of Engineering Frederick Terman began encouraging faculty and graduates to stay in California instead of leaving the state, and develop a high-tech region in the area now known as Silicon Valley. As a result of these efforts, California is regarded as a world center of the entertainment and music industries, of technology, engineering, and the aerospace industry, and as the United States center of agricultural production. Just before the "Dot Com Bust" California had the 5th largest economy in the world among nations. Yet since 1991, and starting in the late 1980s in Southern California, California has seen a net loss of domestic migrants most years. This is often referred to by the media as the California exodus.
However, during the 20th century, two great disasters happened in California. The 1906 San Francisco earthquake and 1928 St. Francis Dam flood remain the deadliest in U.S history.




California is the 3rd largest state in the United States in area, after Alaska and Texas. California is often geographically bisected into two regions, Southern California, comprising the 10 southernmost counties, and Northern California, comprising the 48 northernmost counties.
In the middle of the state lies the California Central Valley, bounded by the Sierra Nevada in the east, the coastal mountain ranges in the west, the Cascade Range to the north and by the Tehachapi Mountains in the south. The Central Valley is California's productive agricultural heartland.
Divided in two by the Sacramento-San Joaquin River Delta, the northern portion, the Sacramento Valley serves as the watershed of the Sacramento River, while the southern portion, the San Joaquin Valley is the watershed for the San Joaquin River. Both valleys derive their names from the rivers that flow through them. With dredging, the Sacramento and the San Joaquin Rivers have remained deep enough for several inland cities to be seaports.
The Sacramento-San Joaquin River Delta is a critical water supply hub for the state. Water is diverted from the delta and through an extensive network of pumps and canals that traverse nearly the length of the state, to the Central Valley and the State Water Projects and other needs. Water from the Delta provides drinking water for nearly 23 million people, almost two-thirds of the state's population as well as water for farmers on the west side of the San Joaquin Valley.
The Channel Islands are located off the Southern coast.
The Sierra Nevada (Spanish for "snowy range") includes the highest peak in the contiguous 48 states, Mount Whitney, at 14,505 feet (4,421 m). The range embraces Yosemite Valley, famous for its glacially carved domes, and Sequoia National Park, home to the giant sequoia trees, the largest living organisms on Earth, and the deep freshwater lake, Lake Tahoe, the largest lake in the state by volume.
To the east of the Sierra Nevada are Owens Valley and Mono Lake, an essential migratory bird habitat. In the western part of the state is Clear Lake, the largest freshwater lake by area entirely in California. Though Lake Tahoe is larger, it is divided by the California/Nevada border. The Sierra Nevada falls to Arctic temperatures in winter and has several dozen small glaciers, including Palisade Glacier, the southernmost glacier in the United States.
About 45 percent of the state's total surface area is covered by forests, and California's diversity of pine species is unmatched by any other state. California contains more forestland than any other state except Alaska. Many of the trees in the California White Mountains are the oldest in the world; an individual bristlecone pine is over 5,000 years old.
In the south is a large inland salt lake, the Salton Sea. The south-central desert is called the Mojave; to the northeast of the Mojave lies Death Valley, which contains the lowest and hottest place in North America, the Badwater Basin at −279 feet (−85 m). The horizontal distance from the bottom of Death Valley to the top of Mount Whitney is less than 90 miles (140 km). Indeed, almost all of southeastern California is arid, hot desert, with routine extreme high temperatures during the summer. The southeastern border of California with Arizona is entirely formed by the Colorado River, from which the southern part of the state gets about half of its water.
California contains both the highest point (Mount Whitney) and the lowest point (Death Valley) in the contiguous United States.
A majority of California's cities are located in either the San Francisco Bay Area or the Sacramento metropolitan area in Northern California; or the Los Angeles area, the Riverside-San Bernardino-Inland Empire, or the San Diego metropolitan area in Southern California. The Los Angeles Area, the Bay Area, and the San Diego metropolitan area are among several major metropolitan areas along the California coast.
As part of the Ring of Fire, California is subject to tsunamis, floods, droughts, Santa Ana winds, wildfires, landslides on steep terrain, and has several volcanoes. It has many earthquakes due to several faults running through the state, in particular the San Andreas Fault. About 37,000 earthquakes are recorded each year, but most are too small to be felt.




Although most of the state has a Mediterranean climate, due to the state's large size, the climate ranges from subarctic to subtropical. The cool California Current offshore often creates summer fog near the coast. Farther inland, there are colder winters and hotter summers. The maritime moderation results in the shoreline summertime temperatures of Los Angeles and San Francisco being the coolest of all major metropolitan areas of the United States and uniquely cool compared to areas on the same latitude in the interior and on the east coast of the North American continent. Even the San Diego shoreline bordering Mexico is cooler in summer than most areas in the contiguous United States. Just a few miles inland, summer temperature extremes are significantly higher, with downtown Los Angeles being several degrees warmer than at the coast. The same microclimate phenomenon is seen in the climate of the Bay Area, where areas sheltered from the sea experience significantly hotter summers than nearby areas that are close to the ocean.
Northern parts of the state have more rain than the south. California's mountain ranges also influence the climate: some of the rainiest parts of the state are west-facing mountain slopes. Northwestern California has a temperate climate, and the Central Valley has a Mediterranean climate but with greater temperature extremes than the coast. The high mountains, including the Sierra Nevada, have an alpine climate with snow in winter and mild to moderate heat in summer.
California's mountains produce rain shadows on the eastern side, creating extensive deserts. The higher elevation deserts of eastern California have hot summers and cold winters, while the low deserts east of the Southern California mountains have hot summers and nearly frostless mild winters. Death Valley, a desert with large expanses below sea level, is considered the hottest location in the world; the highest temperature in the world, 134 °F (56.7 °C), was recorded there on July 10, 1913. The lowest temperature in California was −45 °F (−43 °C) in 1937 in Boca.
The table below lists average temperatures for August and December in a selection of places throughout the state; some highly populated and some not. This includes the relatively cool summers of the Humboldt Bay region around Eureka, the extreme heat of Death Valley, and the mountain climate of Mammoth in the Sierra Nevadas.




California is one of the richest and most diverse parts of the world, and includes some of the most endangered ecological communities. California is part of the Nearctic ecozone and spans a number of terrestrial ecoregions.
California's large number of endemic species includes relict species, which have died out elsewhere, such as the Catalina ironwood (Lyonothamnus floribundus). Many other endemics originated through differentiation or adaptive radiation, whereby multiple species develop from a common ancestor to take advantage of diverse ecological conditions such as the California lilac (Ceanothus). Many California endemics have become endangered, as urbanization, logging, overgrazing, and the introduction of exotic species have encroached on their habitat.




California boasts several superlatives in its collection of flora: the largest trees, the tallest trees, and the oldest trees. California's native grasses are perennial plants. After European contact, these were generally replaced by invasive species of European annual grasses; and, in modern times, California's hills turn a characteristic golden-brown in summer.
Because California has the greatest diversity of climate and terrain, the state has six life zones which are the lower Sonoran (desert); upper Sonoran (foothill regions and some coastal lands), transition (coastal areas and moist northeastern counties); and the Canadian, Hudsonian, and Arctic Zones, comprising the state's highest elevations.
Plant life in the dry climate of the lower Sonoran zone contains a diversity of native cactus, mesquite, and paloverde. The Joshua tree is found in the Mojave Desert. Flowering plants include the dwarf desert poppy and a variety of asters. Fremont cottonwood and valley oak thrive in the Central Valley. The upper Sonoran zone includes the chaparral belt, characterized by forests of small shrubs, stunted trees, and herbaceous plants. Nemophila, mint, Phacelia, Viola, and the California poppy (Eschscholzia californica) – the state flower – also flourish in this zone, along with the lupine, more species of which occur here than anywhere else in the world.
The transition zone includes most of California's forests with the redwood (Sequoia sempervirens) and the "big tree" or giant sequoia (Sequoiadendron giganteum), among the oldest living things on earth (some are said to have lived at least 4,000 years). Tanbark oak, California laurel, sugar pine, madrona, broad-leaved maple, and Douglas-fir also grow here. Forest floors are covered with swordfern, alumnroot, barrenwort, and trillium, and there are thickets of huckleberry, azalea, elder, and wild currant. Characteristic wild flowers include varieties of mariposa, tulip, and tiger and leopard lilies.

The high elevations of the Canadian zone allow the Jeffrey pine, red fir, and lodgepole pine to thrive. Brushy areas are abundant with dwarf manzanita and ceanothus; the unique Sierra puffball is also found here. Right below the timberline, in the Hudsonian zone, the whitebark, foxtail, and silver pines grow. At about 10,500 feet (3,200 m), begins the Arctic zone, a treeless region whose flora include a number of wildflowers, including Sierra primrose, yellow columbine, alpine buttercup, and alpine shooting star.
Common plants that have been introduced to the state include the eucalyptus, acacia, pepper tree, geranium, and Scotch broom. The species that are federally classified as endangered are the Contra Costa wallflower, Antioch Dunes evening primrose, Solano grass, San Clemente Island larkspur, salt marsh bird's beak, McDonald's rock-cress, and Santa Barbara Island liveforever. As of December 1997, 85 plant species were listed as threatened or endangered.
In the deserts of the lower Sonoran zone, the mammals include the jackrabbit, kangaroo rat, squirrel, and opossum. Common birds include the owl, roadrunner, cactus wren, and various species of hawk. The area's reptilian life include the sidewinder viper, desert tortoise, and horned toad. The upper Sonoran zone boasts mammals such as the antelope, brown-footed woodrat, and ring-tailed cat. Birds unique to this zone are the California thrasher, bushtit, and California condor.
In the transition zone, there are Colombian black-tailed deer, black bears, gray foxes, cougars, bobcats, and Roosevelt elk. Reptiles such as the garter snakes and rattlesnakes inhabit the zone. In addition, amphibians such as the water puppy and redwood salamander are common too. Birds such as the kingfisher, chickadee, towhee, and hummingbird thrive here as well.
The Canadian zone mammals include the mountain weasel, snowshoe hare, and several species of chipmunks. Conspicuous birds include the blue-fronted jay, Sierra chickadee. Sierra hermit thrush, water ouzel, and Townsend's solitaire. As one ascends into the Hudsonian zone, birds become scarcer. While the Sierra rosy finch is the only bird native to the high Arctic region, other bird species such as the hummingbird and Clark's nutcracker. Principal mammals found in this region include the Sierra coney, white-tailed jackrabbit, and the bighorn sheep. As of April 2003, the bighorn sheep was listed as endangered by the US Fish and Wildlife Service. The fauna found throughout several zones are the mule deer, coyote, mountain lion, northern flicker, and several species of hawk and sparrow.
Aquatic life in California thrives, from the state's mountain lakes and streams to the rocky Pacific coastline. Numerous trout species are found, among them rainbow, golden, and cutthroat. Migratory species of salmon are common as well. Deep-sea life forms include sea bass, yellowfin tuna, barracuda, and several types of whale. Native to the cliffs of northern California are seals, sea lions, and many types of shorebirds, including migratory species.
As of April 2003, 118 California animals were on the federal endangered list; 181 plants were listed as endangered or threatened. Endangered animals include the San Joaquin kitfox, Point Arena mountain beaver, Pacific pocket mouse, salt marsh harvest mouse, Morro Bay kangaroo rat (and five other species of kangaroo rat), Amargosa vole, California least tern, California condor, loggerhead shrike, San Clemente sage sparrow, San Francisco garter snake, five species of salamander, three species of chub, and two species of pupfish. Eleven butterflies are also endangered and two that are threatened are on the federal list. Among threatened animals are the coastal California gnatcatcher, Paiute cutthroat trout, southern sea otter, and northern spotted owl. California has a total of 290,821 acres (1,176.91 km2) of National Wildlife Refuges. As of September 2010, 123 California animals were listed as either endangered or threatened on the federal list provided by the US Fish & Wildlife Service. Also, as of the same year, 178 species of California plants were listed either as endangered or threatened on this federal list.




The vast majority of rivers in California are dammed as part of two massive water projects: the Central Valley Project, providing water to the agricultural central valley, the California State Water Project diverting water from northern to southern California. The state's coasts, rivers, and other bodies of water are regulated by the California Coastal Commission.
The two most prominent rivers within California are the Sacramento River and the San Joaquin River, which drain the Central Valley and the west slope of the Sierra Nevada and flow to the Pacific Ocean through San Francisco Bay. Several major tributaries feed into the Sacramento and the San Joaquin, including the Pit River, the Tuolumne River, and the Feather River.
The Eel River and Salinas River each drain portions of the California coast, north and south of San Francisco Bay, respectively, and the Eel River is the largest river in the state to remain in its natural un-dammed state. The Mojave River is the primary watercourse in the Mojave Desert, and the Santa Ana River drains much of the Transverse Ranges as it bisects Southern California. Some other important rivers are the Klamath River and the Trinity River in the far north coast, and the Colorado River on the southeast border with Arizona.










The United States Census Bureau estimates that the population of California was 39,144,818 on July 1, 2015, a 5.08% increase since the 2010 United States Census. Between 2000 and 2009, there was a natural increase of 3,090,016 (5,058,440 births minus 2,179,958 deaths). During this time period, international migration produced a net increase of 1,816,633 people while domestic migration produced a net decrease of 1,509,708, resulting in a net in-migration of 306,925 people. The state of California's own statistics show a population of 38,292,687 for January 1, 2009. However, according to the Manhattan Institute for Policy Research, since 1990 almost 3.4 million Californians have moved to other states, with most leaving to Texas, Nevada, and Arizona.
California is the 2nd-most populous subnational entity in the Western Hemisphere and the Americas, with a population second to that of the state of São Paulo in Brazil. California's population is greater than that of all but 34 countries of the world. The Greater Los Angeles Area is the 2nd-largest metropolitan area in the United States, after the New York metropolitan area, while Los Angeles, with nearly half the population of New York, is the 2nd-largest city in the United States. Also, Los Angeles County has held the title of most populous United States county for decades, and it alone is more populous than 42 United States states. Including Los Angeles, four of the top 15 most populous cities in the U.S. are in California: Los Angeles (2nd), San Diego (8th), San Jose (10th), and San Francisco (13th). The center of population of California is located in the town of Buttonwillow, Kern County.




The state has 482 incorporated cities and towns; of which 460 are cities and 22 are towns. Under California law, the terms "city" and "town" are explicitly interchangeable; the name of an incorporated municipality in the state can either be "City of (Name)" or "Town of (Name)".
Sacramento became California's first incorporated city on February 27, 1850. San Jose, San Diego and Benicia tied for California's second incorporated city, each receiving incorporation on March 27, 1850. Jurupa Valley became the state's most recent and 482nd incorporated municipality on July 1, 2011.
The majority of these cities and towns are within one of five metropolitan areas: the Los Angeles Metropolitan Area, the San Francisco Bay Area, the Riverside-San Bernardino Area, the San Diego metropolitan area and the Sacramento metropolitan area.



Starting in the year 2010, for the first time since the California Gold Rush, California-born residents make up the majority of the state's population. Along with the rest of the United States, California's immigration pattern has also shifted over the course of the late 2000s-early 2010s. Immigration from Latin American countries has dropped significantly with most immigrants now coming from Asia. In total for 2011, there were 277,304 immigrants. 57% came from Asian countries vs. 22% from Latin American countries. Net immigration from Mexico, previously the most common country of origin for new immigrants has dropped to zero/less than zero, since more Mexican nationals are departing for their home country than immigrating. As a result it is estimated that Hispanic citizens will constitute 49% of the population by 2060, instead of the previously projected 2050, due primarily to domestic births.
The state's population of undocumented immigrants has been shrinking in recent years, due to increased enforcement and decreased job opportunities for lower-skilled workers. The number of migrants arrested attempting to cross the Mexican border in the Southwest plunged from a high of 1.1 million in 2005 to just 367,000 in 2011. Despite these recent trends, illegal aliens constituted an estimated 7.3 percent of the state's population, the third highest percentage of any state in the country, totaling nearly 2.6 million. In particular, illegal immigrants tended to be concentrated in Los Angeles, Monterey, San Benito, Imperial, and Napa Counties – the latter four of which have significant agricultural industries that depend on manual labor. More than half of illegal immigrants originate from Mexico.




California is considered generally liberal in its policies regarding the LGBT community, and the rights of lesbian, gay, bisexual, and transgender people have received greater recognition since 1960 at both the state and municipal level. California is home to a number of gay villages such as the Castro District in San Francisco, Hillcrest in San Diego, and West Hollywood. Through the Domestic Partnership Act of 1999, California became the first state in the United States to recognize same-sex relationships in any legal capacity. In 2000, voters passed Proposition 22, which restricted state recognition of marriage to opposite-sex couples. This was struck down by the California Supreme Court in May 2008, effectively legalizing same-sex marriage; however, this was overruled later that same year when California voters passed Proposition 8. After further judicial cases, in 2013 the U.S. Supreme Court rendered the law void, allowing same-sex marriages in California to resume.



According to the United States Census Bureau in 2015 the population self-identifies as (alone or in combination):
72.9% White
14.7% Asian
6.5% Black or African American
3.8% Two or More Races
1.7% Native American and Alaska Native
0.5% Native Hawaiian or Pacific Islander
By ethnicity, in 2015 the population was 61.2% non-Hispanic (of any race) and 38.8% Hispanic or Latino (of any race).
As of 2011, 75.1% of California's population younger than age 1 were minorities, meaning that they had at least one parent who was not non-Hispanic white (white Hispanics are counted as minorities).
In terms of total numbers, California has the largest population of White Americans in the United States, an estimated 22,200,000 residents. The state has the 5th largest population of African Americans in the United States, an estimated 2,250,000 residents. California's Asian American population is estimated at 4.4 million, constituting a third of the nation's total. California's Native American population of 285,000 is the most of any state.
According to estimates from 2011, California has the largest minority population in the United States by numbers, making up 60% of the state population. Over the past 25 years, the population of non-Hispanic whites has declined, while Hispanic and Asian populations have grown. Between 1970 and 2011, non-Hispanic whites declined from 80% of the State's population to 40%, while Hispanics grew from 32% in 2000 to 38% in 2011. It is currently projected that Hispanics will rise to 49% of the population by 2060, primarily due to domestic births rather than immigration. With the decline of immigration from Latin America, Asian Americans now constitute the fastest growing racial/ethnic group in California; this growth primarily driven by immigration from China, India and the Philippines, respectively.



English serves as California's de jure and de facto official language. In 2010, the Modern Language Association of America estimated that 57.02% (19,429,309) of California residents age 5 and older spoke only English at home, while 42.98% spoke another primary language at home. According to the 2007 American Community Survey, 73% of people who speak a language other than English at home are able to speak English well or very well, with 9.8% not speaking English at all. Unlike most United States States, California law enshrines English as its official language (rather than it being simply the most commonly used), since the passage of Proposition 63 by California voters. Various government agencies do, and are often required to, furnish documents in the various languages needed to reach their intended audiences.
In total, 16 languages other than English were spoken as primary languages at home by more than 100,000 persons, more than any other state in the nation. New York State, in second place, had 9 languages other than English spoken by more than 100,000 persons. The most common language spoken besides English was Spanish, spoken by 28.46% (9,696,638) of the population. With Asia contributing most of California's new immigrants, California had the highest concentration nationwide of Vietnamese and Chinese speakers, the second highest concentration of Korean, and the third highest concentration of Tagalog speakers.
California has historically been one of the most linguistically diverse areas in the world, with more than 70 indigenous languages derived from 64 root languages in 6 language families. A survey conducted between 2007 and 2009 identified 23 different indigenous languages of Mexico that are spoken among California farmworkers. All of California's indigenous languages are endangered, although there are now efforts toward language revitalization.
As a result of the state's increasing diversity and migration from other areas across the country and around the globe, linguists began noticing a noteworthy set of emerging characteristics of spoken English in California since the late 20th century. This dialect, known as California English, has a vowel shift and several other phonological processes that are different from the dialects used in other regions of the country.




The culture of California is a Western culture and most clearly has its modern roots in the culture of the United States, but also, historically, many Hispanic influences. As a border and coastal state, Californian culture has been greatly influenced by several large immigrant populations, especially those from Latin America and Asia.
California has long been a subject of interest in the public mind and has often been promoted by its boosters as a kind of paradise. In the early 20th century, fueled by the efforts of state and local boosters, many Americans saw the Golden State as an ideal resort destination, sunny and dry all year round with easy access to the ocean and mountains. In the 1960s, popular music groups such as The Beach Boys promoted the image of Californians as laid-back, tanned beach-goers.
The California Gold Rush of the 1850s is still seen as a symbol of California's economic style, which tends to generate technology, social, entertainment, and economic fads and booms and related busts.




The largest religious denominations by number of adherents as a percentage of California's population in 2014 were the Catholic Church with 28 percent, Evangelical Protestants with 20 percent, and Mainline Protestants with 10 percent. Together, all kinds of Protestants accounted for 32 percent. Those unaffiliated with any religion represented 27 percent of the population. The breakdown of other religions is 1% Muslim, 2% Hindu and 2% Buddhist. This is a change from 2008, when the population identified their religion with the Catholic Church with 31 percent; Evangelical Protestants with 18 percent; and Mainline Protestants with 14 percent. In 2008, those unaffiliated with any religion represented 21 percent of the population. The breakdown of other religions in 2008 was 0.5% Muslim, 1% Hindu and 2% Buddhist. The American Jewish Year Book placed the total Jewish population of California at about 1,194,190 in 2006. According to the Association of Religion Data Archives (ARDA) the largest denominations by adherents in 2010 were the Roman Catholic Church with 10,233,334; The Church of Jesus Christ of Latter-day Saints with 763,818; and the Southern Baptist Convention with 489,953.
The first priests to come to California were Roman Catholic missionaries from Spain. Roman Catholics founded 21 missions along the California coast, as well as the cities of Los Angeles and San Francisco. California continues to have a large Roman Catholic population due to the large numbers of Mexicans and Central Americans living within its borders. California has twelve dioceses and two archdioceses, the Archdiocese of Los Angeles and the Archdiocese of San Francisco, the former being the largest archdiocese in the United States.
A Pew Research Center survey revealed that California is somewhat less religious than the rest of the US: 62 percent of Californians say they are "absolutely certain" of their belief in God, while in the nation 71 percent say so. The survey also revealed 48 percent of Californians say religion is "very important", compared to 56 percent nationally.




California has twenty major professional sports league franchises, far more than any other state. The San Francisco Bay Area has seven major league teams spread in its three major cities: San Francisco, San Jose, and Oakland. While the Greater Los Angeles Area is home to ten major league franchises. San Diego has two major league teams, and Sacramento has one. The NFL Super Bowl has been hosted in California 11 times at four different stadiums: Los Angeles Memorial Coliseum, the Rose Bowl, Stanford Stadium, and San Diego's Qualcomm Stadium. A twelfth, Super Bowl 50, was held at Levi's Stadium in Santa Clara on February 7, 2016.
California has long had many respected collegiate sports programs. California is home to the oldest college bowl game, the annual Rose Bowl, among others.
California is the only US state to have hosted both the Summer and Winter Olympics. The 1932 and 1984 Summer Olympics were held in Los Angeles. Squaw Valley Ski Resort in the Lake Tahoe region hosted the 1960 Winter Olympics. Multiple games during the 1994 FIFA World Cup took place in California, with the Rose Bowl hosting eight matches including the final, while Stanford Stadium hosted six matches.

Below is a list of major league sports teams in California:




Public secondary education consists of high schools that teach elective courses in trades, languages, and liberal arts with tracks for gifted, college-bound and industrial arts students. California's public educational system is supported by a unique constitutional amendment that requires a minimum annual funding level for grades K–12 and community colleges that grows with the economy and student enrollment figures.
California had over 6.2 million school students in the 2005–06 school year. Funding and staffing levels in California schools lag behind other states. In expenditure per pupil, California ranked 29th (of the 50 states and the District of Columbia) in 2005–06. In teaching staff expenditure per pupil, California ranked 49th of 51. In overall teacher-pupil ratio, California was also 49th, with 21 students per teacher. Only Arizona and Utah were lower.
A 2007 study concluded that California's public school system was "broken" in that it suffered from over-regulation.
California's public postsecondary education offers three separate systems:
The research university system in the state is the University of California (UC), a public university system. As of fall 2011, the University of California had a combined student body of 234,464 students. There are ten general UC campuses, and a number of specialized campuses in the UC system. The system was originally intended to accept the top one-eighth of California high school students, but several of the schools have become even more selective. The UC system was originally given exclusive authority in awarding Ph.Ds, but this has since changed and the CSU is also able to award several Doctoral degrees.
The California State University (CSU) system has almost 430,000 students, making it the largest university system in the United States. The CSU was originally intended to accept the top one-third of California high school students, but several of the schools have become much more selective. The CSU was originally set up to award only bachelor's and master's degrees, but has since been granted the authority to award several Doctoral degrees.
The California Community Colleges System provides lower division coursework as well as basic skills and workforce training. It is the largest network of higher education in the US, composed of 112 colleges serving a student population of over 2.6 million.
California is also home to such notable private universities as Stanford University, the University of Southern California, the California Institute of Technology, and the Claremont Colleges. California has hundreds of other private colleges and universities, including many religious and special-purpose institutions.




The economy of California is large enough to be comparable to that of the largest of countries. As of 2016, the gross state product (GSP) is about $2.514 trillion, the largest in the United States. California is responsible for 13.9 percent of the United States' approximate $18.1 trillion gross domestic product (GDP). California's GSP is larger than the GDP of all but 5 countries in dollar terms (the United States, China, Japan, Germany, and the United Kingdom), larger than Brazil, France, Russia, Italy, India, Canada, Australia, Spain and Turkey. In Purchasing Power Parity, it is larger than all but 10 countries (the United States, China, India, Japan, Germany, Russia, Brazil, France, the United Kingdom, and Indonesia), larger than Italy, Mexico, Spain, South Korea, Saudi Arabia, Canada and Turkey.
The five largest sectors of employment in California are trade, transportation, and utilities; government; professional and business services; education and health services; and leisure and hospitality. In output, the five largest sectors are financial services, followed by trade, transportation, and utilities; education and health services; government; and manufacturing. As of September 2016, California has an unemployment rate of 5.5%.
California's economy is dependent on trade and international related commerce accounts for about one-quarter of the state's economy. In 2008, California exported $144 billion worth of goods, up from $134 billion in 2007 and $127 billion in 2006. Computers and electronic products are California's top export, accounting for 42 percent of all the state's exports in 2008.
Agriculture is an important sector in California's economy. Farming-related sales more than quadrupled over the past three decades, from $7.3 billion in 1974 to nearly $31 billion in 2004. This increase has occurred despite a 15 percent decline in acreage devoted to farming during the period, and water supply suffering from chronic instability. Factors contributing to the growth in sales-per-acre include more intensive use of active farmlands and technological improvements in crop production. In 2008, California's 81,500 farms and ranches generated $36.2 billion products revenue. In 2011, that number grew to $43.5 billion products revenue. The Agriculture sector accounts for two percent of the state's GDP and employs around three percent of its total workforce. According to the USDA in 2011, the three largest California agricultural products by value were milk and cream, shelled almonds, and grapes.
Per capita GDP in 2007 was $38,956, ranking eleventh in the nation. Per capita income varies widely by geographic region and profession. The Central Valley is the most impoverished, with migrant farm workers making less than minimum wage. According to a 2005 report by the Congressional Research Service, the San Joaquin Valley was characterized as one of the most economically depressed regions in the United States, on par with the region of Appalachia. California has a poverty rate of 23.5%, the highest of any state in the country. Many coastal cities include some of the wealthiest per-capita areas in the United States The high-technology sectors in Northern California, specifically Silicon Valley, in Santa Clara and San Mateo counties, have emerged from the economic downturn caused by the dot-com bust.

In 2010, there were more than 663,000 millionaires in the state, more than any other state in the nation. In 2010, California residents were ranked first among the states with the best average credit score of 754.




State spending increased from $56 billion in 1998 to $127 billion in 2011. California, with 12% of the United States population, has one-third of the nation's welfare recipients. California has the third highest per capita spending on welfare among the states, as well as the highest spending on welfare at $6.67 billion. In January 2011 the California's total debt was at least $265 billion. On June 27, 2013, Governor Jerry Brown signed a balanced budget (no deficit) for the state, its first in decades; however the state's debt remains at $132 billion.
With the passage of Proposition 30 in 2012, California now levies a 13.3% maximum marginal income tax rate with ten tax brackets, ranging from 1% at the bottom tax bracket of $0 annual individual income to 13.3% for annual individual income over $1,000,000. California has a state sales tax of 7.5%, though local governments can and do levy additional sales taxes. Many of these taxes are temporary for a seven-year period (as stipulated in Proposition 30) and afterwards will revert to a previous maximum marginal income tax bracket of 10.3% and state sales tax rate of 7.25%.
All real property is taxable annually; the tax is based on the property's fair market value at the time of purchase or new construction. Property tax increases are capped at 2% per year (see Proposition 13).







Because it is the most populous United States state, California is one of the country's largest users of energy. However because of its high energy rates, conservation mandates, mild weather in the largest population centers and strong environmental movement, its per capita energy use is one of the smallest of any United States state. Due to the high electricity demand, California imports more electricity than any other state, primarily hydroelectric power from states in the Pacific Northwest (via Path 15 and Path 66) and coal- and natural gas-fired production from the desert Southwest via Path 46.
As a result of the state's strong environmental movement, California has some of the most aggressive renewable energy goals in the United States, with a target for California to obtain a third of its electricity from renewables by 2020. Currently, several solar power plants such as the Solar Energy Generating Systems facility are located in the Mojave Desert. California's wind farms include Altamont Pass, San Gorgonio Pass, and Tehachapi Pass. Several dams across the state provide hydro-electric power. It would be possible to convert the total supply to 100% renewable energy, including heating, cooling and mobility, by 2050.
The state's crude oil and natural gas deposits are located in the Central Valley and along the coast, including the large Midway-Sunset Oil Field. Natural gas-fired power plants typically account for more than one-half of state electricity generation.
California is also home to two major nuclear power plants: Diablo Canyon and San Onofre, the latter having been shut down in 2013. Also voters banned the approval of new nuclear power plants since the late 1970s because of concerns over radioactive waste disposal. In addition, several cities such as Oakland, Berkeley and Davis have declared themselves as nuclear-free zones.




California's vast terrain is connected by an extensive system of controlled-access highways ('freeways'), limited-access roads ('expressways'), and highways. California is known for its car culture, giving California's cities a reputation for severe traffic congestion. Construction and maintenance of state roads and statewide transportation planning are primarily the responsibility of the California Department of Transportation, nicknamed "Caltrans". The rapidly growing population of the state is straining all of its transportation networks, and California has some of the worst roads in the United States. The Reason Foundation's 19th Annual Report on the Performance of State Highway Systems ranked California's highways the third-worst of any state, with Alaska second, and Rhode Island first.
The state has been a pioneer in road construction. One of the state's more visible landmarks, the Golden Gate Bridge, was once the longest suspension bridge main span in the world at 4,200 feet (1,300 m) when it opened in 1937. With its orange paint and panoramic views of the bay, this highway bridge is a popular tourist attraction and also accommodates pedestrians and bicyclists. The San Francisco–Oakland Bay Bridge (often abbreviated the "Bay Bridge"), completed in 1936, transports about 280,000 vehicles per day on two-decks. Its two sections meet at Yerba Buena Island through the world's largest diameter transportation bore tunnel, at 76 feet (23 m) wide by 58 feet (18 m) high. The Arroyo Seco Parkway, connecting Los Angeles and Pasadena, opened in 1940 as the first freeway in the Western United States. It was later extended south to the Four Level Interchange in downtown Los Angeles, regarded as the first stack interchange ever built.
Los Angeles International Airport (LAX), the 6th busiest airport in the world, and San Francisco International Airport (SFO), the 21st busiest airport in the world, are major hubs for trans-Pacific and transcontinental traffic. There are about a dozen important commercial airports and many more general aviation airports throughout the state.
California also has several important seaports. The giant seaport complex formed by the Port of Los Angeles and the Port of Long Beach in Southern California is the largest in the country and responsible for handling about a fourth of all container cargo traffic in the United States. The Port of Oakland, fourth largest in the nation, also handles trade entering from the Pacific Rim to the rest of the country. The Port of Stockton is the easternmost port on the west coast of the United States.
The California Highway Patrol is the largest statewide police agency in the United States in employment with over 10,000 employees. They are responsible for providing any police-sanctioned service to anyone on California's state maintained highways and on state property.
The California Department of Motor Vehicles is by far the largest in North America. By the end of 2009, the California DMV had 26,555,006 driver's licenses and ID cards on file. In 2010, there were 1.17 million new vehicle registrations in force.
Intercity rail travel is provided by Amtrak California, which manages the three busiest intercity rail lines in the United States outside the Northeast Corridor, all of which are funded by Caltrans. This service is becoming increasingly popular over flying and ridership is continuing to set records, especially on the LAX-SFO route. Integrated subway and light rail networks are found in Los Angeles (Metro Rail) and San Francisco (MUNI Metro). Light rail systems are also found in San Jose (VTA), San Diego (San Diego Trolley), Sacramento (RT Light Rail), and Northern San Diego County (Sprinter). Furthermore, commuter rail networks serve the San Francisco Bay Area (ACE, BART, Caltrain), Greater Los Angeles (Metrolink), and San Diego County (Coaster).
The California High-Speed Rail Authority was created in 1996 by the state to implement an extensive 700 miles (1,100 km) rail system. Construction was approved by the voters during the November 2008 general election, a $9.95 billion state bond will go toward its construction. Nearly all counties operate bus lines, and many cities operate their own city bus lines as well. Intercity bus travel is provided by Greyhound and Amtrak Thruway Coach.




California's interconnected water system is the world's largest, managing over 40,000,000 acre feet (49 km3) of water per year, centered on six main systems of aqueducts and infrastructure projects. Water use and conservation in California is a politically divisive issue, as the state experiences periodic droughts and has to balance the demands of its large agricultural and urban sectors, especially in the arid southern portion of the state. The state's widespread redistribution of water also invites the frequent scorn of environmentalists.
The California Water Wars, a conflict between Los Angeles and the Owens Valley over water rights, is one of the most well-known examples of the struggle to secure adequate water supplies. Former California Governor Arnold Schwarzenegger said: "We've been in crisis for quite some time because we're now 38 million people and not anymore 18 million people like we were in the late 60s. So it developed into a battle between environmentalists and farmers and between the south and the north and between rural and urban. And everyone has been fighting for the last four decades about water."







The state's capital is Sacramento.
California is organized into three branches of government – the executive branch consisting of the Governor and the other independently elected constitutional officers; the legislative branch consisting of the Assembly and Senate; and the judicial branch consisting of the Supreme Court of California and lower courts. The state also allows ballot propositions: direct participation of the electorate by initiative, referendum, recall, and ratification. Before the passage of California Proposition 14 (2010), California allowed each political party to choose whether to have a closed primary or a primary where only party members and independents vote. After June 8, 2010 when Proposition 14 was approved, excepting only the United States President and county central committee offices, all candidates in the primary elections are listed on the ballot with their preferred party affiliation, but they are not the official nominee of that party. At the primary election, the two candidates with the top votes will advance to the general election regardless of party affiliation. If at a special primary election, one candidate receives more than 50% of all the votes cast, they are elected to fill the vacancy and no special general election will be held.
Executive branch
The California executive branch consists of the Governor of California and seven other elected constitutional officers: Lieutenant Governor, Attorney General, Secretary of State, State Controller, State Treasurer, Insurance Commissioner, and State Superintendent of Public Instruction. They serve four-year terms and may be re-elected only once.
Legislative branch
The California State Legislature consists of a 40-member Senate and 80-member Assembly. Senators serve four-year terms and Assembly members two. Members of the Assembly are subject to term limits of three terms, and members of the Senate are subject to term limits of two terms.
Judicial branch
California's legal system is explicitly based upon English common law (as is the case with all other states except Louisiana) but carries a few features from Spanish civil law, such as community property. California's prison population grew from 25,000 in 1980 to over 170,000 in 2007. Capital punishment is a legal form of punishment and the state has the largest "Death Row" population in the country (though Texas is far more active in carrying out executions).
California's judiciary system is the largest in the United States (with a total of 1,600 judges, while the federal system has only about 840). At the apex is the seven Justices of the Supreme Court of California, while the California Courts of Appeal serve as the primary appellate courts and the California Superior Courts serve as the primary trial courts. Justices of the Supreme Court and Courts of Appeal are appointed by the Governor, but are subject to retention by the electorate every 12 years. The administration of the state's court system is controlled by the Judicial Council, composed of the Chief Justice of the California Supreme Court, 14 judicial officers, four representatives from the State Bar of California, and one member from each house of the state legislature.







California is divided into 58 counties. Per Article 11, Section 1, of the Constitution of California, they are the legal subdivisions of the state. The county government provides countywide services such as law enforcement, jails, elections and voter registration, vital records, property assessment and records, tax collection, public health, health care, social services, libraries, flood control, fire protection, animal control, agricultural regulations, building inspections, ambulance services, and education departments in charge of maintaining statewide standards. In addition, the county serves as the local government for all unincorporated areas. Each county is governed by an elected board of supervisors.



Incorporated cities and towns in California are either charter or general-law municipalities. General-law municipalities owe their existence to state law and are consequently governed by it; charter municipalities are governed by their own city or town charters. Municipalities incorporated in the 19th century tend to be charter municipalities. All ten of the state's most populous cities are charter cities. Most small cities have a council-manager form of government, where the elected city council appoints a city manager to supervise the operations of the city. Some larger cities have a directly-elected mayor who oversees the city government. In many council-manager cities, the city council selects one of its members as a mayor, sometimes rotating through the council membership—but this type of mayoral position is primarily ceremonial.
The Government of San Francisco is the only consolidated city-county in California, where both the city and county governments have been merged into one unified jurisdiction. The San Francisco Board of Supervisors also acts as the city council and the Mayor of San Francisco also serves as the county administrative officer.




About 1,102 school districts, independent of cities and counties, handle California's public education. California school districts may be organized as elementary districts, high school districts, unified school districts combining elementary and high school grades, or community college districts.
There are about 3,400 special districts in California. A special district, defined by California Government Code § 16271(d) as "any agency of the state for the local performance of governmental or proprietary functions within limited boundaries", provides a limited range of services within a defined geographic area. The geographic area of a special district can spread across multiple cities or counties, or could consist of only a portion of one. Most of California's special districts are single-purpose districts, and provide one service.




The state of California sends 53 members to the House of Representatives, the nation's largest congressional state delegation. Consequently California also has the largest number of electoral votes in national presidential elections, with 55. California's U.S. Senators are Dianne Feinstein, a native and former mayor of San Francisco, and Kamala Harris, a native, former District Attorney from San Francisco and former Attorney General of California. In 1992, California became the first state to have a Senate delegation entirely composed of women.




California has an idiosyncratic political culture compared to the rest of the country, and is sometimes regarded as a trendsetter. In socio-cultural mores and national politics, Californians are perceived as more liberal than other Americans, especially those who live in the inland states.
Among the political idiosyncrasies and trendsetting, California was the second state to recall their state governor, the second state to legalize abortion, and the only state to ban marriage for gay couples twice by voters (including Proposition 8 in 2008). Voters also passed Proposition 71 in 2004 to fund stem cell research, and Proposition 14 in 2010 to completely change the state's primary election process. California has also experienced disputes over water rights; and a tax revolt, culminating with the passage of Proposition 13 in 1978, limiting state property taxes.
The state's trend towards the Democratic Party and away from the Republican Party can be seen in state elections. From 1899 to 1939, California had Republican governors. Since 1990, California has generally elected Democratic candidates to federal, state and local offices, including current Governor Jerry Brown; however, the state has elected Republican Governors, though many of its Republican Governors, such as Arnold Schwarzenegger, tend to be considered moderate Republicans and more centrist than the national party.

The Democrats also now hold a majority in both houses of the state legislature. There are 56 Democrats and 24 Republicans in the Assembly; and 26 Democrats and 12 Republicans in the Senate.
The trend towards the Democratic Party is most obvious in presidential elections; Republicans have not won California's electoral votes since 1988.
In the United States House, the Democrats held a 34–19 edge in the CA delegation of the 110th United States Congress in 2007. As the result of gerrymandering, the districts in California were usually dominated by one or the other party, and few districts were considered competitive. In 2008, Californians passed Proposition 20 to empower a 14-member independent citizen commission to redraw districts for both local politicians and Congress. After the 2012 elections, when the new system took effect, Democrats gained 4 seats and held a 38–15 majority in the delegation.
In general, Democratic strength is centered in the populous coastal regions of the Los Angeles metropolitan area and the San Francisco Bay Area. Republican strength is still greatest in eastern parts of the state. Orange County also remains mostly Republican. One study ranked Berkeley, Oakland, Inglewood and San Francisco in the top 20 most liberal American cities; and Bakersfield, Orange, Escondido, Garden Grove, and Simi Valley in the top 20 most conservative cities.
In October 2012, out of the 23,802,577 people eligible to vote, 18,245,970 people were registered to vote. Of the people registered, the three largest registered groups were Democrats (7,966,422), Republicans (5,356,608), and Decline to State (3,820,545). Los Angeles County had the largest number of registered Democrats (2,430,612) and Republicans (1,037,031) of any county in the state.




In California, as of 2009, the U.S. Department of Defense had a total of 117,806 active duty servicemembers of which 88,370 were Sailors or Marines, 18,339 were Airmen, and 11,097 were Soldiers, with 61,365 Department of Defense civilian employees. Additionally, there were a total of 57,792 Reservists and Guardsman in California.
In 2010, Los Angeles County was the largest origin of military recruits in the United States by county, with 1,437 individuals enlisting in the military. However, as of 2002, Californians were relatively under-represented in the military as a proportion to its population.
In 2000, California, had 2,569,340 veterans of United States military service: 504,010 served in World War II, 301,034 in the Korean War, 754,682 during the Vietnam War, and 278,003 during 1990–2000 (including the Persian Gulf War). As of 2010, there were 1,942,775 veterans living in California, of which 1,457,875 served during a period of armed conflict, and just over four thousand served before World War II (the largest population of this group of any state).
California's military forces consist of the Army and Air National Guard, the naval and state military reserve (militia), and the California Cadet Corps.



California has a twinning arrangement with  Catalonia, Spain



Index of California-related articles
Outline of California – organized list of topics about California
Timeline of the far future









Cohen, Saul Bernard (2003). Geopolitics of the World System. Rowman & Littlefield. ISBN 978-0-8476-9907-0. 
Starr, Kevin (2007). California: A History. Modern Library Chronicles. 23. Random House Digital, Inc. ISBN 978-0-8129-7753-0. 



Chartkoff, Joseph L.; Chartkoff, Kerry Kona (1984). The archaeology of California. Stanford: Stanford University Press. ISBN 0-8047-1157-7. OCLC 11351549. 
Fagan, Brian (2003). Before California: An archaeologist looks at our earliest inhabitants. Lanham, MD: Rowman & Littlefield Publishers. ISBN 0-7425-2794-8. OCLC 226025645. 
Hart, James D. (1978). A Companion to California. New York, NY: Oxford University Press. ISBN 0-19-502400-1. 
Matthews, Glenna. The Golden State in the Civil War: Thomas Starr King, the Republican Party, and the Birth of Modern California. New York: Cambridge University Press, 2012.
Moratto, Michael J.; Fredrickson, David A. (1984). California archaeology. Orlando: Academic Press. ISBN 0-12-506182-X. OCLC 228668979. 




State of California
California State Guide, from the Library of Congress
 Geographic data related to California at OpenStreetMap
data.ca.gov: open data portal from California state agencies
California State Facts from USDA
California Drought: Farm and Food Impacts from USDA, Economic Research Service
California at DMOZ
1973 documentary featuring aerial views of the California coastline from Mt. Shasta to Los Angeles
Time-Lapse Tilt-Shift Portrait of California by Ryan and Sheri KillackeyAlaska (/əˈlæskə/) is a U.S. state located in the northwest extremity of North America. The Canadian administrative divisions of British Columbia and Yukon border the state to the east; its most extreme western part is Attu Island; it has a maritime border with Russia to the west across the Bering Strait. To the north are the Chukchi and Beaufort seas–the southern parts of the Arctic Ocean. The Pacific Ocean lies to the south and southwest. Alaska is the largest state in the United States by area, the 3rd least populous and the least densely populated of the 50 United States. Approximately half of Alaska's residents (the total estimated at 738,432 by the U.S. Census Bureau in 2015) live within the Anchorage metropolitan area. Alaska's economy is dominated by the fishing, natural gas, and oil industries, resources which it has in abundance. Military bases and tourism are also a significant part of the economy.
The United States purchased Alaska from the Russian Empire on March 30, 1867, for 7.2 million U.S. dollars at approximately two cents per acre ($4.74/km2). The area went through several administrative changes before becoming organized as a territory on May 11, 1912. It was admitted as the 49th state of the U.S. on January 3, 1959.



The name "Alaska" (Аляска) was introduced in the Russian colonial period when it was used to refer to the peninsula. It was derived from an Aleut, or Unangam idiom, which figuratively refers to the mainland of Alaska. Literally, it means object to which the action of the sea is directed.




Alaska is the northernmost and westernmost state in the United States and has the most easterly longitude in the United States because the Aleutian Islands extend into the Eastern Hemisphere. Alaska is the only non-contiguous U.S. state on continental North America; about 500 miles (800 km) of British Columbia (Canada) separates Alaska from Washington. It is technically part of the continental U.S., but is sometimes not included in colloquial use; Alaska is not part of the contiguous U.S., often called "the Lower 48". The capital city, Juneau, is situated on the mainland of the North American continent but is not connected by road to the rest of the North American highway system.
The state is bordered by Yukon and British Columbia in Canada, to the east, the Gulf of Alaska and the Pacific Ocean to the south and southwest, the Bering Sea, Bering Strait, and Chukchi Sea to the west and the Arctic Ocean to the north. Alaska's territorial waters touch Russia's territorial waters in the Bering Strait, as the Russian Big Diomede Island and Alaskan Little Diomede Island are only 3 miles (4.8 km) apart. Alaska has a longer coastline than all the other U.S. states combined.

Alaska is the largest state in the United States in land area at 663,268 square miles (1,717,856 km2), over twice the size of Texas, the next largest state. Alaska is larger than all but 18 sovereign countries. Counting territorial waters, Alaska is larger than the combined area of the next three largest states: Texas, California, and Montana. It is also larger than the combined area of the 22 smallest U.S. states.



There are no officially defined borders demarcating the various regions of Alaska, but there are six widely accepted regions:




The most populous region of Alaska, containing Anchorage, the Matanuska-Susitna Valley and the Kenai Peninsula. Rural, mostly unpopulated areas south of the Alaska Range and west of the Wrangell Mountains also fall within the definition of South Central, as do the Prince William Sound area and the communities of Cordova and Valdez.




Also referred to as the Panhandle or Inside Passage, this is the region of Alaska closest to the rest of the United States. As such, this was where most of the initial non-indigenous settlement occurred in the years following the Alaska Purchase. The region is dominated by the Alexander Archipelago as well as the Tongass National Forest, the largest national forest in the United States. It contains the state capital Juneau, the former capital Sitka, and Ketchikan, at one time Alaska's largest city. The Alaska Marine Highway provides a vital surface transportation link throughout the area, as only three communities (Haines, Hyder and Skagway) enjoy direct connections to the contiguous North American road system. Officially designated in 1963.




The Interior is the largest region of Alaska; much of it is uninhabited wilderness. Fairbanks is the only large city in the region. Denali National Park and Preserve is located here. Denali is the highest mountain in North America.




Southwest Alaska is a sparsely inhabited region stretching some 500 miles (800 km) inland from the Bering Sea. Most of the population lives along the coast. Kodiak Island is also located in Southwest. The massive Yukon–Kuskokwim Delta, one of the largest river deltas in the world, is here. Portions of the Alaska Peninsula are considered part of Southwest, with the remaining portions included with the Aleutian Islands (see below).




The North Slope is mostly tundra peppered with small villages. The area is known for its massive reserves of crude oil, and contains both the National Petroleum Reserve–Alaska and the Prudhoe Bay Oil Field. Barrow, the northernmost city in the United States, is located here. The Northwest Arctic area, anchored by Kotzebue and also containing the Kobuk River valley, is often regarded as being part of this region. However, the respective Inupiat of the North Slope and of the Northwest Arctic seldom consider themselves to be one people.




More than 300 small volcanic islands make up this chain, which stretches over 1,200 miles (1,900 km) into the Pacific Ocean. Some of these islands fall in the Eastern Hemisphere, but the International Date Line was drawn west of 180° to keep the whole state, and thus the entire North American continent, within the same legal day. Two of the islands, Attu and Kiska, were occupied by Japanese forces during World War II.




With its myriad islands, Alaska has nearly 34,000 miles (54,720 km) of tidal shoreline. The Aleutian Islands chain extends west from the southern tip of the Alaska Peninsula. Many active volcanoes are found in the Aleutians and in coastal regions. Unimak Island, for example, is home to Mount Shishaldin, which is an occasionally smoldering volcano that rises to 10,000 feet (3,048 m) above the North Pacific. It is the most perfect volcanic cone on Earth, even more symmetrical than Japan's Mount Fuji. The chain of volcanoes extends to Mount Spurr, west of Anchorage on the mainland. Geologists have identified Alaska as part of Wrangellia, a large region consisting of multiple states and Canadian provinces in the Pacific Northwest, which is actively undergoing continent building.
One of the world's largest tides occurs in Turnagain Arm, just south of Anchorage – tidal differences can be more than 35 feet (10.7 m).

Alaska has more than three million lakes. Marshlands and wetland permafrost cover 188,320 square miles (487,747 km2) (mostly in northern, western and southwest flatlands). Glacier ice covers some 16,000 square miles (41,440 km2) of land and 1,200 square miles (3,110 km2) of tidal zone. The Bering Glacier complex near the southeastern border with Yukon covers 2,250 square miles (5,827 km2) alone. With over 100,000 glaciers, Alaska has half of all in the world.




According to an October 1998 report by the United States Bureau of Land Management, approximately 65% of Alaska is owned and managed by the U.S. federal government as public lands, including a multitude of national forests, national parks, and national wildlife refuges. Of these, the Bureau of Land Management manages 87 million acres (35 million hectares), or 23.8% of the state. The Arctic National Wildlife Refuge is managed by the United States Fish and Wildlife Service. It is the world's largest wildlife refuge, comprising 16 million acres (6.5 million hectares).
Of the remaining land area, the state of Alaska owns 101 million acres (41 million hectares), its entitlement under the Alaska Statehood Act. A portion of that acreage is occasionally ceded to organized boroughs, under the statutory provisions pertaining to newly formed boroughs. Smaller portions are set aside for rural subdivisions and other homesteading-related opportunities. These are not very popular due to the often remote and roadless locations. The University of Alaska, as a land grant university, also owns substantial acreage which it manages independently.
Another 44 million acres (18 million hectares) are owned by 12 regional, and scores of local, Native corporations created under the Alaska Native Claims Settlement Act (ANCSA) of 1971. Regional Native corporation Doyon, Limited often promotes itself as the largest private landowner in Alaska in advertisements and other communications. Provisions of ANCSA allowing the corporations' land holdings to be sold on the open market starting in 1991 were repealed before they could take effect. Effectively, the corporations hold title (including subsurface title in many cases, a privilege denied to individual Alaskans) but cannot sell the land. Individual Native allotments can be and are sold on the open market, however.
Various private interests own the remaining land, totaling about one percent of the state. Alaska is, by a large margin, the state with the smallest percentage of private land ownership when Native corporation holdings are excluded.




The climate in Southeast Alaska is a mid-latitude oceanic climate (Köppen climate classification: Cfb) in the southern sections and a subarctic oceanic climate (Köppen Cfc) in the northern parts. On an annual basis, Southeast is both the wettest and warmest part of Alaska with milder temperatures in the winter and high precipitation throughout the year. Juneau averages over 50 in (130 cm) of precipitation a year, and Ketchikan averages over 150 in (380 cm). This is also the only region in Alaska in which the average daytime high temperature is above freezing during the winter months.
The climate of Anchorage and south central Alaska is mild by Alaskan standards due to the region's proximity to the seacoast. While the area gets less rain than southeast Alaska, it gets more snow, and days tend to be clearer. On average, Anchorage receives 16 in (41 cm) of precipitation a year, with around 75 in (190 cm) of snow, although there are areas in the south central which receive far more snow. It is a subarctic climate (Köppen: Dfc) due to its brief, cool summers.
The climate of Western Alaska is determined in large part by the Bering Sea and the Gulf of Alaska. It is a subarctic oceanic climate in the southwest and a continental subarctic climate farther north. The temperature is somewhat moderate considering how far north the area is. This region has a tremendous amount of variety in precipitation. An area stretching from the northern side of the Seward Peninsula to the Kobuk River valley (i. e., the region around Kotzebue Sound) is technically a desert, with portions receiving less than 10 in (25 cm) of precipitation annually. On the other extreme, some locations between Dillingham and Bethel average around 100 in (250 cm) of precipitation.
The climate of the interior of Alaska is subarctic. Some of the highest and lowest temperatures in Alaska occur around the area near Fairbanks. The summers may have temperatures reaching into the 90s °F (the low-to-mid 30s °C), while in the winter, the temperature can fall below −60 °F (−51 °C). Precipitation is sparse in the Interior, often less than 10 in (25 cm) a year, but what precipitation falls in the winter tends to stay the entire winter.
The highest and lowest recorded temperatures in Alaska are both in the Interior. The highest is 100 °F (38 °C) in Fort Yukon (which is just 8 mi or 13 km inside the arctic circle) on June 27, 1915, making Alaska tied with Hawaii as the state with the lowest high temperature in the United States. The lowest official Alaska temperature is −80 °F (−62 °C) in Prospect Creek on January 23, 1971, one degree above the lowest temperature recorded in continental North America (in Snag, Yukon, Canada).
The climate in the extreme north of Alaska is Arctic (Köppen: ET) with long, very cold winters and short, cool summers. Even in July, the average low temperature in Barrow is 34 °F (1 °C). Precipitation is light in this part of Alaska, with many places averaging less than 10 in (25 cm) per year, mostly as snow which stays on the ground almost the entire year.







Numerous indigenous peoples occupied Alaska for thousands of years before the arrival of European peoples to the area. Linguistic and DNA studies done here have provided evidence for the settlement of North America by way of the Bering land bridge. The Tlingit people developed a society with a matrilineal kinship system of property inheritance and descent in what is today Southeast Alaska, along with parts of British Columbia and the Yukon. Also in Southeast were the Haida, now well known for their unique arts. The Tsimshian people came to Alaska from British Columbia in 1887, when President Grover Cleveland, and later the U.S. Congress, granted them permission to settle on Annette Island and found the town of Metlakatla. All three of these peoples, as well as other indigenous peoples of the Pacific Northwest Coast, experienced smallpox outbreaks from the late 18th through the mid-19th century, with the most devastating epidemics occurring in the 1830s and 1860s, resulting in high fatalities and social disruption.
The Aleutian Islands are still home to the Aleut people's seafaring society, although they were the first Native Alaskans to be exploited by Russians. Western and Southwestern Alaska are home to the Yup'ik, while their cousins the Alutiiq ~ Sugpiaq lived in what is now Southcentral Alaska. The Gwich'in people of the northern Interior region are Athabaskan and primarily known today for their dependence on the caribou within the much-contested Arctic National Wildlife Refuge. The North Slope and Little Diomede Island are occupied by the widespread Inupiat people.




Some researchers believe that the first Russian settlement in Alaska was established in the 17th century. According to this hypothesis, in 1648 several koches of Semyon Dezhnyov's expedition came ashore in Alaska by storm and founded this settlement. This hypothesis is based on the testimony of Chukchi geographer Nikolai Daurkin, who had visited Alaska in 1764–1765 and who had reported on a village on the Kheuveren River, populated by "bearded men" who "pray to the icons". Some modern researchers associate Kheuveren with Koyuk River.

The first European vessel to reach Alaska is generally held to be the St. Gabriel under the authority of the surveyor M. S. Gvozdev and assistant navigator I. Fyodorov on August 21, 1732 during an expedition of Siberian cossak A. F. Shestakov and Belorussian explorer Dmitry Pavlutsky (1729—1735).
Another European contact with Alaska occurred in 1741, when Vitus Bering led an expedition for the Russian Navy aboard the St. Peter. After his crew returned to Russia with sea otter pelts judged to be the finest fur in the world, small associations of fur traders began to sail from the shores of Siberia toward the Aleutian Islands. The first permanent European settlement was founded in 1784.
Between 1774 and 1800, Spain sent several expeditions to Alaska in order to assert its claim over the Pacific Northwest. In 1789 a Spanish settlement and fort were built in Nootka Sound. These expeditions gave names to places such as Valdez, Bucareli Sound, and Cordova. Later, the Russian-American Company carried out an expanded colonization program during the early-to-mid-19th century.
Sitka, renamed New Archangel from 1804 to 1867, on Baranof Island in the Alexander Archipelago in what is now Southeast Alaska, became the capital of Russian America. It remained the capital after the colony was transferred to the United States. The Russians never fully colonized Alaska, and the colony was never very profitable. Evidence of Russian settlement in names and churches survive throughout southeast Alaska.
William H. Seward, the United States Secretary of State, negotiated the Alaska Purchase (also known as Seward's Folly) with the Russians in 1867 for $7.2 million. Alaska was loosely governed by the military initially, and was administered as a district starting in 1884, with a governor appointed by the President of the United States. A federal district court was headquartered in Sitka.

For most of Alaska's first decade under the United States flag, Sitka was the only community inhabited by American settlers. They organized a "provisional city government," which was Alaska's first municipal government, but not in a legal sense. Legislation allowing Alaskan communities to legally incorporate as cities did not come about until 1900, and home rule for cities was extremely limited or unavailable until statehood took effect in 1959.




Starting in the 1890s and stretching in some places to the early 1910s, gold rushes in Alaska and the nearby Yukon Territory brought thousands of miners and settlers to Alaska. Alaska was officially incorporated as an organized territory in 1912. Alaska's capital, which had been in Sitka until 1906, was moved north to Juneau. Construction of the Alaska Governor's Mansion began that same year. European immigrants from Norway and Sweden also settled in southeast Alaska, where they entered the fishing and logging industries.

During World War II, the Aleutian Islands Campaign focused on the three outer Aleutian Islands – Attu, Agattu and Kiska – that were invaded by Japanese troops and occupied between June 1942 and August 1943. During the occupation, one Alaskan civilian was killed by Japanese troops and nearly fifty were interned in Japan, where about half of them died. Unalaska/Dutch Harbor became a significant base for the United States Army Air Forces and Navy submariners.
The United States Lend-Lease program involved the flying of American warplanes through Canada to Fairbanks and thence Nome; Soviet pilots took possession of these aircraft, ferrying them to fight the German invasion of the Soviet Union. The construction of military bases contributed to the population growth of some Alaskan cities.




Statehood for Alaska was an important cause of James Wickersham early in his tenure as a congressional delegate. Decades later, the statehood movement gained its first real momentum following a territorial referendum in 1946. The Alaska Statehood Committee and Alaska's Constitutional Convention would soon follow. Statehood supporters also found themselves fighting major battles against political foes, mostly in the U.S. Congress but also within Alaska. Statehood was approved by Congress on July 7, 1958. Alaska was officially proclaimed a state on January 3, 1959.
In 1960, the Census Bureau reported Alaska's population as 77.2% White, 3% Black, and 18.8% American Indian and Alaska Native.




On March 27, 1964, the massive Good Friday earthquake killed 133 people and destroyed several villages and portions of large coastal communities, mainly by the resultant tsunamis and landslides. It was the second-most-powerful earthquake in the recorded history of the world, with a moment magnitude of 9.2. It was over one thousand times more powerful than the 1989 San Francisco earthquake. The time of day (5:36 pm), time of year and location of the epicenter were all cited as factors in potentially sparing thousands of lives, particularly in Anchorage.



The 1968 discovery of oil at Prudhoe Bay and the 1977 completion of the Trans-Alaska Pipeline System led to an oil boom. Royalty revenues from oil have funded large state budgets from 1980 onward. That same year, not coincidentally, Alaska repealed its state income tax.
In 1989, the Exxon Valdez hit a reef in the Prince William Sound, spilling over 11 million U.S. gallons (42 megaliters) of crude oil over 1,100 miles (1,800 km) of coastline. Today, the battle between philosophies of development and conservation is seen in the contentious debate over oil drilling in the Arctic National Wildlife Refuge and the proposed Pebble Mine.



The Alaska Heritage Resources Survey (AHRS) is a restricted inventory of all reported historic and prehistoric sites within the state of Alaska; it is maintained by the Office of History and Archaeology. The survey's inventory of cultural resources includes objects, structures, buildings, sites, districts, and travel ways, with a general provision that they are over 50 years old. As of January 31, 2012, over 35,000 sites have been reported.




The United States Census Bureau estimates that the population of Alaska was 738,432 on July 1, 2015, a 3.97% increase since the 2010 United States Census.
In 2010, Alaska ranked as the 47th state by population, ahead of North Dakota, Vermont, and Wyoming (and Washington, D.C.) Alaska is the least densely populated state, and one of the most sparsely populated areas in the world, at 1.2 inhabitants per square mile (0.46/km2), with the next state, Wyoming, at 5.8 inhabitants per square mile (2.2/km2). Alaska is the largest U.S. state by area, and the tenth wealthiest (per capita income). As of November 2014, the state's unemployment rate was 6.6%.



According to the 2010 United States Census, Alaska had a population of 710,231. In terms of race and ethnicity, the state was 66.7% White (64.1% Non-Hispanic White), 14.8% American Indian and Alaska Native, 5.4% Asian, 3.3% Black or African American, 1.0% Native Hawaiian and Other Pacific Islander, 1.6% from Some Other Race, and 7.3% from Two or More Races. Hispanics or Latinos of any race made up 5.5% of the population.
As of 2011, 50.7% of Alaska's population younger than one year of age belonged to minority groups (i.e., did not have two parents of non-Hispanic white ancestry).




According to the 2011 American Community Survey, 83.4% of people over the age of five speak only English at home. About 3.5% speak Spanish at home. About 2.2% speak another Indo-European language at home and about 4.3% speak an Asian language at home. About 5.3% speak other languages at home.
The Alaska Native Language Center at the University of Alaska Fairbanks claims that at least 20 Alaskan native languages exist and there are also some languages with different dialects. Most of Alaska's native languages belong to either the Eskimo–Aleut or Na-Dene language families however some languages are thought to be isolates (e.g. Haida) or have not yet been classified (e.g. Tsimshianic). As of 2014 nearly all of Alaska's native languages were classified as either threatened, shifting, moribund, nearly extinct, or dormant languages.
A total of 5.2% of Alaskans speak one of the state's 20 indigenous languages, known locally as "native languages".
In October 2014, the governor of Alaska signed a bill declaring the state's 20 indigenous languages as official languages. This bill gave the languages symbolic recognition as official languages, though they have not been adopted for official use within the government. The 20 languages that were included in the bill are:
Inupiaq
Siberian Yupik
Central Alaskan Yup’ik
Alutiiq
Unangax
Dena’ina
Deg Xinag
Holikachuk
Koyukon
Upper Kuskokwim
Gwich’in
Tanana
Upper Tanana
Tanacross
Hän
Ahtna
Eyak
Tlingit
Haida
Tsimshian




According to statistics collected by the Association of Religion Data Archives from 2010, about 34% of Alaska residents were members of religious congregations. 100,960 people identified as Evangelical Protestants, 50,866 as Roman Catholic, and 32,550 as mainline Protestants. Roughly 4% are Mormon, 0.5% are Jewish, 1% are Muslim, 0.5% are Buddhist, and 0.5% are Hindu. The largest religious denominations in Alaska as of 2010 were the Catholic Church with 50,866 adherents, non-denominational Evangelical Protestants with 38,070 adherents, The Church of Jesus Christ of Latter-day Saints with 32,170 adherents, and the Southern Baptist Convention with 19,891 adherents. Alaska has been identified, along with Pacific Northwest states Washington and Oregon, as being the least religious states of the USA, in terms of church membership.
In 1795, the First Russian Orthodox Church was established in Kodiak. Intermarriage with Alaskan Natives helped the Russian immigrants integrate into society. As a result, an increasing number of Russian Orthodox churches gradually became established within Alaska. Alaska also has the largest Quaker population (by percentage) of any state. In 2009 there were 6,000 Jews in Alaska (for whom observance of halakha may pose special problems). Alaskan Hindus often share venues and celebrations with members of other Asian religious communities, including Sikhs and Jains.
Estimates for the number of Muslims in Alaska range from 2,000 to 5,000. The Islamic Community Center of Anchorage began efforts in the late 1990s to construct a mosque in Anchorage. They broke ground on a building in south Anchorage in 2010 and were nearing completion in late 2014. When completed, the mosque will be the first in the state and one of the northernmost mosques in the world.




The 2007 gross state product was $44.9 billion, 45th in the nation. Its per capita personal income for 2007 was $40,042, ranking 15th in the nation. According to a 2013 study by Phoenix Marketing International, Alaska had the fifth-largest number of millionaires per capita in the United States, with a ratio of 6.75 percent. The oil and gas industry dominates the Alaskan economy, with more than 80% of the state's revenues derived from petroleum extraction. Alaska's main export product (excluding oil and natural gas) is seafood, primarily salmon, cod, Pollock and crab.
Agriculture represents a very small fraction of the Alaskan economy. Agricultural production is primarily for consumption within the state and includes nursery stock, dairy products, vegetables, and livestock. Manufacturing is limited, with most foodstuffs and general goods imported from elsewhere.
Employment is primarily in government and industries such as natural resource extraction, shipping, and transportation. Military bases are a significant component of the economy in the Fairbanks North Star, Anchorage and Kodiak Island boroughs, as well as Kodiak. Federal subsidies are also an important part of the economy, allowing the state to keep taxes low. Its industrial outputs are crude petroleum, natural gas, coal, gold, precious metals, zinc and other mining, seafood processing, timber and wood products. There is also a growing service and tourism sector. Tourists have contributed to the economy by supporting local lodging.




Alaska has vast energy resources, although its oil reserves have been largely depleted. Major oil and gas reserves were found in the Alaska North Slope (ANS) and Cook Inlet basins, but according to the Energy Information Administration, by February 2014 Alaska had fallen to fourth place in the nation in crude oil production after Texas, North Dakota, and California. Prudhoe Bay on Alaska's North Slope is still the second highest-yielding oil field in the United States, typically producing about 400,000 barrels per day (64,000 m3/d), although by early 2014 North Dakota's Bakken Formation was producing over 900,000 barrels per day (140,000 m3/d). Prudhoe Bay was the largest conventional oil field ever discovered in North America, but was much smaller than Canada's enormous Athabasca oil sands field, which by 2014 was producing about 1,500,000 barrels per day (240,000 m3/d) of unconventional oil, and had hundreds of years of producible reserves at that rate.
The Trans-Alaska Pipeline can transport and pump up to 2.1 million barrels (330,000 m3) of crude oil per day, more than any other crude oil pipeline in the United States. Additionally, substantial coal deposits are found in Alaska's bituminous, sub-bituminous, and lignite coal basins. The United States Geological Survey estimates that there are 85.4 trillion cubic feet (2,420 km3) of undiscovered, technically recoverable gas from natural gas hydrates on the Alaskan North Slope. Alaska also offers some of the highest hydroelectric power potential in the country from its numerous rivers. Large swaths of the Alaskan coastline offer wind and geothermal energy potential as well.

Alaska's economy depends heavily on increasingly expensive diesel fuel for heating, transportation, electric power and light. Though wind and hydroelectric power are abundant and underdeveloped, proposals for statewide energy systems (e.g. with special low-cost electric interties) were judged uneconomical (at the time of the report, 2001) due to low (less than 50¢/gal) fuel prices, long distances and low population. The cost of a gallon of gas in urban Alaska today is usually 30–60¢ higher than the national average; prices in rural areas are generally significantly higher but vary widely depending on transportation costs, seasonal usage peaks, nearby petroleum development infrastructure and many other factors.



The Alaska Permanent Fund is a constitutionally authorized appropriation of oil revenues, established by voters in 1976 to manage a surplus in state petroleum revenues from oil, largely in anticipation of the then recently constructed Trans-Alaska Pipeline System. The fund was originally proposed by Governor Keith Miller on the eve of the 1969 Prudhoe Bay lease sale, out of fear that the legislature would spend the entire proceeds of the sale (which amounted to $900 million) at once. It was later championed by Governor Jay Hammond and Kenai state representative Hugh Malone. It has served as an attractive political prospect ever since, diverting revenues which would normally be deposited into the general fund.
The Alaska Constitution was written so as to discourage dedicating state funds for a particular purpose. The Permanent Fund has become the rare exception to this, mostly due to the political climate of distrust existing during the time of its creation. From its initial principal of $734,000, the fund has grown to $50 billion as a result of oil royalties and capital investment programs. Most if not all the principal is invested conservatively outside Alaska. This has led to frequent calls by Alaskan politicians for the Fund to make investments within Alaska, though such a stance has never gained momentum.
Starting in 1982, dividends from the fund's annual growth have been paid out each year to eligible Alaskans, ranging from an initial $1,000 in 1982 (equal to three years' payout, as the distribution of payments was held up in a lawsuit over the distribution scheme) to $3,269 in 2008 (which included a one-time $1,200 "Resource Rebate"). Every year, the state legislature takes out 8% from the earnings, puts 3% back into the principal for inflation proofing, and the remaining 5% is distributed to all qualifying Alaskans. To qualify for the Permanent Fund Dividend, one must have lived in the state for a minimum of 12 months, maintain constant residency subject to allowable absences, and not be subject to court judgments or criminal convictions which fall under various disqualifying classifications or may subject the payment amount to civil garnishment.
The Permanent Fund is often considered to be one of the leading examples of a "Basic Income" policy in the world.



The cost of goods in Alaska has long been higher than in the contiguous 48 states. Federal government employees, particularly United States Postal Service (USPS) workers and active-duty military members, receive a Cost of Living Allowance usually set at 25% of base pay because, while the cost of living has gone down, it is still one of the highest in the country.
Rural Alaska suffers from extremely high prices for food and consumer goods compared to the rest of the country, due to the relatively limited transportation infrastructure.




Due to the northern climate and short growing season, relatively little farming occurs in Alaska. Most farms are in either the Matanuska Valley, about 40 miles (64 km) northeast of Anchorage, or on the Kenai Peninsula, about 60 miles (97 km) southwest of Anchorage. The short 100-day growing season limits the crops that can be grown, but the long sunny summer days make for productive growing seasons. The primary crops are potatoes, carrots, lettuce, and cabbage.
The Tanana Valley is another notable agricultural locus, especially the Delta Junction area, about 100 miles (160 km) southeast of Fairbanks, with a sizable concentration of farms growing agronomic crops; these farms mostly lie north and east of Fort Greely. This area was largely set aside and developed under a state program spearheaded by Hammond during his second term as governor. Delta-area crops consist predominately of barley and hay. West of Fairbanks lies another concentration of small farms catering to restaurants, the hotel and tourist industry, and community-supported agriculture.
Alaskan agriculture has experienced a surge in growth of market gardeners, small farms and farmers' markets in recent years, with the highest percentage increase (46%) in the nation in growth in farmers' markets in 2011, compared to 17% nationwide. The peony industry has also taken off, as the growing season allows farmers to harvest during a gap in supply elsewhere in the world, thereby filling a niche in the flower market.

Alaska, with no counties, lacks county fairs. However, a small assortment of state and local fairs (with the Alaska State Fair in Palmer the largest), are held mostly in the late summer. The fairs are mostly located in communities with historic or current agricultural activity, and feature local farmers exhibiting produce in addition to more high-profile commercial activities such as carnival rides, concerts and food. "Alaska Grown" is used as an agricultural slogan.
Alaska has an abundance of seafood, with the primary fisheries in the Bering Sea and the North Pacific. Seafood is one of the few food items that is often cheaper within the state than outside it. Many Alaskans take advantage of salmon seasons to harvest portions of their household diet while fishing for subsistence, as well as sport. This includes fish taken by hook, net or wheel.
Hunting for subsistence, primarily caribou, moose, and Dall sheep is still common in the state, particularly in remote Bush communities. An example of a traditional native food is Akutaq, the Eskimo ice cream, which can consist of reindeer fat, seal oil, dried fish meat and local berries.
Alaska's reindeer herding is concentrated on Seward Peninsula, where wild caribou can be prevented from mingling and migrating with the domesticated reindeer.
Most food in Alaska is transported into the state from "Outside", and shipping costs make food in the cities relatively expensive. In rural areas, subsistence hunting and gathering is an essential activity because imported food is prohibitively expensive. Though most small towns and villages in Alaska lie along the coastline, the cost of importing food to remote villages can be high, because of the terrain and difficult road conditions, which change dramatically, due to varying climate and precipitation changes. The cost of transport can reach as high as 50¢ per pound ($1.10/kg) or more in some remote areas, during the most difficult times, if these locations can be reached at all during such inclement weather and terrain conditions. The cost of delivering a 1 US gallon (3.8 L) of milk is about $3.50 in many villages where per capita income can be $20,000 or less. Fuel cost per gallon is routinely 20–30¢ higher than the continental United States average, with only Hawaii having higher prices.







Alaska has few road connections compared to the rest of the U.S. The state's road system covers a relatively small area of the state, linking the central population centers and the Alaska Highway, the principal route out of the state through Canada. The state capital, Juneau, is not accessible by road, only a car ferry, which has spurred several debates over the decades about moving the capital to a city on the road system, or building a road connection from Haines. The western part of Alaska has no road system connecting the communities with the rest of Alaska.
One unique feature of the Alaska Highway system is the Anton Anderson Memorial Tunnel, an active Alaska Railroad tunnel recently upgraded to provide a paved roadway link with the isolated community of Whittier on Prince William Sound to the Seward Highway about 50 miles (80 km) southeast of Anchorage at Portage. At 2.5 miles (4.0 km), the tunnel was the longest road tunnel in North America until 2007. The tunnel is the longest combination road and rail tunnel in North America.




Built around 1915, the Alaska Railroad (ARR) played a key role in the development of Alaska through the 20th century. It links north Pacific shipping through providing critical infrastructure with tracks that run from Seward to Interior Alaska by way of South Central Alaska, passing through Anchorage, Eklutna, Wasilla, Talkeetna, Denali, and Fairbanks, with spurs to Whittier, Palmer and North Pole. The cities, towns, villages, and region served by ARR tracks are known statewide as "The Railbelt". In recent years, the ever-improving paved highway system began to eclipse the railroad's importance in Alaska's economy.
The railroad played a vital role in Alaska's development, moving freight into Alaska while transporting natural resources southward (i.e., coal from the Usibelli coal mine near Healy to Seward and gravel from the Matanuska Valley to Anchorage). It is well known for its summertime tour passenger service.
The Alaska Railroad was one of the last railroads in North America to use cabooses in regular service and still uses them on some gravel trains. It continues to offer one of the last flag stop routes in the country. A stretch of about 60 miles (100 km) of track along an area north of Talkeetna remains inaccessible by road; the railroad provides the only transportation to rural homes and cabins in the area. Until construction of the Parks Highway in the 1970s, the railroad provided the only land access to most of the region along its entire route.
In northern Southeast Alaska, the White Pass and Yukon Route also partly runs through the state from Skagway northwards into Canada (British Columbia and Yukon Territory), crossing the border at White Pass Summit. This line is now mainly used by tourists, often arriving by cruise liner at Skagway. It was featured in the 1983 BBC television series Great Little Railways.
The Alaska Rail network is not connected to Outside. In 2000, the U.S. Congress authorized $6 million to study the feasibility of a rail link between Alaska, Canada, and the lower 48.
Alaska Rail Marine provides car float service between Whittier and Seattle.



Many cities, towns and villages in the state do not have road or highway access; the only modes of access involve travel by air, river, or the sea.

Alaska's well-developed state-owned ferry system (known as the Alaska Marine Highway) serves the cities of southeast, the Gulf Coast and the Alaska Peninsula. The ferries transport vehicles as well as passengers. The system also operates a ferry service from Bellingham, Washington and Prince Rupert, British Columbia in Canada through the Inside Passage to Skagway. The Inter-Island Ferry Authority also serves as an important marine link for many communities in the Prince of Wales Island region of Southeast and works in concert with the Alaska Marine Highway.
In recent years, cruise lines have created a summertime tourism market, mainly connecting the Pacific Northwest to Southeast Alaska and, to a lesser degree, towns along Alaska's gulf coast. The population of Ketchikan may rise by over 10,000 people on many days during the summer, as up to four large cruise ships at a time can dock, debarking thousands of passengers.



Cities not served by road, sea, or river can be reached only by air, foot, dogsled, or snowmachine, accounting for Alaska's extremely well developed bush air services—an Alaskan novelty. Anchorage and, to a lesser extent Fairbanks, is served by many major airlines. Because of limited highway access, air travel remains the most efficient form of transportation in and out of the state. Anchorage recently completed extensive remodeling and construction at Ted Stevens Anchorage International Airport to help accommodate the upsurge in tourism (in 2012–2013, Alaska received almost 2 million visitors).
Regular flights to most villages and towns within the state that are commercially viable are challenging to provide, so they are heavily subsidized by the federal government through the Essential Air Service program. Alaska Airlines is the only major airline offering in-state travel with jet service (sometimes in combination cargo and passenger Boeing 737-400s) from Anchorage and Fairbanks to regional hubs like Bethel, Nome, Kotzebue, Dillingham, Kodiak, and other larger communities as well as to major Southeast and Alaska Peninsula communities.

The bulk of remaining commercial flight offerings come from small regional commuter airlines such as Ravn Alaska, PenAir, and Frontier Flying Service. The smallest towns and villages must rely on scheduled or chartered bush flying services using general aviation aircraft such as the Cessna Caravan, the most popular aircraft in use in the state. Much of this service can be attributed to the Alaska bypass mail program which subsidizes bulk mail delivery to Alaskan rural communities. The program requires 70% of that subsidy to go to carriers who offer passenger service to the communities.
Many communities have small air taxi services. These operations originated from the demand for customized transport to remote areas. Perhaps the most quintessentially Alaskan plane is the bush seaplane. The world's busiest seaplane base is Lake Hood, located next to Ted Stevens Anchorage International Airport, where flights bound for remote villages without an airstrip carry passengers, cargo, and many items from stores and warehouse clubs. In 2006 Alaska had the highest number of pilots per capita of any U.S. state.



Another Alaskan transportation method is the dogsled. In modern times (that is, any time after the mid-late 1920s), dog mushing is more of a sport than a true means of transportation. Various races are held around the state, but the best known is the Iditarod Trail Sled Dog Race, a 1,150-mile (1,850 km) trail from Anchorage to Nome (although the distance varies from year to year, the official distance is set at 1,049 miles or 1,688 km). The race commemorates the famous 1925 serum run to Nome in which mushers and dogs like Togo and Balto took much-needed medicine to the diphtheria-stricken community of Nome when all other means of transportation had failed. Mushers from all over the world come to Anchorage each March to compete for cash, prizes, and prestige. The "Serum Run" is another sled dog race that more accurately follows the route of the famous 1925 relay, leaving from the community of Nenana (southwest of Fairbanks) to Nome.
In areas not served by road or rail, primary transportation in summer is by all-terrain vehicle and in winter by snowmobile or "snow machine," as it is commonly referred to in Alaska.



Alaska's internet and other data transport systems are provided largely through the two major telecommunications companies: GCI and Alaska Communications. GCI owns and operates what it calls the Alaska United Fiber Optic system and as of late 2011 Alaska Communications advertised that it has "two fiber optic paths to the lower 48 and two more across Alaska. In January 2011, it was reported that a $1 billion project to connect Asia and rural Alaska was being planned, aided in part by $350 million in stimulus from the federal government.







Like all other U.S. states, Alaska is governed as a republic, with three branches of government: an executive branch consisting of the Governor of Alaska and the other independently elected constitutional officers; a legislative branch consisting of the Alaska House of Representatives and Alaska Senate; and a judicial branch consisting of the Alaska Supreme Court and lower courts.
The state of Alaska employs approximately 16,000 people statewide.
The Alaska Legislature consists of a 40-member House of Representatives and a 20-member Senate. Senators serve four-year terms and House members two. The Governor of Alaska serves four-year terms. The lieutenant governor runs separately from the governor in the primaries, but during the general election, the nominee for governor and nominee for lieutenant governor run together on the same ticket.
Alaska's court system has four levels: the Alaska Supreme Court, the Alaska Court of Appeals, the superior courts and the district courts. The superior and district courts are trial courts. Superior courts are courts of general jurisdiction, while district courts only hear certain types of cases, including misdemeanor criminal cases and civil cases valued up to $100,000.
The Supreme Court and the Court of Appeals are appellate courts. The Court of Appeals is required to hear appeals from certain lower-court decisions, including those regarding criminal prosecutions, juvenile delinquency, and habeas corpus. The Supreme Court hears civil appeals and may in its discretion hear criminal appeals.




Although in its early years of statehood Alaska was a Democratic state, since the early 1970s it has been characterized as Republican-leaning. Local political communities have often worked on issues related to land use development, fishing, tourism, and individual rights. Alaska Natives, while organized in and around their communities, have been active within the Native corporations. These have been given ownership over large tracts of land, which require stewardship.
Alaska was formerly the only state in which possession of one ounce or less of marijuana in one's home was completely legal under state law, though the federal law remains in force.
The state has an independence movement favoring a vote on secession from the United States, with the Alaskan Independence Party.
Six Republicans and four Democrats have served as governor of Alaska. In addition, Republican Governor Wally Hickel was elected to the office for a second term in 1990 after leaving the Republican party and briefly joining the Alaskan Independence Party ticket just long enough to be reelected. He subsequently officially rejoined the Republican party in 1994.
Alaska's voter initiative making marijuana legal took effect 24 February 2015, placing Alaska alongside Colorado and Washington as the first three U.S. states where recreational marijuana is legal. The new law means people over age 21 can consume small amounts of pot — if they can find it. There is a rather lengthy and involved application process, per Alaska Measure 2 (2014). The first legal marijuana store opened in Valdez in October 2016.



To finance state government operations, Alaska depends primarily on petroleum revenues and federal subsidies. This allows it to have the lowest individual tax burden in the United States. It is one of five states with no state sales tax, one of seven states that do not levy an individual income tax, and one of the two states that has neither. The Department of Revenue Tax Division reports regularly on the state's revenue sources. The Department also issues an annual summary of its operations, including new state laws that directly affect the tax division.
While Alaska has no state sales tax, 89 municipalities collect a local sales tax, from 1.0–7.5%, typically 3–5%. Other local taxes levied include raw fish taxes, hotel, motel, and bed-and-breakfast 'bed' taxes, severance taxes, liquor and tobacco taxes, gaming (pull tabs) taxes, tire taxes and fuel transfer taxes. A part of the revenue collected from certain state taxes and license fees (such as petroleum, aviation motor fuel, telephone cooperative) is shared with municipalities in Alaska.
Fairbanks has one of the highest property taxes in the state as no sales or income taxes are assessed in the Fairbanks North Star Borough (FNSB). A sales tax for the FNSB has been voted on many times, but has yet to be approved, leading lawmakers to increase taxes dramatically on goods such as liquor and tobacco.
In 2014 the Tax Foundation ranked Alaska as having the fourth most "business friendly" tax policy, behind only Wyoming, South Dakota, and Nevada.




Alaska regularly supports Republicans in presidential elections and has done so since statehood. Republicans have won the state's electoral college votes in all but one election that it has participated in (1964). No state has voted for a Democratic presidential candidate fewer times. Alaska was carried by Democratic nominee Lyndon B. Johnson during his landslide election in 1964, while the 1960 and 1968 elections were close. Since 1972, however, Republicans have carried the state by large margins. In 2008, Republican John McCain defeated Democrat Barack Obama in Alaska, 59.49% to 37.83%. McCain's running mate was Sarah Palin, the state's governor and the first Alaskan on a major party ticket. Obama lost Alaska again in 2012, but he captured 40% of the state's vote in that election, making him the first Democrat to do so since 1968.
The Alaska Bush, central Juneau, midtown and downtown Anchorage, and the areas surrounding the University of Alaska Fairbanks campus and Ester have been strongholds of the Democratic Party. The Matanuska-Susitna Borough, the majority of Fairbanks (including North Pole and the military base), and South Anchorage typically have the strongest Republican showing. As of 2004, well over half of all registered voters have chosen "Non-Partisan" or "Undeclared" as their affiliation, despite recent attempts to close primaries to unaffiliated voters.
Because of its population relative to other U.S. states, Alaska has only one member in the U.S. House of Representatives. This seat is held by Republican Don Young, who was re-elected to his 21st consecutive term in 2012. Alaska's At-large congressional district is one of the largest parliamentary constituencies in the world.
In 2008, Governor Sarah Palin became the first Republican woman to run on a national ticket when she became John McCain's running mate. She continued to be a prominent national figure even after resigning from the governor's job in July 2009.
Alaska's United States Senators belong to Class 2 and Class 3. In 2008, Democrat Mark Begich, mayor of Anchorage, defeated long-time Republican senator Ted Stevens. Stevens had been convicted on seven felony counts of failing to report gifts on Senate financial discloser forms one week before the election. The conviction was set aside in April 2009 after evidence of prosecutorial misconduct emerged.
Republican Frank Murkowski held the state's other senatorial position. After being elected governor in 2002, he resigned from the Senate and appointed his daughter, State Representative Lisa Murkowski as his successor. She won full six-year terms in 2004 and 2010.
Alaska's current statewide elected officials




Alaska is not divided into counties, as most of the other U.S. states, but it is divided into boroughs. Many of the more densely populated parts of the state are part of Alaska's 16 boroughs, which function somewhat similarly to counties in other states. However, unlike county-equivalents in the other 49 states, the boroughs do not cover the entire land area of the state. The area not part of any borough is referred to as the Unorganized Borough.
The Unorganized Borough has no government of its own, but the U.S. Census Bureau in cooperation with the state divided the Unorganized Borough into 11 census areas solely for the purposes of statistical analysis and presentation. A recording district is a mechanism for administration of the public record in Alaska. The state is divided into 34 recording districts which are centrally administered under a State Recorder. All recording districts use the same acceptance criteria, fee schedule, etc., for accepting documents into the public record.
Whereas many U.S. states use a three-tiered system of decentralization—state/county/township—most of Alaska uses only two tiers—state/borough. Owing to the low population density, most of the land is located in the Unorganized Borough. As the name implies, it has no intermediate borough government but is administered directly by the state government. In 2000, 57.71% of Alaska's area has this status, with 13.05% of the population.
Anchorage merged the city government with the Greater Anchorage Area Borough in 1975 to form the Municipality of Anchorage, containing the city proper and the communities of Eagle River, Chugiak, Peters Creek, Girdwood, Bird, and Indian. Fairbanks has a separate borough (the Fairbanks North Star Borough) and municipality (the City of Fairbanks).
The state's most populous city is Anchorage, home to 278,700 people in 2006, 225,744 of whom live in the urbanized area. The richest location in Alaska by per capita income is Halibut Cove ($89,895). Yakutat City, Sitka, Juneau, and Anchorage are the four largest cities in the U.S. by area.



As reflected in the 2010 United States Census, Alaska has a total of 355 incorporated cities and census-designated places (CDPs). The tally of cities includes four unified municipalities, essentially the equivalent of a consolidated city–county. The majority of these communities are located in the rural expanse of Alaska known as "The Bush" and are unconnected to the contiguous North American road network. The table at the bottom of this section lists the 100 largest cities and census-designated places in Alaska, in population order.
Of Alaska's 2010 Census population figure of 710,231, 20,429 people, or 2.88% of the population, did not live in an incorporated city or census-designated place. Approximately three-quarters of that figure were people who live in urban and suburban neighborhoods on the outskirts of the city limits of Ketchikan, Kodiak, Palmer and Wasilla. CDPs have not been established for these areas by the United States Census Bureau, except that seven CDPs were established for the Ketchikan-area neighborhoods in the 1980 Census (Clover Pass, Herring Cove, Ketchikan East, Mountain Point, North Tongass Highway, Pennock Island and Saxman East), but have not been used since. The remaining population was scattered throughout Alaska, both within organized boroughs and in the Unorganized Borough, in largely remote areas.




The Alaska Department of Education and Early Development administers many school districts in Alaska. In addition, the state operates a boarding school, Mt. Edgecumbe High School in Sitka, and provides partial funding for other boarding schools, including Nenana Student Living Center in Nenana and The Galena Interior Learning Academy in Galena.
There are more than a dozen colleges and universities in Alaska. Accredited universities in Alaska include the University of Alaska Anchorage, University of Alaska Fairbanks, University of Alaska Southeast, and Alaska Pacific University. Alaska is the only state that has no institutions that are part of the NCAA Division I.
The Alaska Department of Labor and Workforce Development operates AVTEC, Alaska's Institute of Technology. Campuses in Seward and Anchorage offer 1 week to 11-month training programs in areas as diverse as Information Technology, Welding, Nursing, and Mechanics.
Alaska has had a problem with a "brain drain". Many of its young people, including most of the highest academic achievers, leave the state after high school graduation and do not return. As of 2013, Alaska did not have a law school or medical school. The University of Alaska has attempted to combat this by offering partial four-year scholarships to the top 10% of Alaska high school graduates, via the Alaska Scholars Program.




The Alaska State Troopers are Alaska's statewide police force. They have a long and storied history, but were not an official organization until 1941. Before the force was officially organized, law enforcement in Alaska was handled by various federal agencies. Larger towns usually have their own local police and some villages rely on "Public Safety Officers" who have police training but do not carry firearms. In much of the state, the troopers serve as the only police force available. In addition to enforcing traffic and criminal law, wildlife Troopers enforce hunting and fishing regulations. Due to the varied terrain and wide scope of the Troopers' duties, they employ a wide variety of land, air, and water patrol vehicles.
Many rural communities in Alaska are considered "dry," having outlawed the importation of alcoholic beverages. Suicide rates for rural residents are higher than urban.
Domestic abuse and other violent crimes are also at high levels in the state; this is in part linked to alcohol abuse. Alaska has the highest rate of sexual assault in the nation, especially in rural areas. The average age of sexually assaulted victims is 16 years old. In four out of five cases, the suspects were relatives, friends or acquaintances.




Some of Alaska's popular annual events are the Iditarod Trail Sled Dog Race that starts in Anchorage and ends in Nome, World Ice Art Championships in Fairbanks, the Blueberry Festival and Alaska Hummingbird Festival in Ketchikan, the Sitka Whale Fest, and the Stikine River Garnet Fest in Wrangell. The Stikine River attracts the largest springtime concentration of American bald eagles in the world.
The Alaska Native Heritage Center celebrates the rich heritage of Alaska's 11 cultural groups. Their purpose is to encourage cross-cultural exchanges among all people and enhance self-esteem among Native people. The Alaska Native Arts Foundation promotes and markets Native art from all regions and cultures in the State, using the internet.




Influences on music in Alaska include the traditional music of Alaska Natives as well as folk music brought by later immigrants from Russia and Europe. Prominent musicians from Alaska include singer Jewel, traditional Aleut flautist Mary Youngblood, folk singer-songwriter Libby Roderick, Christian music singer/songwriter Lincoln Brewster, metal/post hardcore band 36 Crazyfists and the groups Pamyua and Portugal. The Man.
There are many established music festivals in Alaska, including the Alaska Folk Festival, the Fairbanks Summer Arts Festival, the Anchorage Folk Festival, the Athabascan Old-Time Fiddling Festival, the Sitka Jazz Festival, and the Sitka Summer Music Festival. The most prominent orchestra in Alaska is the Anchorage Symphony Orchestra, though the Fairbanks Symphony Orchestra and Juneau Symphony are also notable. The Anchorage Opera is currently the state's only professional opera company, though there are several volunteer and semi-professional organizations in the state as well.
The official state song of Alaska is "Alaska's Flag", which was adopted in 1955; it celebrates the flag of Alaska.




Alaska's first independent picture entirely made in Alaska was The Chechahcos, produced by Alaskan businessman Austin E. Lathrop and filmed in and around Anchorage. Released in 1924 by the Alaska Moving Picture Corporation, it was the only film the company made.
One of the most prominent movies filmed in Alaska is MGM's Eskimo/Mala The Magnificent, starring Alaska Native Ray Mala. In 1932 an expedition set out from MGM's studios in Hollywood to Alaska to film what was then billed as "The Biggest Picture Ever Made." Upon arriving in Alaska, they set up "Camp Hollywood" in Northwest Alaska, where they lived during the duration of the filming. Louis B. Mayer spared no expense in spite of the remote location, going so far as to hire the chef from the Hotel Roosevelt in Hollywood to prepare meals.
When Eskimo premiered at the Astor Theatre in New York City, the studio received the largest amount of feedback in its history to that point. Eskimo was critically acclaimed and released worldwide; as a result, Mala became an international movie star. Eskimo won the first Oscar for Best Film Editing at the Academy Awards, and showcased and preserved aspects of Inupiat culture on film.
The 1983 Disney movie Never Cry Wolf was at least partially shot in Alaska. The 1991 film White Fang, based on Jack London's novel and starring Ethan Hawke, was filmed in and around Haines. Steven Seagal's 1994 On Deadly Ground, starring Michael Caine, was filmed in part at the Worthington Glacier near Valdez. The 1999 John Sayles film Limbo, starring David Strathairn, Mary Elizabeth Mastrantonio, and Kris Kristofferson, was filmed in Juneau.
The psychological thriller Insomnia, starring Al Pacino and Robin Williams, was shot in Canada, but was set in Alaska. The 2007 film directed by Sean Penn, Into The Wild, was partially filmed and set in Alaska. The film, which is based on the novel of the same name, follows the adventures of Christopher McCandless, who died in a remote abandoned bus along the Stampede Trail west of Healy in 1992.
Many films and television shows set in Alaska are not filmed there; for example, Northern Exposure, set in the fictional town of Cicely, Alaska, was filmed in Roslyn, Washington. The 2007 horror feature 30 Days of Night is set in Barrow, but was filmed in New Zealand.
Many reality television shows are filmed in Alaska. In 2011 the Anchorage Daily News found ten set in the state.




State motto: North to the Future
Nicknames: "The Last Frontier" or "Land of the Midnight Sun" or "Seward's Icebox"
State bird: willow ptarmigan, adopted by the Territorial Legislature in 1955. It is a small (15–17 in or 380–430 mm) Arctic grouse that lives among willows and on open tundra and muskeg. Plumage is brown in summer, changing to white in winter. The willow ptarmigan is common in much of Alaska.
State fish: king salmon, adopted 1962.
State flower: wild/native forget-me-not, adopted by the Territorial Legislature in 1917. It is a perennial that is found throughout Alaska, from Hyder to the Arctic Coast, and west to the Aleutians.
State fossil: woolly mammoth, adopted 1986.
State gem: jade, adopted 1968.
State insect: four-spot skimmer dragonfly, adopted 1995.
State land mammal: moose, adopted 1998.
State marine mammal: bowhead whale, adopted 1983.
State mineral: gold, adopted 1968.
State song: "Alaska's Flag"
State sport: dog mushing, adopted 1972.
State tree: Sitka spruce, adopted 1962.
State dog: Alaskan Malamute, adopted 2010.
State soil: Tanana, adopted unknown.




Index of Alaska-related articles
Outline of Alaska – organized list of topics about Alaska
Sports in Alaska










Alaska at DMOZ
Alaska's Digital Archives
Alaska Inter-Tribal Council
The short film Alaska (1967) is available for free download at the Internet Archive
 Geographic data related to Alaska at OpenStreetMap
US federal government
Alaska State Guide from the Library of Congress
Energy & Environmental Data for Alaska
USGS real-time, geographic, and other scientific resources of Alaska
US Census Bureau
Alaska State Facts
Alaska Statehood Subject Guide from the Eisenhower Presidential Library
Alaska Statehood documents, Dwight D. Eisenhower Presidential Library
Alaska state government
State of Alaska website
Alaska State Databases – Annotated list of searchable databases produced by Alaska state agencies and compiled by the Government Documents Roundtable of the American Library Association.
Alaska Department of Natural Resources, Recorder's OfficeVirginia (/vərˈdʒɪnjə/, UK /vərˈdʒɪni.ə/, officially the Commonwealth of Virginia, is a state located in the Mid-Atlantic region of the United States, as well as the historic Southeast,. Virginia is nicknamed the "Old Dominion" due to its status as the first colonial possession established in mainland British America, and "Mother of Presidents" because eight U.S. presidents were born there, more than any other state. The geography and climate of the Commonwealth are shaped by the Blue Ridge Mountains and the Chesapeake Bay, which provide habitat for much of its flora and fauna. The capital of the Commonwealth is Richmond; Virginia Beach is the most populous city, and Fairfax County is the most populous political subdivision. The Commonwealth's estimated population as of 2014 is over 8.3 million, which is slightly less than New York City.
The area's history begins with several indigenous groups, including the Powhatan. In 1607 the London Company established the Colony of Virginia as the first permanent New World English colony. Slave labor and the land acquired from displaced Native American tribes each played a significant role in the colony's early politics and plantation economy. Virginia was one of the 13 Colonies in the American Revolution and joined the Confederacy in the American Civil War, during which Richmond was made the Confederate capital and Virginia's northwestern counties seceded to form the state of West Virginia. Although the Commonwealth was under one-party rule for nearly a century following Reconstruction, both major national parties are competitive in modern Virginia.
The Virginia General Assembly is the oldest continuous law-making body in the New World. The state government was ranked most effective by the Pew Center on the States in both 2005 and 2008. It is unique in how it treats cities and counties equally, manages local roads, and prohibits its governors from serving consecutive terms. Virginia's economy has many sectors: agriculture in the Shenandoah Valley; federal agencies in Northern Virginia, including the headquarters of the U.S. Department of Defense and Central Intelligence Agency (CIA); and military facilities in Hampton Roads, the site of the region's main seaport. Virginia's economy changed from primarily agricultural to industrial during the 1960s and 1970s, and in 2002 computer chips became the state's leading export by monetary value.




Virginia has a total area of 42,774.2 square miles (110,784.7 km2), including 3,180.13 square miles (8,236.5 km2) of water, making it the 35th-largest state by area. Virginia is bordered by Maryland and Washington, D.C. to the north and east; by the Atlantic Ocean to the east; by North Carolina to the south; by Tennessee to the southwest; by Kentucky to the west; and by West Virginia to the north and west. Virginia's boundary with Maryland and Washington, D.C. extends to the low-water mark of the south shore of the Potomac River. The southern border is defined as the 36° 30′ parallel north, though surveyor error led to deviations of as much as three arcminutes. The border with Tennessee was not settled until 1893, when their dispute was brought to the U.S. Supreme Court.



The Chesapeake Bay separates the contiguous portion of the Commonwealth from the two-county peninsula of Virginia's Eastern Shore. The bay was formed from the drowned river valleys of the Susquehanna River and the James River. Many of Virginia's rivers flow into the Chesapeake Bay, including the Potomac, Rappahannock, York, and James, which create three peninsulas in the bay.

The Tidewater is a coastal plain between the Atlantic coast and the fall line. It includes the Eastern Shore and major estuaries of Chesapeake Bay. The Piedmont is a series of sedimentary and igneous rock-based foothills east of the mountains which were formed in the Mesozoic era. The region, known for its heavy clay soil, includes the Southwest Mountains around Charlottesville. The Blue Ridge Mountains are a physiographic province of the Appalachian Mountains with the highest points in the state, the tallest being Mount Rogers at 5,729 feet (1,746 m). The Ridge and Valley region is west of the mountains and includes the Great Appalachian Valley. The region is carbonate rock based and includes Massanutten Mountain. The Cumberland Plateau and the Cumberland Mountains are in the southwest corner of Virginia, south of the Allegheny Plateau. In this region, rivers flow northwest, with a dendritic drainage system, into the Ohio River basin.
The Virginia Seismic Zone has not had a history of regular earthquake activity. Earthquakes are rarely above 4.5 in magnitude, because Virginia is located away from the edges of the North American Plate. The largest earthquake, at an estimated 5.9 magnitude, was in 1897 near Blacksburg. A 5.8 magnitude earthquake struck central Virginia on August 23, 2011, near Mineral. The earthquake was reportedly felt as far away as Toronto, Atlanta and Florida.
Coal mining takes place in the three mountainous regions at 45 distinct coal beds near Mesozoic basins. Over 62 million tons of other non-fuel resources, such as slate, kyanite, sand, or gravel, were also mined in Virginia in 2012. The state's carbonate rock is filled with more than 4,000 caves, ten of which are open for tourism. 35 million years ago, a bolide impacted what is now eastern Virginia. The resulting crater may explain sinking and earthquakes in the region.




The climate of Virginia is temperate and becomes increasingly warmer and more humid farther south and east. Seasonal extremes vary from average lows of 26 °F (−3 °C) in January to average highs of 86 °F (30 °C) in July. The Atlantic Ocean has a strong effect on eastern and southeastern coastal areas of the state. Influenced by the Gulf Stream, coastal weather is subject to hurricanes, most pronouncedly near the mouth of Chesapeake Bay. In spite of its position adjacent to the Atlantic Ocean, even the coastal areas have a significant continental influence with quite large temperature differences between summer and winter, particularly given the state climate's subtropical classification, which is typical of states in the Upper South.
Virginia has an annual average of 35–45 days of thunderstorm activity, particularly in the western part of the state, and an average annual precipitation of 42.7 inches (108 cm). Cold air masses arriving over the mountains in winter can lead to significant snowfalls, such as the Blizzard of 1996 and winter storms of 2009–2010. The interaction of these elements with the state's topography creates distinct microclimates in the Shenandoah Valley, the mountainous southwest, and the coastal plains. Virginia averages seven tornadoes annually, most F2 or lower on the Fujita scale.
In recent years, the expansion of the southern suburbs of Washington, D.C. into Northern Virginia has introduced an urban heat island primarily caused by increased absorption of solar radiation in more densely populated areas. In the American Lung Association's 2011 report, 11 counties received failing grades for air quality, with Fairfax County having the worst in the state, due to automobile pollution. Haze in the mountains is caused in part by coal power plants.



Forests cover 65% of the state, primarily with deciduous, broad leaf trees in the western part of the state and evergreens and conifers dominant the central and eastern part of Virginia. Lower altitudes are more likely to have small but dense stands of moisture-loving hemlocks and mosses in abundance, with hickory and oak in the Blue Ridge. However, since the early 1990s, Gypsy moth infestations have eroded the dominance of oak forests. In the lowland tidewater and piedmont, yellow pines tend to dominate, with bald cypress wetland forests in the Great Dismal and Nottoway swamps. Other common trees and plants include red bay, wax myrtle, dwarf palmetto, tulip poplar, mountain laurel, milkweed, daisies, and many species of ferns. The largest areas of wilderness are along the Atlantic coast and in the western mountains, where the largest populations of trillium wildflowers in North America are found. The Atlantic coast regions are host to flora commonly associated with the South Atlantic pine forests and lower Southeast Coastal Plain maritime flora, the latter found primarily in eastern and central Virginia.

Mammals include white-tailed deer, black bear, beaver, bobcat, coyote, raccoon, skunk, groundhog, Virginia opossum, gray fox, red fox, and eastern cottontail rabbit. Other mammals include: nutria, fox squirrel, gray squirrel, flying squirrel, chipmunk, brown bat, and weasel. Birds include cardinals (the state bird), barred owls, Carolina chickadees, red-tailed hawks, ospreys, brown pelicans, quail, seagulls, bald eagles, and wild turkeys. Virginia is also home to the pileated woodpecker as well as the red-bellied woodpecker. The peregrine falcon was reintroduced into Shenandoah National Park in the mid-1990s. Walleye, brook trout, Roanoke bass, and blue catfish are among the 210 known species of freshwater fish. Running brooks with rocky bottoms are often inhabited by plentiful amounts of crayfish and salamanders. The Chesapeake Bay is host to many species, including blue crabs, clams, oysters, and rockfish (also known as striped bass).
Virginia has 30 National Park Service units, such as Great Falls Park and the Appalachian Trail, and one national park, the Shenandoah National Park. Shenandoah was established in 1935 and encompasses the scenic Skyline Drive. Almost 40% of the park's area (79,579 acres/322 km2) has been designated as wilderness under the National Wilderness Preservation System. Additionally, there are 34 Virginia state parks and 17 state forests, run by the Department of Conservation and Recreation and the Department of Forestry. The Chesapeake Bay, while not a national park, is protected by both state and federal legislation, and the jointly run Chesapeake Bay Program which conducts restoration on the bay and its watershed. The Great Dismal Swamp National Wildlife Refuge extends into North Carolina, as does the Back Bay National Wildlife Refuge, which marks the beginning of the Outer Banks.




"Jamestown 2007" marked Virginia's quadricentennial year, celebrating 400 years since the establishment of the Jamestown Colony. The celebrations highlighted contributions from Native Americans, Africans, and Europeans, each of which had a significant part in shaping Virginia's history. Warfare, including among these groups, has also had an important role. Virginia was a focal point in conflicts from the French and Indian War, the American Revolution and the Civil War, to the Cold War and the War on Terrorism. Stories about historic figures, such as those surrounding Pocahontas and John Smith, George Washington's childhood, or the plantation elite in the slave society of the antebellum period, have also created potent myths of state history, and have served as rationales for Virginia's ideology.




The first people are estimated to have arrived in Virginia over 12,000 years ago. By 5,000 years ago more permanent settlements emerged, and farming began by 900 AD. By 1500, the Algonquian peoples had founded towns such as Werowocomoco in the Tidewater region, which they referred to as Tsenacommacah. The other major language groups in the area were the Siouan to the west, and the Iroquoians, who included the Nottoway and Meherrin, to the north and south. After 1570, the Algonquians consolidated under Chief Powhatan in response to threats from these other groups on their trade network. Powhatan controlled more than 30 smaller tribes and over 150 settlements, who shared a common Virginia Algonquian language. In 1607, the native Tidewater population was between 13,000 and 14,000.
Several European expeditions, including a group of Spanish Jesuits, explored the Chesapeake Bay during the 16th century. In 1583, Queen Elizabeth I of England granted Walter Raleigh a charter to plant a colony north of Spanish Florida. In 1584, Raleigh sent an expedition to the Atlantic coast of North America. The name "Virginia" may have been suggested then by Raleigh or Elizabeth, perhaps noting her status as the "Virgin Queen," and may also be related to a native phrase, "Wingandacoa," or name, "Wingina." Initially the name applied to the entire coastal region from South Carolina to Maine, plus the island of Bermuda. Later, subsequent royal charters modified the Colony's boundaries. The London Company was incorporated as a joint stock company by the proprietary Charter of 1606, which granted land rights to this area. The company financed the first permanent English settlement in the "New World", Jamestown. Named for King James I, it was founded in May 1607 by Christopher Newport. In 1619, colonists took greater control with an elected legislature called the House of Burgesses. With the bankruptcy of the London Company in 1624, the settlement was taken into royal authority as an English crown colony.

Life in the colony was perilous, and many died during the Starving Time in 1609 and the Anglo-Powhatan Wars, including the Indian massacre of 1622, which fostered the colonists' negative view of all tribes. By 1624, only 3,400 of the 6,000 early settlers had survived. However, European demand for tobacco fueled the arrival of more settlers and servants. The headright system tried to solve the labor shortage by providing colonists with land for each indentured servant they transported to Virginia. African workers were first imported to Jamestown in 1619 initially under the rules of indentured servitude. The shift to a system of African slavery in Virginia was propelled by the legal cases of John Punch, who was sentenced to lifetime slavery in 1640 for attempting to run away, and of John Casor, who was claimed by Anthony Johnson as his servant for life in 1655. Slavery first appears in Virginia statutes in 1661 and 1662, when a law made it hereditary based on the mother's status.
Tensions and the geographic differences between the working and ruling classes led to Bacon's Rebellion in 1676, by which time current and former indentured servants made up as much as 80% of the population. Rebels, largely from the colony's frontier, were also opposed to the conciliatory policy towards native tribes, and one result of the rebellion was the signing at Middle Plantation of the Treaty of 1677, which made the signatory tribes tributary states and was part of a pattern of appropriating tribal land by force and treaty. Middle Plantation saw the founding of The College of William & Mary in 1693 and was renamed Williamsburg as it became the colonial capital in 1699. In 1747, a group of Virginian speculators formed the Ohio Company, with the backing of the British crown, to start English settlement and trade in the Ohio Country west of the Appalachian Mountains. France, which claimed this area as part of their colony of New France, viewed this as a threat, and the ensuing French and Indian War became part of the Seven Years' War (1756–1763). A militia from several British colonies, called the Virginia Regiment, was led by then-Lieutenant Colonel George Washington.




The British Parliament's efforts to levy new taxes following the French and Indian War were deeply unpopular in the colonies. In the House of Burgesses, opposition to taxation without representation was led by Patrick Henry and Richard Henry Lee, among others. Virginians began to coordinate their actions with other colonies in 1773, and sent delegates to the Continental Congress the following year. After the House of Burgesses was dissolved by the royal governor in 1774, Virginia's revolutionary leaders continued to govern via the Virginia Conventions. On May 15, 1776, the Convention declared Virginia's independence from the British Empire and adopted George Mason's Virginia Declaration of Rights, which was then included in a new constitution. Another Virginian, Thomas Jefferson, drew upon Mason's work in drafting the national Declaration of Independence.
When the American Revolutionary War began, George Washington was selected to head the colonial army. During the war, the capital was moved to Richmond at the urging of Governor Thomas Jefferson, who feared that Williamsburg's coastal location would make it vulnerable to British attack. In 1781, the combined action of Continental and French land and naval forces trapped the British army on the Virginia Peninsula, where troops under George Washington and Comte de Rochambeau defeated British General Cornwallis in the Siege of Yorktown. His surrender on October 19, 1781 led to peace negotiations in Paris and secured the independence of the colonies.
Virginians were instrumental in writing the United States Constitution. James Madison drafted the Virginia Plan in 1787 and the Bill of Rights in 1789. Virginia ratified the Constitution on June 25, 1788. The three-fifths compromise ensured that Virginia, with its large number of slaves, initially had the largest bloc in the House of Representatives. Together with the Virginia dynasty of presidents, this gave the Commonwealth national importance. In 1790, both Virginia and Maryland ceded territory to form the new District of Columbia, though the Virginian area was retroceded in 1846. Virginia is called "Mother of States" because of its role in being carved into states like Kentucky, which became the 15th state in 1792, and for the numbers of American pioneers born in Virginia.




In addition to agriculture, slave labor was increasingly used in mining, shipbuilding and other industries. The execution of Gabriel Prosser in 1800, Nat Turner's slave rebellion in 1831 and John Brown's Raid on Harpers Ferry in 1859 marked the growing social discontent over slavery and its role in the plantation economy. By 1860, almost half a million people, roughly 31% of the total population of Virginia, were enslaved. This division contributed to the start of the American Civil War.
Virginia voted to secede from the United States on April 17, 1861, after the Battle of Fort Sumter and Abraham Lincoln's call for volunteers. On April 24, Virginia joined the Confederate States of America, which chose Richmond as its capital. After the 1861 Wheeling Convention, 48 counties in the northwest separated to form a new state of West Virginia, which chose to remain loyal to the Union. Virginian general Robert E. Lee took command of the Army of Northern Virginia in 1862, and led invasions into Union territory, ultimately becoming commander of all Confederate forces. During the war, more battles were fought in Virginia than anywhere else, including Bull Run, the Seven Days Battles, Chancellorsville, and the concluding Battle of Appomattox Court House. After the capture of Richmond in April 1865, the state capital was briefly moved to Lynchburg, while the Confederate leadership fled to Danville. Virginia was formally restored to the United States in 1870, due to the work of the Committee of Nine.
During the post-war Reconstruction era, Virginia adopted a constitution which provided for free public schools, and guaranteed political, civil, and voting rights. The populist Readjuster Party ran an inclusive coalition until the conservative white Democratic Party gained power after 1883. It passed segregationist Jim Crow laws and in 1902 rewrote the Constitution of Virginia to include a poll tax and other voter registration measures that effectively disfranchised most African Americans and many poor European Americans. Though their schools and public services were segregated and underfunded due to a lack of political representation, African Americans were able to unite in communities and take a greater role in Virginia society.




New economic forces also changed the Commonwealth. Virginian James Albert Bonsack invented the tobacco cigarette rolling machine in 1880 leading to new industrial scale production centered on Richmond. In 1886, railroad magnate Collis Potter Huntington founded Newport News Shipbuilding, which was responsible for building six major World War I-era battleships for the U.S. Navy from 1907 to 1923. During the war, German submarines like U-151 attacked ships outside the port. In 1926, Dr. W.A.R. Goodwin, rector of Williamsburg's Bruton Parish Church, began restoration of colonial-era buildings in the historic district with financial backing of John D. Rockefeller, Jr. Though their project, like others in the state, had to contend with the Great Depression and World War II, work continued as Colonial Williamsburg became a major tourist attraction.

Protests started by Barbara Rose Johns in 1951 in Farmville against segregated schools led to the lawsuit Davis v. County School Board of Prince Edward County. This case, filed by Richmond natives Spottswood Robinson and Oliver Hill, was decided in 1954 with Brown v. Board of Education, which rejected the segregationist doctrine of "separate but equal". But, in 1958, under the policy of "massive resistance" led by the influential segregationist Senator Harry F. Byrd and his Byrd Organization, the Commonwealth prohibited desegregated local schools from receiving state funding.
The Civil Rights Movement gained many participants in the 1960s. It achieved the moral force and support to gain passage of national legislation with the Civil Rights Act of 1964 and the Voting Rights Act of 1965. In 1964 the United States Supreme Court ordered Prince Edward County and others to integrate schools. In 1967, the Court also struck down the state's ban on interracial marriage with Loving v. Virginia. From 1969 to 1971, state legislators under Governor Mills Godwin rewrote the constitution, after goals such as the repeal of Jim Crow laws had been achieved. In 1989, Douglas Wilder became the first African American elected as governor in the United States.
The Cold War led to the expansion of national defense government programs housed in offices in Northern Virginia near Washington, D.C., and correlative population growth. The Central Intelligence Agency in Langley was involved in various Cold War events, including as the target of Soviet espionage activities. Also among the federal developments was the Pentagon, built during World War II as the headquarters for the Department of Defense. It was one of the targets of the September 11 attacks; 189 people died at the site when a jet passenger plane was crashed into the building.




Virginia is divided into 95 counties and 38 independent cities, the latter acting in many ways as county-equivalents. This general method of treating cities and counties on par with each other is unique to Virginia, with only three other independent cities in the United States outside Virginia, in three different states. Virginia limits the authority of cities and counties to countermand laws expressly allowed by the Virginia General Assembly under what is known as Dillon's Rule. In addition to independent cities, there are also incorporated towns which operate under their own governments, but are part of a county. Finally there are hundreds of unincorporated communities within the counties. Virginia does not have any further political subdivisions, such as villages or townships.
Virginia has 11 Metropolitan Statistical Areas; Northern Virginia, Hampton Roads, and Richmond-Petersburg are the three most populous. Richmond is the capital of Virginia, and its metropolitan area has a population of over 1.2 million. As of 2010, Virginia Beach is the most populous city in the Commonwealth, with Norfolk and Chesapeake second and third, respectively. Norfolk forms the urban core of the Hampton Roads metropolitan area, which has a population over 1.6 million people and is the site of the world's largest naval base, Naval Station Norfolk. Suffolk, which includes a portion of the Great Dismal Swamp, is the largest city by area at 429.1 square miles (1,111 km2).
Fairfax County is the most populous locality in Virginia, with over one million residents, although that does not include its county seat Fairfax, which is one of the independent cities. Fairfax County has a major urban business and shopping center in Tysons Corner, Virginia's largest office market. Neighboring Prince William County is Virginia's second most populous county, with a population exceeding 450,000, and is home to Marine Corps Base Quantico, the FBI Academy and Manassas National Battlefield Park. Loudoun County, with the county seat at Leesburg, is both the fastest-growing county in Virginia and has the highest median household income ($114,204) in the country as of 2010. Arlington County, the smallest self-governing county in the United States by land area, is an urban community organized as a county. The Roanoke area, with an estimated population of 300,399, is the largest Metropolitan Statistical Area in western Virginia.




The United States Census Bureau estimates that the state population was 8,411,808 on July 1, 2016, a 5.1% increase since the 2010 United States Census. This includes an increase from net migration of 381,969 people into the Commonwealth since the 2010 census. Immigration from outside the United States resulted in a net increase of 159,627 people, and migration within the country produced a net increase of 155,205 people. As of 2000, the center of population is located in Goochland County, near Richmond.
Aside from Virginia, the top birth state for Virginians is New York, having overtaken North Carolina in the 1990s, with the Northeast accounting for the largest number of migrants into the state by region. As of 2015, both the state's population density and median household income are nearly identical to that of Hawaii, while Virginia's total population is closest in size to New Jersey.



The state's most populous ethnic group, Non-Hispanic White, has declined from 76% in 1990 to 62.7% in 2015. In 2011, non-Hispanic Whites were involved in 50.9% of all the births. People of English heritage settled throughout the Commonwealth during the colonial period, and others of British and Irish heritage have since immigrated. Those who self-identify as having "American ethnicity" are predominantly of English descent, but have ancestry that has been in North America for so long that they choose to identify simply as American. Of the English immigrants to Virginia in the 17th century, 75% came as indentured servants. The western mountains have many settlements that were founded by Scots-Irish immigrants before the American Revolution. There are also sizable numbers of people of German descent in the northwestern mountains and Shenandoah Valley, and German ancestry was the most popular response on the 2010 American Community Survey, with 11.7%. 2.9% of Virginians also describe themselves as biracial.
The largest minority group in Virginia is African American, at 19.7% as of 2015. Most African American Virginians have been descendants of enslaved Africans who worked on tobacco, cotton, and hemp plantations. These men, women and children were brought from West and West-Central Africa, primarily from Angola and the Bight of Biafra. The Igbo ethnic group of what is now southern Nigeria were the single largest African group among slaves in Virginia. Though the black population was reduced by the Great Migration, since 1965 there has been a reverse migration of blacks returning south. According to the Pew Research Center, the state has the highest concentration of black and white interracial marriages.
More recent immigration in the late 20th century and early 21st century has fueled new communities of Hispanics and Asians. As of 2015, 9.0% of Virginians are Hispanic or Latino (of any race), and 6.5% are Asian. The state's Hispanic population rose by 92% from 2000 to 2010, with two-thirds of Hispanics living in Northern Virginia. Hispanic citizens in Virginia have higher median household incomes and educational attainment than the general Virginia population. As far as Hispanic groups, there is a large Salvadoran population in the DC suburbs of Northern Virginia, and a large Puerto Rican population in the Hampton Roads region of Southeast Virginia. Northern Virginia also has a significant population of Vietnamese Americans, whose major wave of immigration followed the Vietnam War, and Korean Americans, whose migration has been more recent and was induced in part by the quality school system. The Filipino American community has about 45,000 in the Hampton Roads area, many of whom have ties to the U.S. Navy and armed forces.
Additionally, 0.5% of Virginians are American Indian or Alaska Native, and 0.1% are Native Hawaiian or other Pacific Islander. Virginia has extended state recognition to eight Native American tribes resident in the state, though some lack federal recognition status. Most Native American groups are located in the Tidewater region.
As of 2011, 49.1% of Virginia's population younger than age 1 were minorities (meaning that they had at least one parent who was not non-Hispanic white).



The Piedmont region is known for its dialect's strong influence on Southern American English. While a more homogenized American English is found in urban areas, various accents are also used, including the Tidewater accent, the Old Virginia accent, and the anachronistic Elizabethan of Tangier Island.
As of 2010, 85.87% (6,299,127) of Virginia residents age 5 and older spoke English at home as a primary language, while 6.41% (470,058) spoke Spanish, 0.77% (56,518) Korean, 0.63% (45,881) Vietnamese, 0.57% (42,418) Chinese (which includes Mandarin), and Tagalog was spoken as a main language by 0.56% (40,724) of the population over the age of five. In total, 14.13% (1,036,442) of Virginia's population age 5 and older spoke a mother language other than English. English was passed as the Commonwealth's official language by statutes in 1981 and again in 1996, though the status is not mandated by the Constitution of Virginia.




Virginia is predominantly Christian and Protestant; Baptists are the largest single group with 27% of the population as of 2008. Baptist congregations in Virginia have 763,655 members. Baptist denominational groups in Virginia include the Baptist General Association of Virginia, with about 1,400 member churches, which supports both the Southern Baptist Convention and the moderate Cooperative Baptist Fellowship; and the Southern Baptist Conservatives of Virginia with more than 500 affiliated churches, which supports the Southern Baptist Convention. Roman Catholics are the second-largest religious group with 673,853 members. The Roman Catholic Diocese of Arlington includes most of Northern Virginia's Catholic churches, while the Diocese of Richmond covers the rest.

The Virginia Conference is the regional body of the United Methodist Church in most of the Commonwealth, while the Holston Conference represents much of extreme Southwest Virginia. The Virginia Synod is responsible for the congregations of the Lutheran Church. Presbyterian, Pentecostal, Congregationalist, and Episcopalian adherents each composed less than 2% of the population as of 2010. The Episcopal Diocese of Virginia, Southern Virginia, and Southwestern Virginia support the various Episcopal churches.
In November 2006, 15 conservative Episcopal churches voted to split from the Diocese of Virginia over the ordination of openly gay bishops and clergy in other dioceses of the Episcopal Church; these churches continue to claim affiliation with the larger Anglican Communion through other bodies outside the United States. Though Virginia law allows parishioners to determine their church's affiliation, the diocese claimed the secessionist churches' buildings and properties. The resulting property law case, ultimately decided in favor of the mainline diocese, was a test for Episcopal churches nationwide.
Among other religions, adherents of The Church of Jesus Christ of Latter-day Saints constitute 1% of the population, with 197 congregations in Virginia as of March 2014. Fairfax Station is the site of the Ekoji Buddhist Temple, of the Jodo Shinshu school, and the Hindu Durga Temple. While the state's Jewish population is small, organized Jewish sites date to 1789 with Congregation Beth Ahabah. Muslims are a growing religious group throughout the Commonwealth through immigration. Megachurches in the Commonwealth include Thomas Road Baptist Church, Immanuel Bible Church, and McLean Bible Church. Several Christian universities are also based in the state, including Regent University, Liberty University, and Lynchburg College.




Virginia is an employment-at-will state; its economy has diverse sources of income, including local and federal government, military, farming and business. Virginia has 4.1 million civilian workers, and one-third of the jobs are in the service sector. The unemployment rate in Virginia is among the lowest in the nation, at 4.8%, as of December 2014. The second fastest job growth town in the nation is Leesburg, as of 2011. The Gross Domestic Product of Virginia was $452 billion in 2013. According to the Bureau of Economic Analysis, Virginia had the most counties in the top 100 wealthiest in the United States based upon median income in 2007. Northern Virginia is the highest-income region in Virginia, having six of the twenty highest-income counties in the United States, including the three highest as of 2011. According to CNN Money Magazine the highest-income town in the nation is Great Falls, as of 2011. According to a 2013 study by Phoenix Marketing International, Virginia had the seventh-largest number of millionaires per capita in the United States, with a ratio of 6.64%.




Virginia has the highest defense spending of any state per capita, providing the Commonwealth with around 900,000 jobs. Approximately 12% of all U.S. federal procurement money is spent in Virginia, the second-highest amount after California. Many Virginians work for federal agencies in Northern Virginia, which include the Central Intelligence Agency and the Department of Defense, as well as the National Science Foundation, the United States Geological Survey and the United States Patent and Trademark Office. Many others work for government contractors, including defense and security firms, which hold more than 15,000 federal contracts.
Virginia has one of the highest concentrations of veterans of any state, and is second to California in total Department of Defense employees. The Hampton Roads area has the largest concentration of military personnel and assets of any metropolitan area in the world, including the largest naval base in the world, Naval Station Norfolk. In its state government, Virginia employs 106,143 public employees, who combined have a median income of $44,656 as of 2013.




Virginia has the highest concentration of technology workers of any state, and the fourth-highest number of technology workers after California, Texas, and New York. Computer chips became the state's highest-grossing export in 2006, surpassing its traditional top exports of coal and tobacco combined, reaching a total export value of $717 million in 2015. Northern Virginia, once considered the state's dairy capital, now hosts software, communication technology, defense contracting companies, particularly in the Dulles Technology Corridor.
The state has the highest average and peak Internet speeds in the United States, with the third-highest worldwide. Northern Virginia's data centers can carry up to 70% of the nation's internet traffic, with Loudoun County alone home to as much data center space as northern California.
Virginia companies received the fourth-highest amount of venture capital funding in the first half of 2011 after California, Massachusetts, and New York. In 2009, Forbes magazine named Virginia the best state in the nation for business for the fourth year in a row, while CNBC named it the top state for business in 2007, 2009, and 2011. Additionally, in 2014 a survey of 12,000 small business owners found Virginia to be one of the most friendly states for small businesses. Virginia has 20 Fortune 500 companies, ranking the state eighth nationwide. Tysons Corner is one of the largest business districts in the nation.
Tourism in Virginia supported an estimated 210,000 jobs and generated $21.2 billion in 2012. Arlington County is the top tourist destination in the state by domestic spending, followed by Fairfax County, Loudoun County, and Virginia Beach.




Agriculture occupies 32% of the land in Virginia. As of 2012, about 357,000 Virginian jobs were in agriculture, with over 47,000 farms, averaging 171 acres (0.27 sq mi; 0.69 km2), in a total farmland area of 8.1 million acres (12,656 sq mi; 32,780 km2). Though agriculture has declined significantly since 1960 when there were twice as many farms, it remains the largest single industry in Virginia. Tomatoes surpassed soy as the most profitable crop in Virginia in 2006, with peanuts and hay as other agricultural products. Although it is no longer the primary crop, Virginia is still the fifth-largest producer of tobacco nationwide.
Virginia is the largest producer of seafood on the East Coast, with scallops, oysters, blue crabs, and clams as the largest seafood harvests by value, and France, Canada, and Hong Kong as the top export destinations. Eastern oyster harvests have increased from 23,000 bushels in 2001 to over 500,000 in 2013. Wineries and vineyards in the Northern Neck and along the Blue Ridge Mountains also have begun to generate income and attract tourists. Virginia has the fifth-highest number of wineries in the nation.



Virginia collects personal income tax in five income brackets, ranging from 3.0% to 5.75%. The state sales and use tax rate is 4.3%, while the tax rate on food is 1.5%. There is an additional 1% local tax, for a total of a 5.3% combined sales tax on most Virginia purchases and 2.5% on most food. Virginia's property tax is set and collected at the local government level and varies throughout the Commonwealth. Real estate is also taxed at the local level based on 100% of fair market value. Tangible personal property also is taxed at the local level and is based on a percentage or percentages of original cost.




Virginia's culture was popularized and spread across America and the South by figures such as George Washington, Thomas Jefferson, and Robert E. Lee. Their homes in Virginia represent the birthplace of America and the South. Modern Virginia culture has many sources, and is part of the culture of the Southern United States. The Smithsonian Institution divides Virginia into nine cultural regions.
Besides the general cuisine of the Southern United States, Virginia maintains its own particular traditions. Virginia wine is made in many parts of the state. Smithfield ham, sometimes called "Virginia ham", is a type of country ham which is protected by state law, and can only be produced in the town of Smithfield. Virginia furniture and architecture are typical of American colonial architecture. Thomas Jefferson and many of the state's early leaders favored the Neoclassical architecture style, leading to its use for important state buildings. The Pennsylvania Dutch and their style can also be found in parts of the state.
Literature in Virginia often deals with the state's extensive and sometimes troubled past. The works of Pulitzer Prize winner Ellen Glasgow often dealt with social inequalities and the role of women in her culture. Glasgow's peer and close friend James Branch Cabell wrote extensively about the changing position of gentry in the Reconstruction era, and challenged its moral code with Jurgen, A Comedy of Justice. William Styron approached history in works such as The Confessions of Nat Turner and Sophie's Choice. Tom Wolfe has occasionally dealt with his southern heritage in bestsellers like I Am Charlotte Simmons. Mount Vernon native Matt Bondurant received critical acclaim for his historic novel The Wettest County in the World about moonshiners in Franklin County during prohibition. Virginia also names a state Poet Laureate, currently Ron Smith of Richmond, who will serve until mid-2016.




Rich in cultural heritage, Virginia however ranks near the bottom of U.S. states in terms of public spending on the arts, at nearly half of the national average. The state government does fund some institutions, including the Virginia Museum of Fine Arts and the Science Museum of Virginia. Other museums include the popular Steven F. Udvar-Hazy Center of the National Air and Space Museum and the Chrysler Museum of Art. Besides these sites, many open-air museums are located in the Commonwealth, such as Colonial Williamsburg, the Frontier Culture Museum, and various historic battlefields. The Virginia Foundation for the Humanities works to improve the Commonwealth's civic, cultural, and intellectual life.
Theaters and venues in the Commonwealth are found both in the cities and suburbs. The Harrison Opera House, in Norfolk, is home of the Virginia Opera. The Virginia Symphony Orchestra operates in and around Hampton Roads. Resident and touring theater troupes operate from the American Shakespeare Center in Staunton. The Barter Theatre, designated the State Theatre of Virginia, in Abingdon won the first ever Regional Theatre Tony Award in 1948, while the Signature Theatre in Arlington won it in 2009. There's also a Children's Theater of Virginia, Theatre IV, which is the second largest touring troupe nationwide.
Virginia has launched many award-winning traditional musical artists and internationally successful popular music acts, as well as Hollywood actors. Virginia is known for its tradition in the music genres of old-time string and bluegrass, with groups such as the Carter Family and Stanley Brothers, as well as gospel, blues, and shout bands. Contemporary Virginia is also known for folk rock artists like Dave Matthews and Jason Mraz, hip hop stars like Pharrell Williams and Missy Elliott, as well as thrash metal groups like GWAR and Lamb of God. Notable performance venues include The Birchmere, the Landmark Theater, and Jiffy Lube Live. Wolf Trap National Park for the Performing Arts is located in Vienna and is the only national park intended for use as a performing arts center.




Many counties and localities host county fairs and festivals. The Virginia State Fair is held at the Meadow Event Park every September. Also in September is the Neptune Festival in Virginia Beach, which celebrates the city, the waterfront, and regional artists. Norfolk's Harborfest, in June, features boat racing and air shows. Fairfax County also sponsors Celebrate Fairfax! with popular and traditional music performances. The Virginia Lake Festival is held during the third weekend in July in Clarksville. Wolf Trap hosts the Wolf Trap Opera Company, which produces an opera festival every summer. Each September, Bay Days celebrates the Chesapeake Bay as well as Hampton's 400-year history since 1610, and Isle of Wight County holds a County Fair on the second week of September as well. Both feature live music performances, and other unique events.
On the Eastern Shore island of Chincoteague the annual Pony Swim & Auction of feral Chincoteague ponies at the end of July is a unique local tradition expanded into a week-long carnival. The Shenandoah Apple Blossom Festival is a six-day festival held annually in Winchester that includes parades and bluegrass concerts. The Old Time Fiddlers' Convention in Galax, begun in 1935, is one of the oldest and largest such events worldwide. Two important film festivals, the Virginia Film Festival and the VCU French Film Festival, are held annually in Charlottesville and Richmond, respectively.




The Hampton Roads area is the 45th-largest media market in the United States as ranked by Nielsen Media Research, while the Richmond-Petersburg area is 57th and Roanoke-Lynchburg is 66th as of 2013. Northern Virginia is part of the much larger Washington, D.C. media market.
There are 36 television stations in Virginia, representing each major U.S. network, part of 42 stations which serve Virginia viewers. More than 720 FCC-licensed FM radio stations broadcast in Virginia, with about 300 such AM stations. The nationally available Public Broadcasting Service (PBS) is headquartered in Arlington. Independent PBS affiliates exist throughout Virginia, and the Arlington PBS member station WETA-TV produces programs such as the PBS NewsHour and Washington Week.
The most circulated native newspapers in the Commonwealth are Norfolk's The Virginian-Pilot (142,476 daily subscribers), the Richmond Times-Dispatch (108,559), and The Roanoke Times (78,663), as of 2014. Several Washington, D.C. papers are based in Northern Virginia, such as The Washington Examiner and Politico. The paper with the nation's widest circulation, USA Today, with 1.83 million daily subscriptions, is headquartered in McLean. Besides traditional forms of media, Virginia is the home base for telecommunication companies such as Voxant and XO Communications. In Northern Virginia, The Washington Post is the dominant newspaper, since Northern VA is located in the Washington, DC metropolitan area.




Virginia's educational system consistently ranks in the top ten states on the U.S. Department of Education's National Assessment of Educational Progress, with Virginia students outperforming the average in all subject areas and grade levels tested. The 2011 Quality Counts report ranked Virginia's K–12 education fourth best in the country. All school divisions must adhere to educational standards set forth by the Virginia Department of Education, which maintains an assessment and accreditation regime known as the Standards of Learning to ensure accountability. In 2010, 85% of high school students graduated on-time after four years. Between 2000 and 2008, school enrollment increased 5%, the number of teachers 21%.
Public K–12 schools in Virginia are generally operated by the counties and cities, and not by the state. As of 2011, a total of 1,267,063 students were enrolled in 1,873 local and regional schools in the Commonwealth, including three charter schools, and an additional 109 alternative and special education centers across 132 school divisions. Besides the general public schools in Virginia, there are Governor's Schools and selective magnet schools. The Governor's Schools are a collection of more than 40 regional high schools and summer programs intended for gifted students. The Virginia Council for Private Education oversees the regulation of 320 state accredited and 130 non-accredited private schools. An additional 24,682 students receive homeschooling.
As of 2011, there are 176 colleges and universities in Virginia. In the U.S. News & World Report ranking of public colleges, the University of Virginia is second, The College of William & Mary is sixth, and Virginia Tech is 25th. Virginia Commonwealth University is ranked the top public graduate school in fine arts, while James Madison University has been recognized as the top regional public master's program in The South since 1993. The Virginia Military Institute is the oldest state military college and a top ranked public liberal arts college. George Mason University is the largest university in Virginia with over 32,000 students. Virginia Tech and Virginia State University are the state's land-grant universities. Virginia also operates 23 community colleges on 40 campuses serving over 260,000 students. There are 129 private institutions, including Hampton University, Washington and Lee University, Randolph College, Hampden–Sydney College, Emory & Henry College, Roanoke College, the University of Richmond, and Randolph-Macon College.




Virginia has a mixed health record, and is ranked as the 26th overall healthiest state according to the 2013 United Health Foundation's Health Rankings. Virginia also ranks 21st among the states in the rate of premature deaths, 6,816 per 100,000. In 2008, Virginia reached its lowest ever rate of infant mortality, at 6.7 deaths per 1,000. There are however racial and social health disparities, in 2010 African Americans experienced 28% more premature deaths than whites, while 13% of Virginians lack any health insurance. According to the Centers for Disease Control and Prevention's 2009 survey, 26% of Virginians are obese and another 35% are overweight. 78% of residents claim to have exercised at least once in the past three months. About 30% of Virginia's 10- to 17-year-olds are overweight or obese. Virginia banned smoking in bars and restaurants in January 2010. 19% of Virginians smoke tobacco. Residents of Virginia's 8th congressional district share the longest average life expectancy rate in the nation, over 83 years.
There are 89 hospitals in Virginia listed with the United States Department of Health and Human Services. Notable examples include Inova Fairfax Hospital, the largest hospital in the Washington Metropolitan Area, and the VCU Medical Center, located on the medical campus of Virginia Commonwealth University. The University of Virginia Medical Center, part of the University of Virginia Health System, is highly ranked in endocrinology according to U.S.News & World Report. Virginia has a ratio of 127 primary care physicians per 10,000 residents, which is the 16th highest nationally. Virginia was one of five states to receive a perfect score in disaster preparedness according to a 2008 report by the Trust for America's Health, based on criteria such as detecting pathogens and distributing vaccines and medical supplies.




Because of the 1932 Byrd Road Act, the state government controls most of Virginia's roads, instead of a local county authority as is usual in other states. As of 2011, the Virginia Department of Transportation owns and operates 57,867 miles (93,128 km) of the total 70,105 miles (112,823 km) of roads in the state, making it the third largest state highway system in the United States. Although the Washington Metropolitan Area, which includes Northern Virginia, has the second worst traffic in the nation, Virginia as a whole has the 21st-lowest congestion and the average commute time is 26.9 minutes. Virginia hit peak car usage before the year 2000, making it one of the first such states.

Virginia has Amtrak passenger rail service along several corridors, and Virginia Railway Express (VRE) maintains two commuter lines into Washington, D.C. from Fredericksburg and Manassas. VRE is one of the nation's fastest growing commuter rail services, handling nearly 20,000 passengers a day. The Washington Metro rapid transit system serves Northern Virginia as far west as communities along I-66 in Fairfax County, with expansion plans to reach Loudoun County by 2017. Major freight railroads in Virginia include Norfolk Southern and CSX Transportation. Commuter buses include the Fairfax Connector and the Shenandoah Valley Commuter Bus. The Virginia Department of Transportation operates several free ferries throughout Virginia, the most notable being the Jamestown-Scotland ferry which crosses the James River in Surry County.
Virginia has five major airports: Washington Dulles International and Reagan Washington National in Northern Virginia, both of which handle over 20 million passengers a year; Richmond International; and Newport News/Williamsburg International Airport and Norfolk International serving the Hampton Roads area. Several other airports offer limited commercial passenger service, and sixty-six public airports serve the state's aviation needs. The Virginia Port Authority's main seaports are those in Hampton Roads, which carried 17,726,251 short tons (16,080,984 t) of bulk cargo in 2007, the sixth most of United States ports. The Eastern Shore of Virginia is the site of Wallops Flight Facility, a rocket testing center owned by NASA, and the Mid-Atlantic Regional Spaceport, a commercial spaceport. Space tourism is also offered through Vienna-based Space Adventures.




In colonial Virginia, free men elected the lower house of the legislature, called the House of Burgesses, which together with the Governor's Council, made the "General Assembly". Founded in 1619, the Virginia General Assembly is still in existence as the oldest legislature in the Western Hemisphere. In 2008, the government was ranked by the Pew Center on the States with an A− in terms of its efficiency, effectiveness, and infrastructure, tied with Utah and Washington. This was the second consecutive time that Virginia received the highest grade in the nation.
Since 1971, the government has functioned under the seventh Constitution of Virginia, which provides for a strong legislature and a unified judicial system. Similar to the federal structure, the government is divided in three branches: legislative, executive, and judicial. The legislature is the General Assembly, a bicameral body whose 100-member House of Delegates and 40-member Senate write the laws for the Commonwealth. The Assembly is stronger than the executive, as it selects judges and justices. The Governor and Lieutenant Governor are elected every four years in separate elections. Incumbent governors cannot run for re-election, however the Lieutenant Governor and Attorney General can, and governors may serve non-consecutive terms. The judicial system, the oldest in America, consists of a hierarchy from the Supreme Court of Virginia and the Court of Appeals of Virginia to the Circuit Courts, the trial courts of general jurisdiction, and the lower General District Courts and Juvenile and Domestic Relations District Courts.
The Code of Virginia is the statutory law, and consists of the codified legislation of the General Assembly. The Virginia State Police is the largest law enforcement agency in Virginia. The Virginia Capitol Police is the oldest police department in the United States. The Virginia National Guard consists of 7,500 soldiers in the Virginia Army National Guard and 1,200 airmen in the Virginia Air National Guard. Since the resumption of capital punishment in Virginia in 1982, 107 people have been executed, the second highest number in the nation. The "total crime risk" is 28% lower than the national average. Since Virginia ended prisoner parole in 1995, the rate of recidivism has fallen to 28.3%, among the lowest nationwide. Virginia is an open-carry state.




Over the 20th century, Virginia shifted from a largely rural, politically Southern and conservative state to a more urbanized, pluralistic, and politically moderate environment. Up until the 1970s, Virginia was a racially divided one-party state dominated by the Byrd Organization. The legacy of slavery in the state effectively disfranchised African Americans until after passage of civil rights legislation in the mid-1960s. Enfranchisement and immigration of other groups, especially Hispanics, have placed growing importance on minority voting, while voters that identify as "white working-class" declined by three percent between 2008 and 2012. Regional differences play a large part in Virginia politics. Rural southern and western areas moved to support the Republican Party in response to its "southern strategy", while urban and growing suburban areas, including much of Northern Virginia, form the Democratic Party base. Democratic support also persists in union-influenced Roanoke in Southwest Virginia, college towns such as Charlottesville and Blacksburg, and the southeastern Black Belt Region.
Political party strength in Virginia has likewise been in flux. In the 2007 state elections, Democrats regained control of the State Senate, and narrowed the Republican majority in the House of Delegates to eight seats. Yet elections in 2009 resulted in the election of Republican Bob McDonnell as Governor by a seventeen-point margin, the election of a Republican Lieutenant Governor and Attorney General, as well as Republican gains of six seats in the House of Delegates. In 2011, the Republican caucus took over two-thirds (68–32) of the seats in the House of Delegates, and a majority of the Senate based on the Lieutenant Governor Bill Bolling as the tie-breaker. Following the 2013 elections, Democrat Terry McAuliffe was elected Governor by two percentage points, and Democrat Ralph Northam was elected Lieutenant Governor by double digits. Republicans, however, maintained their super-majority (68–32) in the House of Delegates. State election seasons traditionally start with the annual Shad Planking event in Wakefield.
In federal elections since 2006, both parties have seen successes. Republican Senator George Allen lost close races in 2006, to Democratic newcomer Jim Webb, and again in 2012, to Webb's replacement, former Governor Tim Kaine. In 2008, Democrats won both United States Senate seats; former Governor Mark Warner was elected to replace retiring Republican John Warner. The state went Republican in 11 out of 12 presidential elections from 1948 to 2004, including 10 in a row from 1968 to 2004. However, Democrat Barack Obama carried Virginia's 13 electoral votes in both the 2008 and 2012 presidential elections. In the 2010 elections, Republicans won three United States House of Representatives seats from the Democrats. Of the state's eleven seats in the House of Representatives, Republicans hold eight and Democrats hold three. Virginia is considered a "swing state" in future presidential elections.
In the 2016 Presidential election, Democrat Hillary Clinton carried Virginia, marking the third consecutive win for the Democratic Party at the presidential level. Even so, the gerrymandered Congressional Districts continue to return a majority of Republican Representatives, although a Federal District Court redrew the malapportioned 3rd District as violating the Voting Rights Act. That allowed Virginians to choose in an additional black Representative from the 4th District, and added to the Democratic total.




Virginia is the most populous U.S. state without a major professional sports league franchise. The reasons for this include the lack of any dominant city or market within the state, the proximity of teams in Washington, D.C. and North Carolina, and a reluctance to publicly finance stadiums. However, in recent years, the city of Virginia Beach has proposed a new arena designed to lure a major league franchise. Norfolk is host to two minor league teams: The AAA Norfolk Tides and the ECHL's Norfolk Admirals. The San Francisco Giants' AA team, the Richmond Flying Squirrels, began play at The Diamond in 2010, replacing the AAA Richmond Braves, who relocated after 2008. Additionally, the Washington Nationals, Boston Red Sox, Cleveland Indians, Atlanta Braves, Pittsburgh Pirates, New York Yankees, and Toronto Blue Jays also have Single-A and Rookie-level farm teams in Virginia.
The Washington Redskins have Redskins Park, their headquarters, in Ashburn and their training facility is in Richmond, and the Washington Capitals train at Kettler Capitals Iceplex in Ballston. Virginia has many professional caliber golf courses including the Greg Norman course at Lansdowne Resort and Kingsmill Resort, home of the Kingsmill Championship, an LPGA Tour tournament. NASCAR currently schedules Sprint Cup races on two tracks in Virginia: Martinsville Speedway and Richmond International Raceway. Virginia natives currently competing in the series include Denny Hamlin and Elliott Sadler.
Virginia does not allow state appropriated funds to be used for either operational or capital expenses for intercollegiate athletics. Despite this, both the Virginia Cavaliers and Virginia Tech Hokies have been able to field competitive teams in the Atlantic Coast Conference and maintain modern facilities. Their rivalry is followed statewide. Twelve other universities compete in NCAA Division I, particularly in the Atlantic 10 Conference, Big South Conference, and Colonial Athletic Association. Three historically black schools compete in the Division II Central Intercollegiate Athletic Association, and two others compete in the Division I Mid-Eastern Athletic Conference. Several smaller schools compete in the Old Dominion Athletic Conference and the USA South Athletic Conference of NCAA Division III. The NCAA currently holds its Division III championships in football, men's basketball, volleyball and softball in Salem.




The state nickname is its oldest symbol, though it has never been made official by law. Virginia was given the title "Dominion" by King Charles II of England at the time of The Restoration, because it had remained loyal to the crown during the English Civil War, and the present moniker, "Old Dominion" is a reference to that title. Charles' supporters were called Cavaliers, and "The Cavalier State" nickname was popularized after the American Civil War to romanticize the antebellum period. Sports teams from the University of Virginia are called the Cavaliers. The other nickname, "Mother of Presidents", is also historic, as eight Virginians have served as President of the United States, including four of the first five.
The state's motto, Sic Semper Tyrannis, translates from Latin as "Thus Always to Tyrants", and is used on the state seal, which is then used on the flag. While the seal was designed in 1776, and the flag was first used in the 1830s, both were made official in 1930. The majority of the other symbols were made official in the late 20th century. The Virginia reel is among the square dances classified as the state dance. In March 2015, after 20 years without a state song, Virginia received two: "Our Great Virginia" (official traditional state song) and "Sweet Virginia Breeze" (official popular state song). In 1940, Virginia made "Carry Me Back to Old Virginny" the state song, but it was retired in 1997 and reclassified as the state song emeritus.




National Register of Historic Places listings in Virginia
History of Virginia
History of Virginia on stamps
History of Kentucky
History of West Virginia
Virginia in the American Civil War










Virginia at DMOZ
Encyclopedia Virginia
Government
State Government website
Virginia General Assembly
Virginia's Judicial system
Constitution of Virginia
Virginia State and County Government Websites
Tourism and recreation
Virginia Tourism Website
Virginia State Parks
Virginia Main Street Communities Travel
Culture and history
Virginia Historical Society
Virginia's First People
WPA Guide to the Old Dominion
Library of Virginia
Maps and Demographics
USGS geographic resources of Virginia
Virginia State Climatology Office
Virginia State Facts from USDA, Economic Research Service
 Geographic data related to Virginia at OpenStreetMapMontana /mɒnˈtænə/ is a state in the Western region of the United States. The state's name is derived from the Spanish word montaña (mountain). Montana has several nicknames, although none official, including "Big Sky Country" and "The Treasure State", and slogans that include "Land of the Shining Mountains" and more recently "The Last Best Place". Montana has a 545-mile (877 km) border with three Canadian provinces: British Columbia, Alberta, and Saskatchewan, the only state to do so. It also borders North Dakota and South Dakota to the east, Wyoming to the south, and Idaho to the west and southwest. Montana is ranked 4th in size, but 44th in population and 48th in population density of the 50 United States. The western third of Montana contains numerous mountain ranges. Smaller island ranges are found throughout the state. In total, 77 named ranges are part of the Rocky Mountains. The eastern half of Montana is characterized by western prairie terrain and badlands.
The economy is primarily based on agriculture, including ranching and cereal grain farming. Other significant economic activities include oil, gas, coal and hard rock mining, lumber, and the fastest-growing sector, tourism. The health care, service, and government sectors also are significant to the state's economy. Millions of tourists annually visit Glacier National Park, the Little Bighorn Battlefield National Monument, and Yellowstone National Park.



The name Montana comes from the Spanish word Montaña and the Latin word Montana, meaning "mountain", or more broadly, "mountainous country". Montaña del Norte was the name given by early Spanish explorers to the entire mountainous region of the west. The name Montana was added to a bill by the United States House Committee on Territories, which was chaired at the time by Rep. James Ashley of Ohio, for the territory that would become Idaho Territory. The name was changed by Representatives Henry Wilson (Massachusetts) and Benjamin F. Harding (Oregon), who complained Montana had "no meaning". When Ashley presented a bill to establish a temporary government in 1864 for a new territory to be carved out of Idaho, he again chose Montana Territory. This time Rep. Samuel Cox, also of Ohio, objected to the name. Cox complained that the name was a misnomer given most of the territory was not mountainous and that a Native American name would be more appropriate than a Spanish one. Other names such as Shoshone were suggested, but it was decided that the Committee on Territories could name it whatever they wanted, so the original name of Montana was adopted.




With an area of 147,040 square miles (380,800 km2), Montana is slightly larger than Japan. It is the fourth largest state in the United States after Alaska, Texas, and California; the largest landlocked U.S. state; and the world's 56th largest national state/province subdivision. To the north, Montana shares a 545-mile (877 km) border with three Canadian provinces: British Columbia, Alberta, and Saskatchewan, the only state to do so. It borders North Dakota and South Dakota to the east, Wyoming to the south and Idaho to the west and southwest.



The state's topography is roughly defined by the Continental Divide, which splits much of the state into distinct eastern and western regions. Most of Montana's 100 or more named mountain ranges are in the state's western half, most of which is geologically and geographically part of the Northern Rocky Mountains. The Absaroka and Beartooth ranges in the state's south-central part are technically part of the Central Rocky Mountains. The Rocky Mountain Front is a significant feature in the state's north-central portion, and isolated island ranges that interrupt the prairie landscape common in the central and eastern parts of the state. About 60 percent of the state is prairie, part of the northern Great Plains.
The Bitterroot Mountains—one of the longest continuous ranges in the Rocky Mountain chain from Alaska to Mexico—along with smaller ranges, including the Coeur d'Alene Mountains and the Cabinet Mountains, divide the state from Idaho. The southern third of the Bitterroot range blends into the Continental Divide. Other major mountain ranges west of the Divide include the Cabinet Mountains, the Anaconda Range, the Missions, the Garnet Range, Sapphire Mountains, and Flint Creek Range.

The Divide's northern section, where the mountains rapidly give way to prairie, is part of the Rocky Mountain Front. The front is most pronounced in the Lewis Range, located primarily in Glacier National Park. Due to the configuration of mountain ranges in Glacier National Park, the Northern Divide (which begins in Alaska's Seward Peninsula) crosses this region and turns east in Montana at Triple Divide Peak. It causes the Waterton River, Belly, and Saint Mary rivers to flow north into Alberta, Canada. There they join the Saskatchewan River, which ultimately empties into Hudson Bay.
East of the divide, several roughly parallel ranges cover the state's southern part, including the Gravelly Range, the Madison Range, Gallatin Range, Absaroka Mountains and the Beartooth Mountains. The Beartooth Plateau is the largest continuous land mass over 10,000 feet (3,000 m) high in the continental United States. It contains the state's highest point, Granite Peak, 12,799 feet (3,901 m) high. North of these ranges are the Big Belt Mountains, Bridger Mountains, Tobacco Roots, and several island ranges, including the Crazy Mountains and Little Belt Mountains.

Between many mountain ranges are rich river valleys. The Big Hole Valley, Bitterroot Valley, Gallatin Valley, Flathead Valley, and Paradise Valley have extensive agricultural resources and multiple opportunities for tourism and recreation.
East and north of this transition zone are the expansive and sparsely populated Northern Plains, with tableland prairies, smaller island mountain ranges, and badlands. The isolated island ranges east of the Divide include the Bear Paw Mountains, Bull Mountains, Castle Mountains, Crazy Mountains, Highwood Mountains, Judith Mountains, Little Belt Mountains, Little Rocky Mountains, the Pryor Mountains, Snowy Mountains, Sweet Grass Hills, and—in the state's southeastern corner near Ekalaka—the Long Pines. Many of these isolated eastern ranges were created about 120 to 66 million years ago when magma welling up from the interior cracked and bowed the earth's surface here.
The area east of the divide in the state' north-central portion is known for the Missouri Breaks and other significant rock formations. Three buttes south of Great Falls are major landmarks: Cascade, Crown, Square, Shaw and Buttes. Known as laccoliths, they formed when igneous rock protruded through cracks in the sedimentary rock. The underlying surface consists of sandstone and shale. Surface soils in the area are highly diverse, and greatly affected by the local geology, whether glaciated plain, intermountain basin, mountain foothills, or tableland. Foothill regions are often covered in weathered stone or broken slate, or consist of uncovered bare rock (usually igneous, quartzite, sandstone, or shale). The soil of intermountain basins usually consists of clay, gravel, sand, silt, and volcanic ash, much of it laid down by lakes which covered the region during the Oligocene 33 to 23 million years ago. Tablelands are often topped with argillite gravel and weathered quartzite, occasionally underlain by shale. The glaciated plains are generally covered in clay, gravel, sand, and silt left by the proglacial Lake Great Falls or by moraines or gravel-covered former lake basins left by the Wisconsin glaciation 85,000 to 11,000 years ago. Farther east, areas such as Makoshika State Park near Glendive and Medicine Rocks State Park near Ekalaka contain some of the most scenic badlands regions in the state.

The Hell Creek Formation in Northeast Montana is a major source of dinosaur fossils. Paleontologist Jack Horner of the Museum of the Rockies in Bozeman brought this formation to the world's attention with several major finds.




Montana has thousands of named rivers and creeks, 450 miles (720 km) of which are known for "blue-ribbon" trout fishing. Montana's water resources provide for recreation, hydropower, crop and forage irrigation, mining, and water for human consumption. Montana is one of few geographic areas in the world whose rivers form parts of three major watersheds (i.e. where two continental divides intersect). Its rivers feed the Pacific Ocean, the Gulf of Mexico, and Hudson Bay. The watersheds divide at Triple Divide Peak in Glacier National Park.




West of the divide, the Clark Fork of the Columbia (not to be confused with the Clarks Fork of the Yellowstone River) rises near Butte and flows northwest to Missoula, where it is joined by the Blackfoot River and Bitterroot River. Farther downstream, it is joined by the Flathead River before entering Idaho near Lake Pend Oreille. The Pend Oreille River forms the outflow of Lake Pend Oreille. The Pend Oreille River joined the Columbia River, which flows to the Pacific Ocean—making the 579-mile (932 km) long Clark Fork/Pend Oreille (considered a single river system) the longest river in the Rocky Mountains. The Clark Fork discharges the greatest volume of water of any river exiting the state. The Kootenai River in northwest Montana is another major tributary of the Columbia.



East of the divide the Missouri River, which is formed by the confluence of the Jefferson, Madison and Gallatin rivers near Three Forks, flows due north through the west-central part of the state to Great Falls. From this point, it then flows generally east through fairly flat agricultural land and the Missouri Breaks to Fort Peck reservoir. The stretch of river between Fort Benton and the Fred Robinson Bridge at the western boundary of Fort Peck Reservoir was designated a National Wild and Scenic River in 1976. The Missouri enters North Dakota near Fort Union, having drained more than half the land area of Montana (82,000 square miles (210,000 km2)). Nearly one-third of the Missouri River in Montana lies behind 10 dams: Toston, Canyon Ferry, Hauser, Holter, Black Eagle, Rainbow, Cochrane, Ryan, Morony, and Fort Peck.
The Yellowstone River rises on the continental divide near Younts Peak in Wyoming's Teton Wilderness. It flows north through Yellowstone National Park, enters Montana near Gardiner, and passes through the Paradise Valley to Livingston. It then flows northeasterly across the state through Billings, Miles City, Glendive, and Sidney. The Yellowstone joins the Missouri in North Dakota just east of Fort Union. It is the longest undammed, free-flowing river in the contiguous United States, and drains about a quarter of Montana (36,000 square miles (93,000 km2)).
Other major Montana tributaries of the Missouri include the Smith, Milk, Marias, Judith, and Musselshell Rivers. Montana also claims the disputed title of possessing the world's shortest river, the Roe River, just outside Great Falls. Through the Missouri, these rivers ultimately join the Mississippi River and flow into the Gulf of Mexico.
Major tributaries of the Yellowstone include the Boulder, Stillwater, Clarks Fork, Bighorn, Tongue, and Powder Rivers.



The Northern Divide turns east in Montana at Triple Divide Peak, causing the Waterton River, Belly, and Saint Mary rivers to flow north into Alberta. There they join the Saskatchewan River, which ultimately empties into Hudson Bay.



There are at least 3,223 named lakes and reservoirs in Montana, including Flathead Lake, the largest natural freshwater lake in the western United States. Other major lakes include Whitefish Lake in the Flathead Valley and Lake McDonald and St. Mary Lake in Glacier National Park. The largest reservoir in the state is Fort Peck Reservoir on the Missouri river, which is contained by the second largest earthen dam and largest hydraulically filled dam in the world. Other major reservoirs include Hungry Horse on the Flathead River; Lake Koocanusa on the Kootenai River; Lake Elwell on the Marias River; Clark Canyon on the Beaverhead River; Yellowtail on the Bighorn River, Canyon Ferry, Hauser, Holter, Rainbow; and Black Eagle on the Missouri River.




Vegetation of the state includes lodgepole pine, ponderosa pine; Douglas fir, larch, spruce; aspen, birch, red cedar, hemlock, ash, alder; rocky mountain maple and cottonwood trees. Forests cover approximately 25 percent of the state. Flowers native to Montana include asters, bitterroots, daisies, lupins, poppies, primroses, columbine, lilies, orchids, and dryads. Several species of sagebrush and cactus and many species of grasses are common. Many species of mushrooms and lichens are also found in the state.
Montana is home to a diverse array of fauna that includes 14 amphibian, 90 fish, 117 mammal, 20 reptile and 427 bird species. Additionally, there are over 10,000 invertebrate species, including 180 mollusks and 30 crustaceans. Montana has the largest grizzly bear population in the lower 48 states. Montana hosts five federally endangered species–black-footed ferret, whooping crane, least tern, pallid sturgeon and white sturgeon and seven threatened species including the grizzly bear, Canadian lynx and bull trout. The Montana Department of Fish, Wildlife and Parks manages fishing and hunting seasons for at least 17 species of game fish including seven species of trout, walleye and smallmouth bass and at least 29 species of game birds and animals including ring-neck pheasant, grey partridge, elk, pronghorn antelope, mule deer, whitetail deer, gray wolf and bighorn sheep.




Montana contains Glacier National Park, "The Crown of the Continent"; and portions of Yellowstone National Park, including three of the park's five entrances. Other federally recognized sites include the Little Bighorn National Monument, Bighorn Canyon National Recreation Area, Big Hole National Battlefield, and the National Bison Range. Approximately 31,300,000 acres (127,000 km2), or 35 percent of Montana's land is administered by federal or state agencies. The U.S. Department of Agriculture Forest Service administers 16,800,000 acres (68,000 km2) of forest land in ten National Forests. There are approximately 3,300,000 acres (13,000 km2) of wilderness in 12 separate wilderness areas that are part of the National Wilderness Preservation System established by the Wilderness Act of 1964. The U.S. Department of the Interior Bureau of Land Management controls 8,100,000 acres (33,000 km2) of federal land. The U.S. Department of the Interior Fish and Wildlife Service administers 110,000 acres (450 km2) of 1.1 million acres of National Wildlife Refuges and waterfowl production areas in Montana. The U.S. Department of the Interior Bureau of Reclamation administers approximately 300,000 acres (1,200 km2) of land and water surface in the state. The Montana Department of Fish, Wildlife and Parks operates approximately 275,265 acres (1,113.96 km2) of state parks and access points on the state's rivers and lakes. The Montana Department of Natural Resources and Conservation manages 5,200,000 acres (21,000 km2) of School Trust Land ceded by the federal government under the Land Ordinance of 1785 to the state in 1889 when Montana was granted statehood. These lands are managed by the state for the benefit of public schools and institutions in the state.

Areas managed by the National Park Service include:
Big Hole National Battlefield near Wisdom
Bighorn Canyon National Recreation Area near Fort Smith
Glacier National Park
Grant-Kohrs Ranch National Historic Site at Deer Lodge
Lewis and Clark National Historic Trail
Little Bighorn Battlefield National Monument near Crow Agency
Nez Perce National Historical Park
Yellowstone National Park




Montana is a large state with considerable variation in geography, and the climate is, therefore, equally varied. The state spans from below the 45th parallel (the line equidistant between the equator and North Pole) to the 49th parallel, and elevations range from under 2,000 feet (610 m) to nearly 13,000 feet (4,000 m) above sea level. The western half is mountainous, interrupted by numerous large valleys. Eastern Montana comprises plains and badlands, broken by hills and isolated mountain ranges, and has a semi-arid, continental climate (Köppen climate classification BSk). The Continental Divide has a considerable effect on the climate, as it restricts the flow of warmer air from the Pacific from moving east, and drier continental air from moving west. The area west of the divide has a modified northern Pacific coast climate, with milder winters, cooler summers, less wind and a longer growing season. Low clouds and fog often form in the valleys west of the divide in winter, but this is rarely seen in the east.
Average daytime temperatures vary from 28 °F or −2.2 °C in January to 84.5 °F or 29.2 °C in July. The variation in geography leads to great variation in temperature. The highest observed summer temperature was 117 °F or 47.2 °C at Glendive on July 20, 1893, and Medicine Lake on July 5, 1937. Throughout the state, summer nights are generally cool and pleasant. Extremely hot weather is less common above 4,000 feet or 1,200 metres. Snowfall has been recorded in all months of the year in the more mountainous areas of central and western Montana, though it is rare in July and August.

The coldest temperature on record for Montana is also the coldest temperature for the entire contiguous U.S. On January 20, 1954, −70 °F or −56.7 °C was recorded at a gold mining camp near Rogers Pass. Temperatures vary greatly on cold nights, and Helena, 40 miles (64 km) to the southeast had a low of only −36 °F or −37.8 °C on the same date, and an all-time record low of −42 °F or −41.1 °C. Winter cold spells are usually the result of cold continental air coming south from Canada. The front is often well defined, causing a large temperature drop in a 24-hour period. Conversely, air flow from the southwest results in "chinooks". These steady 25–50 mph (40–80 km/h) (or more) winds can suddenly warm parts of Montana, especially areas just to the east of the mountains, where temperatures sometimes rise up to 50–60 °F (10.0–15.6 °C) for periods of ten days or longer.
Loma is the site of the most extreme recorded temperature change in a 24-hour period in the United States. On January 15, 1972, a chinook wind blew in and the temperature rose from −54 to 49 °F (−47.8 to 9.4 °C).

Average annual precipitation is 15 inches (380 mm), but great variations are seen. The mountain ranges block the moist Pacific air, holding moisture in the western valleys, and creating rain shadows to the east. Heron, in the west, receives the most precipitation, 34.70 inches (881 mm). On the eastern (leeward) side of a mountain range, the valleys are much drier; Lonepine averages 11.45 inches (291 mm), and Deer Lodge 11.00 inches (279 mm) of precipitation. The mountains can receive over 100 inches (2,500 mm), for example the Grinnell Glacier in Glacier National Park gets 105 inches (2,700 mm). An area southwest of Belfry averaged only 6.59 inches (167 mm) over a sixteen-year period. Most of the larger cities get 30 to 50 inches or 0.76 to 1.27 metres of snow each year. Mountain ranges can accumulate 300 inches or 7.62 metres of snow during a winter. Heavy snowstorms may occur from September through May, though most snow falls from November to March.
The climate has become warmer in Montana and continues to do so. The glaciers in Glacier National Park have receded and are predicted to melt away completely in a few decades. Many Montana cities set heat records during July 2007, the hottest month ever recorded in Montana. Winters are warmer, too, and have fewer cold spells. Previously these cold spells had killed off bark beetles, but these are now attacking the forests of western Montana. The warmer winters in the region have allowed various species to expand their ranges and proliferate. The combination of warmer weather, attack by beetles, and mismanagement during past years has led to a substantial increase in the severity of forest fires in Montana. According to a study done for the U.S. Environmental Protection Agency by the Harvard School of Engineering and Applied Science, portions of Montana will experience a 200-percent increase in area burned by wildfires, and an 80-percent increase in related air pollution.
The table below lists average temperatures for the warmest and coldest month for Montana's seven largest cities. The coldest month varies between December and January depending on location, although figures are similar throughout.



Montana is one of only two continental US states (along with Colorado) which is antipodal to land. The Kerguelen Islands are antipodal to the Montana–Saskatchewan–Alberta border. No towns are precisely antipodal to Kerguelen, though Chester and Rudyard are close.




Various indigenous peoples lived in the territory of the present-day state of Montana for thousands of years. Historic tribes encountered by Europeans and settlers from the United States included the Crow in the south-central area; the Cheyenne in the southeast; the Blackfeet, Assiniboine and Gros Ventres in the central and north-central area; and the Kootenai and Salish in the west. The smaller Pend d'Oreille and Kalispel tribes lived near Flathead Lake and the western mountains, respectively.
The land in Montana east of the continental divide was part of the Louisiana Purchase in 1803. Subsequent to and particularly in the decades following the Lewis and Clark Expedition, American, British and French traders operated a fur trade, typically working with indigenous peoples, in both eastern and western portions of what would become Montana. These dealings were not always peaceful, and though the fur trade brought some material gain for indigenous tribal groups it also brought exposure to European diseases and altered their economic and cultural traditions. Until the Oregon Treaty (1846), land west of the continental divide was disputed between the British and U.S. and was known as the Oregon Country. The first permanent settlement by Euro-Americans in what today is Montana was St. Mary's (1841) near present-day Stevensville. In 1847, Fort Benton was established as the uppermost fur-trading post on the Missouri River. In the 1850s, settlers began moving into the Beaverhead and Big Hole valleys from the Oregon Trail and into the Clark's Fork valley.
The first gold discovered in Montana was at Gold Creek near present-day Garrison in 1852. A series of major mining discoveries in the western third of the state starting in 1862 found gold, silver, copper, lead, coal (and later oil) that attracted tens of thousands of miners to the area. The richest of all gold placer diggings was discovered at Alder Gulch, where the town of Virginia City was established. Other rich placer deposits were found at Last Chance Gulch, where the city of Helena now stands, Confederate Gulch, Silver Bow, Emigrant Gulch, and Cooke City. Gold output from 1862 through 1876 reached $144 million; silver then became even more important. The largest mining operations were in the city of Butte, which had important silver deposits and gigantic copper deposits.



Before the creation of Montana Territory (1864–1889), various parts of what is now Montana were parts of Oregon Territory (1848–1859), Washington Territory (1853–1863), Idaho Territory (1863–1864), and Dakota Territory (1861–1864). Montana became a United States territory (Montana Territory) on May 26, 1864. The first territorial capital was at Bannack. The first territorial governor was Sidney Edgerton. The capital moved to Virginia City in 1865 and to Helena in 1875. In 1870, the non-Indian population of Montana Territory was 20,595. The Montana Historical Society, founded on February 2, 1865, in Virginia City is the oldest such institution west of the Mississippi (excluding Louisiana). In 1869 and 1870 respectively, the Cook–Folsom–Peterson and the Washburn–Langford–Doane Expeditions were launched from Helena into the Upper Yellowstone region and directly led to the creation of Yellowstone National Park in 1872.




As white settlers began populating Montana from the 1850s through the 1870s, disputes with Native Americans ensued, primarily over land ownership and control. In 1855, Washington Territorial Governor Isaac Stevens negotiated the Hellgate treaty between the United States Government and the Salish, Pend d'Oreille, and the Kootenai people of western Montana, which established boundaries for the tribal nations. The treaty was ratified in 1859. While the treaty established what later became the Flathead Indian Reservation, trouble with interpreters and confusion over the terms of the treaty led whites to believe that the Bitterroot Valley was opened to settlement, but the tribal nations disputed those provisions. The Salish remained in the Bitterroot Valley until 1891.
The first U.S. Army post established in Montana was Camp Cooke in 1866, on the Missouri River, to protect steamboat traffic going to Fort Benton, Montana. More than a dozen additional military outposts were established in the state. Pressure over land ownership and control increased due to discoveries of gold in various parts of Montana and surrounding states. Major battles occurred in Montana during Red Cloud's War, the Great Sioux War of 1876, the Nez Perce War and in conflicts with Piegan Blackfeet. The most notable of these were the Marias Massacre (1870), Battle of the Little Bighorn (1876), Battle of the Big Hole (1877) and Battle of Bear Paw (1877). The last recorded conflict in Montana between the U.S. Army and Native Americans occurred in 1887 during the Battle of Crow Agency in the Big Horn country. Indian survivors who had signed treaties were generally required to move onto reservations.

Simultaneously with these conflicts, bison, a keystone species and the primary protein source that Native people had survived on for centuries were being destroyed. Some estimates say there were over 13 million bison in Montana in 1870. In 1875, General Philip Sheridan pleaded to a joint session of Congress to authorize the slaughtering of herds in order to deprive the Indians of their source of food. By 1884, commercial hunting had brought bison to the verge of extinction; only about 325 bison remained in the entire United States.



Cattle ranching has been central to Montana's history and economy since Johnny Grant began wintering cattle in the Deer Lodge Valley in the 1850s and traded cattle fattened in fertile Montana valleys with emigrants on the Oregon Trail. Nelson Story brought the first Texas Longhorn cattle into the territory in 1866. Granville Stuart, Samuel Hauser and Andrew J. Davis started a major open range cattle operation in Fergus County in 1879. The Grant-Kohrs Ranch National Historic Site in Deer Lodge is maintained today as a link to the ranching style of the late 19th century. Operated by the National Park Service, it is a 1,900-acre (7.7 km2) working ranch.



Tracks of the Northern Pacific Railroad (NPR) reached Montana from the west in 1881 and from the east in 1882. However, the railroad played a major role in sparking tensions with Native American tribes in the 1870s. Jay Cooke, the NPR president launched major surveys into the Yellowstone valley in 1871, 1872 and 1873 which were challenged forcefully by the Sioux under chief Sitting Bull. These clashes, in part, contributed to the Panic of 1873, a financial crisis that delayed construction of the railroad into Montana. Surveys in 1874, 1875 and 1876 helped spark the Great Sioux War of 1876. The transcontinental NPR was completed on September 8, 1883, at Gold Creek.
Tracks of the Great Northern Railroad (GNR) reached eastern Montana in 1887 and when they reached the northern Rocky Mountains in 1890, the GNR became a significant promoter of tourism to Glacier National Park region. The transcontinental GNR was completed on January 6, 1893, at Scenic, Washington.
In 1881, the Utah and Northern Railway a branch line of the Union Pacific completed a narrow gauge line from northern Utah to Butte. A number of smaller spur lines operated in Montana from 1881 into the 20th century including the Oregon Short Line, Montana Railroad and Milwaukee Road.




Under Territorial Governor Thomas Meagher, Montanans held a constitutional convention in 1866 in a failed bid for statehood. A second constitutional convention was held in Helena in 1884 that produced a constitution ratified 3:1 by Montana citizens in November 1884. For political reasons, Congress did not approve Montana statehood until 1889. Congress approved Montana statehood in February 1889 and President Grover Cleveland signed an omnibus bill granting statehood to Montana, North Dakota, South Dakota and Washington once the appropriate state constitutions were crafted. In July 1889, Montanans convened their third constitutional convention and produced a constitution accepted by the people and the federal government. On November 8, 1889 President Benjamin Harrison proclaimed Montana the forty-first state in the union. The first state governor was Joseph K. Toole. In the 1880s, Helena (the current state capital) had more millionaires per capita than any other United States city.



The Homestead Act of 1862 provided free land to settlers who could claim and "prove-up" 160 acres (0.65 km2) of federal land in the midwest and western United States. Montana did not see a large influx of immigrants from this act because 160 acres was usually insufficient to support a family in the arid territory. The first homestead claim under the act in Montana was made by David Carpenter near Helena in 1868. The first claim by a woman was made near Warm Springs Creek by Gwenllian Evans, the daughter of Deer Lodge Montana pioneer, Morgan Evans. By 1880, there were farms in the more verdant valleys of central and western Montana, but few on the eastern plains.
The Desert Land Act of 1877 was passed to allow settlement of arid lands in the west and allotted 640 acres (2.6 km2) to settlers for a fee of $.25 per acre and a promise to irrigate the land. After three years, a fee of one dollar per acre would be paid and the land would be owned by the settler. This act brought mostly cattle and sheep ranchers into Montana, many of whom grazed their herds on the Montana prairie for three years, did little to irrigate the land and then abandoned it without paying the final fees. Some farmers came with the arrival of the Great Northern and Northern Pacific Railroads throughout the 1880s and 1890s, though in relatively small numbers.

In the early 1900s, James J. Hill of the Great Northern began promoting settlement in the Montana prairie to fill his trains with settlers and goods. Other railroads followed suit. In 1902, the Reclamation Act was passed, allowing irrigation projects to be built in Montana's eastern river valleys. In 1909, Congress passed the Enlarged Homestead Act that expanded the amount of free land from 160 to 320 acres (0.6 to 1.3 km2) per family and in 1912 reduced the time to "prove up" on a claim to three years. In 1916, the Stock-Raising Homestead Act allowed homesteads of 640 acres in areas unsuitable for irrigation.  This combination of advertising and changes in the Homestead Act drew tens of thousands of homesteaders, lured by free land, with World War I bringing particularly high wheat prices. In addition, Montana was going through a temporary period of higher-than-average precipitation. Homesteaders arriving in this period were known as "Honyockers", or "scissorbills." Though the word "honyocker", possibly derived from the ethnic slur "hunyak," was applied in a derisive manner at homesteaders as being "greenhorns", "new at his business" or "unprepared", the reality was that a majority of these new settlers had previous farming experience, though there were also many who did not.
However, farmers faced a number of problems. Massive debt was one. Also, most settlers were from wetter regions, unprepared for the dry climate, lack of trees, and scarce water resources. In addition, small homesteads of fewer than 320 acres (130 ha) were unsuited to the environment. Weather and agricultural conditions are much harsher and drier west of the 100th meridian. Then, the droughts of 1917–1921 proved devastating. Many people left, and half the banks in the state went bankrupt as a result of providing mortgages that could not be repaid. As a result, farm sizes increased while the number of farms decreased
By 1910, homesteaders filed claims on over five million acres, and by 1923, over 93 million acres were farmed. In 1910, the Great Falls land office alone saw over 1,000 homestead filings per month, and the peak of 1917– 1918 saw 14,000 new homesteads each year. But significant drop occurred following drought in 1919.

Honyocker, scissorbill, nester ... He was the Joad of a [half] century ago, swarming into a hostile land: duped when he started, robbed when he arrived; hopeful, courageous, ambitious: he sought independence or adventure, comfort and security ... The honyocker was farmer, spinster, deep-sea diver; fiddler, physician, bartender, cook. He lived in Minnesota or Wisconsin, Massachusetts or Maine. There the news sought him out—Jim Hill's news of free land in the Treasure State ...



As World War I broke out, Jeannette Rankin, the first woman in the United States to be a member of Congress, was a pacifist and voted against the United States' declaration of war. Her actions were widely criticized in Montana, where public support for the war was strong, and wartime sentiment reached levels of hyper-patriotism among many Montanans. In 1917–18, due to a miscalculation of Montana's population, approximately 40,000 Montanans, ten percent of the state's population, either volunteered or were drafted into the armed forces. This represented a manpower contribution to the war that was 25 percent higher than any other state on a per capita basis. Approximately 1500 Montanans died as a result of the war and 2437 were wounded, also higher than any other state on a per capita basis. Montana's Remount station in Miles City provided 10,000 cavalry horses for the war, more than any other Army post in the US. The war created a boom for Montana mining, lumber and farming interests as demand for war materials and food increased.
In June 1917, the U.S. Congress passed the Espionage Act of 1917 which was later extended by the Sedition Act of 1918, enacted in May 1918. In February 1918, the Montana legislature had passed the Montana Sedition Act, which was a model for the federal version. In combination, these laws criminalized criticism of the U.S. government, military, or symbols through speech or other means. The Montana Act led to the arrest of over 200 individuals and the conviction of 78, mostly of German or Austrian descent. Over 40 spent time in prison. In May 2006, then-Governor Brian Schweitzer posthumously issued full pardons for all those convicted of violating the Montana Sedition Act.
The Montanans who opposed U.S. entry into the war included certain immigrant groups of German and Irish heritage as well as pacifist Anabaptist people such as the Hutterites and Mennonites, many of whom were also of Germanic heritage. In turn, pro-War groups formed, such as the Montana Council of Defense, created by Governor Samuel V. Stewart as well as local "loyalty committees."
War sentiment was complicated by labor issues. The Anaconda Copper Company, which was at its historic peak of copper production, was an extremely powerful force in Montana, but also faced criticism and opposition from socialist newspapers and unions struggling to make gains for their members. In Butte, a multi-ethnic community with significant European immigrant population, labor unions, particularly the newly formed Metal Mine Workers' Union, opposed the war on grounds that it mostly profited large lumber and mining interests. In the wake of ramped-up mine production and the Speculator Mine disaster in June 1917, Industrial Workers of the World organizer Frank Little arrived in Butte to organize miners. He gave some speeches with inflammatory anti-war rhetoric. On August 1, 1917, he was dragged from his boarding house by masked vigilantes, and hanged from a railroad trestle, considered a lynching. Little's murder and the strikes that followed resulted in the National Guard being sent to Butte to restore order. Overall, anti-German and anti-labor sentiment increased and created a movement that led to the passage of the Montana Sedition Act the following February. In addition, the Council of Defense was made a state agency with the power to prosecute and punish individuals deemed in violation of the Act. The Council also passed rules limiting public gatherings and prohibiting the speaking of German in public.
In the wake of the legislative action in 1918, emotions rose. U.S. Attorney Burton K. Wheeler and several District Court Judges who hesitated to prosecute or convict people brought up on charges were strongly criticized. Wheeler was brought before the Council of Defense, though he avoided formal proceedings, and a District Court judge from Forsyth was impeached. There were burnings of German-language books and several near-hangings. The prohibition on speaking German remained in effect into the early 1920s. Complicating the wartime struggles, the 1918 Influenza epidemic claimed the lives of over 5,000 Montanans. The period has been dubbed "Montana's Agony" by some historians due to the suppression of civil liberties that occurred.



An economic depression began in Montana after World War I and lasted through the Great Depression until the beginning of World War II. This caused great hardship for farmers, ranchers, and miners. The wheat farms in eastern Montana make the state a major producer; the wheat has a relatively high protein content and thus commands premium prices.



When the U.S. entered World War II on December 8, 1941, many Montanans already had enlisted in the military to escape the poor national economy of the previous decade. Another 40,000-plus Montanans entered the armed forces in the first year following the declaration of war, and over 57,000 joined up before the war ended. These numbers constituted about 10 percent of the state's total population, and Montana again contributed one of the highest numbers of soldiers per capita of any state. Many Native Americans were among those who served, including soldiers from the Crow Nation who became Code Talkers. At least 1500 Montanans died in the war. Montana also was the training ground for the First Special Service Force or "Devil's Brigade," a joint U.S-Canadian commando-style force that trained at Fort William Henry Harrison for experience in mountainous and winter conditions before deployment. Air bases were built in Great Falls, Lewistown, Cut Bank and Glasgow, some of which were used as staging areas to prepare planes to be sent to allied forces in the Soviet Union. During the war, about 30 Japanese balloon bombs were documented to have landed in Montana, though no casualties nor major forest fires were attributed to them.
In 1940, Jeannette Rankin was again elected to Congress. In 1941, as she had in 1917, she voted against the United States' declaration of war after the Japanese attack on Pearl Harbor. Hers was the only vote against the war, and in the wake of public outcry over her vote, Rankin required police protection for a time. Other pacifists tended to be those from "peace churches" who generally opposed war. Many individuals claiming conscientious objector status from throughout the U.S. were sent to Montana during the war as smokejumpers and for other forest fire-fighting duties.



During World War II, the planned battleship USS Montana was named in honor of the state. However, the battleship was never completed. Montana is the only one of the first 48 states lacking a completed battleship being named for it. Alaska and Hawaii have both had nuclear submarines named after them. Montana is the only state in the union without a modern naval ship named in its honor. However, in August 2007 Senator Jon Tester made a request to the Navy that a submarine be christened USS Montana. Secretary of the Navy Ray Mabus announced on September 3, 2015 that Virginia Class attack Submarine SSN-794 will bear the state's namesake. This will be the second commissioned warship to bear the name Montana.



In the post-World War II Cold War era, Montana became host to U.S. Air Force Military Air Transport Service (1947) for airlift training in C-54 Skymasters and eventually, in 1953 Strategic Air Command air and missile forces were based at Malmstrom Air Force Base in Great Falls. The base also hosted the 29th Fighter Interceptor Squadron, Air Defense Command from 1953 to 1968. In December 1959, Malmstrom AFB was selected as the home of the new Minuteman I ballistic missile. The first operational missiles were in-place and ready in early 1962. In late 1962 missiles assigned to the 341st Strategic Missile Wing would play a major role in the Cuban Missile Crisis. When the Soviets removed their missiles from Cuba, President John F. Kennedy said the Soviets backed down because they knew he had an "Ace in the Hole," referring directly to the Minuteman missiles in Montana. Montana eventually became home to the largest ICBM field in the U.S. covering 23,500 square miles (61,000 km2).




The United States Census Bureau estimates that the population of Montana was 1,032,949 on July 1, 2015, a 4.40% increase since the 2010 United States Census. The 2010 census put Montana's population at 989,415 which is an increase of 43,534 people, or 4.40 percent, since 2010. During the first decade of the new century, growth was mainly concentrated in Montana's seven largest counties, with the highest percentage growth in Gallatin County, which saw a 32 percent increase in its population from 2000-2010. The city seeing the largest percentage growth was Kalispell with 40.1 percent, and the city with the largest increase in actual residents was Billings with an increase in population of 14,323 from 2000-2010.
On January 3, 2012, the Census and Economic Information Center (CEIC) at the Montana Department of Commerce estimated Montana had hit the one million population mark sometime between November and December 2011. The United States Census Bureau estimates that the population of Montana was 1,005,141 on July 1, 2012, a 1.6 percent increase since the 2010 United States Census.
According to the 2010 Census, 89.4 percent of the population was White (87.8 percent Non-Hispanic White), 6.3 percent American Indian and Alaska Native, 2.9 percent Hispanics and Latinos of any race, 0.6 percent Asian, 0.4 percent Black or African American, 0.1 percent Native Hawaiian and Other Pacific Islander, 0.6 percent from Some Other Race, and 2.5 percent from two or more races. The largest European ancestry groups in Montana as of 2010 are: German (27.0 percent), Irish (14.8 percent), English (12.6 percent), Norwegian (10.9 percent), French (4.7 percent) and Italian (3.4 percent).



English is the official language in the state of Montana, as it is in many U.S. states. According to the 2000 U.S. Census, 94.8 percent of the population aged 5 and older speak English at home. Spanish is the language most commonly spoken at home other than English. There were about 13,040 Spanish-language speakers in the state (1.4 percent of the population) in 2011. There were also 15,438 (1.7 percent of the state population) speakers of Indo-European languages other than English or Spanish, 10,154 (1.1 percent) speakers of a Native American language, and 4,052 (0.4 percent) speakers of an Asian or Pacific Islander language. Other languages spoken in Montana (as of 2013) include Assiniboine (about 150 speakers in the Montana and Canada), Blackfoot (about 100 speakers), Cheyenne (about 1,700 speakers), Plains Cree (about 100 speakers), Crow (about 3,000 speakers), Dakota (about 18,800 speakers in Minnesota, Montana, Nebraska, North Dakota, and South Dakota), German Hutterite (about 5,600 speakers), Gros Ventre (about 10 speakers), Kalispel-Pend d'Oreille (about 64 speakers), Kutenai (about 6 speakers), and Lakota (about 6,000 speakers in Minnesota, Montana, Nebraska, North Dakota, South Dakota). The United States Department of Education estimated in 2009 that 5,274 students in Montana spoke a language at home other than English. These included a Native American language (64 percent), German (4 percent), Spanish (3 percent), Russian (1 percent), and Chinese (less than 0.5 percent).



Montana has a larger Native American population numerically and percentage-wise than most U.S. states. Although the state ranked 45th in population (according to the 2010 U.S. Census), it ranked 19th in total native people population. Native people constituted 6.5 percent of the state's total population, the sixth highest percentage of all 50 states. Montana has three counties in which Native Americans are a majority: Big Horn, Glacier, and Roosevelt. Other counties with large Native American populations include Blaine, Cascade, Hill, Missoula, and Yellowstone counties. The state's Native American population grew by 27.9 percent between 1980 and 1990 (at a time when Montana's entire population rose just 1.6 percent), and by 18.5 percent between 2000 and 2010. As of 2009, almost two-thirds of Native Americans in the state live in urban areas. Of Montana's 20 largest cities, Polson (15.7 percent), Havre (13.0 percent), Great Falls (5.0 percent), Billings (4.4 percent), and Anaconda (3.1 percent) had the greatest percentage of Native American residents in 2010. Billings (4,619), Great Falls (2,942), Missoula (1,838), Havre (1,210), and Polson (706) have the most Native Americans living there. The state's seven reservations include more than twelve distinct Native American ethnolinguistic groups.
While the largest European-American population in Montana overall is German, pockets of significant Scandinavian ancestry are prevalent in some of the farming-dominated northern and eastern prairie regions, parallel to nearby regions of North Dakota and Minnesota. Farmers of Irish, Scots, and English roots also settled in Montana. The historically mining-oriented communities of western Montana such as Butte have a wider range of European-American ethnicity; Finns, Eastern Europeans and especially Irish settlers left an indelible mark on the area, as well as people originally from British mining regions such as Cornwall, Devon and Wales. The nearby city of Helena, also founded as a mining camp, had a similar mix in addition to a small Chinatown. Many of Montana's historic logging communities originally attracted people of Scottish, Scandinavian, Slavic, English and Scots-Irish descent.
The Hutterites, an Anabaptist sect originally from Switzerland, settled here, and today Montana is second only to South Dakota in U.S. Hutterite population with several colonies spread across the state. Beginning in the mid-1990s, the state also saw an influx of Amish, who relocated to Montana from the increasingly urbanized areas of Ohio and Pennsylvania.
Montana's Hispanic population is concentrated around the Billings area in south-central Montana, where many of Montana's Mexican-Americans have been in the state for generations. Great Falls has the highest percentage of African-Americans in its population, although Billings has more African American residents than Great Falls.
The Chinese in Montana, while a low percentage today, have historically been an important presence. About 2000–3000 Chinese miners were in the mining areas of Montana by 1870, and 2500 in 1890. However, public opinion grew increasingly negative toward them in the 1890s and nearly half of the state's Asian population left the state by 1900. Today, there is a significant Hmong population centered in the vicinity of Missoula. Montanans who claim Filipino ancestry amount to almost 3,000, making them currently the largest Asian American group in the state.



According to the Pew Forum, the religious affiliations of the people of Montana are as follows: Protestant 47%, Catholic 23%, LDS (Mormon) 5%, Jehovah's Witness 2%, Buddhist 1%, Jewish 0.5%, Muslim 0.5%, Hindu 0.5% and Non-Religious at 20%.
The largest denominations in Montana as of 2010 were the Catholic Church with 127,612 adherents, The Church of Jesus Christ of Latter-day Saints with 46,484 adherents, Evangelical Lutheran Church in America with 38,665 adherents, and non-denominational Evangelical Protestant with 27,370 adherents. 




Approximately 66,000 people of Native American heritage live in Montana. Stemming from multiple treaties and federal legislation, including the Indian Appropriations Act (1851), the Dawes Act (1887), and the Indian Reorganization Act (1934), seven Indian reservations, encompassing eleven federally recognized tribal nations, were created in Montana. A twelfth nation, the Little Shell Chippewa is a "landless" people headquartered in Great Falls; it is recognized by the state of Montana but not by the U.S. government. The Blackfeet nation is headquartered on the Blackfeet Indian Reservation (1851) in Browning, Crow on the Crow Indian Reservation (1851) in Crow Agency, Confederated Salish and Kootenai and Pend d'Oreille on the Flathead Indian Reservation (1855) in Pablo, Northern Cheyenne on the Northern Cheyenne Indian Reservation (1884) at Lame Deer, Assiniboine and Gros Ventre on the Fort Belknap Indian Reservation (1888) in Fort Belknap Agency, Assiniboine and Sioux on the Fort Peck Indian Reservation (1888) at Poplar, and Chippewa-Cree on the Rocky Boy's Indian Reservation (1916) near Box Elder. Approximately 63% of all Native people live off the reservations, concentrated in the larger Montana cities, with the largest concentration of urban Indians in Great Falls. The state also has a small Métis population, and 1990 census data indicated that people from as many as 275 different tribes lived in Montana.
Montana's Constitution specifically reads that "the state recognizes the distinct and unique cultural heritage of the American Indians and is committed in its educational goals to the preservation of their cultural integrity." It is the only state in the U.S. with such a constitutional mandate. The Indian Education for All Act (IEFA) was passed in 1999 to provide funding for this mandate and ensure implementation. It mandates that all schools teach American Indian history, culture, and heritage from preschool through college. For kindergarten through 12th-grade students, an "Indian Education for All" curriculum from the Montana Office of Public Instruction is available free to all schools. The state was sued in 2004 because of lack of funding, and the state has increased its support of the program. South Dakota passed similar legislation in 2007, and Wisconsin was working to strengthen its own program based on this model - and the current practices of Montana's schools. Each Indian reservation in the state has a fully accredited tribal colleges. The University of Montana "was the first to establish dual admission agreements with all of the tribal colleges and as such it was the first institution in the nation to actively facilitate student transfer from the tribal colleges"




The Bureau of Economic Analysis estimates that Montana's total state product in 2014 was $44.3 billion. Per capita personal income in 2014 was $40,601, 35th in the nation.
Montana is a relative hub of beer microbrewing, ranking third in the nation in number of craft breweries per capita in 2011. There are significant industries for lumber and mineral extraction; the state's resources include gold, coal, silver, talc, and vermiculite. Ecotaxes on resource extraction are numerous. A 1974 state severance tax on coal (which varied from 20 to 30 percent) was upheld by the Supreme Court of the United States in Commonwealth Edison Co. v. Montana, 453 U.S. 609 (1981).
Tourism is also important to the economy with over ten million visitors a year to Glacier National Park, Flathead Lake, the Missouri River headwaters, the site of the Battle of Little Bighorn and three of the five entrances to Yellowstone National Park.
Montana's personal income tax contains 7 brackets, with rates ranging from 1 percent to 6.9 percent. Montana has no sales tax. In Montana, household goods are exempt from property taxes. However, property taxes are assessed on livestock, farm machinery, heavy equipment, automobiles, trucks, and business equipment. The amount of property tax owed is not determined solely by the property's value. The property's value is multiplied by a tax rate, set by the Montana Legislature, to determine its taxable value. The taxable value is then multiplied by the mill levy established by various taxing jurisdictions—city and county government, school districts and others.
As of June 2015, the state's unemployment rate is 3.9 percent.




Many well-known artists, photographers and authors have documented the land, culture and people of Montana in the last 100 years. Painter and sculptor Charles Marion Russell, known as "the cowboy artist" created more than 2,000 paintings of cowboys, Native Americans, and landscapes set in the Western United States and in Alberta, Canada. The C. M. Russell Museum Complex located in Great Falls, Montana houses more than 2,000 Russell artworks, personal objects, and artifacts.
Evelyn Cameron, a naturalist and photographer from Terry documented early 20th century life on the Montana prairie, taking startlingly clear pictures of everything around her: cowboys, sheepherders, weddings, river crossings, freight wagons, people working, badlands, eagles, coyotes and wolves.
Many notable Montana authors have documented or been inspired by life in Montana in both fiction and non-fiction works. Pulitzer Prize winner Wallace Earle Stegner from Great Falls was often called "The Dean of Western Writers". James Willard Schultz ("Apikuni") from Browning is most noted for his prolific stories about Blackfeet life and his contributions to the naming of prominent features in Glacier National Park.




Montana hosts numerous arts and cultural festivals and events every year. Major events include:
Bozeman was once known as the "Sweet Pea capital of the nation" referencing the prolific edible pea crop. To promote the area and celebrate its prosperity, local business owners began a "Sweet Pea Carnival" that included a parade and queen contest. The annual event lasted from 1906 to 1916. Promoters used the inedible but fragrant and colorful sweet pea flower as an emblem of the celebration. In 1977 the "Sweet Pea" concept was revived as an arts festival rather than a harvest celebration, growing into a three-day event that is one of the largest festivals in Montana.
Montana Shakespeare in the Parks has been performing free, live theatrical productions of Shakespeare and other classics throughout Montana since 1973. The Montana Shakespeare Company is based in Helena.
Since 1909, the Crow Fair and Rodeo, near Hardin, has been an annual event every August in Crow Agency and is currently the largest Northern Native American gathering, attracting nearly 45,000 spectators and participants. Since 1952, North American Indian Days has been held every July in Browning.
Lame Deer hosts the annual Northern Cheyenne Powwow.









The Montana Territory was formed on April 26, 1864, when the U.S. passed the Organic Act. Schools started forming in the area before it was officially a territory as families started settling into the area. The first schools were subscription schools that typically held in the teacher's home. The first formal school on record was at Fort Owen in Bitterroot valley in 1862. The students were Indian children and the children of Fort Owen employees. The first school term started in early winter and only lasted until February 28. Classes were taught by Mr. Robinson. Another early subscription school was started by Thomas Dimsdale in Virginia City in 1863. In this school students were charged $1.75 per week. The Montana Territorial Legislative Assembly had its inaugural meeting in 1864. The first legislature authorized counties to levy taxes for schools, which set the foundations for public schooling. Madison County was the first to take advantage of the newly authorized taxes and it formed fhe first public school in Virginia City in 1886. The first school year was scheduled to begin in January 1866, but severe weather postponed its opening until March. The first school year ran through the summer and didn't end until August 17. One of the first teachers at the school was Sarah Raymond. She was a 25-year-old woman who had traveled to Virginia City via wagon train in 1865. To become a certified teacher, Raymond took a test in her home and paid a $6 fee in gold dust to obtain a teaching certificate. With the help of an assistant teacher, Mrs. Farley, Raymond was responsible for teaching 50 to 60 students each day out of the 81 students enrolled at the school. Sarah Raymond was paid at a rate of $125 per month, and Mrs. Farley was paid $75 per month. There were no textbooks used in the school. In their place was an assortment of books brought in by various emigrants. Sarah quit teaching the following year, but would later become the Madison County superintendent of schools.






There are no major league sports franchises in Montana due to the state's relatively small and dispersed population, but a number of minor league teams play in the state. Baseball is the minor-league sport with the longest heritage in the state, and Montana is currently home to four Minor League Baseball teams, all members of the Pioneer Baseball League: Billings Mustangs, Great Falls Voyagers, Helena Brewers, and Missoula Osprey.



All of Montana's four-year colleges and universities field intercollegiate sports teams. The two largest schools, the University of Montana and Montana State University, are members of the Big Sky Conference and have enjoyed a strong athletic rivalry since the early twentieth century. Six of Montana's smaller four-year schools are members of the Frontier Conference. One is a member of the Great Northwest Athletic Conference.



A variety of sports are offered at Montana high schools. Montana allows the smallest—"Class C"—high schools to utilize six-man football teams, dramatized in the independent 2002 film, The Slaughter Rule.
There are junior ice hockey teams in Montana, five of which are affiliated with the North American 3 Hockey League: Billings Bulls, Bozeman Icedogs, Glacier Nationals, Great Falls Americans, and Helena Bighorns. Others are in the Western States Hockey League: Butte Cobras and the Whitefish Wolverines.



Ski jumping champion and United States Skiing Hall of Fame inductee Casper Oimoen was captain of the U.S. Olympic team at the 1936 Winter Olympics while he was a resident of Anaconda. He placed thirteenth that year, and had previously finished fifth at the 1932 Winter Olympics.
Montana has produced two U.S. champions and Olympic competitors in men's figure skating, both from Great Falls: John Misha Petkevich, lived and trained in Montana before entering college, competed in the 1968 and 1972 Winter Olympics. Scott Davis, also from Great Falls, competed at the 1994 Winter Olympics
Missoulian Tommy Moe won Olympic gold and silver medals at the 1994 Winter Olympics in downhill skiing and super G, the first American skier to win two medals at any Winter Olympics.
Eric Bergoust, also of Missoula, won an Olympic gold medal in freestyle aerial skiing at the 1998 Winter Olympics, also competing in 1994, 2002 and 2006 Olympics plus winning 13 World Cup titles.



Montanans have been a part of several major sporting achievements:
In 1889, Spokane became the first and only Montana horse to win the Kentucky Derby. For this accomplishment, the horse was admitted to the Montana Cowboy Hall of Fame in 2008.
In 1904 a basketball team of young Native American women from Fort Shaw, after playing undefeated during their previous season, went to the Louisiana Purchase Exposition held in St. Louis in 1904, defeated all challenging teams and were declared to be world champions.
In 1923, the controversial Jack Dempsey vs. Tommy Gibbons fight for the heavyweight boxing championship, won by Dempsey, took place in Shelby.



Montana provides year-round recreation opportunities for residents and visitors. Hiking, fishing, hunting, watercraft recreation, camping, golf, cycling, horseback riding, and skiing are popular activities.



Montana has been a destination for its world-class trout fisheries since the 1930s. Fly fishing for several species of native and introduced trout in rivers and lakes is popular for both residents and tourists throughout the state. Montana is the home of the Federation of Fly Fishers and hosts many of the organizations annual conclaves. The state has robust recreational lake trout and kokanee salmon fisheries in the west, walleye can be found in many parts of the state, while northern pike, smallmouth and largemouth bass fisheries as well as catfish and paddlefish can be found in the waters of eastern Montana. Robert Redford's 1992 film of Norman Mclean's novel, A River Runs Through It, was filmed in Montana and brought national attention to fly fishing and the state.
Montana is home to the Rocky Mountain Elk Foundation and has a historic big game hunting tradition. There are fall bow and general hunting seasons for elk, pronghorn antelope, whitetail deer and mule deer. A random draw grants a limited number of permits for moose, mountain goats and bighorn sheep. There is a spring hunting season for black bear and in most years, limited hunting of bison that leave Yellowstone National Park is allowed. Current law allows both hunting and trapping of a specific number of wolves and mountain lions. Trapping of assorted fur bearing animals is allowed in certain seasons and many opportunities exist for migratory waterfowl and upland bird hunting.




Both downhill skiing and cross-country skiing are popular in Montana, which has 15 developed downhill ski areas open to the public, including;
Bear Paw Ski Bowl near Havre, Montana
Big Sky Resort, at Big Sky
Blacktail Mountain near Lakeside
Bridger Bowl Ski Area near Bozeman
Discovery Basin between Philipsburg and Anaconda
Great Divide near Helena, Montana
Lookout Pass off Interstate 90 at the Montana-Idaho border
Lost Trail near Darby, Montana
Maverick Mountain near Dillon, Montana
Moonlight Basin near Big Sky
Red Lodge Mountain Resort near Red Lodge
Showdown Ski Area near White Sulphur Springs, Montana
Snowbowl Ski Area near Missoula
Teton Pass Ski Area near Choteau
Turner Mountain Ski Resort near Libby
Whitefish Mountain Resort near Whitefish
Big Sky, Moonlight Basin, Red Lodge, and Whitefish Mountain are destination resorts, while the remaining areas do not have overnight lodging at the ski area, though several host restaurants and other amenities. These day-use resorts partner with local lodging businesses to offer ski and lodging packages.
Montana also has millions of acres open to cross-country skiing on nine of its national forests plus in Glacier National Park. In addition to cross-country trails at most of the downhill ski areas, there are also 13 private cross-country skiing resorts. Yellowstone National Park also allows cross-country skiing.
Snowmobiling is popular in Montana which boasts over 4000 miles of trails and frozen lakes available in winter. There are 24 areas where snowmobile trails are maintained, most also offering ungroomed trails. West Yellowstone offers a large selection of trails and is the primary starting point for snowmobile trips into Yellowstone National Park, where "oversnow" vehicle use is strictly limited, usually to guided tours, and regulations are in considerable flux.
Snow coach tours are offered at Big Sky, Whitefish, West Yellowstone and into Yellowstone National Park. Equestrian skijoring has a niche in Montana, which hosts the World Skijoring Championships in Whitefish as part of the annual Whitefish Winter Carnival.



Montana does not have a Trauma I hospital, but does have Trauma II hospitals in Missoula, Billings, and Great Falls. In 2013 AARP The Magazine named the Billings Clinic one of the safest hospitals in the United States. Montana is ranked as the least obese state in the U.S., at 19.6%, according to the 2014 Gallup Poll.




As of 2010, Missoula is the 166th largest media market in the United States as ranked by Nielsen Media Research, while Billings is 170th, Great Falls is 190th, the Butte-Bozeman area 191st, and Helena is 206th. There are 25 television stations in Montana, representing each major U.S. network. As of August 2013, there are 527 FCC-licensed FM radio stations broadcast in Montana, with 114 such AM stations.
During the age of the Copper Kings, each Montana copper company had its own newspaper. This changed in 1959 when Lee Enterprises bought several Montana newspapers. Montana's largest circulating daily city newspapers are the Billings Gazette (circulation 39,405), Great Falls Tribune (26,733), and Missoulian (25,439).




Railroads have been an important method of transportation in Montana since the 1880s. Historically, the state was traversed by the main lines of three east-west transcontinental routes: the Milwaukee Road, the Great Northern, and the Northern Pacific. Today, the BNSF Railway is the state's largest railroad, its main transcontinental route incorporating the former Great Northern main line across the state. Montana RailLink, a privately held Class II railroad, operates former Northern Pacific trackage in western Montana.
In addition, Amtrak's Empire Builder train runs through the north of the state, stopping in Libby, Whitefish, West Glacier, Essex, East Glacier Park, Browning, Cut Bank, Shelby, Havre, Malta, Glasgow, and Wolf Point.
Bozeman Yellowstone International Airport is the busiest airport in the state of Montana, surpassing Billings Logan International Airport in the spring of 2013. Montana's other major Airports include Billings Logan International Airport, Missoula International Airport, Great Falls International Airport, Glacier Park International Airport, Helena Regional Airport, Bert Mooney Airport and Yellowstone Airport. Eight smaller communities have airports designated for commercial service under the Essential Air Service program.
Historically, U.S. Route 10 was the primary east-west highway route across Montana, connecting the major cities in the southern half of the state. Still the state's most important east-west travel corridor, the route is today served by Interstate 90 and Interstate 94 which roughly follow the same route as the Northern Pacific. U.S. Routes 2 and 12 and Montana Highway 200 also traverse the entire state from east to west.
Montana's only north-south Interstate Highway is Interstate 15. Other major north-south highways include U.S. Routes 87, 89, 93 and 191. Interstate 25 terminates into I-90 just south of the Montana border in Wyoming.
Montana and South Dakota are the only states to share a land border which is not traversed by a paved road. Highway 212, the primary paved route between the two, passes through the northeast corner of Wyoming between Montana and South Dakota.




The current Governor is Steve Bullock, a Democrat elected in 2012 and sworn in on January 7, 2013. His predecessor in office was two-term governor, Brian Schweitzer. Montana's two U.S. senators are Jon Tester (Democrat) and Steve Daines (Republican). The state's congressional representative is currently Republican Ryan Zinke.
In 1914 Montana granted women the vote and in 1916 became the first state to elect a woman, Progressive Republican Jeannette Rankin, to Congress.
Montana is an Alcoholic beverage control state. It is an equitable distribution and no-fault divorce state. It is one of five states to have no sales tax.




Politics in the state has been competitive, with the Democrats usually holding an edge, thanks to the support among unionized miners and railroad workers. Large-scale battles revolved around the giant Anaconda Copper company, based in Butte and controlled by Rockefeller interests, until it closed in the 1970s. Until 1959, the company owned five of the state's six largest newspapers.
Historically, Montana is a swing state of cross-ticket voters who tend to fill elected offices with individuals from both parties. Through the mid-20th century, the state had a tradition of "sending the liberals to Washington and the conservatives to Helena." Between 1988 and 2006, the pattern flipped, with voters more likely to elect conservatives to federal offices. There have also been long-term shifts of party control. From 1968 through 1988, the state was dominated by the Democratic Party, with Democratic governors for a 20-year period, and a Democratic majority of both the national congressional delegation and during many sessions of the state legislature. This pattern shifted, beginning with the 1988 election, when Montana elected a Republican governor for the first time since 1964 and sent a Republican to the U.S. Senate for the first time since 1948. This shift continued with the reapportionment of the state's legislative districts that took effect in 1994, when the Republican Party took control of both chambers of the state legislature, consolidating a Republican party dominance that lasted until the 2004 reapportionment produced more swing districts and a brief period of Democratic legislative majorities in the mid-2000s.
In more recent presidential elections, Montana has voted for the Republican candidate in all but two elections from 1952 to the present. The state last supported a Democrat for president in 1992, when Bill Clinton won a plurality victory. Overall, since 1889 the state has voted for Democratic governors 60 percent of the time and Republican presidents 40 percent of the time. In the 2008 presidential election, Montana was considered a swing state and was ultimately won by Republican John McCain, albeit by a narrow margin of two percent.
At the state level, the pattern of split ticket voting and divided government holds. Democrats currently hold one of the state's U.S. Senate seats, as well as four of the five statewide offices (Governor, Superintendent of Public Instruction, Secretary of State and State Auditor). The lone congressional district has been Republican since 1996 and in 2014 Steve Daines won one of the state's Senate seats for the GOP. The Legislative branch had split party control between the house and senate most years between 2004 and 2010, when the mid-term elections returned both branches to Republican control. The state Senate is, as of 2015, controlled by the Republicans 29 to 21, and the State House of Representatives at 59 to 41. Historically, Republicans are strongest in the east, while Democrats are strongest in the west.
Montana currently has only one representative in the U.S. House, having lost its second district in the 1990 census reapportionment. Montana's single congressional district holds the largest population of any district in the country, which means its one member in the House of Representatives represents more people than any other member of the U.S. House (see List of U.S. states by population). Montana's population grew at about the national average during the 2000s, and it failed to regain its second seat in 2010. Like other states, Montana has two senators.



An October 2013 Montana State University Billings survey found that 46.6 percent of Montana voters supported the legalization of same-sex marriage, while 42.6 percent opposed it and 10.8 percent were not sure.




Montana has 56 counties with the United States Census Bureau stating Montana's contains 364 "places", broken down into 129 incorporated places and 235 census-designated places. Incorporated places consist of 52 cities, 75 towns, and two consolidated city-counties. Montana has one city, Billings, with a population over 100,000; and two cities with populations over 50,000, Missoula and Great Falls. These three communities are considered the centers of Montana's three Metropolitan Statistical Areas.
The state also has five Micropolitan Statistical Areas centered on Bozeman, Butte, Helena, Kalispell and Havre. These communities, excluding Havre, are colloquially known as the "big 7" Montana cities, as they are consistently the seven largest communities in Montana, with a significant population difference when these communities are compared to those that are 8th and lower on the list. According to the 2010 U.S. Census, the population of Montana's seven most populous cities, in rank order, are Billings, Missoula, Great Falls, Bozeman, Butte, Helena and Kalispell. Based on 2013 census numbers, they collectively contain 35 percent of Montana's population. and the counties containing these communities hold 62 percent of the state's population. The geographic center of population of Montana is located in sparsely populated Meagher County, in the town of White Sulphur Springs.




Montana's motto, Oro y Plata, Spanish for "Gold and Silver", recognizing the significant role of mining, was first adopted in 1865, when Montana was still a territory. A state seal with a miner's pick and shovel above the motto, surrounded by the mountains and the Great Falls of the Missouri River, was adopted during the first meeting of the territorial legislature in 1864–65. The design was only slightly modified after Montana became a state and adopted it as the Great Seal of the State of Montana, enacted by the legislature in 1893. The state flower, the bitterroot, was adopted in 1895 with the support of a group called the Floral Emblem Association, which formed after Montana's Women's Christian Temperance Union adopted the bitterroot as the organization's state flower. All other symbols were adopted throughout the 20th century, save for Montana's newest symbol, the state butterfly, the mourning cloak, adopted in 2001, and the state lullaby, "Montana Lullaby", adopted in 2007.
The state song was not composed until 21 years after statehood, when a musical troupe led by Joseph E. Howard stopped in Butte in September 1910. A former member of the troupe who lived in Butte buttonholed Howard at an after-show party, asking him to compose a song about Montana and got another partygoer, the city editor for the Butte Miner newspaper, Charles C. Cohan, to help. The two men worked up a basic melody and lyrics in about a half-hour for the entertainment of party guests, then finished the song later that evening, with an arrangement worked up the following day. Upon arriving in Helena, Howard's troupe performed 12 encores of the new song to an enthusiastic audience and the governor proclaimed it the state song on the spot, though formal legislative recognition did not occur until 1945. Montana is one of only three states to have a "state ballad", "Montana Melody", chosen by the legislature in 1983. Montana was the first state to also adopt a State Lullaby.
Montana schoolchildren played a significant role in selecting several state symbols. The state tree, the ponderosa pine, was selected by Montana schoolchildren as the preferred state tree by an overwhelming majority in a referendum held in 1908. However, the legislature did not designate a state tree until 1949, when the Montana Federation of Garden Clubs, with the support of the state forester, lobbied for formal recognition. Schoolchildren also chose the western meadowlark as the state bird, in a 1930 vote, and the legislature acted to endorse this decision in 1931. Similarly, the secretary of state sponsored a children's vote in 1981 to choose a state animal, and after 74 animals were nominated, the grizzly bear won over the elk by a 2–1 margin. The students of Livingston started a statewide school petition drive plus lobbied the governor and the state legislature to name the Maiasaura as the state fossil in 1985.
Various community civic groups also played a role in selecting the state grass and the state gemstones. When broadcaster Norma Ashby discovered there was no state fish, she initiated a drive via her television show, Today in Montana, and an informal citizen's election to select a state fish resulted in a win for the blackspotted cutthroat trout after hot competition from the Arctic grayling. The legislature in turn adopted this recommendation by a wide margin.




Outline of Montana
Index of Montana-related articles













Census of Montana
General Information About Montana
 Geographic data related to Montana at OpenStreetMap
List of Searchable Databases Produced by Montana State Agencies
Montana Energy Data & Statistics – From the U.S. Department of Energy
Montana Historical Society
Montana Official Travel Information Site
Montana Official Website
Montana at DMOZ
Montana State Facts From the U.S. Department of Agriculture
USGS Real-time, Geographic, and Other Scientific Resources of MontanaGermany (/ˈdʒɜːrməni/; German: Deutschland, pronounced [ˈdɔʏtʃlant]), officially the Federal Republic of Germany (German: Bundesrepublik Deutschland,  listen ), is a federal parliamentary republic in central-western Europe. It includes 16 constituent states, covers an area of 357,021 square kilometres (137,847 sq mi), and has a largely temperate seasonal climate. With about 82 million inhabitants, Germany is the most populous member state of the European Union. After the United States, it is the second most popular immigration destination in the world. Germany's capital and largest metropolis is Berlin. Other major cities include Hamburg, Munich, Cologne, Frankfurt, Stuttgart and Düsseldorf.
Various Germanic tribes have inhabited the northern parts of modern Germany since classical antiquity. A region named Germania was documented before 100 AD. During the Migration Period the Germanic tribes expanded southward. Beginning in the 10th century, German territories formed a central part of the Holy Roman Empire. During the 16th century, northern German regions became the centre of the Protestant Reformation.
In 1871, Germany became a nation state when most of the German states unified into the Prussian-dominated German Empire. After World War I and the German Revolution of 1918–1919, the Empire was replaced by the parliamentary Weimar Republic. The establishment of the national socialist dictatorship in 1933 led to World War II and a genocide. After a period of Allied occupation, two German states were founded: the Federal Republic of Germany and the German Democratic Republic. In 1990, the country was reunified.
In the 21st century, Germany is a great power and has the world's fourth-largest economy by nominal GDP, as well as the fifth-largest by PPP. As a global leader in several industrial and technological sectors, it is both the world's third-largest exporter and importer of goods. Germany is a developed country with a very high standard of living sustained by a skilled and productive society. It upholds a social security and universal health care system, environmental protection and a tuition-free university education.
Germany was a founding member of the European Union in 1993. It is part of the Schengen Area, and became a co-founder of the Eurozone in 1999. Germany is a member of the United Nations, NATO, the G8, the G20, and the OECD. The national military expenditure is the 9th highest in the world. Known for its rich cultural history, Germany has been continuously the home of influential artists, philosophers, musicians, sportspeople, entrepreneurs, scientists, engineers, and inventors.




The English word Germany derives from the Latin Germania, which came into use after Julius Caesar adopted it for the peoples east of the Rhine. The German term Deutschland, originally diutisciu land ("the German lands") is derived from deutsch (cf. dutch), descended from Old High German diutisc "popular" (i.e. belonging to the diot or diota "people"), originally used to distinguish the language of the common people from Latin and its Romance descendants. This in turn descends from Proto-Germanic *þiudiskaz "popular" (see also the Latinised form Theodiscus), derived from *þeudō, descended from Proto-Indo-European *tewtéh₂- "people", from which the word "Teutons" also originates.




The discovery of the Mauer 1 mandible shows that ancient humans were present in Germany at least 600,000 years ago. The oldest complete hunting weapons found anywhere in the world were discovered in a coal mine in Schöningen where three 380,000-year-old wooden javelins were unearthed. The Neander Valley was the location where the first ever non-modern human fossil was discovered; the new species of human was called the Neanderthal. The Neanderthal 1 fossils are known to be 40,000 years old. Evidence of modern humans, similarly dated, has been found in caves in the Swabian Jura near Ulm. The finds include 42,000-year-old bird bone and mammoth ivory flutes which are the oldest musical instruments ever found, the 40,000-year-old Ice Age Lion Man which is the oldest uncontested figurative art ever discovered, and the 35,000-year-old Venus of Hohle Fels which is the oldest uncontested human figurative art ever discovered. The Nebra sky disk is a bronze artefact created during the European Bronze Age attributed to a site near Nebra, Saxony-Anhalt. It is part of UNESCO's Memory of the World Programme.




The Germanic tribes are thought to date from the Nordic Bronze Age or the Pre-Roman Iron Age. From southern Scandinavia and north Germany, they expanded south, east and west from the 1st century BC, coming into contact with the Celtic tribes of Gaul as well as Iranian, Baltic, and Slavic tribes in Central and Eastern Europe. Under Augustus, Rome began to invade Germania (an area extending roughly from the Rhine to the Ural Mountains). In 9 AD, three Roman legions led by Varus were defeated by the Cheruscan leader Arminius. By 100 AD, when Tacitus wrote Germania, Germanic tribes had settled along the Rhine and the Danube (the Limes Germanicus), occupying most of the area of modern Germany; Austria, Baden Württemberg, southern Bavaria, southern Hessen and the western Rhineland, however, were Roman provinces.
In the 3rd century a number of large West Germanic tribes emerged: Alemanni, Franks, Chatti, Saxons, Frisii, Sicambri, and Thuringii. Around 260, the Germanic peoples broke into Roman-controlled lands. After the invasion of the Huns in 375, and with the decline of Rome from 395, Germanic tribes moved further south-west. Simultaneously several large tribes formed in what is now Germany and displaced or absorbed smaller Germanic tribes. Large areas known since the Merovingian period as Austrasia, Neustria, and Aquitaine were conquered by the Franks who established the Frankish Kingdom, and pushed further east to subjugate Saxony and Bavaria. Areas of what is today the eastern part of Germany were inhabited by Western Slavic tribes of Sorbs, Veleti and the Obotritic confederation.




In 800, the Frankish king Charlemagne was crowned emperor and founded the Carolingian Empire, which was later divided in 843 among his heirs. Following the break up of the Frankish Realm, for 900 years, the history of Germany was intertwined with the history of the Holy Roman Empire, which subsequently emerged from the eastern portion of Charlemagne's original empire. The territory initially known as East Francia stretched from the Rhine in the west to the Elbe River in the east and from the North Sea to the Alps.
The Ottonian rulers (919–1024) consolidated several major duchies and the German king Otto I was crowned Holy Roman Emperor of these regions in 962. In 996 Gregory V became the first German Pope, appointed by his cousin Otto III, whom he shortly after crowned Holy Roman Emperor. The Holy Roman Empire absorbed northern Italy and Burgundy under the reign of the Salian emperors (1024–1125), although the emperors lost power through the Investiture Controversy.

In the 12th century, under the Hohenstaufen emperors (1138–1254), German princes increased their influence further south and east into territories inhabited by Slavs; they encouraged German settlement in these areas, called the eastern settlement movement (Ostsiedlung). Members of the Hanseatic League, which included mostly north German cities and towns, prospered in the expansion of trade. In the south, the Greater Ravensburg Trade Corporation (Große Ravensburger Handelsgesellschaft) served a similar function. The edict of the Golden Bull issued in 1356 by Emperor Charles IV provided the basic constitutional structure of the Empire and codified the election of the emperor by seven prince-electors who ruled some of the most powerful principalities and archbishoprics.
Population declined in the first half of the 14th century, starting with the Great Famine in 1315, followed by the Black Death of 1348–50. Despite the decline, however, German artists, engineers, and scientists developed a wide array of techniques similar to those used by the Italian artists and designers of the time who flourished in such merchant city-states as Venice, Florence and Genoa. Artistic and cultural centres throughout the German states produced such artists as the Augsburg painters Hans Holbein and his son, and Albrecht Dürer. Johannes Gutenberg introduced moveable-type printing to Europe, a development that laid the basis for the spread of learning to the masses.

In 1517, the Wittenberg monk Martin Luther publicised The Ninety-Five Theses, challenging the Roman Catholic Church and initiating the Protestant Reformation. In 1555, the Peace of Augsburg established Lutheranism as an acceptable alternative to Catholicism, but also decreed that the faith of the prince was to be the faith of his subjects, a principle called Cuius regio, eius religio. The agreement at Augsburg failed to address other religious creed: for example, the Reformed faith was still considered a heresy and the principle did not address the possible conversion of an ecclesiastic ruler, such as happened in Electorate of Cologne in 1583. From the Cologne War until the end of the Thirty Years' Wars (1618–1648), religious conflict devastated German lands. The latter reduced the overall population of the German states by about 30 per cent, and in some places, up to 80 per cent. The Peace of Westphalia ended religious warfare among the German states. German rulers were able to choose either Roman Catholicism, Lutheranism or the Reformed faith as their official religion after 1648.
In the 18th century, the Holy Roman Empire consisted of approximately 1,800 territories. The elaborate legal system initiated by a series of Imperial Reforms (approximately 1450–1555) created the Imperial Estates and provided for considerable local autonomy among ecclesiastical, secular, and hereditary states, reflected in Imperial Diet. The House of Habsburg held the imperial crown from 1438 until the death of Charles VI in 1740. Having no male heirs, he had convinced the Electors to retain Habsburg hegemony in the office of the emperor by agreeing to the Pragmatic Sanction. This was finally settled through the War of Austrian Succession; in the Treaty of Aix-la-Chapelle, Maria Theresa's husband became Holy Roman Emperor, and she ruled the Empire as Empress Consort. From 1740, the dualism between the Austrian Habsburg Monarchy and the Kingdom of Prussia dominated the German history.
In 1772, then again in 1793 and 1795, the two dominant German states of Prussia and Austria, along with the Russian Empire, agreed to the Partitions of Poland; dividing among themselves the lands of the Polish-Lithuanian Commonwealth. As a result of the partitions, millions of Polish speaking inhabitants fell under the rule of the two German monarchies. However, the annexed territories though incorporated into the Kingdom of Prussia and the Habsburg Realm, were not legally considered as a part of the Holy Roman Empire.
During the period of the French Revolutionary Wars, along with the arrival of the Napoleonic era and the subsequent final meeting of the Imperial Diet, most of the secular Free Imperial Cities were annexed by dynastic territories; the ecclesiastical territories were secularised and annexed. In 1806 the Imperium was dissolved; German states, particularly the Rhineland states, fell under the influence of France. Until 1815, France, Russia, Prussia and the Habsburgs competed for hegemony in the German states during the Napoleonic Wars.




Following the fall of Napoleon, the Congress of Vienna (convened in 1814) founded the German Confederation (Deutscher Bund), a loose league of 39 sovereign states. The appointment of the Emperor of Austria as the permanent president of the Confederation reflected the Congress's failure to accept Prussia's influence among the German states, and acerbated the long-standing competition between the Hohenzollern and Habsburg interests. Disagreement within restoration politics partly led to the rise of liberal movements, followed by new measures of repression by Austrian statesman Metternich. The Zollverein, a tariff union, furthered economic unity in the German states. National and liberal ideals of the French Revolution gained increasing support among many, especially young, Germans. The Hambach Festival in May 1832 was a main event in support of German unity, freedom and democracy. In the light of a series of revolutionary movements in Europe, which established a republic in France, intellectuals and commoners started the Revolutions of 1848 in the German states. King Frederick William IV of Prussia was offered the title of Emperor, but with a loss of power; he rejected the crown and the proposed constitution, leading to a temporary setback for the movement.

King William I appointed Otto von Bismarck as the new Minister President of Prussia in 1862. Bismarck successfully concluded war on Denmark in 1864, which promoted German over Danish interests in the Jutland peninsula. The subsequent (and decisive) Prussian victory in the Austro-Prussian War of 1866 enabled him to create the North German Confederation (Norddeutscher Bund) which excluded Austria from the federation's affairs. After the French defeat in the Franco-Prussian War, the German princes proclaimed the founding of the German Empire in 1871 at Versailles, uniting all scattered parts of Germany except Austria. Prussia was the dominant constituent state of the new empire; the Hohenzollern King of Prussia ruled as its concurrent Emperor, and Berlin became its capital.

In the Gründerzeit period following the unification of Germany, Bismarck's foreign policy as Chancellor of Germany under Emperor William I secured Germany's position as a great nation by forging alliances, isolating France by diplomatic means, and avoiding war. Under Wilhelm II, Germany, like other European powers, took an imperialistic course, leading to friction with neighbouring countries. Most alliances in which Germany had previously been involved were not renewed. This resulted in creation of a dual alliance with the multinational realm of Austria-Hungary, promoting at least benevolent neutrality if not outright military support. Subsequently, the Triple Alliance of 1882 included Italy, completing a Central European geographic alliance that illustrated German, Austrian and Italian fears of incursions against them by France and/or Russia. Similarly, Britain, France and Russia also concluded alliances that would protect them against Habsburg interference with Russian interests in the Balkans or German interference against France.
At the Berlin Conference in 1884, Germany claimed several colonies including German East Africa, German South-West Africa, Togoland, and Kamerun. Later, Germany further expanded its colonial empire to include German New Guinea, German Micronesia and German Samoa in the Pacific, and Kiautschou Bay in China. In what became known as the "First Genocide of the Twentieth-Century", between 1904 and 1907, the German colonial government in South-West Africa (present-day Namibia) ordered the annihilation of the local Herero and Namaqua peoples, as a punitive measure for an uprising against German colonial rule. In total, around 100,000 people—80% of the Herero and 50% of the Namaqua—perished form imprisonment in concentration camps, where the majority died of disease, abuse, and exhaustion, or from dehydration and starvation in the countryside after being deprived of food and water.
The assassination of Austria's crown prince on 28 June 1914 provided the pretext for the Austrian Empire to attack Serbia and trigger World War I. After four years of warfare, in which approximately two million German soldiers were killed, a general armistice ended the fighting on 11 November, and German troops returned home. In the German Revolution (November 1918), Emperor Wilhelm II and all German ruling princes abdicated their positions and responsibilities. Germany's new political leadership signed the Treaty of Versailles in 1919. In this treaty, Germany, as part of the Central Powers, accepted defeat by the Allies in one of the bloodiest conflicts of all time. Germans perceived the treaty as humiliating and unjust and it was later seen by historians as influential in the rise of Adolf Hitler. After the defeat in the First World War, Germany lost around thirteen per cent of its European territory (areas predominantly inhabited by ethnic Polish, French and Danish populations, which were lost following the Greater Poland Uprising, the return of Alsace-Lorraine and the Schleswig plebiscites), and all of its colonial possessions in Africa and the South Sea.




Germany was declared a republic at the beginning of the German Revolution in November 1918. On 11 August 1919 President Friedrich Ebert signed the democratic Weimar Constitution. In the subsequent struggle for power, the radical-left Communists seized power in Bavaria, but conservative elements in other parts of Germany attempted to overthrow the Republic in the Kapp Putsch. It was supported by parts of the Reichswehr (military) and other conservative, nationalistic and monarchist factions. After a tumultuous period of bloody street fighting in the major industrial centres, the occupation of the Ruhr by Belgian and French troops and the rise of inflation culminating in the hyperinflation of 1922–23, a debt restructuring plan and the creation of a new currency in 1924 ushered in the Golden Twenties, an era of increasing artistic innovation and liberal cultural life. Underneath it all, though, lay a current of animosity and frustration over the Treaty of Versailles, widely perceived as a stab in the back, which provided the basis of much of the anti-Semitism rife in the next two decades. The economic situation remained volatile. Historians describe the period between 1924 and 1929 as one of "partial stabilisation." The worldwide Great Depression hit Germany in 1929. After the federal election of 1930, Chancellor Heinrich Brüning's government was enabled by President Paul von Hindenburg to act without parliamentary approval. Brüning's government pursued a policy of fiscal austerity and deflation which caused high unemployment of nearly 30% by 1932.
The Nazi Party led by Adolf Hitler won the special federal election of 1932. After a series of unsuccessful cabinets, Hindenburg appointed Hitler as Chancellor of Germany in 1933. After the Reichstag fire, a decree abrogated basic civil rights and within weeks the first Nazi concentration camp at Dachau opened. The Enabling Act of 1933 gave Hitler unrestricted legislative power; subsequently, his government established a centralised totalitarian state, withdrew from the League of Nations following a national referendum, and began military rearmament.

Using deficit spending, a government-sponsored programme for economic renewal focused on public works projects. In public work projects of 1934, 1.7 million Germans immediately were put to work, which gave them an income and social benefits. The most famous of the projects was the high speed roadway, the Reichsautobahn, known as the German autobahns. Other capital construction projects included hydroelectric facilities such as the Rur Dam, water supplies such as Zillierbach Dam, and transportation hubs such as Zwickau Hauptbahnhof. Over the next five years, unemployment plummeted and average wages both per hour and per week rose.
In 1935, the regime withdrew from the Treaty of Versailles and introduced the Nuremberg Laws which targeted Jews and other minorities. Germany also reacquired control of the Saar in 1935, annexed Austria in 1938, and despite the Munich Agreement, occupied Czechoslovakia in early 1939.
In August 1939, Hitler's government negotiated and signed the Molotov–Ribbentrop pact that divided Eastern Europe into German and Soviet spheres of influence. Following the agreement, on 1 September 1939 Germany invaded Poland, marking the beginning of World War II. In response to Hitler's actions, Britain and France declared war on Germany. In the spring of 1940, Germany conquered Denmark and Norway, the Netherlands, Belgium, Luxembourg, and France forcing the French government to sign an armistice after German troops occupied most of the country. The British repelled German air attacks in the Battle of Britain in the same year. In 1941, German troops invaded Yugoslavia, Greece and the Soviet Union. By 1942, Germany and other Axis powers controlled most of continental Europe and North Africa, but following the Soviet Union's victory at the Battle of Stalingrad, the allies' reconquest of North Africa and invasion of Italy in 1943, German forces suffered repeated military defeats. In June 1944, the Western allies landed in France and the Soviets pushed into Eastern Europe. By late 1944, the Western allies had entered Germany despite one final German counter offensive in the Ardennes Forest. Following Hitler's suicide during the Battle of Berlin, German armed forces surrendered on 8 May 1945, ending World War II in Europe.

In what later became known as The Holocaust, the German government persecuted minorities and used a network of concentration and death camps across Europe to conduct a genocide of what they considered to be inferior races. In total, over 10 million civilians were systematically murdered, including 6 million Jews, between 220,000 and 1,500,000 Romani, 275,000 persons with disabilities, thousands of Jehovah's Witnesses, thousands of homosexuals, and hundreds of thousands of members of the political and religious opposition from Germany and occupied countries (Nacht und Nebel). Nazi policies in the German occupied countries resulted in the deaths of 2.7 million Poles, 1.3 million Ukrainians, and an estimated 2.8 million Soviet war prisoners. German military war casualties were estimated at between 3.2 million and 5.3 million soldiers, and up to 2 million German civilians. German territorial losses resulted in the expulsion of circa 12 million of ethnic Germans from Eastern Europe. Germany ceded roughly one-quarter of its pre-war territory. Strategic bombing and land warfare destroyed many cities and cultural heritage sites. After World War II, former members of the Nazi regime were tried for war crimes at the Nuremberg trials.




After Germany surrendered, the Allies partitioned Berlin and Germany's remaining territory into four military occupation zones. The western sectors, controlled by France, the United Kingdom, and the United States, were merged on 23 May 1949 to form the Federal Republic of Germany (Bundesrepublik Deutschland); on 7 October 1949, the Soviet Zone became the German Democratic Republic (Deutsche Demokratische Republik). They were informally known as "West Germany" and "East Germany". East Germany selected East Berlin as its capital, while West Germany chose Bonn as a provisional capital, to emphasise its stance that the two-state solution was an artificial and temporary status quo.
West Germany was established as a federal parliamentary republic with a "social market economy". Starting in 1948 West Germany became a major recipient of reconstruction aid under the Marshall Plan and used this to rebuild its industry. Konrad Adenauer was elected the first Federal Chancellor (Bundeskanzler) of Germany in 1949 and remained in office until 1963. Under his and Ludwig Erhard's leadership, the country enjoyed prolonged economic growth beginning in the early 1950s, that became known as an "economic miracle" (Wirtschaftswunder). West Germany joined NATO in 1955 and was a founding member of the European Economic Community in 1957.

East Germany was an Eastern Bloc state under political and military control by the USSR via occupation forces and the Warsaw Pact. Although East Germany claimed to be a democracy, political power was exercised solely by leading members (Politbüro) of the communist-controlled Socialist Unity Party of Germany, supported by the Stasi, an immense secret service controlling many aspects of the society. A Soviet-style command economy was set up and the GDR later became a Comecon state. While East German propaganda was based on the benefits of the GDR's social programmes and the alleged constant threat of a West German invasion, many of its citizens looked to the West for freedom and prosperity. The Berlin Wall, built in 1961 to stop East Germans from escaping to West Germany, became a symbol of the Cold War. It was the site of Ronald Reagan's "Mr. Gorbachov, Tear down this wall!" speech of 12 June 1987, which echoed John F. Kennedy's famous Ich bin ein Berliner speech of 26 June 1963. The fall of the Wall in 1989 became a symbol of the Fall of Communism, German Reunification and Die Wende.
Tensions between East and West Germany were reduced in the early 1970s by Chancellor Willy Brandt's Ostpolitik. In summer 1989, Hungary decided to dismantle the Iron Curtain and open the borders, causing the emigration of thousands of East Germans to West Germany via Hungary. This had devastating effects on the GDR, where regular mass demonstrations received increasing support. The East German authorities eased the border restrictions, allowing East German citizens to travel to the West; originally intended to help retain East Germany as a state, the opening of the border actually led to an acceleration of the Wende reform process. This culminated in the Two Plus Four Treaty a year later on 12 September 1990, under which the four occupying powers renounced their rights under the Instrument of Surrender, and Germany regained full sovereignty. This permitted German reunification on 3 October 1990, with the accession of the five re-established states of the former GDR.




The united Germany is considered to be the enlarged continuation of the Federal Republic of Germany and not a successor state. As such, it retained all of West Germany's memberships in international organisations. Based on the Berlin/Bonn Act, adopted in 1994, Berlin once again became the capital of the reunified Germany, while Bonn obtained the unique status of a Bundesstadt (federal city) retaining some federal ministries. The relocation of the government was completed in 1999. Following the 1998 elections, SPD politician Gerhard Schröder became the first Chancellor of a red–green coalition with the Alliance '90/The Greens party.
The modernisation and integration of the eastern German economy is a long-term process scheduled to last until the year 2019, with annual transfers from west to east amounting to roughly $80 billion.

Since reunification, Germany has taken a more active role in the European Union. Together with its European partners Germany signed the Maastricht Treaty in 1992, established the Eurozone in 1999, and signed the Lisbon Treaty in 2007. Germany sent a peacekeeping force to secure stability in the Balkans and sent a force of German troops to Afghanistan as part of a NATO effort to provide security in that country after the ousting of the Taliban. These deployments were controversial since Germany was bound by domestic law only to deploy troops for defence roles.
In the 2005 elections, Angela Merkel became the first female Chancellor of Germany as the leader of a grand coalition. In 2009 the German government approved a €50 billion economic stimulus plan to protect several sectors from a downturn.
In 2009, a liberal-conservative coalition under Merkel assumed leadership of the country. In 2013, a grand coalition was established in a Third Merkel cabinet. Among the major German political projects of the early 21st century are the advancement of European integration, the energy transition (Energiewende) for a sustainable energy supply, the "Debt Brake" for balanced budgets, measures to increase the fertility rate significantly (pronatalism), and high-tech strategies for the future transition of the German economy, summarised as Industry 4.0.
Germany was affected by the European migrant crisis in 2015 as it became the final destination of choice for most migrants entering the EU. The country took in over a million refugees and developed a quota system which redistributed migrants around its federal states based on their tax income and existing population density.




Germany is in Western and Central Europe, with Denmark bordering to the north, Poland and the Czech Republic to the east, Austria to the southeast, Switzerland to the south-southwest, France, Luxembourg and Belgium lie to the west, and the Netherlands to the northwest. It lies mostly between latitudes 47° and 55° N and longitudes 5° and 16° E. Germany is also bordered by the North Sea and, at the north-northeast, by the Baltic Sea. With Switzerland and Austria, Germany also shares a border on the fresh-water Lake Constance, the third largest lake in Central Europe. German territory covers 357,021 km2 (137,847 sq mi), consisting of 349,223 km2 (134,836 sq mi) of land and 7,798 km2 (3,011 sq mi) of water. It is the seventh largest country by area in Europe and the 62nd largest in the world.
Elevation ranges from the mountains of the Alps (highest point: the Zugspitze at 2,962 metres or 9,718 feet) in the south to the shores of the North Sea (Nordsee) in the northwest and the Baltic Sea (Ostsee) in the northeast. The forested uplands of central Germany and the lowlands of northern Germany (lowest point: Wilstermarsch at 3.54 metres or 11.6 feet below sea level) are traversed by such major rivers as the Rhine, Danube and Elbe. Germany's alpine glaciers are experiencing deglaciation. Significant natural resources include iron ore, coal, potash, timber, lignite, uranium, copper, natural gas, salt, nickel, arable land and water.



Most of Germany has a temperate seasonal climate dominated by humid westerly winds. The country is situated in between the oceanic Western European and the continental Eastern European climate. The climate is moderated by the North Atlantic Drift, the northern extension of the Gulf Stream. This warmer water affects the areas bordering the North Sea; consequently in the northwest and the north the climate is oceanic. Germany gets an average of 789 mm (31 in) of precipitation per year; there is no consistent dry season. Winters are cool and summers tend to be warm: temperatures can exceed 30 °C (86 °F).
The east has a more continental climate: winters can be very cold and summers very warm, and longer dry periods can occur. Central and southern Germany are transition regions which vary from moderately oceanic to continental. In addition to the maritime and continental climates that predominate over most of the country, the Alpine regions in the extreme south and, to a lesser degree, some areas of the Central German Uplands have a mountain climate, with lower temperatures and more precipitation.



The territory of Germany can be subdivided into two ecoregions: European-Mediterranean montane mixed forests and Northeast-Atlantic shelf marine. As of 2008 the majority of Germany is covered by either arable land (34%) or forest and woodland (30.1%); only 13.4% of the area consists of permanent pastures, 11.8% is covered by settlements and streets.

Plants and animals include those generally common to Central Europe. Beeches, oaks, and other deciduous trees constitute one-third of the forests; conifers are increasing as a result of reforestation. Spruce and fir trees predominate in the upper mountains, while pine and larch are found in sandy soil. There are many species of ferns, flowers, fungi, and mosses. Wild animals include roe deer, wild boar, mouflon (a subspecies of wild sheep), fox, badger, hare, and small numbers of the Eurasian beaver. The blue cornflower was once a German national symbol.
The 16 national parks in Germany include the Jasmund National Park, the Vorpommern Lagoon Area National Park, the Müritz National Park, the Wadden Sea National Parks, the Harz National Park, the Hainich National Park, the Black Forest National Park, the Saxon Switzerland National Park, the Bavarian Forest National Park and the Berchtesgaden National Park. In addition, there are 15 Biosphere Reserves, as well as 98 nature parks. More than 400 registered zoos and animal parks operate in Germany, which is believed to be the largest number in any country. The Berlin Zoo, opened in 1844, is the oldest zoo in Germany, and presents the most comprehensive collection of species in the world.




Germany has a number of large cities. There are 11 officially recognised metropolitan regions in Germany. 34 cities have been identified as regiopolis. The largest conurbation is the Rhine-Ruhr region (11.7 million in 2008), including Düsseldorf (the capital of North Rhine-Westphalia), Cologne, Bonn, Dortmund, Essen, Duisburg, and Bochum.




Germany is a federal, parliamentary, representative democratic republic. The German political system operates under a framework laid out in the 1949 constitutional document known as the Grundgesetz (Basic Law). Amendments generally require a two-thirds majority of both chambers of parliament; the fundamental principles of the constitution, as expressed in the articles guaranteeing human dignity, the separation of powers, the federal structure, and the rule of law are valid in perpetuity.
The president, Joachim Gauck (18 March 2012–present), is the head of state and invested primarily with representative responsibilities and powers. He is elected by the Bundesversammlung (federal convention), an institution consisting of the members of the Bundestag and an equal number of state delegates. The second-highest official in the German order of precedence is the Bundestagspräsident (President of the Bundestag), who is elected by the Bundestag and responsible for overseeing the daily sessions of the body. The third-highest official and the head of government is the Chancellor, who is appointed by the Bundespräsident after being elected by the Bundestag.

The chancellor, Angela Merkel (22 November 2005–present), is the head of government and exercises executive power, similar to the role of a Prime Minister in other parliamentary democracies. Federal legislative power is vested in the parliament consisting of the Bundestag (Federal Diet) and Bundesrat (Federal Council), which together form the legislative body. The Bundestag is elected through direct elections, by proportional representation (mixed-member). The members of the Bundesrat represent the governments of the sixteen federated states and are members of the state cabinets.
Since 1949, the party system has been dominated by the Christian Democratic Union and the Social Democratic Party of Germany. So far every chancellor has been a member of one of these parties. However, the smaller liberal Free Democratic Party (in parliament from 1949 to 2013) and the Alliance '90/The Greens (in parliament since 1983) have also played important roles.
The debt-to-GDP ratio of Germany had its peak in 2010 when it stood at 80.3% and decreased since then. According to Eurostat, the government gross debt of Germany amounts to €2,152.0 billion or 71.9% of its GDP in 2015. The federal government achieved a budget surplus of €12.1 billion ($13.1 billion) in 2015. Germany's credit rating by credit rating agencies Standard & Poor's, Moody's and Fitch Ratings stands at the highest possible rating AAA with a stable outlook in 2016.




Germany has a civil law system based on Roman law with some references to Germanic law. The Bundesverfassungsgericht (Federal Constitutional Court) is the German Supreme Court responsible for constitutional matters, with power of judicial review. Germany's supreme court system, called Oberste Gerichtshöfe des Bundes, is specialised: for civil and criminal cases, the highest court of appeal is the inquisitorial Federal Court of Justice, and for other affairs the courts are the Federal Labour Court, the Federal Social Court, the Federal Finance Court and the Federal Administrative Court.
Criminal and private laws are codified on the national level in the Strafgesetzbuch and the Bürgerliches Gesetzbuch respectively. The German penal system seeks the rehabilitation of the criminal and the protection of the public. Except for petty crimes, which are tried before a single professional judge, and serious political crimes, all charges are tried before mixed tribunals on which lay judges (Schöffen) sit side by side with professional judges. Many of the fundamental matters of administrative law remain in the jurisdiction of the states.
Germany has a low murder rate with 0.9 murders per 100,000 in 2014.




Germany comprises sixteen federal states which are collectively referred to as Bundesländer. Each state has its own state constitution and is largely autonomous in regard to its internal organisation. Because of differences in size and population the subdivisions of these states vary, especially as between city states (Stadtstaaten) and states with larger territories (Flächenländer). For regional administrative purposes five states, namely Baden-Württemberg, Bavaria, Hesse, North Rhine-Westphalia and Saxony, consist of a total of 22 Government Districts (Regierungsbezirke). As of 2013 Germany is divided into 402 districts (Kreise) at a municipal level; these consist of 295 rural districts and 107 urban districts.




Germany has a network of 227 diplomatic missions abroad and maintains relations with more than 190 countries. As of 2011, Germany is the largest contributor to the budget of the European Union (providing 20%) and the third largest contributor to the UN (providing 8%). Germany is a member of NATO, the OECD, the G8, the G20, the World Bank and the IMF. It has played an influential role in the European Union since its inception and has maintained a strong alliance with France and all neighbouring countries since 1990. Germany promotes the creation of a more unified European political, economic and security apparatus.
The development policy of Germany is an independent area of foreign policy. It is formulated by the Federal Ministry for Economic Cooperation and Development and carried out by the implementing organisations. The German government sees development policy as a joint responsibility of the international community. It was the world's third biggest aid donor in 2009 after the United States and France.
In 1999, Chancellor Gerhard Schröder's government defined a new basis for German foreign policy by taking part in the NATO decisions surrounding the Kosovo War and by sending German troops into combat for the first time since 1945. The governments of Germany and the United States are close political allies. Cultural ties and economic interests have crafted a bond between the two countries resulting in Atlanticism.




Germany's military, the Bundeswehr, is organised into Heer (Army and special forces KSK), Marine (Navy), Luftwaffe (Air Force), Bundeswehr Joint Medical Service and Streitkräftebasis (Joint Support Service) branches. In absolute terms, German military expenditure is the 9th highest in the world. In 2015, military spending was at €32.9 billion, about 1.2% of the country's GDP, well below the NATO target of 2%.
As of December 2015 the Bundeswehr employed roughly 178,000 service members, including 9,500 volunteers. Reservists are available to the Armed Forces and participate in defence exercises and deployments abroad. Since 2001 women may serve in all functions of service without restriction. About 19,000 female soldiers are on active duty. According to SIPRI, Germany was the fourth largest exporter of major arms in the world in 2014.

In peacetime, the Bundeswehr is commanded by the Minister of Defence. In state of defence, the Chancellor would become commander-in-chief of the Bundeswehr.
The role of the Bundeswehr is described in the Constitution of Germany as defensive only. But after a ruling of the Federal Constitutional Court in 1994 the term "defence" has been defined to not only include protection of the borders of Germany, but also crisis reaction and conflict prevention, or more broadly as guarding the security of Germany anywhere in the world. As of January 2015, the German military has about 2,370 troops stationed in foreign countries as part of international peacekeeping forces, including about 850 Bundeswehr troops in the NATO-led ISAF force in Afghanistan and Uzbekistan, 670 German soldiers in Kosovo, and 120 troops with UNIFIL in Lebanon.
Until 2011, military service was compulsory for men at age 18, and conscripts served six-month tours of duty; conscientious objectors could instead opt for an equal length of Zivildienst (civilian service), or a six-year commitment to (voluntary) emergency services like a fire department or the Red Cross. In 2011 conscription was officially suspended and replaced with a voluntary service.




Germany has a social market economy with a highly skilled labour force, a large capital stock, a low level of corruption, and a high level of innovation. It is the world's third largest exporter of goods, and has the largest national economy in Europe which is also the world's fourth largest by nominal GDP and the fifth one by PPP.
The service sector contributes approximately 71% of the total GDP (including information technology), industry 28%, and agriculture 1%. The unemployment rate published by Eurostat amounts to 4.7% in January 2015, which is the lowest rate of all 28 EU member states. With 7.1% Germany also has the lowest youth unemployment rate of all EU member states. According to the OECD Germany has one of the highest labour productivity levels in the world.

Germany is part of the European single market which represents more than 508 million consumers. Several domestic commercial policies are determined by agreements among European Union (EU) members and by EU legislation. Germany introduced the common European currency, the Euro in 2002. It is a member of the Eurozone which represents around 338 million citizens. Its monetary policy is set by the European Central Bank, which is headquartered in Frankfurt, the financial centre of continental Europe.
Being home to the modern car, the automotive industry in Germany is regarded as one of the most competitive and innovative in the world, and is the fourth largest by production. The top 10 exports of Germany are vehicles, machinery, chemical goods, electronic products, electrical equipments, pharmaceuticals, transport equipments, basic metals, food products, and rubber and plastics.



Of the world's 500 largest stock-market-listed companies measured by revenue in 2014, the Fortune Global 500, 28 are headquartered in Germany. 30 Germany-based companies are included in the DAX, the German stock market index. Well-known international brands include Mercedes-Benz, BMW, SAP, Volkswagen, Audi, Siemens, Allianz, Adidas, Porsche, and DHL.
Germany is recognised for its large portion of specialised small and medium enterprises, known as the Mittelstand model. Around 1,000 of these companies are global market leaders in their segment and are labelled hidden champions. Berlin developed a thriving, cosmopolitan hub for startup companies and became a leading location for venture capital funded firms in the European Union.
The list includes the largest German companies by revenue in 2014:




With its central position in Europe, Germany is a transport hub for the continent. Like its neighbours in Western Europe, Germany's road network is among the densest in the world. The motorway (Autobahn) network ranks as the third-largest worldwide in length and is known for its lack of a general speed limit.
Germany has established a polycentric network of high-speed trains. The InterCityExpress or ICE network of the Deutsche Bahn serves major German cities as well as destinations in neighbouring countries with speeds up to 300 km/h (190 mph). The German railways are subsidised by the government, receiving €17.0 billion in 2014.
The largest German airports are Frankfurt Airport and Munich Airport, both hubs of Lufthansa, while Air Berlin has hubs at Berlin Tegel and Düsseldorf. Other major airports include Berlin Schönefeld, Hamburg, Cologne/Bonn and Leipzig/Halle. The Port of Hamburg is one of the top twenty largest container ports in the world.




In 2008, Germany was the world's sixth-largest consumer of energy, and 60% of its primary energy was imported. In 2014, energy sources were: oil (35.0%); coal, including lignite (24.6%); natural gas (20.5%); nuclear (8.1%); hydro-electric and renewable sources (11.1%). The government and the nuclear power industry agreed to phase out all nuclear power plants by 2021. It also enforces energy conservation, green technologies, emission reduction activities, and aims to meet the country's electricity demands using 40% renewable sources by 2020. Germany is committed to the Kyoto protocol and several other treaties promoting biodiversity, low emission standards, water management, and the renewable energy commercialisation. The country's household recycling rate is among the highest in the world—at around 65%. Nevertheless, the country's total greenhouse gas emissions were the highest in the EU in 2010. The German energy transition (Energiewende) is the recognised move to a sustainable economy by means of energy efficiency and renewable energy.




Germany is a global leader in science and technology as it achievements in the fields of science and technology have been significant. Research and development efforts form an integral part of the economy. The Nobel Prize has been awarded to 106 German laureates. It produces the second highest number of graduates in science and engineering (31%) after South Korea. In the beginning of the 20th century, German laureates had more awards than those of any other nation, especially in the sciences (physics, chemistry, and physiology or medicine).
Notable German physicists before the 20th century include Hermann von Helmholtz, Joseph von Fraunhofer and Gabriel Daniel Fahrenheit, among others. Albert Einstein introduced the relativity theories for light and gravity in 1905 and 1915 respectively. Along with Max Planck, he was instrumental in the introduction of quantum mechanics, in which Werner Heisenberg and Max Born later made major contributions. Wilhelm Röntgen discovered X-rays. Otto Hahn was a pioneer in the fields of radiochemistry and discovered nuclear fission, while Ferdinand Cohn and Robert Koch were founders of microbiology. Numerous mathematicians were born in Germany, including Carl Friedrich Gauss, David Hilbert, Bernhard Riemann, Gottfried Leibniz, Karl Weierstrass, Hermann Weyl and Felix Klein.

Germany has been the home of many famous inventors and engineers, including Hans Geiger, the creator of the Geiger counter; and Konrad Zuse, who built the first fully automatic digital computer. Such German inventors, engineers and industrialists as Count Ferdinand von Zeppelin, Otto Lilienthal, Gottlieb Daimler, Rudolf Diesel, Hugo Junkers and Karl Benz helped shape modern automotive and air transportation technology. German institutions like the German Aerospace Center (DLR) are the largest contributor to ESA. Aerospace engineer Wernher von Braun developed the first space rocket at Peenemünde and later on was a prominent member of NASA and developed the Saturn V Moon rocket. Heinrich Rudolf Hertz's work in the domain of electromagnetic radiation was pivotal to the development of modern telecommunication.
Research institutions in Germany include the Max Planck Society, the Helmholtz Association and the Fraunhofer Society. The Wendelstein 7-X in Greifswald hosts a facility in the research of fusion power for instance. The Gottfried Wilhelm Leibniz Prize is granted to ten scientists and academics every year. With a maximum of €2.5 million per award it is one of highest endowed research prizes in the world.




Germany is the seventh most visited country in the world, with a total of 407 million overnights during 2012. This number includes 68.83 million nights by foreign visitors. In 2012, over 30.4 million international tourists arrived in Germany. Berlin has become the third most visited city destination in Europe. Additionally, more than 30% of Germans spend their holiday in their own country, with the biggest share going to Mecklenburg-Vorpommern. Domestic and international travel and tourism combined directly contribute over EUR43.2 billion to German GDP. Including indirect and induced impacts, the industry contributes 4.5% of German GDP and supports 2 million jobs (4.8% of total employment).
Germany is well known for its diverse tourist routes, such as the Romantic Road, the Wine Route, the Castle Road, and the Avenue Road. The German Timber-Frame Road (Deutsche Fachwerkstraße) connects towns with examples of these structures. There are 41 UNESCO World Heritage Sites in Germany, including the old town cores of Regensburg, Bamberg, Lübeck, Quedlinburg, Weimar, Stralsund and Wismar. Germany's most-visited landmarks include e.g. Neuschwanstein Castle, Cologne Cathedral, Berlin Bundestag, Hofbräuhaus Munich, Heidelberg Castle, Dresden Zwinger, Fernsehturm Berlin and Aachen Cathedral. The Europa-Park near Freiburg is Europe's second most popular theme park resort.




With a population of 80.2 million according to the 2011 census, rising to 81.5 million as at 30 June 2015 and to at least 81.9 million as at 31 December 2015, Germany is the most populous country in the European Union, the second most populous country in Europe after Russia, and ranks as the 16th most populous country in the world. Its population density stands at 227 inhabitants per square kilometre (588 per square mile). The overall life expectancy in Germany at birth is 80.19 years (77.93 years for males and 82.58 years for females). The fertility rate of 1.41 children born per woman (2011 estimates), or 8.33 births per 1000 inhabitants, is one of the lowest in the world. Since the 1970s, Germany's death rate has exceeded its birth rate. However, Germany is witnessing increased birth rates and migration rates since the beginning of the 2010s, particularly a rise in the number of well-educated migrants.
Four sizable groups of people are referred to as "national minorities" because their ancestors have lived in their respective regions for centuries. There is a Danish minority (about 50,000) in the northernmost state of Schleswig-Holstein. The Sorbs, a Slavic population of about 60,000, are in the Lusatia region of Saxony and Brandenburg. The Roma and Sinti live throughout the whole federal territory and the Frisians live on Schleswig-Holstein's western coast, and in the north-western part of Lower Saxony.
Approximately 5 million Germans live abroad.




In 2014, about seven million of Germany's 81 million residents did not have German citizenship. Sixty-nine per cent of these people lived in western Germany and mostly in urban areas.
In the 1960s and 1970s, the German governments invited "guest workers" (Gastarbeiter) to migrate to Germany for work in the German industries. Many companies preferred to keep these workers employed in Germany after they had trained them and Germany's immigrant population has steadily increased. As of 2011, about six million foreign citizens (7.7% of the population) were registered in Germany.
The Federal Statistical Office classifies the citizens by immigrant background. Regarding immigrant background, 20% of the country's residents, or more than 16 million people, were of immigrant or partially immigrant descent in 2009 (including persons descending or partially descending from ethnic German repatriates). In 2010, 29% of families with children under 18 had at least one parent with immigrant roots.
In 2015, the Population Division of the United Nations Department of Economic and Social Affairs listed Germany as host to the second-highest number of international migrants worldwide, about 5% or 12 million of all 244 million migrants. Germany ranks 7th amongst EU countries and 37th globally in terms of the per centage of migrants who made up part of the country's population. As of 2014, the largest national group was from Turkey (2,859,000), followed by Poland (1,617,000), Russia (1,188,000), and Italy (764,000). Since 1987, around 3 million ethnic Germans, mostly from the former Eastern Bloc countries, have exercised their right of return and emigrated to Germany.




Since its foundation in 1871, Germany has been about two-thirds Protestant and one-third Roman Catholic, with a notable Jewish minority. Other faiths existed in the state, but never achieved a demographic significance and cultural impact of these three confessions. Germany almost lost its Jewish minority during the Holocaust. Religious makeup changed gradually in the decades following 1945, with West Germany becoming more religiously diversified through immigration and East Germany becoming overwhelmingly irreligious through state policies. It continues to diversify after the German reunification in 1990, with an accompanying substantial decline in religiosity throughout all of Germany and a contrasting increase of Evangelical Protestants and Muslims.
Geographically, Protestantism is concentrated in the northern, central and eastern parts of the country. These are mostly members of the EKD, which encompasses Lutheran, Reformed and administrative or confessional unions of both traditions dating back to the Prussian Union of 1817. Roman Catholicism is concentrated in the south and west.
According to the 2011 German Census, Christianity is the largest religion in Germany, claiming 66.8% of the total population. Relative to the whole population, 31.7% declared themselves as Protestants, including members of the Evangelical Church in Germany (EKD) (30.8%) and the free churches (German: Evangelische Freikirchen) (0.9%), and 31.2% declared themselves as Roman Catholics. Orthodox believers constituted 1.3%, while Jews–0.1%. Other religions accounted for 2.7%. In 2014, the Catholic Church accounted for 23.9 million members (29.5% of the population) and the Evangelical Church for 22.6 million (27.9% of the population). Both large churches have lost significant numbers of adherents in recent years.
In 2011, 33% of Germans were not members of officially recognised religious associations with special status. Irreligion in Germany is strongest in the former East Germany and major metropolitan areas.
Islam is the second largest religion in the country. In the 2011 census, 1.9% of Germans declared themselves to be Muslims. More recent estimates are that there are between 2.1 and 4.3 million Muslims living in Germany. Most of the Muslims are Sunnis and Alevites from Turkey, but there are a small number of Shi'ites, Ahmadiyyas and other denominations.
Other religions comprising less than one per cent of Germany's population are Buddhism with 250,000 adherents (roughly 0.3%) and Hinduism with some 100,000 adherents (0.1%). All other religious communities in Germany have fewer than 50,000 adherents each.




German is the official and predominant spoken language in Germany. It is one of 24 official and working languages of the European Union, and one of the three working languages of the European Commission. German is the most widely spoken first language in the European Union, with around 100 million native speakers.
Recognised native minority languages in Germany are Danish, Low German, Sorbian, Romany, and Frisian; they are officially protected by the European Charter for Regional or Minority Languages. The most used immigrant languages are Turkish, Kurdish, Polish, the Balkan languages, and Russian. Germans are typically multilingual: 67% of German citizens claim to be able to communicate in at least one foreign language and 27% in at least two.
Standard German is a West Germanic language and is closely related to and classified alongside Low German, Dutch, Frisian and English languages. To a lesser extent, it is also related to the East Germanic (extinct) and North Germanic languages. Most German vocabulary is derived from the Germanic branch of the Indo-European language family. Significant minorities of words are derived from Latin and Greek, with a smaller amount from French and most recently English (known as Denglisch). German is written using the Latin alphabet. German dialects, traditional local varieties traced back to the Germanic tribes, are distinguished from varieties of standard German by their lexicon, phonology, and syntax.




Responsibility for educational supervision in Germany is primarily organised within the individual federal states. Optional kindergarten education is provided for all children between three and six years old, after which school attendance is compulsory for at least nine years. Primary education usually lasts for four to six years. Secondary education includes three traditional types of schools focused on different academic levels: the Gymnasium enrols the most gifted children and prepares students for university studies; the Realschule for intermediate students lasts six years and the Hauptschule prepares pupils for vocational education. The Gesamtschule unifies all secondary education.
A system of apprenticeship called Duale Ausbildung leads to a skilled qualification which is almost comparable to an academic degree. It allows students in vocational training to learn in a company as well as in a state-run trade school. This model is well regarded and reproduced all around the world.
Most of the German universities are public institutions, and students traditionally study without fee payment. The general requirement for university is the Abitur. However, there are a number of exceptions, depending on the state, the college and the subject. Tuition free academic education is open to international students and is increasingly common. According to an OECD report in 2014, Germany is the world's third leading destination for international study.
Germany has a long tradition of higher education reflecting the global status as a modern economy. The established universities in Germany include some of the oldest in the world, with Heidelberg University (established in 1386) being the oldest. It is followed by the Leipzig University (1409), the Rostock University (1419) and the Greifswald University (1456). The University of Berlin, founded in 1810 by the liberal educational reformer Wilhelm von Humboldt, became the academic model for many European and Western universities. In the contemporary era Germany has developed eleven Universities of Excellence: Humboldt University Berlin, the University of Bremen, the University of Cologne, TU Dresden, the University of Tübingen, RWTH Aachen, FU Berlin, Heidelberg University, the University of Konstanz, LMU Munich, and the Technical University of Munich.




Germany's system of hospices, called spitals, dates from medieval times, and today, Germany has the world's oldest universal health care system, dating from Bismarck's social legislation of the 1880s, Since the 1880s, reforms and provisions have ensured a balanced health care system. Currently the population is covered by a health insurance plan provided by statute, with criteria allowing some groups to opt for a private health insurance contract. According to the World Health Organization, Germany's health care system was 77% government-funded and 23% privately funded as of 2013. In 2005, Germany spent 11% of its GDP on health care. Germany ranked 20th in the world in life expectancy with 77 years for men and 82 years for women, and it had a very low infant mortality rate (4 per 1,000 live births).
In 2010, the principal cause of death was cardiovascular disease, at 41%, followed by malignant tumours, at 26%. In 2008, about 82,000 Germans had been infected with HIV/AIDS and 26,000 had died from the disease (cumulatively, since 1982). According to a 2005 survey, 27% of German adults are smokers. Obesity in Germany has been increasingly cited as a major health issue. A 2007 study shows Germany has the highest number of overweight people in Europe.




Culture in German states has been shaped by major intellectual and popular currents in Europe, both religious and secular. Historically Germany has been called Das Land der Dichter und Denker ("the land of poets and thinkers"), because of the major role its writers and philosophers have played in the development of Western thought.
Germany is well known for such folk festival traditions as Oktoberfest and Christmas customs, which include Advent wreaths, Christmas pageants, Christmas trees, Stollen cakes, and other practices. As of 2016 UNESCO inscribed 41 properties in Germany on the World Heritage List. There are a number of public holidays in Germany determined by each state; 3 October has been a national day of Germany since 1990, celebrated as the Tag der Deutschen Einheit (German Unity Day).
In the 21st century Berlin has emerged as a major international creative centre. According to the Anholt–GfK Nation Brands Index, in 2014 Germany was the world's most respected nation among 50 countries (ahead of US, UK, and France). A global opinion poll for the BBC revealed that Germany is recognised for having the most positive influence in the world in 2013 and 2014.




German classical music includes works by some of the world's most well-known composers. Dieterich Buxtehude composed oratorios for organ, which influenced the later work of Johann Sebastian Bach and Georg Friedrich Händel; these men were influential composers of the Baroque period. During his tenure as violinist and teacher at the Salzburg cathedral, Augsburg-born composer Leopold Mozart mentored one of the most noted musicians of all time: Wolfgang Amadeus Mozart. Ludwig van Beethoven was a crucial figure in the transition between the Classical and Romantic eras. Carl Maria von Weber and Felix Mendelssohn were important in the early Romantic period. Robert Schumann and Johannes Brahms composed in the Romantic idiom. Richard Wagner was known for his operas. Richard Strauss was a leading composer of the late Romantic and early modern eras. Karlheinz Stockhausen and Hans Zimmer are important composers of the 20th and early 21st centuries.
Germany is the second largest music market in Europe, and fourth largest in the world. German popular music of the 20th and 21st century includes the movements of Neue Deutsche Welle, pop, Ostrock, heavy metal/rock, punk, pop rock, indie and schlager pop. German electronic music gained global influence, with Kraftwerk and Tangerine Dream pioneering in this genre. DJs and artists of the techno and house music scenes of Germany have become well known (e.g. Paul van Dyk, Paul Kalkbrenner, and Scooter).




German painters have influenced western art. Albrecht Dürer, Hans Holbein the Younger, Matthias Grünewald and Lucas Cranach the Elder were important German artists of the Renaissance, Peter Paul Rubens and Johann Baptist Zimmermann of the Baroque, Caspar David Friedrich and Carl Spitzweg of Romanticism, Max Liebermann of Impressionism and Max Ernst of Surrealism. Such German sculptors as Otto Schmidt-Hofer, Franz Iffland, and Julius Schmidt-Felling made important contributions to German art history in the late 19th and early 20th centuries.
Several German art groups formed in the 20th century, such as the November Group or Die Brücke (The Bridge) and Der Blaue Reiter (The Blue Rider), by the Russian-born Wassily Kandinsky, influenced the development of Expressionism in Munich and Berlin. The New Objectivity arose as a counter-style to it during the Weimar Republic. Post-World War II art trends in Germany can broadly be divided into Neo-expressionism, performance art and Conceptualism. Especially notable neo-expressionists include Georg Baselitz, Anselm Kiefer, Jörg Immendorff, A. R. Penck, Markus Lüpertz, Peter Robert Keil and Rainer Fetting. Other notable artists who work with traditional media or figurative imagery include Martin Kippenberger, Gerhard Richter, Sigmar Polke, and Neo Rauch. Leading German conceptual artists include or included Bernd and Hilla Becher, Hanne Darboven, Hans-Peter Feldmann, Hans Haacke, Joseph Beuys, HA Schult, Aris Kalaizis, Neo Rauch (New Leipzig School) and Andreas Gursky (photography). Major art exhibitions and festivals in Germany are the documenta, the Berlin Biennale, transmediale and Art Cologne.




Architectural contributions from Germany include the Carolingian and Ottonian styles, which were precursors of Romanesque. Brick Gothic is a distinctive medieval style that evolved in Germany. Also in Renaissance and Baroque art, regional and typically German elements evolved (e.g. Weser Renaissance and Dresden Baroque). Among many renowned Baroque masters were Pöppelmann, Balthasar Neumann, Knobelsdorff and the Asam brothers. The Wessobrunner School exerted a decisive influence on, and at times even dominated, the art of stucco in southern Germany in the 18th century. The Upper Swabian Baroque Route offers a baroque-themed tourist route that highlights the contributions of such artists and craftsmen as the sculptor and plasterer Johann Michael Feuchtmayer, one of the foremost members of the Feuchtmayer family and the brothers Johann Baptist Zimmermann and Dominikus Zimmermann. Vernacular architecture in Germany is often identified by its timber framing (Fachwerk) traditions and varies across regions, and among carpentry styles.
When industrialisation spread across Europe, Classicism and a distinctive style of historism developed in Germany, sometimes referred to as Gründerzeit style, due to the economical boom years at the end of the 19th century. Regional historicist styles include the Hanover School, Nuremberg Style and Dresden's Semper-Nicolai School. Among the most famous of German buildings, the Schloss Neuschwanstein represents Romanesque Revival. Notable sub-styles that evolved since the 18th century are the German spa and seaside resort architecture. German artists, writers and gallerists like Siegfried Bing, Georg Hirth and Bruno Möhring also contributed to the development of Art Nouveau at the turn of the 20th century, known as Jugendstil in German.

Expressionist architecture developed in the 1910s in Germany and influenced Art Deco and other modern styles, with e.g. Fritz Höger, Erich Mendelsohn, Dominikus Böhm and Fritz Schumacher being influential architects. Germany was particularly important in the early modernist movement: it is the home of Werkbund initiated by Hermann Muthesius (New Objectivity), and of the Bauhaus movement founded by Walter Gropius. Consequently, Germany is often considered the cradle of modern architecture and design. Ludwig Mies van der Rohe became one of the world's most renowned architects in the second half of the 20th century. He conceived of the glass façade skyscraper. Renowned contemporary architects and offices include Hans Kollhoff, Sergei Tchoban, KK Architekten, Helmut Jahn, Behnisch, GMP, Ole Scheeren, J. Mayer H., OM Ungers, Gottfried Böhm and Frei Otto (the last two being Pritzker Prize winners).




German literature can be traced back to the Middle Ages and the works of writers such as Walther von der Vogelweide and Wolfram von Eschenbach. Well-known German authors include Johann Wolfgang von Goethe, Friedrich Schiller, Gotthold Ephraim Lessing and Theodor Fontane. The collections of folk tales published by the Brothers Grimm popularised German folklore on an international level. The Grimms also gathered and codified regional variants of the German language, grounding their work in historical principles; their Deutsches Wörterbuch, or German Dictionary, sometimes called the Grimm dictionary, was begun in 1838 and the first volumes published in 1854.
Influential authors of the 20th century include Gerhart Hauptmann, Thomas Mann, Hermann Hesse, Heinrich Böll and Günter Grass. The German book market is the third largest in the world, after the United States and China. The Frankfurt Book Fair is the most important in the world for international deals and trading, with a tradition spanning over 500 years. The Leipzig Book Fair also retains a major position in Europe.
German philosophy is historically significant: Gottfried Leibniz's contributions to rationalism; the enlightenment philosophy by Immanuel Kant; the establishment of classical German idealism by Johann Gottlieb Fichte, Georg Wilhelm Friedrich Hegel and Friedrich Wilhelm Joseph Schelling; Arthur Schopenhauer's composition of metaphysical pessimism; the formulation of communist theory by Karl Marx and Friedrich Engels; Friedrich Nietzsche's development of perspectivism; Gottlob Frege's contributions to the dawn of analytic philosophy; Martin Heidegger's works on Being; the development of the Frankfurt school by Max Horkheimer, Theodor Adorno, Herbert Marcuse and Jürgen Habermas have been particularly influential.




The largest internationally operating media companies in Germany are the Bertelsmann enterprise, Axel Springer SE and ProSiebenSat.1 Media. The German Press Agency DPA is also significant. Germany's television market is the largest in Europe, with some 38 million TV households. Around 90% of German households have cable or satellite TV, with a variety of free-to-view public and commercial channels. There are more than 500 public and private radio stations in Germany, with the public Deutsche Welle being the main German radio and television broadcaster in foreign languages. Germany's national radio network is the Deutschlandradio while ARD stations are covering local services.
Many of Europe's best-selling newspapers and magazines are produced in Germany. The papers (and internet portals) with the highest circulation are Bild (a tabloid), Die Zeit, Süddeutsche Zeitung, Frankfurter Allgemeine Zeitung and Die Welt, the largest magazines include Der Spiegel, Stern and Focus.
The German video gaming market is one of the largest in the world. The Gamescom in Cologne is the world's leading gaming convention. Popular game series from Germany include Turrican, the Anno series, The Settlers series, the Gothic series, SpellForce, the FIFA Manager series, Far Cry and Crysis. Relevant game developers and publishers are Blue Byte, Crytek, Deep Silver, Kalypso Media, Piranha Bytes, Yager Development, and some of the largest social network game companies like Bigpoint, Gameforge, Goodgame and Wooga.




German cinema has made major technical and artistic contributions to film. The first works of the Skladanowsky Brothers were shown to an audience in 1895. The renowned Babelsberg Studio in Berlin's suburb Potsdam was established in 1912, thus being the first large-scale film studio in the world. Today it is Europe's largest studio. Other early and still active studios include UFA and Bavaria Film. Early German cinema was particularly influential with German expressionists such as Robert Wiene and Friedrich Wilhelm Murnau. Director Fritz Lang's Metropolis (1927) is referred to as the first major science-fiction film. In 1930 Josef von Sternberg directed The Blue Angel, the first major German sound film, with Marlene Dietrich. Films of Leni Riefenstahl set new artistic standards, in particular Triumph of the Will.

After 1945, many of the films of the immediate post-war period can be characterised as Trümmerfilm (rubble film). Such films included Wolfgang Staudte's Die Mörder sind unter uns (The Murderers are among us, 1946) and Irgendwo in Berlin (Somewhere in Berlin, 1946) by Werner Krien. Notable East German films were largely produced by DEFA and included Ehe im Schatten (Marriage in the Shadows) by Kurt Maetzig (1947), Der Untertan (1951); Die Geschichte vom kleinen Muck (The Story of Little Muck, 1953), Konrad Wolf's Der geteilte Himmel (Divided Heaven) (1964) and Frank Beyer's Jacob the Liar (1975). The defining film genre in West Germany of the 1950s was arguably the Heimatfilm ("homeland film"); these films depicted the beauty of the land and the moral integrity of the people living in it. Characteristic for the films of the 1960s were genre films including Edgar Wallace and Karl May adaptations. One of the most successful German movie series of the 1970s included the sex reports called Schulmädchen-Report (Schoolgirl Report). During the 1970s and 1980s, New German Cinema directors such as Volker Schlöndorff, Werner Herzog, Wim Wenders, and Rainer Werner Fassbinder brought West German auteur cinema to critical acclaim.
Among the box office hits, there were films such as Chariots of the Gods (1970), Das Boot (The Boat, 1981), The Never Ending Story (1984), Otto – The Movie (1985), Run Lola Run (1998), Manitou's Shoe (2001), the Resident Evil series (2002–2016), Good Bye, Lenin! (2003), Head On (2004), The White Ribbon (2009), Animals United (2010), and Cloud Atlas (2012). The Academy Award for Best Foreign Language Film ("Oscar") went to the German production Die Blechtrommel (The Tin Drum) in 1979, to Nirgendwo in Afrika (Nowhere in Africa) in 2002, and to Das Leben der Anderen (The Lives of Others) in 2007. Various Germans won an "Oscar" award for their performances in other films.
The annual European Film Awards ceremony is held every other year in Berlin, home of the European Film Academy. The Berlin International Film Festival, known as "Berlinale", awarding the "Golden Bear" and held annually since 1951, is one of the world's leading film festivals. The "Lolas" are annually awarded in Berlin, at the German Film Awards, that have been presented since 1951.




German cuisine varies from region to region and often neighbouring regions share some culinary similarities (e.g. the southern regions of Bavaria and Swabia share some traditions with Switzerland and Austria). International varieties such as pizza, sushi, Chinese food, Greek food, Indian cuisine and doner kebab are also popular and available, thanks to diverse ethnic communities.
Bread is a significant part of German cuisine and German bakeries produce about 600 main types of bread and 1,200 different types of pastries and rolls (Brötchen). German cheeses account for about a third of all cheese produced in Europe. In 2012 over 99% of all meat produced in Germany was either pork, chicken or beef. Germans produce their ubiquitous sausages in almost 1,500 varieties, including Bratwursts, Weisswursts, and Currywursts. In 2012, organic foods accounted for 3.9% of total food sales.
Although wine is becoming more popular in many parts of Germany, especially in German wine regions, the national alcoholic drink is beer. German beer consumption per person stands at 110 litres (24 imp gal; 29 US gal) in 2013 and remains among the highest in the world. German beer purity regulations date back to the 15th century.
The 2015 Michelin Guide awarded eleven restaurants in Germany three stars, the highest designation, while 38 more received two stars and 233 one star. German restaurants have become the world's second-most decorated after France.




Twenty-seven million Germans are members of a sports club and an additional twelve million pursue sports individually. Association football is the most popular sport. With more than 6.3 million official members, the German Football Association (Deutscher Fußball-Bund) is the largest sports organisation of its kind worldwide, and the German top league, the Bundesliga, attracts the second highest average attendance of all professional sports leagues in the world. The German men's national football team won the FIFA World Cup in 1954, 1974, 1990, and 2014, and the UEFA European Championship in 1972, 1980 and 1996. Germany hosted the FIFA World Cup in 1974 and 2006 and the UEFA European Championship in 1988.
Other popular spectator sports include winter sports, boxing, basketball, handball, volleyball, ice hockey, tennis, horse riding and golf. Water sports like sailing, rowing, and swimming are popular in Germany as well.
Germany is one of the leading motor sports countries in the world. Constructors like BMW and Mercedes are prominent manufacturers in motor sport. Porsche has won the 24 Hours of Le Mans race 17 times, and Audi 13 times (as of 2015). The driver Michael Schumacher has set many motor sport records during his career, having won seven Formula One World Drivers' Championships, more than any other. He is one of the highest paid sportsmen in history. Sebastian Vettel is also among the top five most successful Formula One drivers of all time. Also Nico Rosberg won the Formula One World Championship.
Historically, German athletes have been successful contenders in the Olympic Games, ranking third in an all-time Olympic Games medal count (when combining East and West German medals). Germany was the last country to host both the summer and winter games in the same year, in 1936 the Berlin Summer Games and the Winter Games in Garmisch-Partenkirchen. In Munich it hosted the Summer Games of 1972.




German designers were leaders of modern product design, with the Bauhaus designers like Mies van der Rohe, and Dieter Rams of Braun being essential.
Germany is a leading country in the fashion industry. The German textile industry consisted of about 1,300 companies with more than 130,000 employees in 2010, which generated a revenue of 28 billion Euro. Almost 44 per cent of the products are exported. The Berlin Fashion Week and the fashion trade fair Bread & Butter are held twice a year.
Munich, Hamburg and Düsseldorf are also important design, production and trade hubs of the domestic fashion industry, among smaller towns. Renowned fashion designers from Germany include Karl Lagerfeld, Jil Sander, Wolfgang Joop, Philipp Plein and Michael Michalsky. Important brands include Hugo Boss, Escada, Adidas, Puma and Triumph. The German supermodels Claudia Schiffer, Heidi Klum, Tatjana Patitz and Nadja Auermann have come to international fame.




Index of Germany-related articles
Outline of Germany










Government
Official site of the Federal Government
Official site of the Federal President
Official site of the German Chancellor
Official Germany Tourism website
General information
Germany from the BBC News
"Germany". The World Factbook. Central Intelligence Agency. 
Germany from UCB Libraries GovPubs
Germany at DMOZ
Germany Encyclopædia Britannica entry
Germany from the OECD
Germany at the EU
 Wikimedia Atlas of Germany
 Geographic data related to Germany at OpenStreetMap
Key Development Forecasts for Germany from International FuturesBrazil (/brəˈzɪl/; Portuguese: Brasil [bɾaˈziw]), officially the Federative Republic of Brazil (Portuguese: República Federativa do Brasil,  listen ), is the largest country in both South America and Latin America. As the world's fifth-largest country by both area and population, it is the largest country to have Portuguese as an official language and the only one in the Americas. Bounded by the Atlantic Ocean on the east, Brazil has a coastline of 7,491 km (4,655 mi). It borders all other South American countries except Ecuador and Chile and covers 47.3% of the continent's land area. Its Amazon River basin includes a vast tropical forest, home to diverse wildlife, a variety of ecological systems, and extensive natural resources spanning numerous protected habitats. This unique environmental heritage makes Brazil one of 17 megadiverse countries, and is the subject of significant global interest and debate regarding deforestation and environmental protection.
Brazil was inhabited by numerous tribal nations prior to the landing in 1500 of explorer Pedro Álvares Cabral, who claimed the area for the Portuguese Empire. Brazil remained a Portuguese colony until 1808, when the capital of the empire was transferred from Lisbon to Rio de Janeiro. In 1815, the colony was elevated to the rank of kingdom upon the formation of the United Kingdom of Portugal, Brazil and the Algarves. Independence was achieved in 1822 with the creation of the Empire of Brazil, a unitary state governed under a constitutional monarchy and a parliamentary system. The ratification of the first constitution in 1824 led to the formation of a bicameral legislature, now called the National Congress. The country became a presidential republic in 1889 following a military coup d'état. An authoritarian military junta came to power in 1964 and ruled until 1985, after which civilian governance resumed. Brazil's current constitution, formulated in 1988, defines it as a democratic federal republic. The federation is composed of the union of the Federal District, the 26 states, and the 5,570 municipalities.
Brazil's economy is the world's ninth-largest by nominal GDP and seventh-largest by GDP (PPP) as of 2015. A member of the BRICS group, Brazil until 2010 had one of the world's fastest growing major economies, with its economic reforms giving the country new international recognition and influence. Brazil's national development bank plays an important role for the country's economic growth. Brazil is a founding member of the United Nations, the G20, BRICS, Unasul, Mercosul, Organization of American States, Organization of Ibero-American States, CPLP, and the Latin Union. Brazil is a regional power in Latin America and a middle power in international affairs, with some analysts identifying it as an emerging global power. One of the world's major breadbaskets, Brazil has been the largest producer of coffee for the last 150 years.




It is likely that the word "Brazil" comes from the Portuguese word for brazilwood, a tree that once grew plentifully along the Brazilian coast. In Portuguese, brazilwood is called pau-brasil, with the word brasil commonly given the etymology "red like an ember", formed from Latin brasa ("ember") and the suffix -il (from -iculum or -ilium). As brazilwood produces a deep red dye, it was highly valued by the European cloth industry and was the earliest commercially exploited product from Brazil. Throughout the 16th century, massive amounts of brazilwood were harvested by indigenous peoples (mostly Tupi) along the Brazilian coast, who sold the timber to European traders (mostly Portuguese, but also French) in return for assorted European consumer goods.
The official Portuguese name of the land, in original Portuguese records, was the "Land of the Holy Cross" (Terra da Santa Cruz), but European sailors and merchants commonly called it simply the "Land of Brazil" (Terra do Brasil) on account of the brazilwood trade. The popular appellation eclipsed and eventually supplanted the official Portuguese name. Early sailors sometimes also called it the "Land of Parrots" (Terra di Papaga).
In the Guarani language, an official language of Paraguay, Brazil is called "Pindorama". This was the name the indigenous population gave to the region, meaning "land of the palm trees".







The earliest human remains found in the Americas, Luzia Woman, were found in the area of Pedro Leopoldo, Minas Gerais and provide evidence of human habitation going back at least 11,000 years. The earliest pottery ever found in the Western Hemisphere was excavated in the Amazon basin of Brazil and radiocarbon dated to 8,000 years ago (6000 BC). The pottery was found near Santarém and provides evidence that the tropical forest region supported a complex prehistoric culture.
Around the time of the Portuguese arrival, the territory of current day Brazil had an estimated indigenous population of 7 million people, mostly semi-nomadic who subsisted on hunting, fishing, gathering, and migrant agriculture. The indigenous population of Brazil comprised several large indigenous ethnic groups (e.g. the Tupis, Guaranis, Gês and Arawaks). The Tupí people were subdivided into the Tupiniquins and Tupinambás, and there were also many subdivisions of the other groups.
Before the arrival of Europeans, the boundaries between these groups and their subgroups were marked by wars that arose from differences in culture, language and moral beliefs. These wars also involved large-scale military actions on land and water, with cannibalistic rituals on prisoners of war. While heredity had some weight, leadership status was more subdued over time, than allocated in succession ceremonies and conventions. Slavery among the Indians had a different meaning than it had for Europeans, since it originated from a diverse socio-economic organization, in which asymmetries were translated into kinship relations.




The land now called Brazil was claimed for the Portuguese Empire on 22 April 1500, with the arrival of the Portuguese fleet commanded by Pedro Álvares Cabral. The Portuguese encountered indigenous peoples divided into several tribes, most of whom spoke languages of the Tupi–Guarani family, and fought among themselves. Though the first settlement was founded in 1532, colonization was effectively begun in 1534, when King Dom João III of Portugal divided the territory into the fifteen private and autonomous Captaincy Colonies of Brazil.
However, the decentralized and unorganized tendencies of the captaincy colonies proved problematic, and in 1549 the Portuguese king restructured them into the Governorate General of Brazil, a single and centralized Portuguese colony in South America. In the first two centuries of colonization, Indigenous and European groups lived in constant war, establishing opportunistic alliances in order to gain advantages against each other. By the mid-16th century, cane sugar had become Brazil's most important exportation product, and slaves purchased in Sub-Saharan Africa, in the slave market of Western Africa (not only those from Portuguese allies of their colonies in Angola and Mozambique), had become its largest import, to cope with plantations of sugarcane, due to increasing international demand for Brazilian sugar.

By the end of the 17th century, sugarcane exports began to decline, and the discovery of gold by bandeirantes in the 1690s would become the new backbone of the colony's economy, fostering a Brazilian Gold Rush which attracted thousands of new settlers to Brazil from Portugal and all Portuguese colonies around the world. This increased level of immigration in turn caused some conflicts between newcomers and old settlers.
Portuguese expeditions known as Bandeiras gradually advanced the Portugal colonial original frontiers in South America to approximately the current Brazilian borders. In this era other European powers tried to colonize parts of Brazil, in incursions that the Portuguese had to fight, notably the French in Rio during the 1560s, in Maranhão during the 1610s, and the Dutch in Bahia and Pernambuco, during the Dutch–Portuguese War, after the end of Iberian Union.
The Portuguese colonial administration in Brazil had two objectives that would ensure colonial order and the monopoly of Portugal's wealthiest and largest colony: to keep under control and eradicate all forms of slave rebellion and resistance, such as the Quilombo of Palmares, and to repress all movements for autonomy or independence, such as the Minas Conspiracy.




In late 1807, Spanish and Napoleonic forces threatened the security of continental Portugal, causing Prince Regent João, in the name of Queen Maria I, to move the royal court from Lisbon to Brazil. There they established some of Brazil's first financial institutions, such as its local stock exchanges, and its National Bank, additionally ending the Portuguese monopoly on Brazilian trade and opening Brazil to other nations. In 1809, in retaliation for being forced into exile, the Prince Regent ordered the Portuguese conquest of French Guiana.
With the end of the Peninsular War in 1814, the courts of Europe demanded that Queen Maria I and Prince Regent João return to Portugal, deeming it unfit for the head of an ancient European monarchy to reside in a colony. In 1815, in order to justify continuing to live in Brazil, where the royal court had thrived for the past six years, the Crown established the United Kingdom of Portugal, Brazil, and the Algarves, thus creating a pluricontinental transatlantic monarchic state. However, such a ploy didn't last long, since the leadership in Portugal resentful with the new status of its larger colony, continued to require the return of court to Lisbon (as postulated by the Liberal Revolution of 1820), as well as groups of Brazilians, impatient for practical and real changes still demanded independence and a republic, as showed by the 1817 Pernambucan Revolt. In 1821, as a demand of revolutionaries who had taken the city of Porto, D. João VI was unable to hold out any longer, and departed for Lisbon. There he swore oath to the new constitution, leaving his son, Prince Pedro de Alcântara, as Regent of the Kingdom of Brazil.




Tensions between Portuguese and Brazilians increased, and the Portuguese Cortes, guided by the new political regime imposed by the 1820 Liberal Revolution, tried to re-establish Brazil as a colony. The Brazilians refused to yield, and Prince Pedro decided to stand with them, declaring the country's independence from Portugal on 7 September 1822. A month later, Prince Pedro was declared the first Emperor of Brazil, with the regnal title of Dom Pedro I, resulting in the foundation of the Empire of Brazil.
The Brazilian War of Independence, which had already begun along this process, spread through northern, northeastern regions and in Cisplatina province. With the last Portuguese soldiers surrendering on 8 March 1824, Portugal officially recognized Brazil on 29 August 1825.
On 7 April 1831, worn down by years of administrative turmoil and political dissensions with both liberal and conservative sides of politics, including an attempt of republican secession, as well as unreconciled with the way that absolutists in Portugal had given to the succession of King John VI, Pedro I went to Portugal to reclaim his daughter's crown, abdicating the Brazilian throne in favor of his five-year-old son and heir (who thus became the Empire's second monarch, with the regnal title of Dom Pedro II).

As the new Emperor could not exert his constitutional powers until he became of age, a regency was set up by the National Assembly. In the absence of a charismatic figure who could represent a moderate face of power, during this period a series of localized rebellions took place, as the Cabanagem, the Malê Revolt, the Balaiada, the Sabinada, and the Ragamuffin War, which emerged from the dissatisfaction of the provinces with the central power, coupled with old and latent social tensions peculiar of a vast, slaveholding and newly independent nation state. This period of internal political and social upheaval, which included the Praieira revolt, was overcome only at the end of the 1840s, years after the end of the regency, which occurred with the premature coronation of Pedro II in 1841.
During the last phase of the monarchy, internal political debate was centered on the issue of slavery. The Atlantic slave trade was abandoned in 1850, as a result of the British Aberdeen Act, but only in May 1888 after a long process of internal mobilization and debate for an ethical and legal dismantling of slavery in the country, was the institution formally abolished.
The foreign affairs in the monarchy were basically related issues with the countries of the Southern Cone with which Brazil has borders. Long after the Cisplatine War that resulted in independence for Uruguay, Brazil won three international wars during the 58-year reign of Pedro II. These were the Platine War, the Uruguayan War and the devastating Paraguayan War, the largest war effort in Brazilian history.
On 15 November 1889, worn out by years of economic stagnation, in attrition with the majority of Army officers, as well as with rural and financial elites (for different reasons), the monarchy was overthrown by a military coup.




The "early republican government was little more than a military dictatorship, with army dominating affairs both at Rio de Janeiro and in the states. Freedom of the press disappeared and elections were controlled by those in power". In 1894, following the unfoldings of two severe crises, an economic along with a military one, the republican civilians rose to power.
Little by little, a cycle of general instability sparked by these crises undermined the regime to such an extent, that in the wake of the murder of his running mate, the defeated opposition presidential candidate Getúlio Vargas supported by most of the military, successfully led the Brazilian Revolution of 1930. Vargas was supposed to assume power temporarily, but instead closed the Congress, extinguished the Constitution, ruled with emergency powers and replaced the states' governors with his own supporters.

In the 1930s, three major attempts to remove Vargas and his supporters from power occurred. The first was the failed Constitutionalist Revolution of 1932, the second was the anti-fascist Brazilian uprising of 1935 led by communists, and the fascist Integralist Movement attempted a coup in May 1938. The 1935 uprising created a security crisis in which the Congress transferred more power to the executive. The 1937 coup d'état resulted in the cancellation of the 1938 election, installed Vargas as a dictator, and began the Estado Novo era, noted for government brutality and censorship of the press.
In foreign policy, the success in resolving border disputes with neighboring countries in the early years of the republican period, was followed by a failed attempt to exert a prominent role in the League of Nations, after its involvement in World War I. In World War II Brazil remained neutral until August 1942, when the country entered on the allied side, after suffering retaliations undertaken by Nazi Germany and Fascist Italy, due to the country having severed diplomatic relations with Axis powers in the wake of the Pan-American Conference.
With the allied victory in 1945 and the end of the Nazi-fascist regimes in Europe, Vargas's position became unsustainable and he was swiftly overthrown in another military coup, with Democracy being "reinstated" by the same army that had discontinued it 15 years before. Vargas committed suicide in August 1954 amid a political crisis, after having returned to power by election in 1950.




Several brief interim governments succeeded after Vargas's suicide. Juscelino Kubitschek became president in 1956 and assumed a conciliatory posture towards the political opposition that allowed him to govern without major crises. The economy and industrial sector grew remarkably, but his greatest achievement was the construction of the new capital city of Brasília, inaugurated in 1960. His successor was Jânio Quadros, who resigned in 1961 less than a year after taking office. His vice-president, João Goulart, assumed the presidency, but aroused strong political opposition and was deposed in April 1964 by a coup that resulted in a military regime.
The new regime was intended to be transitory but it gradually closed in on itself and became a full dictatorship with the promulgation of the Fifth Institutional Act in 1968. The oppression was not limited to only those who resorted to guerrilla tactics to fight the regime, but also reached institutional opponents, artists, journalists and other members of civil society, inside and outside the country (through the infamous "Operation Condor"). Despite its brutality, like other totalitarian regimes in history, due to an economic boom, known as an "economic miracle", the regime reached its highest level of popularity in the early 1970s.

Slowly however, the wear and tear of years of dictatorial power that had not slowed the repression, even after the defeat of the leftist guerrillas, plus the inability to deal with the economic crises of the period and popular pressure, made an opening policy inevitable, which from the regime side was led by Generals Geisel and Golbery. With the enactment of the Amnesty Law in 1979, Brazil began its slow return to democracy, which would be completed during the 1980s.
Civilians returned to power in 1985 when José Sarney assumed the presidency, becoming unpopular during his tenure due to his failure in controlling the economic crisis and hyperinflation inherited from the military regime. Sarney's unsuccessful government allowed the election in 1989 of the almost unknown Fernando Collor, who was subsequently impeached by the National Congress in 1992.
Collor was succeeded by his Vice-President Itamar Franco, who appointed Fernando Henrique Cardoso as Minister of Finance. In 1994, Cardoso produced a highly successful Plano Real, that, after decades of failed economic plans made by previous governments attempting to curb hyperinflation, finally granted stability to the Brazilian economy, leading Cardoso to be elected that year, and again in 1998.
The peaceful transition of power from Fernando Henrique to his main opposition leader, Luiz Inácio Lula da Silva (elected in 2002 and re-elected in 2006), was seen as a proof that Brazil had finally succeeded in achieving a long-sought political stability. However, sparked by indignation and frustrations accumulated over decades (against corruption, police brutality, inefficiencies of political establishment and public service), numerous peaceful protests erupted in Brazil from the middle of first term of Dilma Rousseff (who succeeded Lula in 2010). Enhanced by a political and economic crises with evidences of involvement of politicians from all main political parties in several bribery and tax evasion schemes, with large street protests for and against her, Rousseff was impeached by the Brazilian Congress in 2016.




Brazil occupies a large area along the eastern coast of South America and includes much of the continent's interior, sharing land borders with Uruguay to the south; Argentina and Paraguay to the southwest; Bolivia and Peru to the west; Colombia to the northwest; and Venezuela, Guyana, Suriname and France (French overseas region of French Guiana) to the north. It shares a border with every South American country except Ecuador and Chile. It also encompasses a number of oceanic archipelagos, such as Fernando de Noronha, Rocas Atoll, Saint Peter and Paul Rocks, and Trindade and Martim Vaz. Its size, relief, climate, and natural resources make Brazil geographically diverse. Including its Atlantic islands, Brazil lies between latitudes 6°N and 34°S, and longitudes 28° and 74°W.
Brazil is the fifth largest country in the world, and third largest in the Americas, with a total area of 8,515,767.049 km2 (3,287,956 sq mi), including 55,455 km2 (21,411 sq mi) of water. It spans four time zones; from UTC−5 comprising the state of Acre and the westernmost portion of Amazonas, to UTC−4 in the western states, to UTC−3 in the eastern states (the national time) and UTC−2 in the Atlantic islands.
Brazil is the only country in the world that has the equator and the Tropic of Capricorn running through it. It is also the only country to have contiguous territory both inside and outside the tropics. Brazilian topography is also diverse and includes hills, mountains, plains, highlands, and scrublands. Much of the terrain lies between 200 metres (660 ft) and 800 metres (2,600 ft) in elevation. The main upland area occupies most of the southern half of the country. The northwestern parts of the plateau consist of broad, rolling terrain broken by low, rounded hills.
The southeastern section is more rugged, with a complex mass of ridges and mountain ranges reaching elevations of up to 1,200 metres (3,900 ft). These ranges include the Mantiqueira and Espinhaço mountains and the Serra do Mar. In the north, the Guiana Highlands form a major drainage divide, separating rivers that flow south into the Amazon Basin from rivers that empty into the Orinoco River system, in Venezuela, to the north. The highest point in Brazil is the Pico da Neblina at 2,994 metres (9,823 ft), and the lowest is the Atlantic Ocean.
Brazil has a dense and complex system of rivers, one of the world's most extensive, with eight major drainage basins, all of which drain into the Atlantic. Major rivers include the Amazon (the world's second-longest river and the largest in terms of volume of water), the Paraná and its major tributary the Iguaçu (which includes the Iguazu Falls), the Negro, São Francisco, Xingu, Madeira and Tapajós rivers.
Geography of Brazil




The climate of Brazil comprises a wide range of weather conditions across a large area and varied topography, but most of the country is tropical. According to the Köppen system, Brazil hosts five major climatic subtypes: equatorial, tropical, semiarid, highland tropical, temperate, and subtropical. The different climatic conditions produce environments ranging from equatorial rainforests in the north and semiarid deserts in the northeast, to temperate coniferous forests in the south and tropical savannas in central Brazil. Many regions have starkly different microclimates.
An equatorial climate characterizes much of northern Brazil. There is no real dry season, but there are some variations in the period of the year when most rain falls. Temperatures average 25 °C (77 °F), with more significant temperature variation between night and day than between seasons.
Over central Brazil rainfall is more seasonal, characteristic of a savanna climate. This region is as extensive as the Amazon basin but has a very different climate as it lies farther south at a higher altitude. In the interior northeast, seasonal rainfall is even more extreme. The semiarid climatic region generally receives less than 800 millimetres (31.5 in) of rain, most of which generally falls in a period of three to five months of the year and occasionally less than this, creating long periods of drought. Brazil's 1877–78 Grande Seca (Great Drought), the worst in Brazil's history, caused approximately half a million deaths. A similarly devastating drought occurred in 1915.
South of Bahia, near the coasts, and more southerly most of the state of São Paulo, the distribution of rainfall changes, with rain falling throughout the year. The south enjoys subtropical conditions, with cool winters and average annual temperatures not exceeding 18 °C (64.4 °F); winter frosts and snowfall are not rare in the highest areas.
Climate of Brazil




Brazil's large territory comprises different ecosystems, such as the Amazon rainforest, recognized as having the greatest biological diversity in the world, with the Atlantic Forest and the Cerrado, sustaining the greatest biodiversity. In the south, the Araucaria pine forest grows under temperate conditions. The rich wildlife of Brazil reflects the variety of natural habitats. Scientists estimate that the total number of plant and animal species in Brazil could approach four million, mostly invertebrates.

Larger mammals include carnivores pumas, jaguars, ocelots, rare bush dogs, and foxes, and herbivores peccaries, tapirs, anteaters, sloths, opossums, and armadillos. Deer are plentiful in the south, and many species of New World monkeys are found in the northern rain forests. Concern for the environment has grown in response to global interest in environmental issues. Brazil's Amazon Basin is home to an extremely diverse array of fish species, including the red-bellied piranha. Despite its reputation as a ferocious freshwater fish, the red-bellied piranha is actually a generally timid scavenger. Biodiversity can contribute to agriculture, livestock, forestry and fisheries extraction. However, almost all economically exploited species of plants, such as soybeans and coffee, or animals, such as chickens, are imported from other countries, and the economic use of native species still crawls. In the Brazilian GDP, the forest sector represents just over 1% and fishing 0.4%.
The natural heritage of Brazil is severely threatened by cattle ranching and agriculture, logging, mining, resettlement, oil and gas extraction, over-fishing, wildlife trade, dams and infrastructure, water pollution, climate change, fire, and invasive species. In many areas of the country, the natural environment is threatened by development. Construction of highways has opened up previously remote areas for agriculture and settlement; dams have flooded valleys and inundated wildlife habitats; and mines have scarred and polluted the landscape. At least 70 dams are said to be planned for the Amazon region, including the controversial Belo Monte hydroelectric dam.
Biodiversity of Brazil




The form of government is that of a democratic federative republic, with a presidential system. The president is both head of state and head of government of the Union and is elected for a four-year term, with the possibility of re-election for a second successive term. The current president is Michel Temer, who replaced Dilma Rousseff after her impeachment. The President appoints the Ministers of State, who assist in government. Legislative houses in each political entity are the main source of law in Brazil. The National Congress is the Federation's bicameral legislature, consisting of the Chamber of Deputies and the Federal Senate. Judiciary authorities exercise jurisdictional duties almost exclusively. Brazil is a democracy, according to the Democracy Index 2010.
The Brazilian Federation is the "indissoluble union" of the States, the Municipalities and the Federal District. The Union, the states and the Federal District, and the municipalities, are the "spheres of government". The federation is set on five fundamental principles: sovereignty, citizenship, dignity of human beings, the social values of labour and freedom of enterprise, and political pluralism. The classic tripartite branches of government (executive, legislative and judicial under a checks and balances system) are formally established by the Constitution. The executive and legislative are organized independently in all three spheres of government, while the judiciary is organized only at the federal and state/Federal District spheres.

All members of the executive and legislative branches are directly elected. Judges and other judicial officials are appointed after passing entry exams. For most of its democratic history, Brazil has had a multi-party system, proportional representation. Voting is compulsory for the literate between 18 and 70 years old and optional for illiterates and those between 16 and 18 or beyond 70.
Together with several smaller parties, four political parties stand out: Workers' Party (PT), Brazilian Social Democracy Party (PSDB), Brazilian Democratic Movement Party (PMDB) and Democrats (DEM). Fifteen political parties are represented in Congress. It is common for politicians to switch parties, and thus the proportion of congressional seats held by particular parties changes regularly. Almost all governmental and administrative functions are exercised by authorities and agencies affiliated to the Executive.




Brazilian law is based on Roman-Germanic traditions and civil law concepts prevail over common law practice. Most of Brazilian law is codified, although non-codified statutes also represent a substantial part, playing a complementary role. Court decisions set out interpretive guidelines; however, they are seldom binding on other specific cases. Doctrinal works and the works of academic jurists have strong influence in law creation and in law cases.

The legal system is based on the Federal Constitution, which was promulgated on 5 October 1988, and is the fundamental law of Brazil. All other legislation and court decisions must conform to its rules. As of April 2007, there have been 53 amendments. States have their own constitutions, which must not contradict the Federal Constitution. Municipalities and the Federal District have "organic laws" (leis orgânicas), which act in a similar way to constitutions. Legislative entities are the main source of statutes, although in certain matters judiciary and executive bodies may enact legal norms. Jurisdiction is administered by the judiciary entities, although in rare situations the Federal Constitution allows the Federal Senate to pass on legal judgments. There are also specialized military, labor, and electoral courts. The highest court is the Supreme Federal Court.
This system has been criticized over the last few decades for the slow pace of decision-making. Lawsuits on appeal may take several years to resolve, and in some cases more than a decade elapses before definitive rulings. Nevertheless, the Supreme Federal Tribunal was the first court in the world to transmit its sessions on television, and also via YouTube. More recently, in December 2009, the Supreme Court adopted Twitter to display items on the day planner of the ministers, to inform the daily actions of the Court and the most important decisions made by them.




The armed forces of Brazil are the second largest in Latin America by active personnel and the largest in terms of military equipment. It consists of the Brazilian Army (including the Army Aviation Command), the Brazilian Navy (including the Marine Corps and Naval Aviation), and the Brazilian Air Force. Brazil's conscription policy gives it one of the world's largest military forces, estimated at more than 1.6 million reservist annually.
Numbering close to 236,000 active personnel, the Brazilian Army has the largest number of armored vehicles in South America, including armored transports and tanks It is also unique in Latin America for its large, elite forces specializing in unconventional missions, the Brazilian Special Operations Command, and the versatile Strategic Rapid Action Force, made up of highly mobilized and prepared (Special Operations Brigade, Infantry Brigade Parachutist, 1st Jungle Infantry Battalion (Airmobile) and 12th Brigade Light Infantry (Airmobile) to act anywhere in the country, in short time, the external aggression hypothesis. The states' Military Police and the Military Firefighters Corps are described as an ancillary forces of the Army by the constitution, but are under the control of each state's governor.

The Navy once operated some of the most powerful warships in the world with the two Minas Geraes-class dreadnoughts, which sparked a South American dreadnought race between Argentina, Brazil, and Chile. Today, it is a green water force and has a group of specialized elite in retaking ships and naval facilities, GRUMEC, unit specially trained to protect Brazilian oil platforms along its coast. It's the only navy in Latin America that operates an aircraft carrier, NAe São Paulo, and one of the ten navies of the world to operate this type of ship.
The Air Force, it is the largest in Latin America has about 700 manned aircraft in service and effective about 67 thousand military.
Brazil has not been invaded since 1865 during the Paraguayan War. Additionally, Brazil has no contested territorial disputes with any of its neighbours and neither does it have rivalries, like Chile and Bolivia have with each other. The Brazilian military has also three times intervened militarily to overthrow the Brazilian government. It has built a tradition of participating in UN peacekeeping missions such as in Haiti and East Timor.




Brazil's international relations are based on Article 4 of the Federal Constitution, which establishes non-intervention, self-determination, international cooperation and the peaceful settlement of conflicts as the guiding principles of Brazil's relationship with other countries and multilateral organizations. According to the Constitution, the President has ultimate authority over foreign policy, while the Congress is tasked with reviewing and considering all diplomatic nominations and international treaties, as well as legislation relating to Brazilian foreign policy.
Brazil's foreign policy is a by-product of the country's unique position as a regional power in Latin America, a leader among developing countries, and an emerging world power. Brazilian foreign policy has generally been based on the principles of multilateralism, peaceful dispute settlement, and non-intervention in the affairs of other countries.
An increasingly well-developed tool of Brazil's foreign policy is providing aid as a donor to other developing countries. Brazil does not just use its growing economic strength to provide financial aid, but it also provides high levels of expertise and most importantly of all, a quiet non-confrontational diplomacy to improve governance levels. Total aid is estimated to be around $1 billion per year that includes:
technical cooperation of around $480 million ($30 million in 2010 provided directly by the Brazilian Cooperation Agency (ABC))
an estimated $450 million for in-kind expertise provided by Brazilian institutions specialising in technical cooperation
In addition, Brazil manages a peacekeeping mission in Haiti ($350 million) and makes in-kind contributions to the World Food Programme ($300 million). This is in addition to humanitarian assistance and contributions to multilateral development agencies. The scale of this aid places it on par with China and India. The Brazilian South-South aid has been described as a "global model in waiting."




In Brazil, the Constitution establishes five different police agencies for law enforcement: Federal Police Department, Federal Highway Police, Federal Railroad Police, Military Police and Civil Police. Of these, the first three are affiliated with federal authorities and the last two are subordinate to state governments. All police forces are the responsibility of the executive branch of any of the federal or state powers. The National Public Security Force also can act in public disorder situations arising anywhere in the country.
The country still has above-average levels of violent crime and particularly high levels of gun violence and homicide. In 2012, the World Health Organization (WHO) estimated the number of 32 deaths per 100,000 inhabitants, one of the highest rates of intentional homicide of the world. The number considered tolerable by the WHO is about 10 homicides per 100,000 inhabitants. However, there are differences between the crime rates in the Brazilian states. While in São Paulo the homicide rate registered in 2013 was 10.8 deaths per 100,000 inhabitants, in Alagoas it was 64.7 homicides per 100,000 inhabitants.
Brazil also has high levels of incarceration and the third largest prison population in the world (behind only China and the United States), with an estimated total of approximately 700,000 prisoners around the country (June 2014), an increase of about 300% compared to the index registered in 1992. The high number of prisoners eventually overloaded the Brazilian prison system, leading to a shortfall of about two hundred thousand accommodations.




Brazil is a federation composed of 26 States, one Federal district (which contains the capital city, Brasília) and Municipalities. States have autonomous administrations, collect their own taxes and receive a share of taxes collected by the Federal government. They have a governor and a unicameral legislative body elected directly by their voters. They also have independent Courts of Law for common justice. Despite this, states have much less autonomy to create their own laws than in the United States. For example, criminal and civil laws can be voted by only the federal bicameral Congress and are uniform throughout the country.
The states and the federal district may be grouped into regions: Northern, Northeast, Central-West, Southeast and Southern. The Brazilian regions are merely geographical, not political or administrative divisions, and they do not have any specific form of government. Although defined by law, Brazilian regions are useful mainly for statistical purposes, and also to define the distribution of federal funds in development projects.
Municipalities, as the states, have autonomous administrations, collect their own taxes and receive a share of taxes collected by the Union and state government. Each has a mayor and an elected legislative body, but no separate Court of Law. Indeed, a Court of Law organized by the state can encompass many municipalities in a single justice administrative division called comarca (county).




Brazil is the largest national economy in Latin America, the world's eight largest economy at market exchange rates and the seventh largest in purchasing power parity (PPP), according to the International Monetary Fund and the World Bank. Brazil has a mixed economy with abundant natural resources. After rapid growth in preceding decades, the country entered an ongoing recession in 2014 amid a political corruption scandal and nationwide protests.
Its GDP (PPP) per capita was $15,048 in 2016 putting Brazil in the 77th position according to IMF data. Active in agricultural, mining, manufacturing and service sectors Brazil has a labor force of over a 107 million (ranking 6th worldwide) and unemployment of 6.2% (ranking 64th worldwide). The country has been expanding its presence in international financial and commodities markets, and is one of a group of four emerging economies called the BRIC countries. Brazil has been the world's largest producer of coffee for the last 150 years.

Brazil has become the fourth largest car market in the world. Major export products include aircraft, electrical equipment, automobiles, ethanol, textiles, footwear, iron ore, steel, coffee, orange juice, soybeans and corned beef. In total, Brazil ranks 23rd worldwide in value of exports.
Brazil pegged its currency, the real, to the U.S. dollar in 1994. However, after the East Asian financial crisis, the Russian default in 1998 and the series of adverse financial events that followed it, the Central Bank of Brazil temporarily changed its monetary policy to a managed-float scheme while undergoing a currency crisis, until definitively changing the exchange regime to free-float in January 1999.
Brazil received an International Monetary Fund rescue package in mid-2002 of $30.4 billion, then a record sum. Brazil's central bank paid back the IMF loan in 2005, although it was not due to be repaid until 2006. One of the issues the Central Bank of Brazil recently dealt with was an excess of speculative short-term capital inflows to the country, which may have contributed to a fall in the value of the U.S. dollar against the real during that period. Nonetheless, foreign direct investment (FDI), related to long-term, less speculative investment in production, is estimated to be $193.8 billion for 2007. Inflation monitoring and control currently plays a major part in the Central bank's role of setting out short-term interest rates as a monetary policy measure.
Between 1993 and 2010, 7012 mergers & acquisitions with a total known value of $707 billion with the involvement of Brazilian firms have been announced. The year 2010 was a new record in terms of value with 115 billion USD of transactions. The largest transaction with involvement of Brazilian companies has been: Cia. Vale do Rio Doce acquired Inco in a tender offer valued at US$18.9 billion.
Corruption costs Brazil almost $41 billion a year alone, with 69.9% of the country's firms identifying the issue as a major constraint in successfully penetrating the global market. Local government corruption is so prevalent that voters perceive it as a problem only if it surpasses certain levels, and only if a local media e.g. a radio station is present to divulge the findings of corruption charges. Initiatives, like this exposure, strengthen awareness which is indicated by the Transparency International's Corruption Perceptions Index; ranking Brazil 69th out of 178 countries in 2012. The purchasing power in Brazil is eroded by the so-called Brazil cost.




Brazil's diversified economy includes agriculture, industry, and a wide range of services. Agriculture and allied sectors like forestry, logging and fishing accounted for 5.1% of the gross domestic product in 2007. Brazil is one of the largest producer of oranges, coffee, sugar cane, cassava and sisal, soybeans and papayas.
The industry – from automobiles, steel and petrochemicals to computers, aircraft and consumer durables – accounted for 30.8% of the gross domestic product. Industry is highly concentrated in metropolitan São Paulo, Rio de Janeiro, Campinas, Porto Alegre, and Belo Horizonte.
Brazil is the world's tenth largest energy consumer with much of its energy coming from renewable sources, particularly hydroelectricity and ethanol; the Itaipu Dam is the world's largest hydroelectric plant by energy generation. The first car with an ethanol engine was produced in 1978 and the first airplane engine running on ethanol in 2005. Recent oil discoveries in the Pre-salt layer have opened the door for a large increase in oil production. The governmental agencies responsible for the energy policy are the Ministry of Mines and Energy, the National Council for Energy Policy, the National Agency of Petroleum, Natural Gas and Biofuels, and the National Agency of Electricity.




Tourism in Brazil is a growing sector and key to the economy of several regions of the country. The country had 5 million visitors in 2010, ranking in terms of international tourist arrivals as the second destination in South America, and third in Latin America after Mexico and Argentina. Revenues from international tourists reached US$6 billion in 2010, showing a recovery from the 2008–2009 economic crisis. Historical records of 5.4 million visitors and US$6.8 billion in receipts were reached in 2011.
Natural areas are its most popular tourism product, a combination of ecotourism with leisure and recreation, mainly sun and beach, and adventure travel, as well as cultural tourism. Among the most popular destinations are the Amazon Rainforest, beaches and dunes in the Northeast Region, the Pantanal in the Center-West Region, beaches at Rio de Janeiro and Santa Catarina, cultural tourism in Minas Gerais and business trips to São Paulo city.
In terms of the 2015 Travel and Tourism Competitiveness Index (TTCI), which is a measurement of the factors that make it attractive to develop business in the travel and tourism industry of individual countries, Brazil ranked in the 28st place at the world's level, third in the Americas, after Canada and United States.
Brazil's main competitive advantages are its natural resources, which ranked 1st on this criteria out of all countries considered, and ranked 23rd for its cultural resources, due to its many World Heritage sites. The TTCI report notes Brazil's main weaknesses: its ground transport infrastructure remains underdeveloped (ranked 116th), with the quality of roads ranking in 105th place; and the country continues to suffer from a lack of price competitiveness (ranked 114th), due in part to high ticket taxes and airport charges, as well as high prices and high taxation. Safety and security have improved significantly: 75th in 2011, up from 128th in 2008.
According to the World Tourism Organization (WTO), international travel to Brazil accelerated in 2000, particularly during 2004 and 2005. However, in 2006 a slow-down took place, and international arrivals had almost no growth in 2007–08.

In spite of this trend, revenues from international tourism continued to rise, from USD 4 billion in 2005 to 5 billion in 2007, despite 330 000 fewer arrivals. This favorable trend is the result of the strong devaluation of the US dollar against the Brazilian Real, which began in 2004, but which makes Brazil a more expensive international destination. This trend changed in 2009, when both visitors and revenues fell as a result of the Great Recession of 2008–09. By 2010, the industry had recovered, and arrivals grew above 2006 levels to 5.2 million international visitors, and receipts from these visitors reached USD 6 billion. In 2011 the historical record was reached with 5.4 million visitors and US$6.8 billion in receipts.
Despite continuing record-breaking international tourism revenues, the number of Brazilian tourists travelling overseas has been growing steadily since 2003, resulting in a net negative foreign exchange balance, as more money is spent abroad by Brazilians than comes in as receipts from international tourists visiting Brazil. Tourism expenditures abroad grew from USD 5.8 billion in 2006, to USD 8.2 billion in 2007, a 42% increase, representing a net deficit of USD 3.3 billion in 2007, as compared to USD 1.5 billion in 2006, a 125% increase from the previous year. This trend is caused by Brazilians taking advantage of the stronger Real to travel and making relatively cheaper expenditures abroad. Brazilians traveling overseas in 2006 represented 4% of the country's population.
In 2005, tourism contributed with 3.2% of the country's revenues from exports of goods and services, and represented 7% of direct and indirect employment in the Brazilian economy. In 2006 direct employment in the sector reached 1.9 million people. Domestic tourism is a fundamental market segment for the industry, as 51 million people traveled throughout the country in 2005, and direct revenues from Brazilian tourists reached USD 22 billion, 5.6 times more receipts than international tourists in 2005.
In 2005, Rio de Janeiro, Foz do Iguaçu, São Paulo, Florianópolis and Salvador were the most visited cities by international tourists for leisure trips. The most popular destinations for business trips were São Paulo, Rio de Janeiro and Porto Alegre. In 2006 Rio de Janeiro and Fortaleza were the most popular destinations for business trips.







Technological research in Brazil is largely carried out in public universities and research institutes, with the majority of funding for basic research coming from various government agencies. Brazil's most esteemed technological hubs are the Oswaldo Cruz Institute, the Butantan Institute, the Air Force's Aerospace Technical Center, the Brazilian Agricultural Research Corporation and the INPE. The Brazilian Space Agency has the most advanced space program in Latin America, with significant resources to launch vehicles, and manufacture of satellite. Owner of relative technological sophistication, the country develops submarines, aircraft, as well as being involved in space research, having a Vehicle Launch Center Light and being the only country in the Southern Hemisphere the integrate team building International Space Station (ISS).
The country is also a pioneer in the search for oil in deep water, from where extracts 73% of its reserves. Uranium is enriched at the Resende Nuclear Fuel Factory, mostly for research purposes (as Brazil obtains 88% from its electricity from hydroelectricity) and the country's first nuclear submarine will be delivered in 2015 (by France). Brazil is one of the three countries in Latin America with an operational Synchrotron Laboratory, a research facility on physics, chemistry, material science and life sciences. And Brazil is the only Latin American country to have a semiconductor company with its own fabrication plant, the CEITEC. According to the Global Information Technology Report 2009-2010 of the World Economic Forum, Brazil is the 61 world's largest developer of information technology.
Brazil also has a large number of outstanding scientific personalities. Among the most renowned Brazilian inventors are priests Bartolomeu de Gusmão, Landell de Moura and Francisco João de Azevedo, besides Alberto Santos-Dumont, Evaristo Conrado Engelberg, Manuel Dias de Abreu, Andreas Pavel e Nélio José Nicolai. Brazilian science is represented by the likes of César Lattes (Brazilian physicist Pathfinder of Pi Meson), Mário Schenberg (considered the greatest theoretical physicist of Brazil), José Leite Lopes (only Brazilian physicist holder of UNESCO Science Prize), Artur Ávila (the first Latin American winner of Fields Medal) and Fritz Müller (pioneer in factual support the theory of evolution by Charles Darwin).




Brazilian roads are the primary carriers of freight and passenger traffic. The road system totaled 1.98 million km (1.23 million mi) in 2002. The total of paved roads increased from 35,496 km (22,056 mi) (22,056 mi) in 1967 to 184,140 km (114,419 mi) (114,425 mi) in 2002.
The first investments in road infrastructure have given up in the 1920s, the government of Washington Luis, being pursued in the governments of Getúlio Vargas and Eurico Gaspar Dutra. President Juscelino Kubitschek (1956–61), who designed and built the capital Brasília, was another supporter of highways. Kubitschek was responsible for the installation of major car manufacturers in the country (Volkswagen, Ford and General Motors arrived in Brazil during his rule) and one of the points used to attract them was, of course, support for the construction of highways. With the implementation of Fiat in 1976 ending an automobile market closed loop, from the end of the 1990s the country has received large foreign direct investments installing in its territory other major car manufacturers and utilities, such as Iveco, Renault, Peugeot, Citroen, Honda, Mitsubishi, Mercedes-Benz, BMW, Hyundai, Toyota among others. Brazil is the seventh most important country in the auto industry.
Brazil's railway system has been declining since 1945, when emphasis shifted to highway construction. The total length of railway track was 30,875 km (19,185 mi) in 2002, as compared with 31,848 km (19,789 mi) in 1970. Most of the railway system belonged to the Federal Railroad Corporation RFFSA, which was privatized in 2007. The São Paulo Metro was the first underground transit system in Brazil. The other metro systems are in Rio de Janeiro, Porto Alegre, Recife, Belo Horizonte, Brasília, Teresina and Fortaleza.
The country has an extensive rail network of 28,538 kilometres (17,733 miles) in length, the tenth largest network in the world. Currently, the Brazilian government, unlike the past, seeks to encourage this mode of transport; an example of this incentive is the project of the Rio–São Paulo high-speed rail, that will connect the two main cities of the country to carry passengers.
There are about 2,500 airports in Brazil, including landing fields: the second largest number in the world, after the United States. São Paulo-Guarulhos International Airport, near São Paulo, is the largest and busiest airport with nearly 20 million passengers annually, while handling the vast majority of commercial traffic for the country.
For freight transport waterways are of importance, e.g. the industrial zones of Manaus can be reached only by means of the Solimões- Amazonas waterway (3,250 kilometres (2,020 miles) with 6 metres (20 feet) minimum depth). The country also has 50,000 kilometres (31,000 miles) of waterways.
Coastal shipping links widely separated parts of the country. Bolivia and Paraguay have been given free ports at Santos. Of the 36 deep-water ports, Santos, Itajaí, Rio Grande, Paranaguá, Rio de Janeiro, Sepetiba, Vitória, Suape, Manaus and São Francisco do Sul are the most important. Bulk carriers have to wait up to 18 days before being serviced, container ships 36,3 hours on average.




Among the achievements in the water supply and sanitation sector is an increase in access to water piped on premises from 79% to 92% between 1990 and 2010; an increase in access to Improved sanitation from 68% to 79% in the same period; a functioning national system to finance water and sanitation infrastructure; a high level of cost recovery compared to most other developing countries; as well as a number of notable technical and financial innovations such as Condominial sewerage and an output-based subsidy for treated wastewater called PRODES.
Among the challenges is the still high number of poor Brazilians living in urban slums (Favela) and in rural areas without access to piped water or sanitation; water scarcity in the Northeast of Brazil; water pollution, especially in the South-East of the country; the low share of collected wastewater that is being treated (35% in 2000); and long-standing tensions between the federal, state and municipal governments about their respective roles in the sector.




The Brazilian public health system, the National Health System (SUS), is managed and provided by all levels of government. The public health services are universal and available to all citizens of the country for free. Nevertheless, millions of affluent Brazilians have private health care coverage.
According to the Brazilian Government, the most serious health problems are:
Childhood mortality: about 2.51% of childhood mortality, reaching 3.77% in the northeast region.
Motherhood mortality: about 73.1 deaths per 100,000 born children in 2002.
Mortality by non-transmissible illness: 151.7 deaths per 100,000 inhabitants caused by heart and circulatory diseases, along with 72.7 deaths per 100,000 inhabitants caused by cancer.
Mortality caused by external causes (transportation, violence and suicide): 71.7 deaths per 100,000 inhabitants (14.9% of all deaths in the country), reaching 82.3 deaths in the southeast region.
In 2002, Brazil accounted for 40% of malaria cases in the Americas. Nearly 99% are concentrated in the Legal Amazon Region, which is home to not more than 12% of the population.




The Federal Constitution and the Law of Guidelines and Bases of National Education determine that the Federal Government, States, Federal District and municipalities must manage and organize their respective education systems. Each of these public educational systems is responsible for its own maintenance, which manages funds as well as the mechanisms and funding sources. The constitution reserves 25% of the state budget and 18% of federal taxes and municipal taxes for education.
According to the IBGE, in 2011, the literacy rate of the population was 90.4%, meaning that 13 million (9.6% of population) people are still illiterate in the country; functional illiteracy has reached 21.6% of the population. Illiteracy is highest in the Northeast, where 19.9% of the population is illiterate.
Higher education starts with undergraduate or sequential courses, which may offer different options of specialization in academic or professional careers. Depending on the choice, students can improve their educational background with courses of post-graduate studies or broad sense. To attend a higher education institution is required, by Law of Guidelines and Bases of Education, completing all levels of education suited to the needs of all students of teaching kindergarten, elementary and medium, provided the student does not hold any disability, whether physical, mental, visual or hearing. Of the top 10 universities in Latin America, eight are Brazilian, according QS World University Rankings.




The Brazilian press has its beginnings in 1808 with the arrival of the Portuguese royal family to Brazil, hitherto forbidden any activity of the press – was the publication of newspapers or books. The Brazilian press was officially born in Rio de Janeiro on 13 May 1808, with the creation of the Royal Printing, National Press by the Prince Regent Dom João.
The Gazeta do Rio de Janeiro, the first newspaper published in the country, began to circulate on 10 September 1808. The largest newspapers nowadays are Folha de S.Paulo (from the state of São Paulo), Super Notícia (Minas Gerais 296.799), O Globo (RJ 277.876) and O Estado de S. Paulo (SP 235.217).
Radio broadcasting began on 7 September 1922, with a speech by then President Pessoa, and was formalized on 20 April 1923 with the creation of "Radio Society of Rio de Janeiro."
Television in Brazil began officially on 18 September 1950, with the founding of TV Tupi by Assis Chateaubriand. Since then television has grown in the country, creating large public networks such as Globo, SBT, Record and Bandeirantes. Today it is the most important factor in popular culture of Brazilian society, indicated by research showing that as much as 67% of the general population follow the same daily soap opera broadcast. Digital Television, using the SBTVD standard (based on the Japanese standard ISDB-T), was adopted 29 June 2006 and launched on 2 November 2007. In May 2010, Brazil launched TV Brasil Internacional, an international television station, initially broadcasting to 49 countries.




The population of Brazil, as recorded by the 2008 PNAD, was approximately 190 million (22.31 inhabitants per square kilometre or 57.8/sq mi), with a ratio of men to women of 0.95:1 and 83.75% of the population defined as urban. The population is heavily concentrated in the Southeastern (79.8 million inhabitants) and Northeastern (53.5 million inhabitants) regions, while the two most extensive regions, the Center-West and the North, which together make up 64.12% of the Brazilian territory, have a total of only 29.1 million inhabitants.
The first census in Brazil was carried out in 1872 and recorded a population of 9,930,478. From 1880 to 1930, 4 million Europeans arrived. Brazil's population increased significantly between 1940 and 1970, because of a decline in the mortality rate, even though the birth rate underwent a slight decline. In the 1940s the annual population growth rate was 2.4%, rising to 3.0% in the 1950s and remaining at 2.9% in the 1960s, as life expectancy rose from 44 to 54 years and to 72.6 years in 2007. It has been steadily falling since the 1960s, from 3.04% per year between 1950 and 1960 to 1.05% in 2008 and is expected to fall to a negative value of –0.29% by 2050 thus completing the demographic transition.
In 2008, the illiteracy rate was 11.48% and among the youth (ages 15–19) 1.74%. It was highest (20.30%) in the Northeast, which had a large proportion of rural poor. Illiteracy was high (24.18%) among the rural population and lower (9.05%) among the urban population.




According to the National Research by Household Sample (PNAD) of 2008, 48.43% of the population (about 92 million) described themselves as White; 43.80% (about 83 million) as Pardo (brown), 6.84% (about 13 million) as Black; 0.58% (about 1.1 million) as Asian; and 0.28% (about 536 thousand) as Amerindian (officially called indígena, Indigenous), while 0.07% (about 130 thousand) did not declare their race.
In 2007, the National Indian Foundation estimated that Brazil has 67 different uncontacted tribes, up from their estimate of 40 in 2005. Brazil is believed to have the largest number of uncontacted peoples in the world.

Since the arrival of the Portuguese in 1500, considerable miscegenation between Amerindians, Europeans, and Africans has taken place in all regions of the country (with European ancestry being dominant nationwide according to the vast majority of all autosomal studies undertaken covering the entire population, accounting for between 65% to 77%).
Brazilian society is more markedly divided by social class lines, although a high income disparity is found between race groups, so racism and classism can be conflated. Socially significant closeness to one racial group is taken in account more in the basis of appearance (phenotypes) rather than ancestry, to the extent that full siblings can pertain to different "racial" groups. Socioeconomic factors are also significant, because a minority of pardos are likely to start declaring themselves White or Black if socially upward. Skin color and facial features do not line quite well with ancestry (usually, Afro-Brazilians are evenly mixed and European ancestry is dominant in Whites and pardos with a significant non-European contribution, but the individual variation is great).
The brown population (officially called pardo in Portuguese, also colloquially moreno) is a broad category that includes caboclos (assimilated Amerindians in general, and descendants of Whites and Natives), mulatos (descendants of primarily Whites and Afro-Brazilians) and cafuzos (descendants of Afro-Brazilians and Natives). People of considerable Amerindian ancestry form the majority of the population in the Northern, Northeastern and Center-Western regions.
Higher percents of Blacks, mulattoes and tri-racials can be found in the eastern coast of the Northeastern region from Bahia to Paraíba and also in northern Maranhão, southern Minas Gerais and in eastern Rio de Janeiro. From the 19th century, Brazil opened its borders to immigration. About five million people from over 60 countries migrated to Brazil between 1808 and 1972, most of them of Portuguese, Italian, Spanish, German, Ukrainian, Polish, Jewish, Russian, Chinese, Japanese, and Arab origin.




Religion in Brazil formed from the meeting of the Catholic Church with the religious traditions of enslaved African peoples and indigenous peoples. This confluence of faiths during the Portuguese colonization of Brazil led to the development of a diverse array of syncretistic practices within the overarching umbrella of Brazilian Catholic Church, characterized by traditional Portuguese festivities, and in some instances, Allan Kardec's Spiritism (a religion which incorporates elements of spiritualism and Christianity). Religious pluralism increased during the 20th century, and the Protestant community has grown to include over 22% of the population. The most common Protestant denominations are Pentecostal and Evangelical ones. Other Protestant branches with a notable presence in the country include the Baptists, Seventh-day Adventists, Lutherans and the Reformed tradition.
Roman Catholicism is the country's predominant faith. Brazil has the world's largest Catholic population. According to the 2000 Demographic Census (the PNAD survey does not inquire about religion), 73.57% of the population followed Roman Catholicism; 15.41% Protestantism; 1.33% Kardecist spiritism; 1.22% other Christian denominations; 0.31% Afro-Brazilian religions; 0.13% Buddhism; 0.05% Judaism; 0.02% Islam; 0.01% Amerindian religions; 0.59% other religions, undeclared or undetermined; while 7.35% have no religion.
However, in the last ten years Protestantism, particularly Pentecostalism and Evangelicalism, has spread in Brazil, while the proportion of Catholics has dropped significantly. After Protestantism, individuals professing no religion are also a significant group, exceeding 7% of the population as of the 2000 census. The cities of Boa Vista, Salvador, and Porto Velho have the greatest proportion of Irreligious residents in Brazil. Teresina, Fortaleza, and Florianópolis were the most Roman Catholic in the country. Greater Rio de Janeiro, not including the city proper, is the most irreligious and least Roman Catholic Brazilian periphery, while Greater Porto Alegre and Greater Fortaleza are on the opposite sides of the lists, respectively.




According to IBGE (Brazilian Institute of Geography and Statistics) urban areas already concentrate 84.35% of the population, while the Southeast region remains the most populated one, with over 80 million inhabitants. The largest metropolitan areas in Brazil are São Paulo, Rio de Janeiro, and Belo Horizonte – all in the Southeastern Region – with 19.5, 11.5, and 5.1 million inhabitants respectively. The majority of state capitals are the largest cities in their states, except for Vitória, the capital of Espírito Santo, and Florianópolis, the capital of Santa Catarina. There are also non-capital metropolitan areas in the states of São Paulo (Campinas, Santos and the Paraíba Valley), Minas Gerais (Steel Valley), Rio Grande do Sul (Sinos Valley) and Santa Catarina (Joinville) and (Itajaí Valley).




The official language of Brazil is Portuguese (Article 13 of the Constitution of the Federal Republic of Brazil), which almost all of the population speaks and is virtually the only language used in newspapers, radio, television, and for business and administrative purposes. The most famous exception to this is a strong sign language law that was passed by the National Congress of Brazil. Legally recognized in 2002, the law was regulated in 2005. The law mandates the use of the Brazilian Sign Language, more commonly known by its Portuguese acronym LIBRAS, in education and government services. The language must be taught as a part of the education and speech and language pathology curricula. LIBRAS teachers, instructors and translators are recognized professionals. Schools and health services must provide access ("inclusion") to deaf people.
Brazilian Portuguese has had its own development, mostly similar to 16th-century Central and Southern dialects of European Portuguese (despite a very substantial number of Portuguese colonial settlers, and more recent immigrants, coming from Northern regions, and in minor degree Portuguese Macaronesia), with a few influences from the Amerindian and African languages, especially West African and Bantu restricted to the vocabulary only. As a result, the language is somewhat different, mostly in phonology, from the language of Portugal and other Portuguese-speaking countries (the dialects of the other countries, partly because of the more recent end of Portuguese colonialism in these regions, have a closer connection to contemporary European Portuguese). These differences are comparable to those between American and British English.
Brazil is the only Portuguese-speaking nation in the Americas, making the language an important part of Brazilian national identity and giving it a national culture distinct from those of its Spanish-speaking neighbors.

In 1990, the Community of Portuguese Language Countries (CPLP), which included representatives from all countries with Portuguese as the official language, reached an agreement on the reform of the Portuguese orthography to unify the two standards then in use by Brazil on one side and the remaining lusophone countries on the other. This spelling reform went into effect in Brazil on 1 January 2009. In Portugal, the reform was signed into law by the President on 21 July 2008 allowing for a 6-year adaptation period, during which both orthographies will co-exist. The remaining CPLP countries are free to establish their own transition timetables.
Minority languages are spoken throughout the nation. One hundred and eighty Amerindian languages are spoken in remote areas and a significant number of other languages are spoken by immigrants and their descendants. In the municipality of São Gabriel da Cachoeira, Nheengatu (a currently endangered South American creole language – or an 'anti-creole', according to some linguists – with mostly Indigenous Brazilian languages lexicon and Portuguese-based grammar that, together with its southern relative língua geral paulista, once was a major lingua franca in Brazil, being replaced by Portuguese only after governmental prohibition led by major political changes), Baniwa and Tucano languages had been granted co-official status with Portuguese.
There are significant communities of German (mostly the Brazilian Hunsrückisch, a High German language dialect) and Italian (mostly the Talian, a Venetian dialect) origins in the Southern and Southeastern regions, whose ancestors' native languages were carried along to Brazil, and which, still alive there, are influenced by the Portuguese language. Talian is officially a historic patrimony of Rio Grande do Sul, and two German dialects possess co-official status in a few municipalities.
Learning at least one second language (generally English or Spanish) is mandatory for all the 12 grades of the mandatory education system (primary and secondary education, there called ensino fundamental and ensino médio respectively). Brazil is the first country in South America to offer Esperanto to secondary students.




The core culture of Brazil is derived from Portuguese culture, because of its strong colonial ties with the Portuguese empire. Among other influences, the Portuguese introduced the Portuguese language, Roman Catholicism and colonial architectural styles. The culture was, however, also strongly influenced by African, indigenous and non-Portuguese European cultures and traditions.
Some aspects of Brazilian culture were influenced by the contributions of Italian, German and other European as well Japanese, Jewish and Arab immigrants who arrived in large numbers in the South and Southeast of Brazil. The indigenous Amerindians influenced Brazil's language and cuisine; and the Africans influenced language, cuisine, music, dance and religion.
Brazilian art has developed since the 16th century into different styles that range from Baroque (the dominant style in Brazil until the early 19th century) to Romanticism, Modernism, Expressionism, Cubism, Surrealism and Abstractionism. Brazilian cinema dates back to the birth of the medium in the late 19th century and has gained a new level of international acclaim since the 1960s.




The music of Brazil was formed mainly from the fusion of European and African elements. Until the nineteenth century, Portugal was the gateway to most of the influences that built Brazilian music, although many of these elements were not of Portuguese origin, but generally European. The first was José Maurício Nunes Garcia, author of sacred pieces with influence of Viennese classicism. The major contribution of the African element was the rhythmic diversity and some dances and instruments that had a bigger role in the development of popular music and folk, flourishing especially in the twentieth century.
Popular music since the late eighteenth century began to show signs of forming a characteristically Brazilian sound, with samba considered the most typical and on the UNESCO cultural heritage list. Maracatu and Afoxê are two Afro-Brazilian music traditions that have been popularized by their appearance in the annual Brazilian Carnivals. The sport of capoeira is usually played with its own music referred to as capoeira music, which is usually considered to be a call-and-response type of folk music.
Choro is a very popular music instrumental style. Its origins are in 19th-century Rio de Janeiro. In spite of the name, the style often has a fast and happy rhythm, characterized by virtuosity, improvisation, subtle modulations and full of syncopation and counterpoint. Bossa nova is also a well-known style of Brazilian music developed and popularized in the 1950s and 1960s. The phrase "bossa nova" means literally "new trend". A lyrical fusion of samba and jazz, bossa nova acquired a large following starting in the 1960s.




Brazilian literature dates back to the 16th century, to the writings of the first Portuguese explorers in Brazil, such as Pêro Vaz de Caminha, filled with descriptions of fauna, flora and commentary about the indigenous population that fascinated European readers.
Brazil produced significant works in Romanticism – novelists like Joaquim Manuel de Macedo and José de Alencar wrote novels about love and pain. Alencar, in his long career, also treated indigenous people as heroes in the Indigenist novels O Guarani, Iracema and Ubirajara. Machado de Assis, one of his contemporaries, wrote in virtually all genres and continues to gain international prestige from critics worldwide.
The Brazilian Modernism, evidenced by the Week of Modern Art in 1922, was concerned with a nationalist avant-garde literature, while Post-Modernism brought a generation of distinct poets like João Cabral de Melo Neto, Carlos Drummond de Andrade, Vinicius de Moraes, Cora Coralina, Graciliano Ramos, Cecília Meireles, and internationally known writers dealing with universal and regional subjects like Jorge Amado, João Guimarães Rosa, Clarice Lispector and Manuel Bandeira.




Brazilian cuisine varies greatly by region, reflecting the country's varying mix of indigenous and immigrant populations. This has created a national cuisine marked by the preservation of regional differences. Examples are Feijoada, considered the country's national dish; and regional foods such as vatapá, moqueca, polenta and acarajé.
The national beverage is coffee and cachaça is Brazil's native liquor. Cachaça is distilled from sugar cane and is the main ingredient in the national cocktail, Caipirinha.
An typical meal consists mostly of rice and beans with beef, salad, french fries and a fried egg. Often, it's mixed with cassava flour (farofa). Fried potatoes, fried cassava, fried banana, fried meat and fried cheese are very often eaten in lunch and served in most typical restaurants. Popular snacks are pastel (a fired pastry); coxinha (a variation of chicken croquete); pão de queijo (cheese bread and cassava flour / tapioca); pamonha (corn and milk paste); esfirra (A variation of Lebanese pastry); kibbeh (from Arabic cuisine); empanada (pastry) and empada, little salt pies filled with shrimps or heart of palm.
Brazil has a variety of candies such as brigadeiros (chocolate fudge balls), cocada (a coconut sweet), beijinhos (coconut truffles and clove) and romeu e julieta (cheese with a guava jam known as goiabada). Peanuts are used to make paçoca, rapadura and pé-de-moleque. Local common fruits like açaí, cupuaçu, mango, papaya, cocoa, cashew, guava, orange, lime, passionfruit, pineapple, and hog plum are turned in juices and used to make chocolates, popsicles and ice cream.




The Brazilian film industry began in the late 19th century, during the early days of the Belle Époque. While there were national film productions during the early 20th century, American films such as Rio the Magnificent were made in Rio de Janeiro to promote tourism in the city. The films Limite (1931) and Ganga Bruta (1933), the latter being produced by Adhemar Gonzaga through the prolific studio Cinédia, were poorly received at release and failed at the box office, but are acclaimed nowadays and placed among the finest Brazilian films of all time. The 1941 unfinished film It's All True was divided in four segments, two of which were filmed in Brazil and directed by Orson Welles; it was originally produced as part of the United States' Good Neighbor Policy during Getúlio Vargas' Estado Novo government.
During the 1960s the Cinema Novo movement rose to prominence with directors such as Glauber Rocha, Nelson Pereira dos Santos, Paulo Cesar Saraceni and Arnaldo Jabor. Rocha's films Deus e o Diabo na Terra do Sol (1964) and Terra em Transe (1967) are considered to be some of the greatest and most influential in Brazilian film history.
During the 1990s Brazil saw a surge of critical and commercial success with films such as O Quatrilho (1995), O Que É Isso, Companheiro? (1997) and Central do Brasil (1998), all of which were nominated for the Academy Award for Best Foreign Language Film, the latter receiving a Best Actress nomination for Fernanda Montenegro. The 2002 crime film City of God, directed by Fernando Meirelles, was critically acclaimed, scoring 90% on Rotten Tomatoes, being placed in Roger Ebert's Best Films of the Decade list and receiving four Academy Award nominations in 2004, including Best Director. Notable film festivals in Brazil include the São Paulo and Rio de Janeiro International Film Festivals and the Gramado Festival.




The most popular sport in Brazil is football. The Brazilian men's national team is ranked among the best in the world according to the FIFA World Rankings, and has won the World Cup tournament a record five times.
Volleyball, basketball, auto racing, and martial arts also attract large audiences. The Brazil men's national volleyball team, for example, currently holds the titles of the World League, World Grand Champions Cup, World Championship and the World Cup.
Some sport variations have their origins in Brazil: beach football, futsal (indoor football) and footvolley emerged in Brazil as variations of football. In martial arts, Brazilians developed Capoeira, Vale tudo, and Brazilian Jiu-Jitsu.
In auto racing, three Brazilian drivers have won the Formula One world championship eight times.
Brazil has hosted several high-profile international sporting events, like the 1950 FIFA World Cup and recently has hosted the 2014 FIFA World Cup. The São Paulo circuit, Autódromo José Carlos Pace, hosts the annual Grand Prix of Brazil.
São Paulo organized the IV Pan American Games in 1963, and Rio de Janeiro hosted the XV Pan American Games in 2007. On 2 October 2009, Rio de Janeiro was selected to host the 2016 Olympic Games and 2016 Paralympic Games, making it the first South American city to host the games and second in Latin America after Mexico City. Furthermore, the country hosted the FIBA Basketball World Cups in 1954 and 1963. At the 1963 event, the Brazil national basketball team won one of its two world championship titles.






Index of Brazil-related articles
Outline of Brazil
Brazilian passport
Visa policy of Brazil
Visa requirements for Brazilian citizens
List of diplomatic missions of Brazil













Government
Brazilian Federal Government
Official Tourist Guide of Brazil
Brazilian Institute of Geography and Statistics
General information
"Brazil". The World Factbook. Central Intelligence Agency. 
Brazil Encyclopædia Britannica entry
Brazil at UCB Libraries GovPubs
Brazil at DMOZ
Country Profile from the U.S. Library of Congress (1997)
Brazil from the BBC News
Key Development Forecasts for Brazil from International Futures
 Wikimedia Atlas of Brazil
 Geographic data related to Brazil at OpenStreetMapArgentina (/ˌɑːrdʒənˈtiːnə/; Spanish: [aɾxenˈtina]), officially the Argentine Republic (Spanish: República Argentina), is a federal republic in the southern half of South America. Sharing the bulk of the Southern Cone with its neighbor Chile to the west, the country is also bordered by Bolivia and Paraguay to the north, Brazil to the northeast, Uruguay and the South Atlantic Ocean to the east, and the Drake Passage to the south. With a mainland area of 2,780,400 km2 (1,073,500 sq mi), Argentina is the eighth-largest country in the world, the second largest in Latin America, and the largest Spanish-speaking one. The country is subdivided into twenty-three provinces (Spanish: provincias, singular provincia) and one autonomous city (ciudad autónoma), Buenos Aires, which is the federal capital of the nation (Spanish: Capital Federal) as decided by Congress. The provinces and the capital have their own constitutions, but exist under a federal system.
Argentina claims sovereignty over part of Antarctica, the Falkland Islands (Spanish: Islas Malvinas), and South Georgia and the South Sandwich Islands. The earliest recorded human presence in the area of modern-day Argentina dates back to the Paleolithic period. The country has its roots in Spanish colonization of the region during the 16th century. Argentina rose as the successor state of the Viceroyalty of the Río de la Plata, a Spanish overseas viceroyalty founded in 1776. The declaration and fight for independence (1810–1818) was followed by an extended civil war that lasted until 1861, culminating in the country's reorganization as a federation of provinces with Buenos Aires as its capital city. The country thereafter enjoyed relative peace and stability, with massive waves of European immigration radically reshaping its cultural and demographic outlook. The almost-unparalleled increase in prosperity led to Argentina becoming the seventh wealthiest developed nation in the world by the early 20th century.
After 1930 Argentina descended into political instability and periodic economic crisis that pushed it back into underdevelopment, though it nevertheless remained among the fifteen richest countries until the mid-20th century. Argentina retains its historic status as a middle power in international affairs, and is a prominent regional power in the Southern Cone and Latin America. Argentina has the second largest economy in South America, the third-largest in Latin America and is a member of the G-15 and G-20 major economies. It is also a founding member of the United Nations, World Bank, World Trade Organization, Mercosur, Union of South American Nations, Community of Latin American and Caribbean States and the Organization of Ibero-American States. It is the country with the highest Human Development Index in Latin America with a rating of "very high". Because of its stability, market size and growing high-tech sector, Argentina is classified as a high-income economy.



The name "Argentina" is derived from Latin argentum ("silver", plata in Spanish), a noun associated with the silver mountains legend, widespread among the first European explorers of the La Plata Basin.
The first written use of the name can be traced to La Argentina, a 1602 poem by Martín del Barco Centenera describing the region and the foundation of Buenos Aires. Although "Argentina" was already in common usage by the 18th century, the country was formally named "Viceroyalty of the Río de la Plata" by the Spanish Empire, and "United Provinces of the Río de la Plata" after independence.
The 1826 constitution included the first use of the name "Argentine Republic" in legal documents. The name "Argentine Confederation" was also commonly used and was formalized in the Argentine Constitution of 1853. In 1860 a presidential decree settled the country's name as "Argentine Republic", and that year's constitutional amendment ruled all the names since 1810 as legally valid.
In the English language the country was traditionally called "the Argentine", mimicking the typical Spanish usage la Argentina and perhaps resulting from a mistaken shortening of the fuller name 'Argentine Republic'. 'The Argentine' fell out of fashion during the mid-to-late 20th century, and now the country is simply referred to as "Argentina".
In the Spanish language "Argentina" is feminine ("La [República] Argentina"), taking the feminine article "La" as the initial syllable of "Argentina" is unstressed.







The earliest traces of human life in the area now known as Argentina are dated from the Paleolithic period, with further traces in the Mesolithic and Neolithic. Until the period of European colonization, Argentina was relatively sparsely populated by a wide number of diverse cultures with different social organizations, which can be divided into three main groups. The first group are basic hunters and food gatherers without development of pottery, such as the Selknam and Yaghan in the extreme south. The second group are advanced hunters and food gatherers which include the Puelche, Querandí and Serranos in the center-east; and the Tehuelche in the south—all of them conquered by the Mapuche spreading from Chile—and the Kom and Wichi in the north. The last group are farmers with pottery, like the Charrúa, Minuane and Guaraní in the northeast, with slash and burn semisedentary existence; the advanced Diaguita sedentary trading culture in the northwest, which was conquered by the Inca Empire around 1480; the Toconoté and Hênîa and Kâmîare in the country's center, and the Huarpe in the center-west, a culture that raised llama cattle and was strongly influenced by the Incas.




Europeans first arrived in the region with the 1502 voyage of Amerigo Vespucci. The Spanish navigators Juan Díaz de Solís and Sebastian Cabot visited the territory that is now Argentina in 1516 and 1526, respectively. In 1536 Pedro de Mendoza founded the small settlement of Buenos Aires, which was abandoned in 1541.
Further colonization efforts came from Paraguay—establishing the Governorate of the Río de la Plata—Peru and Chile. Francisco de Aguirre founded Santiago del Estero in 1553. Londres was founded in 1558; Mendoza, in 1561; San Juan, in 1562; San Miguel de Tucumán, in 1565. Juan de Garay founded Santa Fe in 1573 and the same year Jerónimo Luis de Cabrera set up Córdoba. Garay went further south to re-found Buenos Aires in 1580. San Luis was established in 1596.
The Spanish Empire subordinated the economic potential of the Argentine territory to the immediate wealth of the silver and gold mines in Bolivia and Peru, and as such it became part of the Viceroyalty of Peru until the creation of the Viceroyalty of the Río de la Plata in 1776 with Buenos Aires as its capital.
Buenos Aires repelled two ill-fated British invasions in 1806 and 1807. The ideas of the Age of Enlightenment and the example of the first Atlantic Revolutions generated criticism of the absolutist monarchy that ruled the country. As in the rest of Spanish America, the overthrow of Ferdinand VII during the Peninsular War created great concern.




Beginning a process from which Argentina was to emerge as successor state to the Viceroyalty, the 1810 May Revolution replaced the viceroy Baltasar Hidalgo de Cisneros with the First Junta, a new government in Buenos Aires composed by locals. In the first clashes of the Independence War the Junta crushed a royalist counter-revolution in Córdoba, but failed to overcome those of the Banda Oriental, Upper Peru and Paraguay, which later became independent states.
Revolutionaries split into two antagonist groups: the Centralists and the Federalists—a move that would define Argentina's first decades of independence. The Assembly of the Year XIII appointed Gervasio Antonio de Posadas as Argentina's first Supreme Director.
In 1816 the Congress of Tucumán formalized the Declaration of Independence. One year later General Martín Miguel de Güemes stopped royalists on the north, and General José de San Martín took an army across the Andes and secured the independence of Chile; then he led the fight to the Spanish stronghold of Lima and proclaimed the independence of Peru. In 1819 Buenos Aires enacted a centralist constitution that was soon abrogated by federalists.
The 1820 Battle of Cepeda, fought between the Centralists and the Federalists, resulted in the end of the Supreme Director rule. In 1826 Buenos Aires enacted another centralist constitution, with Bernardino Rivadavia being appointed as the first president of the country. However, the interior provinces soon rose against him, forced his resignation and discarded the constitution. Centralists and Federalists resumed the civil war; the latter prevailed and formed the Argentine Confederation in 1831, led by Juan Manuel de Rosas. During his regime he faced a French blockade (1838–1840), the War of the Confederation (1836–1839), and a combined Anglo-French blockade (1845–1850), but remained undefeated and prevented further loss of national territory. His trade restriction policies, however, angered the interior provinces and in 1852 Justo José de Urquiza, another powerful caudillo, beat him out of power. As new president of the Confederation, Urquiza enacted the liberal and federal 1853 Constitution. Buenos Aires seceded but was forced back into the Confederation after being defeated in the 1859 Battle of Cepeda.




Overpowering Urquiza in the 1861 Battle of Pavón, Bartolomé Mitre secured Buenos Aires predominance and was elected as the first president of the reunified country. He was followed by Domingo Faustino Sarmiento and Nicolás Avellaneda; these three presidencies set up the bases of the modern Argentine State.
Starting with Julio Argentino Roca in 1880, ten consecutive federal governments emphasized liberal economic policies. The massive wave of European immigration they promoted—second only to the United States'—led to a near-reinvention of Argentine society and economy that by 1908 had placed the country as the seventh wealthiest developed nation in the world. Driven by this immigration wave and decreasing mortality, the Argentine population grew fivefold and the economy 15-fold: from 1870 to 1910 Argentina's wheat exports went from 100,000 to 2,500,000 t (110,000 to 2,760,000 short tons) per year, while frozen beef exports increased from 25,000 to 365,000 t (28,000 to 402,000 short tons) per year, placing Argentina as one of the world's top five exporters. Its railway mileage rose from 503 to 31,104 km (313 to 19,327 mi). Fostered by a new public, compulsory, free and secular education system, literacy skyrocketed from 22% to 65%, a level higher than most Latin American nations would reach even fifty years later. Furthermore, real GDP grew so fast that despite the huge immigration influx, per capita income between 1862 and 1920 went from 67% of developed country levels to 100%: In 1865, Argentina was already one of the top 25 nations by per capita income which by 1908, it had surpassed Denmark, Canada and The Netherlands to reach 7th place—behind Switzerland, New Zealand, Australia, the United States, the United Kingdom and Belgium. Argentina's per capita income was 70% higher than Italy's, 90% higher than Spain's, 180% higher than Japan's and 400% higher than Brazil's. Despite these unique achievements, the country was slow to meet its original goals of industrialization: after steep development of capital-intensive local industries in the 1920s, a significant part of the manufacture sector remained labor-intensive in the 1930s.
In 1912, President Roque Sáenz Peña enacted universal and secret male suffrage, which allowed Hipólito Yrigoyen, leader of the Radical Civic Union (or UCR), to win the 1916 election. He enacted social and economic reforms and extended assistance to family farmers and small businesses. Argentina stayed neutral during World War I. The second administration of Yrigoyen faced an economic crisis, influenced by the Great Depression.




In 1930, Yrigoyen was ousted from power by the military led by José Félix Uriburu. Although Argentina remained among the fifteen richest countries until mid-century, this coup d'état marks the start of the steady economic and social decline that pushed the country back into underdevelopment.
Uriburu ruled for two years; then Agustín Pedro Justo was elected in a fraudulent election, and signed a controversial treaty with the United Kingdom. Argentina stayed neutral during World War II, a decision that had full British support but was rejected by the United States after the attack on Pearl Harbor. A new military coup toppled the government, and Argentina declared war on the Axis Powers a month before the end of World War II in Europe. The minister of welfare, Juan Domingo Perón, was fired and jailed because of his high popularity among workers. His liberation was forced by a massive popular demonstration, and he went on to win the 1946 election.




Perón created a political movement known as Peronism. He nationalized strategic industries and services, improved wages and working conditions, paid the full external debt and achieved nearly full employment. The economy, however, began to decline in 1950 because of over-expenditure. His highly popular wife, Eva Perón, played a central political role. She pushed Congress to enact women's suffrage in 1947, and developed an unprecedented social assistance to the most vulnerable sectors of society. However, her declining health did not allow her to run for the vice-presidency in 1951, and she died of cancer the following year. Perón was reelected in 1951, even surpassing his 1946 performance. In 1955 the Navy bombed the Plaza de Mayo in an ill-fated attempt to kill the President. A few months later, during the self-called Liberating Revolution coup, he resigned and went into exile in Spain.
The new head of State, Pedro Eugenio Aramburu, proscribed Peronism and banned all of its manifestations; nevertheless, Peronists kept organized underground. Arturo Frondizi from the UCR won the following elections. He encouraged investment to achieve energetic and industrial self-sufficiency, reversed a chronic trade deficit and lifted Peronism proscription; yet his efforts to stay in good terms with Peronists and the military earned him the rejection of both and a new coup forced him out. But Senate Chief José María Guido reacted swiftly and applied the anti-power vacuum legislation, becoming president instead; elections were repealed and Peronism proscribed again. Arturo Illia was elected in 1963 and led to an overall increase in prosperity; however his attempts to legalize Peronism resulted in his overthrow in 1966 by the Juan Carlos Onganía-led coup d'état called the Argentine Revolution, a new military government that sought to rule indefinitely.




The "Dirty War" (Spanish: Guerra Sucia) was the name used by the Argentine Government for a period of state terrorism in Argentina against political dissidents, with military and security forces conducting urban and rural guerrilla violence against left-wing guerrillas, political dissidents, and anyone believed to be associated with socialism. Victims of the violence included an estimated 15,000 to 30,000 left-wing activists and militants, including trade unionists, students, journalists, Marxists, Peronist guerrillas and alleged sympathizers. Some 10,000 of the "disappeared" were believed to be guerrillas of the Montoneros (MPM), and the Marxist People's Revolutionary Army (ERP). The guerrillas were responsible for causing at least 6,000 casualties among the military, police forces and civilian population according to a National Geographic Magazine article in the mid-1980s. The disappeared ones were considered to be a political or ideological threat to the military junta and their disappearances an attempt to silence the opposition and break the determination of the guerillas.

Declassified documents of the Chilean secret police cite an official estimate by the Batallón de Inteligencia 601 of 22,000 killed or "disappeared" between 1975 and mid-1978. During this period, in which it was later revealed 8,625 "disappeared" in the form of PEN (Poder Ejecutivo Nacional, anglicized as "National Executive Power") detainees who were held in clandestine detention camps throughout Argentina before eventually being freed under diplomatic pressure. The number of people believed to have been killed or "disappeared," depending on the source, range from 9,089 to 30,000 in the period from 1976 to 1983, when the military was forced from power following Argentina's defeat in the Falklands War. The National Commission on the Disappearance of Persons estimates that around 13,000 were disappeared.
After democratic government was restored, Congress passed legislation to provide compensation to victims' families. Some 11,000 Argentines have applied to the relevant authorities and received up to US $200,000 each as monetary compensation for the loss of loved ones during the military dictatorship.
The exact chronology of the repression is still debated, however, as in some senses the long political war started in 1969. Trade unionists were targeted for assassination by the Peronist and Marxist paramilitary as early as 1969, and individual cases of state-sponsored terrorism against Peronism and the left can be traced back to the Bombing of Plaza de Mayo in 1955. The Trelew massacre of 1972, the actions of the Argentine Anticommunist Alliance since 1973, and Isabel Martínez de Perón's "annihilation decrees" against left-wing guerrillas during Operativo Independencia (translates to Operation of Independence) in 1975, have also been suggested as dates for the beginning of the Dirty War.
Onganía shut down Congress, banned all political parties and dismantled student and worker unions. In 1969, popular discontent led to two massive protests: the Cordobazo and the Rosariazo. The terrorist guerrilla organization Montoneros kidnapped and executed Aramburu. The newly chosen head of government, Alejandro Agustín Lanusse, seeking to ease the growing political pressure, let Héctor José Cámpora be the Peronist candidate instead of Perón. Cámpora won the March 1973 election, issued a pardon for condemned guerrilla members and then secured Perón's return from his exile in Spain.
On the day Perón returned to Argentina, the clash between Peronist internal factions—right-wing union leaders and left-wing youth from Montoneros—resulted in the Ezeiza Massacre. Cámpora resigned, overwhelmed by political violence, and Perón won the September 1973 election with his third wife Isabel as vice-president. He expelled Montoneros from the party and they became once again a clandestine organization. José López Rega organized the Argentine Anticommunist Alliance (AAA) to fight against them and the People's Revolutionary Army (ERP). Perón died in July 1974 and was succeeded by his wife, who signed a secret decree empowering the military and the police to "annihilate" the left-wing subversion, stopping ERP's attempt to start a rural insurgence in Tucumán province. Isabel Perón was ousted one year later by a junta of the three armed forces, led by army general Jorge Rafael Videla. They initiated the National Reorganization Process, often shortened to Proceso.
The Proceso shut down Congress, removed the judges of the Supreme Court, banned political parties and unions, and resorted to the forced disappearance of suspected guerrilla members and of anyone believed to be associated with the left-wing. By the end of 1976 Montoneros had lost near 2,000 members; by 1977, the ERP was completely defeated. A severely weakened Montoneros launched a counterattack in 1979, which was quickly annihilated, ending the guerrilla threat. Nevertheless, the junta stayed in power. Then head of state General Leopoldo Galtieri launched Operation Rosario, which escalated into the Falklands War (Spanish: Guerra de Malvinas); within two months Argentina was defeated by the United Kingdom. Reynaldo Bignone replaced Galtieri and began to organize the transition to democratic rule.




Raúl Alfonsín won the 1983 elections campaigning for the prosecution of those responsible for human rights violations during the Proceso: the Trial of the Juntas and other martial courts sentenced all the coup's leaders but, under military pressure, he also enacted the Full Stop and Due Obedience laws, which halted prosecutions further down the chain of command. The worsening economic crisis and hyperinflation reduced his popular support and the Peronist Carlos Menem won the 1989 election. Soon after, riots forced Alfonsín to an early resignation.
Menem embraced neo-liberal policies: a fixed exchange rate, business deregulation, privatizations and dismantling of protectionist barriers normalized the economy for a while. He pardoned the officers who had been sentenced during Alfonsín's government. The 1994 Constitutional Amendment allowed Menem to be elected for a second term. The economy began to decline in 1995, with increasing unemployment and recession; led by Fernando de la Rúa, the UCR returned to the presidency in the 1999 elections.

De la Rúa kept Menem's economic plan despite the worsening crisis, which led to growing social discontent. A massive capital flight was responded to with a freezing of bank accounts, generating further turmoil. The December 2001 riots forced him to resign. Congress appointed Eduardo Duhalde as acting president, who abrogated the fixed exchange rate established by Menem. By the late 2002 the economic crisis began to recess, but the assassination of two piqueteros by the police caused political commotion, prompting Duhalde to move elections forward. Néstor Kirchner was elected as the new president.
Boosting the neo-Keynesian economic policies laid by Duhalde, Kirchner ended the economic crisis attaining significant fiscal and trade surpluses, and steep GDP growth. Under his administration Argentina restructured its defaulted debt with an unprecedented discount of about 70% on most bonds, paid off debts with the International Monetary Fund, purged the military of officers with doubtful human rights records, nullified and voided the Full Stop and Due Obedience laws, ruled them as unconstitutional, and resumed legal prosecution of the Juntas' crimes. He did not run for reelection, promoting instead the candidacy of his wife, senator Cristina Fernández de Kirchner, who was elected in 2007 and reelected in 2011.
On 22 November 2015, after a tie in the first round of presidential elections on 25 October, Mauricio Macri won the first ballotage in Argentina's history, beating Front for Victory candidate Daniel Scioli and becoming president-elect. Macri is the first democratically elected non-radical or peronist president since 1916, although he had the support of the first mentioned. He took office on 10 December 2015. In April 2016, the Macri Government introduced austerity measures intended to tackle inflation and public deficits.




With a mainland surface area of 2,780,400 km2 (1,073,518 sq mi), Argentina is located in southern South America, sharing land borders with Chile across the Andes to the west; Bolivia and Paraguay to the north; Brazil to the northeast, Uruguay and the South Atlantic Ocean to the east; and the Drake Passage to the south; for an overall land border length of 9,376 km (5,826 mi). Its coastal border over the Río de la Plata and South Atlantic Ocean is 5,117 km (3,180 mi) long.
Argentina's highest point is Aconcagua in the Mendoza province (6,959 m (22,831 ft) above sea level), also the highest point in the Southern and Western Hemispheres. The lowest point is Laguna del Carbón in the San Julián Great Depression Santa Cruz province (−105 m (−344 ft) below sea level, also the lowest point in the Southern and Western Hemispheres, and the seventh lowest point on Earth)
The northernmost point is at the confluence of the Grande de San Juan and Río Mojinete rivers in Jujuy province; the southernmost is Cape San Pío in Tierra del Fuego province; the easternmost is northeast of Bernardo de Irigoyen, Misiones and the westernmost is within Los Glaciares National Park in Santa Cruz province. The maximum north–south distance is 3,694 km (2,295 mi), while the maximum east–west one is 1,423 km (884 mi).
Some of the major rivers are the Paraná, Uruguay—which join to form the Río de la Plata, Paraguay, Salado, Negro, Santa Cruz, Pilcomayo, Bermejo and Colorado. These rivers are discharged into the Argentine Sea, the shallow area of the Atlantic Ocean over the Argentine Shelf, an unusually wide continental platform. Its waters are influenced by two major ocean currents: the warm Brazil Current and the cold Falklands Current.




Argentina is divided into seven geographical regions:
Northwest, a continuation of the high Puna with even higher, more rugged topography to the far-west; the arid precordillera, filled with narrow valleys or quebradas to the mid-west; and an extension of the mountainous Yungas jungles to the east.
Mesopotamia, a subtropical wedge covering the western Paraná Plateau and neighboring lowlands enclosed by the Paraná and Uruguay rivers.
Gran Chaco, a large, subtropical and tropical low-lying, gently sloping alluvial plain between Mesopotamia and the Andes.
Sierras Pampeanas, a series of medium-height mountain chains located in the center.
Cuyo, a basin and range area in the central Andes piedmont, to the west.
Pampas, a massive and hugely fertile alluvial plain located in the center east.
Patagonia, a large southern plateau consisting mostly of arid, rocky steppes to the east; with moister cold grasslands to the south and dense subantarctic forests to the west.




Argentina is a megadiverse country hosting one of the greatest ecosystem varieties in the world: 15 continental zones, 3 oceanic zones, and the Antarctic region are all represented in its territory. This huge ecosystem variety has led to a biological diversity that is among the world's largest:
9,372 cataloged vascular plant species (ranked 24th)
1,038 cataloged bird species (ranked 14th)
375 cataloged mammal species (ranked 12th)
338 cataloged reptilian species (ranked 16th)
162 cataloged amphibian species (ranked 19th)




Although the most populated areas are generally temperate, Argentina has an exceptional climate diversity, ranging from subtropical in the north to subpolar in the far south. The average annual precipitation ranges from 150 millimetres (6 in) in the driest parts of Patagonia to over 2,000 millimetres (79 in) in the westernmost parts of Patagonia and the northeastern parts of the country. Mean annual temperatures range from 5 °C (41 °F) in the far south to 25 °C (77 °F) in the north.
Major wind currents include the cool Pampero Winds blowing on the flat plains of Patagonia and the Pampas; following the cold front, warm currents blow from the north in middle and late winter, creating mild conditions. The Sudestada usually moderates cold temperatures but brings very heavy rains, rough seas and coastal flooding. It is most common in late autumn and winter along the central coast and in the Río de la Plata estuary. The Zonda, a hot dry wind, affects Cuyo and the central Pampas. Squeezed of all moisture during the 6,000 m (19,685 ft) descent from the Andes, Zonda winds can blow for hours with gusts up to 120 km/h (75 mph), fueling wildfires and causing damage; between June and November, when the Zonda blows, snowstorms and blizzard (viento blanco) conditions usually affect higher elevations.







Argentina is a federal constitutional republic and representative democracy. The government is regulated by a system of checks and balances defined by the Constitution of Argentina, the country's supreme legal document. The seat of government is the city of Buenos Aires, as designated by Congress. Suffrage is universal, equal, secret and mandatory.
The federal government is composed of three branches:
The Legislative branch consists of the bicameral Congress, made up of the Senate and Deputy chambers, which makes federal law, declares war, approves treaties and has the power of the purse and of impeachment, by which it can remove sitting members of the government. The Chamber of Deputies represents the people and has 257 voting members elected to a four-year term. Seats are apportioned among the provinces by population every tenth year. As of 2014 ten provinces have just five deputies while the Buenos Aires Province, being the most populous one, has 70. The Chamber of Senators represents the provinces, has 72 members elected at-large to six-year terms, with each province having three seats; one third of Senate seats are up for election every other year. At least one-third of the candidates presented by the parties must be women.

In the Executive branch, the President is the commander-in-chief of the military, can veto legislative bills before they become law—subject to Congressional override—and appoints the members of the Cabinet and other officers, who administer and enforce federal laws and policies. The President is elected directly by the vote of the people, serves a four-year term and may be elected to office no more than twice in a row.
The Judicial branch includes the Supreme Court and lower federal courts interpret laws and overturn those they find unconstitutional. The Judicial is independent of the Executive and the Legislative. The Supreme Court has seven members appointed by the President—subject to Senate approval—who serve for life. The lower courts' judges are proposed by the Council of Magistrates (a secretariat composed of representatives of judges, lawyers, researchers, the Executive and the Legislative), and appointed by the President on Senate approval.




Argentina is a federation of twenty-three provinces and one autonomous city, Buenos Aires. Provinces are divided for administration purposes into departments and municipalities, except for Buenos Aires Province, which is divided into partidos. The City of Buenos Aires is divided into communes.
Provinces hold all the power that they chose not to delegate to the federal government; they must be representative republics and must not contradict the Constitution. Beyond this they are fully autonomous: they enact their own constitutions, freely organize their local governments, and own and manage their natural and financial resources. Some provinces have bicameral legislatures, while others have unicameral ones.
During the War of Independence the main cities and their surrounding countrysides became provinces though the intervention of their cabildos. The Anarchy of the Year XX completed this process, shaping the original thirteen provinces. Jujuy seceded from Salta in 1834, and the thirteen provinces became fourteen. After seceding for a decade, Buenos Aires accepted the 1853 Constitution of Argentina in 1861, and was made a federal territory in 1880.
An 1862 law designated as national territories those under federal control but outside the frontiers of the provinces. In 1884 they served as bases for the establishment of the governorates of Misiones, Formosa, Chaco, La Pampa, Neuquén, Río Negro, Chubut, Santa Cruz and Tierra del Fuego. The agreement about a frontier dispute with Chile in 1900 created the National Territory of Los Andes; its lands were incorporated into Jujuy, Salta and Catamarca in 1943. La Pampa and Chaco became provinces in 1951. Misiones did so in 1953, and Formosa, Neuquén, Río Negro, Chubut and Santa Cruz, in 1955. The last national territory, Tierra del Fuego, became the Tierra del Fuego, Antártida e Islas del Atlántico Sur Province in 1990. Argentina is divided into seven main geographical regions, many provinces having their territories across more than one.




Foreign policy is officially handled by the Ministry of Foreign Affairs, International Trade and Worship, which answers to the President.
An historical and current middle power, Argentina bases its foreign policies on the guiding principles of non-intervention, human rights, self-determination, international cooperation, disarmament and peaceful settlement of conflicts. The country is one of the G-15 and G-20 major economies of the world, and a founding member of the UN, WBG, WTO and OAS. In 2012 Argentina was elected again to a two-year non-permanent position on the United Nations Security Council and is participating in major peacekeeping operations in Haiti, Cyprus, Western Sahara and the Middle East.
A prominent Latin American and Southern Cone regional power, Argentina co-founded OEI, CELAC and UNASUR, of which the former president Néstor Kirchner was first Secretary General. It is also a founding member of the Mercosur block, having Brazil, Paraguay, Uruguay and Venezuela as partners. Since 2002 the country has emphasized its key role in Latin American integration, and the block—which has some supranational legislative functions—is its first international priority.
Argentina claims 965,597 km2 (372,819 sq mi) in Antarctica, where it has the world's oldest continuous state presence, since 1904. This overlaps claims by Chile and the United Kingdom, though all such claims fall under the provisions of the 1961 Antarctic Treaty, of which Argentina is a founding signatory and permanent consulting member, with the Antarctic Treaty Secretariat being based in Buenos Aires.
Argentina disputes sovereignty over the Falkland Islands (Spanish: Islas Malvinas), and South Georgia and the South Sandwich Islands, which are administered by the United Kingdom as Overseas Territories.




The President holds the title of commander-in-chief of the Argentine Armed Forces, as part of a legal framework that imposes a strict separation between national defense and internal security systems:
The National Defense System, an exclusive responsibility of the federal government, coordinated by the Ministry of Defense, and comprising the Army, the Navy and the Air Force. Ruled and monitored by Congress through the Houses' Defense Committees, it is organized on the essential principle of legitimate self-defense: the repelling of any external military aggression in order to guarantee freedom of the people, national sovereignty, and territorial integrity. Its secondary missions include committing to multinational operations within the framework of the United Nations, participating in internal support missions, assisting friendly countries, and establishing a sub-regional defense system.

Military service is voluntary, with enlistment age between 18 and 24 years old and no conscription. Argentina's defense has historically been one of the best equipped in the region, even managing its own weapon research facilities, shipyards, ordnance, tank and plane factories. However, real military expenditures declined steadily after 1981 and the defense budget in 2011 was about 0.74% of GDP, a historical minimum, below the Latin American average.
The Interior Security System, jointly administered by the federal and subscribing provincial governments. At the federal level it is coordinated by the Interior, Security and Justice ministries, and monitored by Congress. It is enforced by the Federal Police; the Prefecture, which fulfills coast guard duties; the Gendarmerie, which serves border guard tasks; and the Airport Security Police. At the provincial level it is coordinated by the respective internal security ministries and enforced by local police agencies.
Argentina was the only South American country to send warships and cargo planes in 1991 to the Gulf War under UN mandate and has remained involved in peacekeeping efforts in multiple locations like UNPROFOR in Croatia/Bosnia, Gulf of Fonseca, UNFICYP in Cyprus (where among Army and Marines troops the Air Force provided the UN Air contingent since 1994) and MINUSTAH in Haiti. Argentina is the only Latin American country to maintain troops in Kosovo during SFOR (and later EUFOR) operations where combat engineers of the Argentine Armed Forces are embedded in an Italian brigade.
In 2007, an Argentine contingent including helicopters, boats and water purification plants was sent to help Bolivia against their worst floods in decades. In 2010 the Armed Forces were also involved in Haiti and Chile humanitarian responses after their respective earthquakes.




Benefiting from rich natural resources, a highly literate population, a diversified industrial base, and an export-oriented agricultural sector, the economy of Argentina is Latin America's third-largest, and the second largest in South America. It has a "very high" rating on the Human Development Index and a relatively high GDP per capita, with a considerable internal market size and a growing share of the high-tech sector.

A middle emerging economy and one of the world's top developing nations, Argentina is a member of the G-20 major economies. Historically, however, its economic performance has been very uneven, with high economic growth alternating with severe recessions, income maldistribution and—in the recent decades—increasing poverty. Early in the 20th century Argentina achieved development, and became the world's seventh richest country. Although managing to keep a place among the top fifteen economies until mid-century, it suffered a long and steady decline and now it's just an upper middle-income country.
High inflation—a weakness of the Argentine economy for decades—has become a trouble once again, with rates in 2013 between the official 10.2% and the privately estimated 25%, causing heated public debate over manipulated statistics. Income distribution, having improved since 2002, is classified as "medium", still considerably unequal.
Argentina ranks 107th out of 175 countries in the Transparency International's 2014 Corruption Perceptions Index. While the country has settled most of its debts, it faces a technical debt crisis since 31 July 2014. A New York judge blocked Argentina's payments to 93% of its bonds unless it pays to "Vulture funds" the full value of the defaulted bonds they bought after its 2001 default. Argentina vowed not to capitulate to what it considered the ransom tactics of the funds.




In 2012 manufacturing accounted for 20.3% of GDP—the largest goods-producing sector in the nation's economy. Well-integrated into Argentine agriculture, half of the industrial exports have rural origin.
With a 6.5% production growth rate in 2011, the diversified manufacturing sector rests on a steadily growing network of industrial parks (314 as of 2013)
In 2012 the leading sectors by volume were: food processing, beverages and tobacco products; motor vehicles and auto parts; textiles and leather; refinery products and biodiesel; chemicals and pharmaceuticals; steel, aluminum and iron; industrial and farm machinery; home appliances and furniture; plastics and tires; glass and cement; and recording and print media. In addition, Argentina has since long been one of the top five wine-producing countries in the world. However, it has also been classified as one of the 74 countries where instances of child labor and forced labor have been observed and mentioned in a 2014 report published by the Bureau of International Labor Affairs. The ILAB's List of Goods Produced by Child Labor or Forced Labor shows that many of the goods produced by child labor and/or forced labor comes from the agricultural sector.
Córdoba is Argentina's major industrial center, hosting metalworking, motor vehicle and auto parts manufactures. Next in importance are the Greater Buenos Aires area (food processing, metallurgy, motor vehicles and auto parts, chemicals and petrochemicals, consumer durables, textiles and printing); Rosario (food processing, metallurgy, farm machinery, oil refining, chemicals, and tanning); San Miguel de Tucumán (sugar refining); San Lorenzo (chemicals and pharmaceuticals); San Nicolás de los Arroyos (steel milling and metallurgy); and Ushuaia and Bahía Blanca (oil refining). Other manufacturing enterprises are located in the provinces of Santa Fe (zinc and copper smelting, and flour milling); Mendoza and Neuquén (wineries and fruit processing); Chaco (textiles and sawmills); and Santa Cruz, Salta and Chubut (oil refining)
The electric output of Argentina in 2009 totaled over 122 TWh (440 PJ), of which about 37% was consumed by industrial activities.




Argentina has the largest railway system in Latin America, with 36,966 km (22,970 mi) of operating lines in 2008, out of a full network of almost 48,000 km (29,826 mi). This system links all 23 provinces plus Buenos Aires City, and connects with all neighboring countries. There are four incompatible gauges in use; this forces virtually all interregional freight traffic to pass through Buenos Aires. The system has been in decline since the 1940s: regularly running up large budgetary deficits, by 1991 it was transporting 1,400 times less goods than it did in 1973. However, in recent years the system has experienced a greater degree of investment from the state, in both commuter rail lines and long distance lines, renewing rolling stock and infrastructure. In April 2015, by overwhelming majority the Argentine Senate passed a law which re-created Ferrocarriles Argentinos (2015), effectively re-nationalising the country's railways, a move which saw support from all major political parties on both sides of the political spectrum.

By 2004 Buenos Aires, all provincial capitals except Ushuaia, and all medium-sized towns were interconnected by 69,412 km (43,131 mi) of paved roads, out of a total road network of 231,374 km (143,769 mi). Most important cities are linked by a growing number of expressways, including Buenos Aires–La Plata, Rosario–Córdoba, Córdoba–Villa Carlos Paz, Villa Mercedes–Mendoza, National Route 14 General José Gervasio Artigas and Provincial Route 2 Juan Manuel Fangio, among others. Nevertheless, this road infrastructure is still inadequate and cannot handle the sharply growing demand caused by deterioration of the railway system.
In 2012 there were about 11,000 km (6,835 mi) of waterways, mostly comprising the La Plata, Paraná, Paraguay and Uruguay rivers, with Buenos Aires, Zárate, Campana, Rosario, San Lorenzo, Santa Fe, Barranqueras and San Nicolas de los Arroyos as the main fluvial ports. Some of the largest sea ports are La Plata–Ensenada, Bahía Blanca, Mar del Plata, Quequén–Necochea, Comodoro Rivadavia, Puerto Deseado, Puerto Madryn, Ushuaia and San Antonio Oeste. Buenos Aires has historically been the most important port; however since the 1990s the Up-River port region has become dominant: stretching along 67 km (42 mi) of the Paraná river shore in Santa Fe province, it includes 17 ports and in 2013 accounted for 50% of all exports.
In 2013 there were 161 airports with paved runways out of more than a thousand. The Ezeiza International Airport, about 35 km (22 mi) from downtown Buenos Aires, is the largest in the country, followed by Cataratas del Iguazú in Misiones, and El Plumerillo in Mendoza. Aeroparque, in the city of Buenos Aires, is the most important domestic airport.




Print media industry is highly developed in Argentina, with more than two hundred newspapers. The major national ones include Clarín (centrist, Latin America's best-seller and the second most widely circulated in the Spanish-speaking world), La Nación (center-right, published since 1870), Página/12 (leftist, founded in 1987), the Buenos Aires Herald (Latin America's most prestigious English language daily, liberal, dating back to 1876), La Voz del Interior (center, founded in 1904), and the Argentinisches Tageblatt (German weekly, liberal, published since 1878)
Argentina began the world's first regular radio broadcasting on 27 August 1920, when Richard Wagner's Parsifal was aired by a team of medical students led by Enrique Telémaco Susini in Buenos Aires' Teatro Coliseo. By 2002 there were 260 AM and 1150 FM registered radio stations in the country.
The Argentine television industry is large, diverse and popular across Latin America, with many productions and TV formats having been exported abroad. Since 1999 Argentines enjoy the highest availability of cable and satellite television in Latin America, as of 2014 totaling 87.4% of the country's households, a rate similar to those in the United States, Canada and Europe.
By 2011 Argentina also had the highest coverage of networked telecommunications among Latin American powers: about 67% of its population had internet access and 137.2%, mobile phone subscriptions.




Argentines have three Nobel Prizes laureates in the Sciences. Bernardo Houssay, the first Latin American among them, discovered the role of pituitary hormones in regulating glucose in animals. César Milstein did extensive research in antibodies. Luis Leloir discovered how organisms store energy converting glucose into glycogen and the compounds which are fundamental in metabolizing carbohydrates. Argentine research has led to the treatment of heart diseases and several forms of cancer. Domingo Liotta designed and developed the first artificial heart successfully implanted in a human being in 1969. René Favaloro developed the techniques and performed the world's first ever coronary bypass surgery.
Argentina's nuclear programme has been highly successful. In 1957 Argentina was the first country in Latin America to design and build a research reactor with homegrown technology, the RA-1 Enrico Fermi. This reliance in the development of own nuclear related technologies, instead of simply buying them abroad, was a constant of Argentina's nuclear programme conducted by the civilian National Atomic Energy Commission (CNEA). Nuclear facilities with Argentine technology have been built in Peru, Algeria, Australia and Egypt. In 1983, the country admitted having the capability of producing weapon-grade uranium, a major step needed to assemble nuclear weapons; since then, however, Argentina has pledged to use nuclear power only for peaceful purposes. As a member of the Board of Governors of the International Atomic Energy Agency, Argentina has been a strong voice in support of nuclear non-proliferation efforts and is highly committed to global nuclear security. In 1974 it was the first country in Latin America to put in-line a commercial nuclear power plant, Atucha I. Although the Argentine built parts for that station amounted to 10% of the total, the nuclear fuel it uses are since entirely built in the country. Later nuclear power stations employed a higher percentage of Argentine built components; Embalse, finished in 1983, a 30% and the 2011 Atucha II reactor a 40%.

Despite its modest budget and numerous setbacks, academics and the sciences in Argentina have enjoyed an international respect since the turn of the 1900s, when Dr. Luis Agote devised the first safe and effective means of blood transfusion as well as René Favaloro, who was a pioneer in the improvement of the coronary artery bypass surgery. Argentine scientists are still on the cutting edge in fields such as nanotechnology, physics, computer sciences, molecular biology, oncology, ecology, and cardiology. Juan Maldacena, an Argentine-American scientist, is a leading figure in string theory.
Space research has also become increasingly active in Argentina. Argentine built satellites include LUSAT-1 (1990), Víctor-1 (1996), PEHUENSAT-1 (2007), and those developed by CONAE, the Argentine space agency, of the SAC series. Argentina has its own satellite programme, nuclear power station designs (4th generation) and public nuclear energy company INVAP, which provides several countries with nuclear reactors. Established in 1991, the CONAE has since launched two satellites successfully and, in June 2009, secured an agreement with the European Space Agency for the installation of a 35-m diameter antenna and other mission support facilities at the Pierre Auger Observatory, the world's foremost cosmic ray observatory. The facility will contribute to numerous ESA space probes, as well as CONAE's own, domestic research projects. Chosen from 20 potential sites and one of only three such ESA installations in the world, the new antenna will create a triangulation which will allow the ESA to ensure mission coverage around the clock 




Tourism in Argentina is characterized by its cultural offerings and its ample and varied natural assets. The country had 5.57 million visitors in 2013, ranking in terms of the international tourist arrivals as the top destination in South America, and second in Latin America after Mexico. Revenues from international tourists reached US$4.41 billion in 2013, down from US$4.89 billion in 2012. The country's capital city, Buenos Aires, is the most visited city in South America. There are 30 National Parks of Argentina including many World Heritage Sites in Argentina.




The tariffs for water supply and sanitation in Argentina are relatively low, the service quality reasonable. However, according to the WHO, 21% of the total population remains without access to house connections and 52% of the urban population do not have access to sewerage.
Between 1991 and 1999, as part of one of the world's largest privatization programs, water and sanitation concessions with the private sector were signed. After the 2001 economic crisis, many concessions were renegotiated.
Most service providers barely recover operation and maintenance costs and have no capacity to self-finance investments. While private operators were able to achieve higher levels of cost recovery, since the Argentine financial crisis in 2002 tariffs have been frozen and the self-financing capacity of utilities has disappeared.




In the 2001 census [INDEC], Argentina had a population of 36,260,130, and preliminary results from the 2010 census were of 40,091,359 inhabitants. Argentina ranks third in South America in total population and 33rd globally. Population density is of 15 persons per square kilometer of land area, well below the world average of 50 persons. The population growth rate in 2010 was an estimated 1.03% annually, with a birth rate of 17.7 live births per 1,000 inhabitants and a mortality rate of 7.4 deaths per 1,000 inhabitants. The net migration rate has ranged from zero to four immigrants per 1,000 inhabitants per year.
The proportion of people under 15 is 25.6%, a little below the world average of 28%, and the proportion of people 65 and older is relatively high at 10.8%. In Latin America this is second only to Uruguay and well above the world average, which is currently 7%. Argentina has one of Latin America's lowest population growth rates, recently about 1% a year, as well as a comparatively low infant mortality rate. Its birth rate of 2.3 children per woman is still nearly twice as high as that in Spain or Italy, compared here as they have similar religious practices and proportions. The median age is approximately 30 years and life expectancy at birth is 77.14 years.
Argentina became in 2010 the first country in Latin America and the second in the Americas to allow same-sex marriage nationwide. It was the tenth country to allow same-sex marriage.




As with other areas of new settlement such as the United States, Canada, Australia, New Zealand, Brazil and Uruguay, Argentina is considered a country of immigrants. Argentines usually refer to the country as a crisol de razas (crucible of races, or melting pot).
During the 19th and 20th centuries especially, Argentina was the country with the second biggest immigration wave in the world, with 6.6 million, second only to the United States in the numbers of immigrants received (27 millions) and ahead of such other areas of new settlement like Canada, Brazil and Australia.
Strikingly, at those times, the national population doubled every two decades. This belief is endured in the popular saying "los argentinos descienden de los barcos" (Argentines descend from the ships). Therefore, most Argentines are descended from the 19th- and 20th-century immigrants of the great immigration wave to Argentina (1850–1955), with a great majority of these immigrants coming from diverse European countries. The majority of these European immigrants came from Italy and Spain. The majority of Argentines descend from multiple European ethnic groups, primarily of Italian and Spanish descent (over 25 million individuals in Argentina, almost 60% of the population have some partial Italian origins), while 17% of the population also have partial French origins, and a sizeable number of Germans.
Argentina is home to a significant population of Arab and partial Arab background, mostly of Syrian and Lebanese origin (in Argentina they are considered among the white people, just like in the United States Census), The majority of Arab Argentines are Christians who belong to the Maronite Church, Roman Catholic, Eastern Orthodox and Eastern Rite Catholic Churches. A scant number are Muslims of Middle Eastern origins. The Asian population in the country numbers at around 180,000 individuals, most of whom are of Chinese and Korean descent, although an older Japanese community that traces back to the early 20th century still exists.
A study conducted on 218 individuals in 2010 by the Argentine geneticist Daniel Corach, has established that the genetic map of Argentina is composed by 79% from different European ethnicities (mainly Spanish and Italian ethnicities), 18% of different indigenous ethnicities, and 4.3% of African ethnic groups, in which 63.6% of the tested group had at least one ancestor who was Indigenous.
From the 1970s, immigration has mostly been coming from Bolivia, Paraguay and Peru, with smaller numbers from Dominican Republic, Ecuador and Romania. The Argentine government estimates that 750,000 inhabitants lack official documents and has launched a program to encourage illegal immigrants to declare their status in return for two-year residence visas—so far over 670,000 applications have been processed under the program.




The de facto official language is Spanish, spoken by almost all Argentines. The country is the largest Spanish-speaking society that universally employs voseo, the use of the pronoun vos instead of tú ("you"), which imposes the use of alternate verb forms as well. Due to the extensive Argentine geography, Spanish has a strong variation among regions, although the prevalent dialect is Rioplatense, primarily spoken in the La Plata Basin and accented similarly to Neapolitan language. Italian and other European immigrants influenced Lunfardo—the regional slang—permeating the vernacular vocabulary of other Latin American countries as well.
There are several second-languages in widespread use among the Argentine population:
English, taught since elementary school. 42.3% of Argentines claim to speak it, with 15.4% of them claiming to have a high level of language comprehension.
Italian, by 1.5 million people.
Arabic, specially its Northern Levantine dialect, by one million people.
Standard German, by 400,000 people.
Yiddish, by 200,000 people, the largest Jewish population in Latin America and 7th in the world.
Guaraní, by 200,000 people, mostly in Corrientes (where it is official de jure) and Misiones.
Catalan, by 174,000 people.
French, including the rare Occitan language.
Quechua, by 65,000 people, mostly in the Northwest.
Wichí, by 53,700 people, mainly in Chaco where, along with Kom and Moqoit, it is official de jure.
Vlax Romani, by 52,000 people.
Japanese, by 32,000 people.
Aymara, by 30,000 people, mostly in the Northwest.
Ukrainian, by 27,000 people.
Welsh, including its Patagonian dialect, in which 25,000 people are fluent. Some districts have recently incorporated it as an educational language.




The Constitution guarantees freedom of religion. Although it enforces neither an official nor a state faith, it gives Roman Catholicism a differential status.
According to a CONICET poll, Argentines are 76.5% Catholic, 11.3% Agnostics and Atheists, 9% Evangelical Protestants, 1.2% Jehovah's Witnesses, 0.9% Mormons; 1.2% follow other religions, including Islam, Judaism and Buddhism.
The country is home to both the largest Muslim and largest Jewish communities in Latin America, the latter being the 7th most populous in the world. Argentina is a member of the International Holocaust Remembrance Alliance.
Argentines show high individualization and de-institutionalization of religious beliefs; 23.8% of them claim to always attend religious services; 49.1%, to seldom do and 26.8%, to never do.
On 13 March 2013, Argentine Jorge Mario Bergoglio, the Cardinal Archbishop of Buenos Aires, was elected Bishop of Rome and Supreme Pontiff of the Catholic Church. He took the name "Francis", and he became the first Pope from either the Americas or from the Southern Hemisphere. He is the first Pope born outside of Europe since the election of Pope Gregory III (who was Syrian) in 741. He is also the first Jesuit Pope.




Argentina is highly urbanized, with 92% of its population living in cities: the ten largest metropolitan areas account for half of the population. About 3 million people live in the city of Buenos Aires, and including the Greater Buenos Aires metropolitan area it totals around 13 million, making it one of the largest urban areas in the world.
The metropolitan areas of Córdoba and Rosario have around 1.3 million inhabitants each. Mendoza, San Miguel de Tucumán, La Plata, Mar del Plata, Salta and Santa Fe have at least half a million people each.
The population is unequally distributed: about 60% live in the Pampas region (21% of the total area), including 15 million people in Buenos Aires province. The provinces of Córdoba and Santa Fe, and the city of Buenos Aires have 3 million each. Seven other provinces have over one million people each: Mendoza, Tucumán, Entre Ríos, Salta, Chaco, Corrientes and Misiones. With 64.3 inhabitants per square kilometre (167/sq mi), Tucumán is the only Argentine province more densely populated than the world average; by contrast, the southern province of Santa Cruz has around 1.1/km2 (2.8/sq mi).




The Argentine education system consists of four levels:
An initial level for children between 45 days to 5 years old, with the last two years being compulsory.
An elementary or lower school mandatory level lasting 6 or 7 years. In 2010 the literacy rate was 98.07%.
A secondary or high school mandatory level lasting 5 or 6 years. In 2010 18.3% of people over age 15 had completed secondary school.
A higher level, divided in tertiary, university and post-graduate sub-levels. in 2013 there were 47 national public universities across the country, as well as 46 private ones. In 2010 6.3% of people over age 20 had graduated from university. The public universities of Buenos Aires, Córdoba, La Plata, Rosario, and the National Technological University are some of the most important.
The Argentine state guarantees universal, secular and free-of-charge public education for all levels. Responsibility for educational supervision is organized at the federal and individual provincial states. In the last decades the role of the private sector has grown across all educational stages.




Health care is provided through a combination of employer and labor union-sponsored plans (Obras Sociales), government insurance plans, public hospitals and clinics and through private health insurance plans. Health care cooperatives number over 300 (of which 200 are related to labor unions) and provide health care for half the population; the national INSSJP (popularly known as PAMI) covers nearly all of the five million senior citizens.
There are more than 153,000 hospital beds, 121,000 physicians and 37,000 dentists (ratios comparable to developed nations). The relatively high access to medical care has historically resulted in mortality patterns and trends similar to developed nations': from 1953 to 2005, deaths from cardiovascular disease increased from 20% to 23% of the total, those from tumors from 14% to 20%, respiratory problems from 7% to 14%, digestive maladies (non-infectious) from 7% to 11%, strokes a steady 7%, injuries, 6%, and infectious diseases, 4%. Causes related to senility led to many of the rest. Infant deaths have fallen from 19% of all deaths in 1953 to 3% in 2005.
The availability of health care has also reduced infant mortality from 70 per 1000 live births in 1948 to 12.1 in 2009 and raised life expectancy at birth from 60 years to 76. Though these figures compare favorably with global averages, they fall short of levels in developed nations and in 2006, Argentina ranked fourth in Latin America.




Argentina is a multicultural country with significant European influences. Its cities are largely characterized by both the prevalence of people of European descent, and of conscious imitation of European styles in fashion, architecture and design. Modern Argentine culture has been largely influenced by Italian, Spanish and other European immigration like France, United Kingdom, Germany among others. Argentina is largely characterized by both the prevalence of people of European descent, and of conscious imitation of European styles in architecture. Museums, cinemas, and galleries are abundant in all the large urban centers, as well as traditional establishments such as literary bars, or bars offering live music of a variety of genres although there are lesser elements of Amerindian and African influences, particularly in the fields of music and art.  The other big influence is the gauchos and their traditional country lifestyle of self-reliance. Finally, indigenous American traditions have been absorbed into the general cultural milieu. Argentine writer Ernesto Sabato has reflected on the nature of the culture of Argentina as follows:




Although Argentina's rich literary history began around 1550, it reached full independence with Esteban Echeverría's El Matadero, a romantic landmark that played a significant role in the development of 19th century's Argentine narrative, split by the ideological divide between the popular, federalist epic of José Hernández' Martín Fierro and the elitist and cultured discourse of Sarmiento's masterpiece, Facundo.
The Modernist movement advanced into the 20th century including exponents such as Leopoldo Lugones and poet Alfonsina Storni; it was followed by Vanguardism, with Ricardo Güiraldes's Don Segundo Sombra as an important reference.
Jorge Luis Borges, Argentina's most acclaimed writer and one of the foremost figures in the history of literature, found new ways of looking at the modern world in metaphor and philosophical debate and his influence has extended to authors all over the globe. Short stories such as Ficciones and The Aleph are among his most famous books. He was a friend and collaborator with Adolfo Bioy Casares, who wrote one of the most praised science fiction novels, The Invention of Morel. Julio Cortázar, one of the leading members of the Latin American Boom and a major name in 20th century literature, influenced an entire generation of writers in the Americas and Europe.
Other highly regarded Argentine writers, poets and essayists include Estanislao del Campo, Eugenio Cambaceres, Pedro Bonifacio Palacios, Hugo Wast, Benito Lynch, Enrique Banchs, Oliverio Girondo, Ezequiel Martínez Estrada, Victoria Ocampo, Leopoldo Marechal, Silvina Ocampo, Roberto Arlt, Eduardo Mallea, Manuel Mujica Láinez, Ernesto Sábato, Silvina Bullrich, Rodolfo Walsh, María Elena Walsh, Tomás Eloy Martínez, Manuel Puig, Alejandra Pizarnik, and Osvaldo Soriano.




Tango, a Rioplatense musical genre with European and African influences, is one of Argentina's international cultural symbols. The golden age of tango (1930 to mid-1950s) mirrored that of jazz and swing in the United States, featuring large orchestras like those of Osvaldo Pugliese, Aníbal Troilo, Francisco Canaro, Julio de Caro and Juan d'Arienzo. After 1955, virtuoso Astor Piazzolla popularized Nuevo tango, a subtler and more intellectual trend for the genre. Tango enjoys worldwide popularity nowadays with groups like Gotan Project, Bajofondo and Tanghetto.
Argentina developed strong classical music and dance scenes that gave rise to renowned artists such as Alberto Ginastera, composer; Alberto Lysy, violinist; Martha Argerich and Eduardo Delgado, pianists; Daniel Barenboim, pianist and symphonic orchestra director; José Cura and Marcelo Álvarez, tenors; and to ballet dancers Jorge Donn, José Neglia, Norma Fontenla, Maximiliano Guerra, Paloma Herrera, Marianela Núñez, Iñaki Urlezaga and Julio Bocca.
A national Argentine folk style emerged in the 1930s from dozens of regional musical genres and went to influence the entirety of Latin American music. Some of its interpreters, like Atahualpa Yupanqui and Mercedes Sosa, achieved worldwide acclaim.
The romantic ballad genre included singers of international fame such as Sandro de América.
Argentine rock developed as a distinct musical style in the mid-1960s, when Buenos Aires and Rosario became cradles of aspiring musicians. Founding bands like Los Gatos, Sui Generis, Almendra and Manal were followed by Seru Giran, Los Abuelos de la Nada, Soda Stereo and Patricio Rey y sus Redonditos de Ricota, with prominent artists including Gustavo Cerati, Litto Nebbia, Andrés Calamaro, Luis Alberto Spinetta, Charly García, Fito Páez and León Gieco.
Tenor saxophonist Leandro "Gato" Barbieri and composer and big band conductor Lalo Schifrin are among the most internationally successful Argentine jazz musicians.




Buenos Aires is one of the great theater capitals of the world, with a scene of international caliber centered on Corrientes Avenue, "the street that never sleeps", sometimes referred to as an intellectual Broadway in Buenos Aires. Teatro Colón is a global landmark for opera and classical performances; its acoustics are considered among the world's top five. Other important theatrical venues include Teatro General San Martín, Cervantes, both in Buenos Aires City; Argentino in La Plata, El Círculo in Rosario, Independencia in Mendoza, and Libertador in Córdoba. Griselda Gambaro, Copi, Roberto Cossa, Marco Denevi, Carlos Gorostiza, and Alberto Vaccarezza are a few of the most prominent Argentine playwrights.
Argentine theatre traces its origins to Viceroy Juan José de Vértiz y Salcedo's creation of the colony's first theatre, La Ranchería, in 1783. In this stage, in 1786, a tragedy entitled Siripo had its premiere. Siripo is now a lost work (only the second act is conserved), and can be considered the first Argentine stage play, because it was written by Buenos Aires poet Manuel José de Lavardén, it was premiered in Buenos Aires, and its plot was inspired by an historical episode of the early colonization of the Río de la Plata Basin: the destruction of Sancti Spiritu colony by aboriginals in 1529. La Ranchería theatre operated until its destruction in a fire in 1792. The second theatre stage in Buenos Aires was Teatro Coliseo, opened in 1804 during the term of Viceroy Rafael de Sobremonte. It was the nation's longest-continuously operating stage. The musical creator of the Argentine National Anthem, Blas Parera, earned fame as a theatre score writer during the early 19th century. The genre suffered during the regime of Juan Manuel de Rosas, though it flourished alongside the economy later in the century. The national government gave Argentine theatre its initial impulse with the establishment of the Colón Theatre, in 1857, which hosted classical and operatic, as well as stage performances. Antonio Petalardo's successful 1871 gambit on the opening of the Teatro Opera, inspired others to fund the growing art in Argentina.




The Argentine film industry has historically been one of the three most developed in Latin American cinema, along with those produced in Mexico and Brazil. Started in 1896; by the early 1930s it had already become Latin America's leading film producer, a place it kept until the early 1950s. The world's first animated feature films were made and released in Argentina, by cartoonist Quirino Cristiani, in 1917 and 1918.

Argentine films have achieved worldwide recognition: the country has won two Academy Award for Best Foreign Language Film, with The Official Story (1985) and The Secret in Their Eyes (2009) with seven nominations:
The Truce (La Tregua) in 1974
Camila (Camila) in 1984
The Official Story (La Historia Oficial) in 1985
Tango (Tango) in 1998
Son of the Bride (El hijo de la novia) in 2001
The Secret in Their Eyes (El Secreto de sus Ojos) in 2009
Wild Tales (Relatos Salvajes) in 2015
In addition, Argentine composers Luis Enrique Bacalov and Gustavo Santaolalla have been honored with Academy Award for Best Original Score in 2006 and 2007 nods and Armando Bo and Nicolás Giacobone have been honored with Academy Award for Best Original Screenplay in 2015. Also, the Argentine French actress Berenice Bejo received a nomination for the Academy Award for Best Supporting Actress in 2011 and won the César Award for Best Actress and won the Best Actress award in the Cannes Film Festival for her role in the film The Past.
Argentina also has won sixteen Goya Awards for Best Spanish Language Foreign Film with A King and His Movie (1986), A Place in the World (1992), Gatica, el mono (1993), Autumn Sun (1996), Ashes of Paradise (1997), The Lighthouse (1998), Burnt Money (2000), The Escape (2001), Intimate Stories (2003), Blessed by Fire (2005), The Hands (2006), XXY (2007), The Secret in Their Eyes (2009), Chinese Take-Away (2011), Wild Tales (2014) and The Clan (2015) being by far the most awarded in Latin America with twenty three nominations.
Many other Argentine films have been acclaimed by the international critique: Camila (1984), Man Facing Southeast (1986), A Place in the World (1992), Pizza, Beer, and Cigarettes (1997), Nine Queens (2000), A Red Bear (2002), The Motorcycle Diaries (2004), The Aura (2005), Chinese Take-Away (2011) and Wild Tales (2014) being some of them.
In 2013 about 100 full-length motion pictures were being created annually.




Some of the best-known Argentine painters are Cándido López and Florencio Molina Campos (Naïve style); Ernesto de la Cárcova and Eduardo Sívori (Realism); Fernando Fader (Impressionism); Pío Collivadino, Atilio Malinverno and Cesáreo Bernaldo de Quirós (Postimpressionism); Emilio Pettoruti (Cubism); Julio Barragán (Concretism and Cubism) Antonio Berni (Neofigurativism); Roberto Aizenberg and Xul Solar (Surrealism); Gyula Košice (Constructivism); Eduardo Mac Entyre (Generative art); Luis Seoane, Carlos Torrallardona, Luis Aquino, and Alfredo Gramajo Gutiérrez (Modernism); Lucio Fontana (Spatialism); Tomás Maldonado and Guillermo Kuitca (Abstract art); León Ferrari and Marta Minujín (Conceptual art); and Gustavo Cabral (Fantasy art).
In 1946 Gyula Košice and others created The Madí Movement in Argentina, which then spread to Europe and United States, where it had a significant impact. Tomás Maldonado was one of the main theorists of the Ulm Model of design education, still highly influential globally.
Other Argentine artists of worldwide fame include Adolfo Bellocq, whose lithographs have been influential since the 1920s, and Benito Quinquela Martín, the quintessential port painter, inspired by the immigrant-bound La Boca neighborhood.
Internationally laureate sculptors Erminio Blotta, Lola Mora and Rogelio Yrurtia authored many of the classical evocative monuments of the Argentine cityscape.




The colonization brought the Spanish Baroque architecture, which can still be appreciated in its simpler Rioplatense style in the reduction of San Ignacio Miní, the Cathedral of Córdoba, and the Cabildo of Luján. Italian and French influences increased at the beginning of the 19th century with strong eclectic overtones that gave the local architecture a unique feeling.
Numerous Argentine architects have enriched their own country's cityscape and those around the world: Juan Antonio Buschiazzo helped popularize Beaux-Arts architecture and Francisco Gianotti combined Art Nouveau with Italianate styles, each adding flair to Argentine cities during the early 20th century. Francisco Salamone and Viktor Sulčič left an Art Deco legacy, and Alejandro Bustillo created a prolific body of Neoclassical and Rationalist architecture. Alberto Prebisch and Amancio Williams were highly influenced by Le Corbusier, while Clorindo Testa introduced Brutalist architecture locally. César Pelli's and Patricio Pouchulu's Futurist creations have graced cities worldwide: Pelli's 1980s throwbacks to the Art Deco glory of the 1920s made him one of the world's most prestigious architects, with the Norwest Center and the Petronas Towers among his most celebrated creations.




Pato is the national sport, an ancient horseback game locally originated in the early 1600s and predecessor of horseball. The most popular sport is Football. Along with France, the men's national team is the only to have won the most important international triplet: World Cup, Confederations Cup, and Olympic Gold Medal. It has also won 14 Copas América, 6 Pan American Gold Medals, and many other trophies. Alfredo Di Stéfano, Diego Maradona, and Lionel Messi are among the best players in the game's history.
The country's women's field hockey team Las Leonas is one of the world's most successful, with four Olympic medals, two World Cups, a World League and seven Champions Trophy. Luciana Aymar is recognized as the best female player in the history of the sport, being the only player to have received the FIH Player of the Year Award eight times.
Basketball is a very popular sport. The men's national team is the only one in the FIBA Americas zone that has won the quintuplet crown: World Championship, Olympic Gold Medal, Diamond Ball, Americas Championship, and Pan American Gold Medal. It has also conquered 13 South American Championships, and many other tournaments. Emanuel Ginóbili, Luis Scola, Andrés Nocioni, Fabricio Oberto, Pablo Prigioni, Carlos Delfino and Juan Ignacio Sánchez are a few of the country's most acclaimed players, all of them part of the NBA. Argentina hosted the Basketball World Cup in 1950 and 1990.
Rugby is another popular sport in Argentina. As of 2014 the men's national team, known as 'Los Pumas' has competed at the Rugby World Cup each time it has been held, achieving their highest ever result in 2007 when they came third. Since 2012 the Los Pumas have competed against Australia, New Zealand & South Africa in The Rugby Championship, the premier international Rugby competition in the Southern Hemisphere. Since 2009 the men's national 'A' team known as the 'Jaguares' has competed against the USA & Canada 'A' teams along with Uruguay in the Americas Rugby Championship, The Los Jaguares have won every year the competition has been competed.

Argentina has produced some of the most formidable champions for Boxing, including Carlos Monzón, the best middleweight in history; Pascual Pérez, one of the most decorated flyweight boxers of all times; Víctor Galíndez, as of 2009 record holder for consecutive world light heavyweight title defenses; and Nicolino Locche, nicknamed "The Untouchable" for his masterful defense; they are all inductees into the International Boxing Hall of Fame.
Tennis has been quite popular among people of all ages. Guillermo Vilas is the greatest Latin American player of the Open Era, while Gabriela Sabatini is the most accomplished Argentine female player of all time—having reached #3 in the WTA Ranking, are both inductees into the International Tennis Hall of Fame.
Argentina reigns undisputed in Polo, having won more international championships than any other country and been seldom beaten since the 1930s. The Argentine Polo Championship is the sport's most important international team trophy. The country is home to most of the world's top players, among them Adolfo Cambiaso, the best in Polo history.
Historically, Argentina has had a strong showing within Auto racing. Juan Manuel Fangio was five times Formula One world champion under four different teams, winning 102 of his 184 international races, and is widely ranked as the greatest driver of all time. Other distinguished racers were Oscar Alfredo Gálvez, Juan Gálvez, José Froilán González, and Carlos Reutemann.




Besides many of the pasta, sausage and dessert dishes common to continental Europe, Argentines enjoy a wide variety of Indigenous and Criollo creations, including empanadas (a small stuffed pastry), locro (a mixture of corn, beans, meat, bacon, onion, and gourd), humita and mate.
The country has the highest consumption of red meat in the world, traditionally prepared as asado, the Argentine barbecue. It is made with various types of meats, often including chorizo, sweetbread, chitterlings, and blood sausage.
Common desserts include facturas (Viennese-style pastry), cakes and pancakes filled with dulce de leche (a sort of milk caramel jam), alfajores (shortbread cookies sandwiched together with chocolate, dulce de leche or a fruit paste), and tortas fritas (fried cakes)
Argentine wine, one of the world's finest, is an integral part of the local menu. Malbec, Torrontés, Cabernet Sauvignon, Syrah and Chardonnay are some of the most sought-after varieties.




Some of Argentina's national symbols are defined by law, while others are traditions lacking formal designation. The Flag of Argentina consists of three horizontal stripes equal in width and colored light blue, white and light blue, with the Sun of May in the center of the middle white stripe. The flag was designed by Manuel Belgrano in 1812; it was adopted as a national symbol on 20 July 1816. The Coat of Arms, which represents the union of the provinces, came into use in 1813 as the seal for official documents. The Argentine National Anthem was written by Vicente López y Planes with music by Blas Parera, and was adopted in 1813. The National Cockade was first used during the May Revolution of 1810 and was made official two years later. The Virgin of Luján is Argentina's patron saint.
The hornero, living across most of the national territory, was chosen as the national bird in 1928 after a lower school survey. The ceibo is the national floral emblem and national tree, while the quebracho colorado is the national forest tree. Rhodochrosite is known as the national gemstone. The national sport is pato, an equestrian game that was popular among gauchos.
Argentine wine is the national liquor, and mate, the national infusion. Asado and locro are considered the national dishes.




Index of Argentina-related articles
Outline of Argentina









Legal documents

Articles

Books




Government
Official website (Spanish)
Travel & tourism
Argentina Ministry of Tourism
National Institute of Tourism Promotion
Overview
"Argentina". The World Factbook. Central Intelligence Agency. 
Argentina at DMOZ
Argentina at the Latin American Network Information Center
Argentina at the University Libraries – University of Colorado Boulder
Key Development Forecasts for Argentina at International Futures
 Geographic data related to Argentina at OpenStreetMap
 Argentina – Wikipedia book
 Wikimedia Atlas of ArgentinaMexico (Spanish: México, pronounced: [ˈme.xi.ko], modern Nahuatl ), officially the United Mexican States (Spanish: Estados Unidos Mexicanos,  listen ), is a federal republic in the southern half of North America. It is bordered to the north by the United States; to the south and west by the Pacific Ocean; to the southeast by Guatemala, Belize, and the Caribbean Sea; and to the east by the Gulf of Mexico. Covering almost two million square kilometers (over 760,000 sq mi), Mexico is the sixth largest country in the Americas by total area and the 13th largest independent nation in the world. With an estimated population of over 120 million, it is the eleventh most populous country and the most populous Spanish-speaking country in the world while being the second most populous country in Latin America. Mexico is a federation comprising 31 states and a federal district that is also its capital and most populous city. Other metropolises include Guadalajara, Monterrey, Puebla, Toluca, Tijuana and León.
Pre-Columbian Mexico was home to many advanced Mesoamerican civilizations, such as the Olmec, Toltec, Teotihuacan, Zapotec, Maya and Aztec before first contact with Europeans. In 1521, the Spanish Empire conquered and colonized the territory from its base in Mexico-Tenochtitlan, which was administered as the viceroyalty of New Spain. Three centuries later, this territory became Mexico following recognition in 1821 after the colony's Mexican War of Independence. The tumultuous post-independence period was characterized by economic instability and many political changes. The Mexican–American War (1846–48) led to the territorial cession of the extensive northern borderlands, one-third of its territory, to the United States. The Pastry War, the Franco-Mexican War, a civil war, two empires and a domestic dictatorship occurred through the 19th century. The dictatorship was overthrown in the Mexican Revolution of 1910, which culminated with the promulgation of the 1917 Constitution and the emergence of the country's current political system.
Mexico has the fifteenth largest nominal GDP and the eleventh largest by purchasing power parity. The Mexican economy is strongly linked to those of its North American Free Trade Agreement (NAFTA) partners, especially the United States. Mexico was the first Latin American member of the Organisation for Economic Co-operation and Development (OECD), joining in 1994. It is classified as an upper-middle income country by the World Bank and a newly industrialized country by several analysts. By 2050, Mexico could become the world's fifth or seventh largest economy. The country is considered both a regional power and middle power, and is often identified as an emerging global power. Due to its rich culture and history, Mexico ranks first in the Americas and seventh in the world by number of UNESCO World Heritage Sites. In 2015 it was the 9th most visited country in the world, with 32.1 million international arrivals. Mexico is a member of the United Nations, the World Trade Organization, the G8+5, the G20, the Uniting for Consensus, and has been an observer of the Organisation internationale de la Francophonie since 2014.




Mēxihco is the Nahuatl term for the heartland of the Aztec Empire, namely, the Valley of Mexico, and its people, the Mexica, and surrounding territories. This became the future State of Mexico as a division of New Spain prior to independence (compare Latium). It is generally considered to be a toponym for the valley which became the primary ethnonym for the Aztec Triple Alliance as a result, or vice versa. After New Spain won independence from Spain, representatives decided to name the new country after its capital, Mexico City. This was founded in 1524 on top of the ancient Mexica capital of Mexico-Tenochtitlan.
Traditionally, the name Tenochtitlan was thought to come from Nahuatl tetl [ˈtetɬ] ("rock") and nōchtli [ˈnoːtʃtɬi] ("prickly pear") and is often thought to mean "Among the prickly pears [growing among] rocks". However, one attestation in the late 16th-century manuscript known as "the Bancroft dialogues" suggests the second vowel was short, so that the true etymology remains uncertain.
The suffix -co is the Nahuatl locative, making the word a place name. Beyond that, the etymology is uncertain. It has been suggested that it is derived from Mextli or Mēxihtli, a secret name for the god of war and patron of the Mexica, Huitzilopochtli, in which case Mēxihco means "Place where Huitzilopochtli lives". Another hypothesis suggests that Mēxihco derives from a portmanteau of the Nahuatl words for "Moon" (Mētztli) and navel (xīctli). This meaning ("Place at the Center of the Moon") might refer to Tenochtitlan's position in the middle of Lake Texcoco. The system of interconnected lakes, of which Texcoco formed the center, had the form of a rabbit, which the Mesoamericans pareidolically associated with the Moon. Still another hypothesis suggests that the word is derived from Mēctli, the goddess of maguey.
The name of the city-state was transliterated to Spanish as México with the phonetic value of the letter 'x' in Medieval Spanish, which represented the voiceless postalveolar fricative [ʃ]. This sound, as well as the voiced postalveolar fricative [ʒ], represented by a 'j', evolved into a voiceless velar fricative [x] during the 16th century. This led to the use of the variant Méjico in many publications in Spanish, most notably in Spain, whereas in Mexico and most other Spanish–speaking countries México was the preferred spelling. In recent years the Real Academia Española, which regulates the Spanish language, determined that both variants are acceptable in Spanish but that the normative recommended spelling is México. The majority of publications in all Spanish-speaking countries now adhere to the new norm, even though the alternative variant is still occasionally used. In English, the 'x' in Mexico represents neither the original nor the current sound, but the consonant cluster [ks].
The official name of the country has changed as the form of government has changed. On three occasions (1325–1521, 1821–1823, and 1863–1867), the country was known as Imperio Mexicano (Mexican Empire). All three federal constitutions (1824, 1857 and 1917, the current constitution) used the name Estados Unidos Mexicanos—or the variant Estados-Unidos Mexicanos, all of which have been translated as "United Mexican States". The phrase República Mexicana, "Mexican Republic", was used in the 1836 Constitutional Laws. On November 22, 2012, president Felipe Calderón sent to the Mexican Congress a piece of legislation to change the country's name officially to simply Mexico. To be implemented, the bill needed to be passed by both houses of Congress, as well as a majority of Mexico's 31 State legislatures. As this legislation was proposed just a week before Calderón turned power over to Enrique Peña Nieto, Calderón's critics saw this as a symbolic gesture.







The earliest human artifacts in Mexico are chips of stone tools found near campfire remains in the Valley of Mexico and radiocarbon-dated to circa 10,000 years ago. Mexico is the site of the domestication of maize, tomato and beans, which produced an agricultural surplus. This enabled the transition from paleo-Indian hunter-gatherers to sedentary agricultural villages beginning around 5000 BCE.
In the subsequent formative eras, maize cultivation and cultural traits such as a mythological and religious complex, and a vigesimal numeric system, were diffused from the Mexican cultures to the rest of the Mesoamerican culture area. In this period, villages became more dense in terms of population, becoming socially stratified with an artisan class, and developing into chiefdoms. The most powerful rulers had religious and political power, organizing construction of large ceremonial centers developed.
The earliest complex civilization in Mexico was the Olmec culture, which flourished on the Gulf Coast from around 1500 BCE. Olmec cultural traits diffused through Mexico into other formative-era cultures in Chiapas, Oaxaca and the Valley of Mexico. The formative period saw the spread of distinct religious and symbolic traditions, as well as artistic and architectural complexes. The formative-era of Mesoamerica is considered one of the six independent cradles of civilization.
In the subsequent pre-classical period, the Maya and Zapotec civilizations developed complex centers at Calakmul and Monte Albán, respectively. During this period the first true Mesoamerican writing systems were developed in the Epi-Olmec and the Zapotec cultures. The Mesoamerican writing tradition reached its height in the Classic Maya Hieroglyphic script.
In Central Mexico, the height of the classic period saw the ascendancy of Teotihuacan, which formed a military and commercial empire whose political influence stretched south into the Maya area as well as north. Teotihuacan, with a population of more than 150,000 people, had some of the largest pyramidal structures in the pre-Columbian Americas. After the collapse of Teotihuacán around 600 CE, competition ensued between several important political centers in central Mexico such as Xochicalco and Cholula. At this time, during the Epi-Classic, Nahua peoples began moving south into Mesoamerica from the North, and became politically and culturally dominant in central Mexico, as they displaced speakers of Oto-Manguean languages.




During the early post-classic, Central Mexico was dominated by the Toltec culture, Oaxaca by the Mixtec, and the lowland Maya area had important centers at Chichén Itzá and Mayapán. Toward the end of the post-Classic period, the Mexica established dominance.
Alexander von Humboldt originated the modern usage of "Aztec" as a collective term applied to all the people linked by trade, custom, religion, and language to the Mexica state and Ēxcān Tlahtōlōyān, the Triple Alliance. In 1843, with the publication of the work of William H. Prescott, it was adopted by most of the world, including 19th-century Mexican scholars who considered it a way to distinguish present-day Mexicans from pre-conquest Mexicans. This usage has been the subject of debate since the late 20th century.
The Aztec empire was an informal or hegemonic empire because it did not exert supreme authority over the conquered lands; it was satisfied with the payment of tributes from them. It was a discontinuous empire because not all dominated territories were connected; for example, the southern peripheral zones of Xoconochco were not in direct contact with the center. The hegemonic nature of the Aztec empire was demonstrated by their restoration of local rulers to their former position after their city-state was conquered. The Aztec did not interfere in local affairs, as long as the tributes were paid.
The Aztec of Central Mexico built a tributary empire covering most of central Mexico. The Aztec were noted for practicing human sacrifice on a large scale. Along with this practice, they avoided killing enemies on the battlefield. Their warring casualty rate was far lower than that of their Spanish counterparts, whose principal objective was immediate slaughter during battle. This distinct Mesoamerican cultural tradition of human sacrifice ended with the Spanish conquest in the 16th century. Over the next centuries Mexican indigenous cultures were gradually subjected to Spanish colonial rule.




The Spanish first learned of Mexico during the Juan de Grijalva expedition of 1518. The natives kept "repeating: Colua, Colua, and Mexico, Mexico, but we [explorers] did not know what Colua or Mexico meant", until encountering Montezuma's governor at the mouth of the Rio de las Banderas. The Spanish conquest of the Aztec Empire began in February 1519 when Hernán Cortés arrived at the port in Veracruz with ca. 500 conquistadores. After taking control of that city, he moved on to the Aztec capital. In his search for gold and other riches, Cortés decided to invade and conquer the Aztec empire.

When the Spaniards arrived, the ruler of the Aztec empire was Moctezuma II, who was later killed. His successor and brother Cuitláhuac took control of the Aztec empire, but was among the first to fall from the first smallpox epidemic in the area a short time later. Unintentionally introduced by Spanish conquerors, among whom smallpox was endemic, the infectious disease ravaged Mesoamerica in the 1520s. It killed more than 3 million natives as they had no immunity. Other sources, however, mentioned that the death toll of the Aztecs might have reached 15 million (out of a population of less than 30 million) although such a high number conflicts with the 350,000 Aztecs who ruled an empire of 5 million or 10 million. Severely weakened, the Aztec empire was easily defeated by Cortés and his forces on his second return with the help of state of Tlaxcala whose population estimate was 300,000. The native population declined 80–90% by 1600 to 1–2.5 million. Any population estimate of pre-Columbian Mexico is bound to be a guess but 8–12 million is often suggested for the area encompassed by the modern nation.
Smallpox was a devastating disease: it generally killed Aztecs but not Spaniards, who as Europeans had already been exposed to it in their cities for centuries and therefore had developed acquired immunity. The deaths caused by smallpox are believed to have triggered a rapid growth of Christianity in Mexico and the Americas. At first, the Aztecs believed the epidemic was a punishment from an angry god, but they later accepted their fate and no longer resisted the Spanish rule. Many of the surviving Aztecs believed that smallpox could be credited to the superiority of the Christian god, which resulted in their acceptance of Catholicism and yielding to the Spanish rule throughout Mexico.
The territory became part of the Spanish Empire under the name of New Spain. Mexico City was systematically rebuilt by Cortés following the Fall of Tenochtitlan in 1521. Much of the identity, traditions and architecture of Mexico developed during the 300-year colonial period.



The capture of Tenochtitlan marked the beginning of a 300-year-long colonial period, during which Mexico was known as "New Spain".




Contrary to a widespread misconception, Spain did not conquer all of the Aztec Empire nor the rest of Mesoamerica when Cortes took Tenochtitlan. It required another two centuries to complete the conquest: rebellions broke out within the old empire and wars continued with other native peoples. After the fall of Tenochtitlan, it took the Spaniards decades of sporadic warfare to subdue the rest of Mesoamerica. Particularly fierce was the Chichimeca War (1576–1606) and the Tepehuán Revolt (1616–1620) in the north.
The Council of Indies and the mendicant religious, which arrived in Mesoamerica as early as 1524, labored to generate capital for the crown of Spain and convert the Indian populations to Catholicism. During this period and the following Colonial periods, the efforts of the mendicant friars resulted in religious syncretism which combined the Pre-Hispanic cultures with Spanish socio-religious concepts.
The result was mixed culture in multi-ethnic State that relied on the "repartimiento" system of peasant "Republics of Indians" which performed the work. The pre-Hispanic Mesoamerican system was replaced by the encomienda feudal-style system of Spain probably adapted to the pre-Hispanic tradition. This in turn was replaced in the 19th century by a debt-based inscription of labor. Its burden was a catalyst for political revitalization movements and prompted the revolution that ended colonial New Spain in 1821.
According to West, "the first gold and silver that the Spaniards obtained in Mexico came not from mining but in the form of native Indian artifacts looted from palaces, temples, graves and other sacred places, where the Aztec and Tarascan nobility had accumulated such treasures for generations." Cortes soon claimed the Tamazula area as a source of silver. Silver lodes were discovered by the 1530s and mined in Zumpango del Rio and an area called the "Provincia de la Plata". The Provincia consisted of five major asientos de minas in the areas of Amatepec, Sultepec, Taxco, Zacualpan, and Temascaltepec.




During this period, Mexico was part of the much larger Viceroyalty of New Spain, which included Cuba, Puerto Rico, Central America as far south as Costa Rica, Florida, the southwestern United States and the Philippines. Spain during the 16th century focused its energies on areas with dense populations that had produced Pre-Columbian civilizations, since these areas could provide the settlers with a disciplined labor force and a population to catechize.
The indigenous population stabilized around one to one and a half million individuals in the 17th century from the most commonly accepted five to ten million pre-contact population. The population decline was primarily the result of communicable diseases (particularly small pox) introduced during the Columbian Exchange. During the three hundred years of the colonial era, Mexico received some 400,000 to half a million Europeans, 200,000 to 250,000 Africans and 40,000 to 120,000 Asians. The 18th century saw a great increase in the percentage of mestizos (The Penguin Atlas of World Population History, pp. 291–92). The Spanish explored a good part of North America, even seeking the fabled "El Dorado." They made no concerted effort to settle the northern desert regions in what is now the United States until the end of the 16th century (Santa Fe, 1598).
Colonial law with Spanish roots was introduced and attached to native customs creating a hierarchy between local jurisdiction (the Cabildos) and the Crown, whereby upper administrative offices were closed to the natives, even those of pure Spanish blood. Administration was based on the racial separation of the population among "Republics" of Spaniards, Indians and Mestizos, autonomous and directly dependent on the king himself.

From an economic point of view, New Spain was administered principally for the benefit of the Empire and its military and defensive efforts. Mexico provided more than half of the Empire's taxes and supported the administration of all North and Central America. Competition with Spain was discouraged to the extent that activities like cultivation of grapes and olives, introduced by Cortez himself, was banned out of fear that these crops would compete with Spain's.
In order to protect Mexico from the attacks of English, French and Dutch pirates and protect the Crown's monopoly of revenue only two ports were open to foreign trade—Veracruz on the Atlantic and Acapulco on the Pacific. The pirates attacked, plundered and ravaged several cities like Campeche (1557), Veracruz (1568) and Alvarado (1667).
Education was encouraged by the Crown from the very beginning. Mexico boasts the first primary school (Texcoco, 1523), first university (1551) and the first printing house (1524) in the Americas. Indigenous languages were studied mainly by the religious orders during the first centuries. They became official languages in the so-called Republic of Indians to be outlawed and ignored after independence by the prevailing Spanish-speaking creoles.
Mexico produced important cultural achievements during the colonial period, like the literature of Sor Juana Inés de la Cruz and Ruiz de Alarcón, as well as cathedrals, civil monuments, forts and colonial cities such as Puebla, Mexico City, Querétaro, Zacatecas and others, today part of Unesco's World Heritage.
The syncretism between indigenous and Spanish cultures in New Spain gave birth to many of today's Mexican cultural traits like tequila (first distilled in the 16th century), mariachi (18th), jarabe (17th), charros (17th) and Mexican cuisine – a mixture of European and indigenous ingredients and techniques.




On September 16, 1810, a "loyalist revolt" against the ruling Junta was declared by priest Miguel Hidalgo y Costilla, in the small town of Dolores, Guanajuato. The first insurgent group was formed by Hidalgo, the Spanish viceregal army captain Ignacio Allende, the militia captain Juan Aldama and "La Corregidora" Josefa Ortiz de Domínguez. Hidalgo and some of his soldiers were captured and executed by firing squad in Chihuahua, on July 31, 1811. Following his death, the leadership was assumed by priest José María Morelos, who occupied key southern cities.
In 1813 the Congress of Chilpancingo was convened and, on November 6, signed the "Solemn Act of the Declaration of Independence of Northern America". Morelos was captured and executed on December 22, 1815.
In subsequent years, the insurgency was near collapse, but in 1820 Viceroy Juan Ruiz de Apodaca sent an army under the criollo general Agustín de Iturbide against the troops of Vicente Guerrero. Instead, Iturbide approached Guerrero to join forces, and on August 24, 1821 representatives of the Spanish Crown and Iturbide signed the "Treaty of Córdoba" and the "Declaration of Independence of the Mexican Empire", which recognized the independence of Mexico under the terms of the "Plan of Iguala".
Mexico’s short recovery after the War of Independence was soon cut short again by the civil wars and institutional instability of the 1850s, which lasted until the government of Porfirio Díaz reestablished conditions that paved the way for economic growth. The conflicts that arose from the mid-1850s had a profound effect because they were widespread and made themselves perceptible in the vast rural areas of the countries, involved clashes between castes, different ethnic groups and haciendas, and entailed a deepening of the political and ideological divisions between republicans and monarchists.







Agustín de Iturbide immediately proclaimed himself emperor of the First Mexican Empire. A revolt against him in 1823 established the United Mexican States. In 1824, a Republican Constitution was drafted and Guadalupe Victoria became the first president of the newly born country. In 1829 president Guerrero abolished slavery. The first decades of the post-independence period were marked by economic instability, which led to the Pastry War in 1836. There was constant strife between liberales, supporters of a federal form of government, and conservadores, who proposed a hierarchical form of government.
During this period, the frontier borderlands to the north became quite isolated from the government in Mexico City, and its monopolistic economic policies caused suffering. With limited trade, the people had difficulty meeting tax payments and resented the central government's actions in collecting customs. Resentment built up from California to Texas. Both the mission system and the presidios had collapsed after the Spanish withdrew from the colony, causing great disruption especially in Alta California and New Mexico. The people in the borderlands had to raise local militias to protect themselves from hostile Native Americans. These areas developed in different directions from the center of the country.
Wanting to stabilize and develop the frontier, Mexico encouraged immigration into present-day Texas, as they were unable to persuade people from central Mexico to move into those areas. They allowed for religious freedom for the new settlers, who were primarily Protestant English speakers from the United States. Within several years, the Anglos far outnumbered the Tejano in the area. Itinerant traders traveled through the area, working by free market principles. The Tejano grew more separate from the government and due to its neglect, many supported the idea of independence and joined movements to that end, collaborating with the English-speaking Americans.
General Antonio López de Santa Anna, a centralist and two-time dictator, approved the Siete Leyes in 1836, a radical amendment that institutionalized the centralized form of government. When he suspended the 1824 Constitution, civil war spread across the country. Three new governments declared independence: the Republic of Texas, the Republic of the Rio Grande and the Republic of Yucatán.
Texas successfully achieved independence as a republic in 1836 and joined the United States. A border dispute between the US and Mexico led to the Mexican–American War, which began in 1846 and lasted for two years. Many Southerners intended that slavery should be extended to the west in these newly acquired territories but the United States generally would not permit it, except in Texas. The War was settled via the Treaty of Guadalupe Hidalgo. Mexico was forced to give up more than one-third of its land to the U.S., including Alta California, New Mexico, and the disputed parts of Texas. A much smaller transfer of territory in what is today southern Arizona and southwestern New Mexico—known as the Gadsden Purchase—occurred in 1854.
The Caste War of Yucatán, the Mayan uprising that began in 1847, was one of the most successful modern Native American revolts. Maya rebels, or Cruzob, maintained relatively independent enclaves in the peninsula until the 1930s.
Dissatisfaction with Santa Anna's return to power led to the liberal "Plan of Ayutla", initiating an era known as La Reforma. The new Constitution drafted in 1857 established a secular state, federalism as the form of government, and several freedoms. As the conservadores refused to recognize it, the Reform War began in 1858, during which both groups had their own governments. The war ended in 1861 with victory by the Liberals, led by president Benito Juárez, who was Amerindian.
In the 1860s Mexico was occupied by France, which established the Second Mexican Empire under the rule of the Habsburg Archduke Ferdinand Maximilian of Austria with support from the Roman Catholic clergy and the conservadores. The latter switched sides and joined the liberales. Maximilian surrendered, was tried on June 14, 1867, and was executed a few days later on June 19.



Porfirio Díaz, a republican general during the French intervention, ruled Mexico from 1876 to 1880 and then from 1884 to 1911 in five consecutive reelections, period known as the Porfiriato, characterized by remarkable economic achievements, investments in the arts and sciences, but also of economic inequality and political repression.




President Díaz announced in 1908 that he would retire in 1911, resulting in the development of new coalitions. But then he ran for reelection anyway and in a show of U.S. support, Díaz and William Howard Taft planned a summit in El Paso, Texas, and Ciudad Juárez, Mexico, for October 16, 1909, an historic first meeting between a Mexican and a U.S. president and also the first time an American president would cross the border into Mexico. Both sides agreed that the disputed Chamizal strip connecting El Paso to Ciudad Juárez would be considered neutral territory with no flags present during the summit, but the meeting focused attention on this territory and resulted in assassination threats and other serious security concerns.
On the day of the summit, Frederick Russell Burnham, the celebrated scout, and Private C.R. Moore, a Texas Ranger, discovered a man holding a concealed palm pistol standing at the El Paso Chamber of Commerce building along the procession route, and they disarmed the assassin within only a few feet of Díaz and Taft. Both presidents were unharmed and the summit was held. Díaz was re-elected in 1910, but alleged electoral fraud forced him into exile in France and sparked the 1910 Mexican Revolution, initially led by Francisco I. Madero.
Madero was elected president but overthrown and murdered in a coup d'état two years later directed by conservative general Victoriano Huerta. That event re-ignited the civil war, involving figures such as Francisco Villa and Emiliano Zapata, who formed their own forces. A third force, the constitutional army led by Venustiano Carranza managed to bring an end to the war, and radically amended the 1857 Constitution to include many of the social premises and demands of the revolutionaries into what was eventually called the 1917 Constitution. It is estimated that the war killed 900,000 of the 1910 population of 15 million.
Assassinated in 1920, Carranza was succeeded by another revolutionary hero, Álvaro Obregón, who in turn was succeeded by Plutarco Elías Calles. Obregón was reelected in 1928 but assassinated before he could assume power. Although this period is usually referred to as the Mexican Revolution, it might also be termed a civil war since president Díaz (1909) narrowly escaped assassination and presidents Francisco I. Madero (1913), Venustiano Carranza (1920), Álvaro Obregón (1928), and former revolutionary leaders Emiliano Zapata (1919) and Pancho Villa (1923) all were assassinated during this period.



In 1929, Calles founded the National Revolutionary Party (PNR), later renamed the Institutional Revolutionary Party (PRI), and started a period known as the Maximato, which ended with the election of Lázaro Cárdenas, who implemented many economic and social reforms. This included the Mexican oil expropriation in March 1938, which nationalized the U.S. and Anglo-Dutch oil company known as the Mexican Eagle Petroleum Company. This movement would result in the creation of the state-owned Mexican oil company known as Pemex. This sparked a diplomatic crisis with the countries whose citizens had lost businesses by Cárdenas' radical measure, but since then the company has played an important role in the economic development of Mexico.

Between 1940 and 1980, Mexico remained a poor country but experienced substantial economic growth that some historians call the "Mexican miracle". Although the economy continued to flourish for some, social inequality remained a factor of discontent. Moreover, the PRI rule became increasingly authoritarian and at times oppressive in what is now referred to as 'Mexico's dirty war' (see the 1968 Tlatelolco massacre, which claimed the life of around 300 protesters based on conservative estimates and as many as 800 protesters).
Electoral reforms and high oil prices followed the administration of Luis Echeverría, mismanagement of these revenues led to inflation and exacerbated the 1982 Crisis. That year, oil prices plunged, interest rates soared, and the government defaulted on its debt. President Miguel de la Madrid resorted to currency devaluations which in turn sparked inflation.
In the 1980s the first cracks emerged in PRI's monopolistic position. In Baja California, Ernesto Ruffo Appel was elected as governor. In 1988, alleged electoral fraud prevented the leftist candidate Cuauhtémoc Cárdenas from winning the national presidential elections, giving Carlos Salinas de Gortari the presidency and leading to massive protests in Mexico City.

Salinas embarked on a program of neoliberal reforms which fixed the exchange rate, controlled inflation and culminated with the signing of the North American Free Trade Agreement (NAFTA), which came into effect on January 1, 1994. The same day, the Zapatista Army of National Liberation (EZLN) started a two-week-long armed rebellion against the federal government, and has continued as a non-violent opposition movement against neoliberalism and globalization.



In 1994, Salinas was succeeded by Ernesto Zedillo, followed by the Mexican peso crisis and a $50 billion IMF bailout. Major macroeconomic reforms were started by President Zedillo, and the economy rapidly recovered and growth peaked at almost 7% by the end of 1999.
In 2000, after 71 years, the PRI lost a presidential election to Vicente Fox of the opposition National Action Party (PAN). In the 2006 presidential election, Felipe Calderón from the PAN was declared the winner, with a very narrow margin over leftist politician Andrés Manuel López Obrador of the Party of the Democratic Revolution (PRD). López Obrador, however, contested the election and pledged to create an "alternative government".
After twelve years, in 2012, the PRI won the Presidency again with the election of Enrique Peña Nieto, the governor of the State of Mexico from 2005–2011. However, he won with only a plurality of about 38%, and did not have a legislative majority.




Mexico is located between latitudes 14° and 33°N, and longitudes 86° and 119°W in the southern portion of North America. Almost all of Mexico lies in the North American Plate, with small parts of the Baja California peninsula on the Pacific and Cocos Plates. Geophysically, some geographers include the territory east of the Isthmus of Tehuantepec (around 12% of the total) within Central America. Geopolitically, however, Mexico is entirely considered part of North America, along with Canada and the United States.
Mexico's total area is 1,972,550 km2 (761,606 sq mi), making it the world's 14th largest country by total area, and includes approximately 6,000 km2 (2,317 sq mi) of islands in the Pacific Ocean (including the remote Guadalupe Island and the Revillagigedo Islands), Gulf of Mexico, Caribbean, and Gulf of California. From its farthest land points, Mexico is a little over 2,000 mi (3,219 km) in length.
On its north, Mexico shares a 3,141 km (1,952 mi) border with the United States. The meandering Río Bravo del Norte (known as the Rio Grande in the United States) defines the border from Ciudad Juárez east to the Gulf of Mexico. A series of natural and artificial markers delineate the United States-Mexican border west from Ciudad Juárez to the Pacific Ocean. On its south, Mexico shares an 871 km (541 mi) border with Guatemala and a 251 km (156 mi) border with Belize.
Mexico is crossed from north to south by two mountain ranges known as Sierra Madre Oriental and Sierra Madre Occidental, which are the extension of the Rocky Mountains from northern North America. From east to west at the center, the country is crossed by the Trans-Mexican Volcanic Belt also known as the Sierra Nevada. A fourth mountain range, the Sierra Madre del Sur, runs from Michoacán to Oaxaca.
As such, the majority of the Mexican central and northern territories are located at high altitudes, and the highest elevations are found at the Trans-Mexican Volcanic Belt: Pico de Orizaba (5,700 m or 18,701 ft), Popocatepetl (5,462 m or 17,920 ft) and Iztaccihuatl (5,286 m or 17,343 ft) and the Nevado de Toluca (4,577 m or 15,016 ft). Three major urban agglomerations are located in the valleys between these four elevations: Toluca, Greater Mexico City and Puebla.




The Tropic of Cancer effectively divides the country into temperate and tropical zones. Land north of the twenty-fourth parallel experiences cooler temperatures during the winter months. South of the twenty-fourth parallel, temperatures are fairly constant year round and vary solely as a function of elevation. This gives Mexico one of the world's most diverse weather systems.
Areas south of the 24th parallel with elevations up to 1,000 m (3,281 ft) (the southern parts of both coastal plains as well as the Yucatán Peninsula), have a yearly median temperature between 24 to 28 °C (75.2 to 82.4 °F). Temperatures here remain high throughout the year, with only a 5 °C (9 °F) difference between winter and summer median temperatures. Both Mexican coasts, except for the south coast of the Bay of Campeche and northern Baja, are also vulnerable to serious hurricanes during the summer and fall. Although low-lying areas north of the 24th parallel are hot and humid during the summer, they generally have lower yearly temperature averages (from 20 to 24 °C or 68.0 to 75.2 °F) because of more moderate conditions during the winter.
Many large cities in Mexico are located in the Valley of Mexico or in adjacent valleys with altitudes generally above 2,000 m (6,562 ft). This gives them a year-round temperate climate with yearly temperature averages (from 16 to 18 °C or 60.8 to 64.4 °F) and cool nighttime temperatures throughout the year.
Many parts of Mexico, particularly the north, have a dry climate with sporadic rainfall while parts of the tropical lowlands in the south average more than 2,000 mm (78.7 in) of annual precipitation. For example, many cities in the north like Monterrey, Hermosillo, and Mexicali experience temperatures of 40 °C (104 °F) or more in summer. In the Sonoran Desert temperatures reach 50 °C (122 °F) or more.
In 2012, Mexico passed a comprehensive climate change bill, a first in the developing world, that has set a goal for the country to generate 35% of its energy from clean energy sources by 2024, and to cut emissions by 50% by 2050, from the level found in 2000. During the 2016 North American Leaders' Summit, the target of 50% of electricity generated from renewable sources by 2025 was announced.
Climate of Mexico




Mexico is one of the 17 megadiverse countries of the world. With over 200,000 different species, Mexico is home of 10–12% of the world's biodiversity. Mexico ranks first in biodiversity in reptiles with 707 known species, second in mammals with 438 species, fourth in amphibians with 290 species, and fourth in flora, with 26,000 different species. Mexico is also considered the second country in the world in ecosystems and fourth in overall species. Approximately 2,500 species are protected by Mexican legislations.
In 2002, Mexico had the second fastest rate of deforestation in the world, second only to Brazil. The government has taken another initiative in the late 1990s to broaden the people's knowledge, interest and use of the country's esteemed biodiversity, through the Comisión Nacional para el Conocimiento y Uso de la Biodiversidad.
In Mexico, 170,000 square kilometres (65,637 sq mi) are considered "Protected Natural Areas." These include 34 biosphere reserves (unaltered ecosystems), 67 national parks, 4 natural monuments (protected in perpetuity for their aesthetic, scientific or historical value), 26 areas of protected flora and fauna, 4 areas for natural resource protection (conservation of soil, hydrological basins and forests) and 17 sanctuaries (zones rich in diverse species).
The discovery of the Americas brought to the rest of the world many widely used food crops and edible plants. Some of Mexico's native culinary ingredients include: chocolate, avocado, tomato, maize, vanilla, guava, chayote, epazote, camote, jícama, nopal, zucchini, tejocote, huitlacoche, sapote, mamey sapote, many varieties of beans, and an even greater variety of chiles, such as the habanero and the jalapeño. Most of these names come from indigenous languages like Nahuatl.
Because of its high biodiversity Mexico has also been a frequent site of bioprospecting by international research bodies. The first highly successful instance being the discovery in 1947 of the tuber "Barbasco" (Dioscorea composita) which has a high content of diosgenin, revolutionizing the production of synthetic hormones in the 1950s and 1960s and eventually leading to the invention of combined oral contraceptive pills.







The United Mexican States are a federation whose government is representative, democratic and republican based on a presidential system according to the 1917 Constitution. The constitution establishes three levels of government: the federal Union, the state governments and the municipal governments. According to the constitution, all constituent states of the federation must have a republican form of government composed of three branches: the executive, represented by a governor and an appointed cabinet, the legislative branch constituted by a unicameral congress and the judiciary, which will include a state Supreme Court of Justice. They also have their own civil and judicial codes.
The federal legislature is the bicameral Congress of the Union, composed of the Senate of the Republic and the Chamber of Deputies. The Congress makes federal law, declares war, imposes taxes, approves the national budget and international treaties, and ratifies diplomatic appointments.
The federal Congress, as well as the state legislatures, are elected by a system of parallel voting that includes plurality and proportional representation. The Chamber of Deputies has 500 deputies. Of these, 300 are elected by plurality vote in single-member districts (the federal electoral districts) and 200 are elected by proportional representation with closed party lists for which the country is divided into five electoral constituencies. The Senate is made up of 128 senators. Of these, 64 senators (two for each state and two for the Federal District) are elected by plurality vote in pairs; 32 senators are the first minority or first-runner up (one for each state and one for the Federal District), and 32 are elected by proportional representation from national closed party lists.
The executive is the President of the United Mexican States, who is the head of state and government, as well as the commander-in-chief of the Mexican military forces. The President also appoints the Cabinet and other officers. The President is responsible for executing and enforcing the law, and has the power to veto bills.

The highest organ of the judicial branch of government is the Supreme Court of Justice, the national supreme court, which has eleven judges appointed by the President and approved by the Senate. The Supreme Court of Justice interprets laws and judges cases of federal competency. Other institutions of the judiciary are the Federal Electoral Tribunal, collegiate, unitary and district tribunals, and the Council of the Federal Judiciary.




Three parties have historically been the dominant parties in Mexican politics: the National Action Party: a conservative party founded in 1939 and belonging to the Christian Democrat Organization of America; the Institutional Revolutionary Party, a center-left party and member of Socialist International that was founded in 1929 to unite all the factions of the Mexican Revolution and held an almost hegemonic power in Mexican politics since then; the Party of the Democratic Revolution: a left-wing party, founded in 1989 as the successor of the coalition of socialists and liberal parties.




Public security is enacted at the three levels of government, each of which has different prerogatives and responsibilities. Local and state police departments are primarily in charge of law enforcement, whereas the Mexican Federal Police are in charge of specialized duties. All levels report to the Secretaría de Seguridad Pública (Secretary of Public Security). The General Attorney's Office (Procuraduría General de la República, PGR) is the executive power's agency in charge of investigating and prosecuting crimes at the federal level, mainly those related to drug and arms trafficking, espionage, and bank robberies. The PGR operates the Federal Investigations Agency (Agencia Federal de Investigación, AFI) an investigative and preventive agency.
While the government generally respects the human rights of its citizens, serious abuses of power have been reported in security operations in the southern part of the country and in indigenous communities and poor urban neighborhoods. The National Human Rights Commission has had little impact in reversing this trend, engaging mostly in documentation but failing to use its powers to issue public condemnations to the officials who ignore its recommendations. By law, all defendants have the rights that assure them fair trials and humane treatment; however, the system is overburdened and overwhelmed with several problems.
Despite the efforts of the authorities to fight crime and fraud, most Mexicans have low confidence in the police or the judicial system, and therefore, few crimes are actually reported by the citizens. The Global Integrity Index which measures the existence and effectiveness of national anti-corruption mechanisms rated Mexico 31st behind Kenya, Thailand, and Russia. In 2008, president Calderón proposed a major reform of the judicial system, which was approved by the Congress of the Union, which included oral trials, the presumption of innocence for defendants, the authority of local police to investigate crime—until then a prerogative of special police units—and several other changes intended to speed up trials.




Drug cartels are a major concern in Mexico. Mexico's drug war has left over 60,000 dead and perhaps another 20,000 missing. The Mexican drug cartels have as many as 100,000 members. The Mexican government's National Geography and Statistics Institute estimated that there were 41,563 crimes per 100,000 residents in 2012.
President Felipe Calderón made abating organized crime one of the top priorities of his administration by deploying military personnel to cities where drug cartels operate. This move was criticized by the opposition parties and the National Human Rights Commission for escalating the violence, but its effects have been positively evaluated by the US State Department's Bureau for International Narcotics and Law Enforcement Affairs as having obtained "unprecedented results" with "many important successes".
Since President Felipe Calderón launched a crackdown against cartels in 2006, more than 28,000 alleged criminals have been killed. Of the total drug-related violence 4% are innocent people, mostly by-passers and people trapped in between shootings; 90% accounts for criminals and 6% for military personnel and police officers. In October 2007, President Calderón and US president George W. Bush announced the Mérida Initiative, a plan of law enforcement cooperation between the two countries.




The foreign relations of Mexico are directed by the President of Mexico and managed through the Ministry of Foreign Affairs. The principles of the foreign policy are constitutionally recognized in the Article 89, Section 10, which include: respect for international law and legal equality of states, their sovereignty and independence, non-intervention in the domestic affairs of other countries, peaceful resolution of conflicts, and promotion of collective security through active participation in international organizations. Since the 1930s, the Estrada Doctrine has served as a crucial complement to these principles.
Mexico is one of the founding members of several international organizations, most notably the United Nations, the Organization of American States, the Organization of Ibero-American States, the OPANAL and the Rio Group. In 2008, Mexico contributed over 40 million dollars to the United Nations regular budget. In addition, it was the only Latin American member of the Organisation for Economic Co-operation and Development since it joined in 1994 until Chile gained full membership in 2010.
Mexico is considered a regional power hence its presence in major economic groups such as the G8+5 and the G-20. In addition, since the 1990s Mexico has sought a reform of the United Nations Security Council and its working methods with the support of Canada, Italy, Pakistan and other nine countries, which form a group informally called the Coffee Club.
After the War of Independence, the relations of Mexico were focused primarily on the United States, its northern neighbor, largest trading partner, and the most powerful actor in hemispheric and world affairs. Mexico supported the Cuban government since its establishment in the early 1960s, the Sandinista revolution in Nicaragua during the late 1970s, and leftist revolutionary groups in El Salvador during the 1980s. Felipe Calderón's administration put a greater emphasis on relations with Latin America and the Caribbean.




The Mexican Armed Forces have two branches: the Mexican Army (which includes the Mexican Air Force), and the Mexican Navy. The Mexican Armed Forces maintain significant infrastructure, including facilities for design, research, and testing of weapons, vehicles, aircraft, naval vessels, defense systems and electronics; military industry manufacturing centers for building such systems, and advanced naval dockyards that build heavy military vessels and advanced missile technologies.

In recent years, Mexico has improved its training techniques, military command and information structures and has taken steps to becoming more self-reliant in supplying its military by designing as well as manufacturing its own arms, missiles, aircraft, vehicles, heavy weaponry, electronics, defense systems, armor, heavy military industrial equipment and heavy naval vessels. Since the 1990s, when the military escalated its role in the war on drugs, increasing importance has been placed on acquiring airborne surveillance platforms, aircraft, helicopters, digital war-fighting technologies, urban warfare equipment and rapid troop transport.
Mexico has the capabilities to manufacture nuclear weapons, but abandoned this possibility with the Treaty of Tlatelolco in 1968 and pledged to only use its nuclear technology for peaceful purposes. In 1970, Mexico's national institute for nuclear research successfully refined weapons grade uranium which is used in the manufacture of nuclear weapons but in April 2010, Mexico agreed to turn over its weapons grade uranium to the United States.
Historically, Mexico has remained neutral in international conflicts, with the exception of World War II. However, in recent years some political parties have proposed an amendment of the Constitution in order to allow the Mexican Army, Air Force or Navy to collaborate with the United Nations in peacekeeping missions, or to provide military help to countries that officially ask for it.




The United Mexican States are a federation of 31 free and sovereign states, which form a union that exercises a degree of jurisdiction over the Federal District and other territories.
Each state has its own constitution, congress, and a judiciary, and its citizens elect by direct voting a governor for a six-year term, and representatives to their respective unicameral state congresses for three-year terms.
The Federal District is a special political division that belongs to the federation as a whole and not to a particular state, and as such, has more limited local rule than the nation's states.
The states are divided into municipalities, the smallest administrative political entity in the country, governed by a mayor or municipal president (presidente municipal), elected by its residents by plurality.




Mexico has the 15th largest nominal GDP and the 11th largest by purchasing power parity. GDP annual average growth for the period of 1995–2002 was 5.1%. Mexico's Gross Domestic Product (GDP) in purchasing power parity (PPP) was estimated at US $2.2602 trillion in 2015, and $1.3673 trillion in nominal exchange rates. Mexico's GDP in PPP per capita was US $18,714.05. The World Bank reported in 2009 that the country's Gross National Income in market exchange rates was the second highest in Latin America, after Brazil at US $1,830.392 billion, which lead to the highest income per capita in the region at $14,400. Mexico is now firmly established as an upper middle-income country. After the slowdown of 2001 the country has recovered and has grown 4.2, 3.0 and 4.8 percent in 2004, 2005 and 2006, even though it is considered to be well below Mexico's potential growth. Furthermore, after the 2008–2009 recession, the economy grew an average of 3.32 percent per year from 2010 to 2014.
From the late 1990s onwards, the majority of the population has been part of the growing middle class. But from 2004 to 2008 the portion of the population who received less than half of the median income has risen from 17% to 21% and the absolute levels of poverty rose from 2006 to 2010, with a rise in persons living in extreme or moderate poverty rising from 35 to 46% (52 million persons). This is also reflected by the fact that infant mortality in Mexico is three times higher than the average among OECD nations, and the literacy levels are in the median range of OECD nations. Nevertheless, according to Goldman Sachs, by 2050 Mexico will have the 5th largest economy in the world.
Among the OECD countries, Mexico has the second highest degree of economic disparity between the extremely poor and extremely rich, after Chile – although it has been falling over the last decade, being only one of few countries in which this is the case. The bottom ten percent in the income hierarchy disposes of 1.36% of the country's resources, whereas the upper ten percent dispose of almost 36%. OECD also notes that Mexico's budgeted expenses for poverty alleviation and social development is only about a third of the OECD average – both in absolute and relative numbers.
According to a 2008 UN report the average income in a typical urbanized area of Mexico was $26,654, while the average income in rural areas just miles away was only $8,403. Daily minimum wages are set annually by law and determined by zone; $67.29 Mexican pesos ($5.13 USD) in Zone A and $63.77 Mexican pesos ($4.86 USD) in Zone B.
The electronics industry of Mexico has grown enormously within the last decade. Mexico has the sixth largest electronics industry in the world after China, United States, Japan, South Korea, and Taiwan. Mexico is the second largest exporter of electronics to the United States where it exported $71.4 billion worth of electronics in 2011. The Mexican electronics industry is dominated by the manufacture and OEM design of televisions, displays, computers, mobile phones, circuit boards, semiconductors, electronic appliances, communications equipment and LCD modules. The Mexican electronics industry grew 20% between 2010 and 2011, up from its constant growth rate of 17% between 2003 and 2009. Currently electronics represent 30% of Mexico's exports.
Mexico produces the most automobiles of any North American nation. The industry produces technologically complex components and engages in some research and development activities. The "Big Three" (General Motors, Ford and Chrysler) have been operating in Mexico since the 1930s, while Volkswagen and Nissan built their plants in the 1960s. In Puebla alone, 70 industrial part-makers cluster around Volkswagen. In the 2010s expansion of the sector was surging. In 2014 alone, more than $10 billion in investment was committed. Kia Motors in August 2014 announced plans for a $1 billion factory in Nuevo León. At the time Mercedes-Benz and Nissan were already building a $1.4 billion plant near Puebla, while BMW was planning a $1-billion assembly plant in San Luis Potosí. Additionally, Audi began building a $1.3 billion factory near Puebla in 2013.
The domestic car industry is represented by DINA S.A., which has built buses and trucks since 1962, and the new Mastretta company that builds the high-performance Mastretta MXT sports car. In 2006, trade with the United States and Canada accounted for almost 50% of Mexico's exports and 45% of its imports. During the first three quarters of 2010, the United States had a $46.0 billion trade deficit with Mexico. In August 2010 Mexico surpassed France to become the 9th largest holder of US debt. The commercial and financial dependence on the US is a cause for concern.
The remittances from Mexican citizens working in the United States account for 0.2% of Mexico's GDP which was equal to US$20 billion per year in 2004 and is the tenth largest source of foreign income after oil, industrial exports, manufactured goods, electronics, heavy industry, automobiles, construction, food, banking and financial services. According to Mexico's central bank, remittances in 2008 amounted to $25bn.
Major players in the broadcasting industry are Televisa, the largest Spanish media company in the Spanish-speaking world, and TV Azteca.




The telecommunications industry is mostly dominated by Telmex (Teléfonos de México), privatized in 1990. By 2006, Telmex had expanded its operations to Colombia, Peru, Chile, Argentina, Brazil and Uruguay and the United States. Other players in the domestic industry are Axtel and Maxcom. Because of Mexican orography, providing a landline telephone service at remote mountainous areas is expensive, and the penetration of line-phones per capita is low compared to other Latin American countries, at 40 percent; however, 82% of Mexicans over the age of 14 own a mobile phone. Mobile telephony has the advantage of reaching all areas at a lower cost, and the total number of mobile lines is almost two times that of landlines, with an estimation of 63 million lines. The telecommunication industry is regulated by the government through Cofetel (Comisión Federal de Telecomunicaciones).
The Mexican satellite system is domestic and operates 120 earth stations. There is also extensive microwave radio relay network and considerable use of fiber-optic and coaxial cable. Mexican satellites are operated by Satélites Mexicanos (Satmex), a private company, leader in Latin America and servicing both North and South America. It offers broadcast, telephone and telecommunication services to 37 countries in the Americas, from Canada to Argentina. Through business partnerships Satmex provides high-speed connectivity to ISPs and Digital Broadcast Services. Satmex maintains its own satellite fleet with most of the fleet being designed and built in Mexico.
The use of radio, television, and Internet in Mexico is prevalent. There are approximately 1,410 radio broadcast stations and 236 television stations (excluding repeaters). Major players in the broadcasting industry are Televisa—the largest media company in the Spanish-speaking world—and TV Azteca.




Energy production in Mexico is managed by state-owned companies: the Federal Commission of Electricity and Pemex.
Pemex, the public company in charge of exploration, extraction, transportation and marketing of crude oil and natural gas, as well as the refining and distribution of petroleum products and petrochemicals, is one of the largest companies in the world by revenue, making US $86 billion in sales a year. Mexico is the sixth-largest oil producer in the world, with 3.7 million barrels per day. In 1980 oil exports accounted for 61.6% of total exports; by 2000 it was only 7.3%.
The largest hydro plant in Mexico is the 2,400 MW Manuel Moreno Torres Dam in Chicoasén, Chiapas, in the Grijalva River. This is the world's fourth most productive hydroelectric plant.
Mexico is the country with the world's third largest solar potential. The country's gross solar potential is estimated at 5kWh/m2 daily, which corresponds to 50 times national electricity generation. Currently, there is over 1 million square meters of solar thermal panels installed in Mexico, while in 2005, there were 115,000 square meters of solar PV (photo-voltaic). It is expected that in 2012 there will be 1,8 million square meters of installed solar thermal panels.
The project named SEGH-CFE 1, located in Puerto Libertad, Sonora, Northwest of Mexico, will have capacity of 46.8 MW from an array of 187,200 solar panels when complete in 2013. All of the electricity will be sold directly to the CFE and absorbed into the utility's transmission system for distribution throughout their existing network. At an installed capacity of 46.8 MWp, when complete in 2013, the project will be the first utility scale project of its kind in Mexico and the largest solar project of any kind in Latin America.




The National Autonomous University of Mexico was officially established in 1910, and the university become one of the most important institutes of higher learning in Mexico. UNAM provides world class education in science, medicine, and engineering. Many scientific institutes and new institutes of higher learning, such as National Polytechnic Institute (founded in 1936), were established during the first half of the 20th century. Most of the new research institutes were created within UNAM. Twelve institutes were integrated into UNAM from 1929 to 1973. In 1959, the Mexican Academy of Sciences was created to coordinate scientific efforts between academics.
In 1995, the Mexican chemist Mario J. Molina shared the Nobel Prize in Chemistry with Paul J. Crutzen and F. Sherwood Rowland for their work in atmospheric chemistry, particularly concerning the formation and decomposition of ozone. Molina, an alumnus of UNAM, became the first Mexican citizen to win the Nobel Prize in science.
In recent years, the largest scientific project being developed in Mexico was the construction of the Large Millimeter Telescope (Gran Telescopio Milimétrico, GMT), the world's largest and most sensitive single-aperture telescope in its frequency range. It was designed to observe regions of space obscured by stellar dust.




Mexico has traditionally been among the most visited countries in the world according to the World Tourism Organization and it is the most visited country in the Americas, after the United States. The most notable attractions are the Mesoamerican ruins, cultural festivals, colonial cities, nature reserves and the beach resorts. The nation's wide range of climates, from temperate to tropical, and unique culture – a fusion of the European and the Mesoamerican – make Mexico an attractive destination. The peak tourism seasons in the country are during December and the mid-Summer, with brief surges during the week before Easter and Spring break, when many of the beach resort sites become popular destinations for college students from the United States.
Mexico has the 23rd highest income from tourism in the world, and the highest in Latin America. The vast majority of tourists come to Mexico from the United States and Canada followed by Europe and Asia. A smaller number also come from other Latin American countries. In the 2011 Travel and Tourism Competitiveness Index report, Mexico was ranked 43rd in the world, which was 4th in the Americas .
The coastlines of Mexico harbor many stretches of beaches that are frequented by sun bathers and other visitors. On the Yucatán peninsula, one of the most popular beach destinations is the resort town of Cancún, especially among university students during spring break. Just offshore is the beach island of Isla Mujeres, and to the east is the Isla Holbox. To the south of Cancun is the coastal strip called Riviera Maya which includes the beach town of Playa del Carmen and the ecological parks of Xcaret and Xel-Há. A day trip to the south of Cancún is the historic port of Tulum. In addition to its beaches, the town of Tulum is notable for its cliff-side Mayan ruins.
On the Pacific coast is the notable tourist destination of Acapulco. Once the destination for the rich and famous, the beaches have become crowded and the shores are now home to many multi-story hotels and vendors. Acapulco is home to renowned cliff divers: trained divers who leap from the side of a vertical cliff into the surf below.
At the southern tip of the Baja California peninsula is the resort town of Cabo San Lucas, a town noted for its beaches and marlin fishing. Further north along the Sea of Cortés is the Bahía de La Concepción, another beach town known for its sports fishing. Closer to the United States border is the weekend draw of San Felipe, Baja California.




The roadway network in Mexico is extensive and all areas in the country are covered by it. The roadway network in Mexico has an extent of 366,095 km (227,481 mi), of which 116,802 km (72,577 mi) are paved, making it the largest paved-roadway network in Latin America. Of these, 10,474 km (6,508 mi) are multi-lane expressways: 9,544 km (5,930 mi) are four-lane highways and the rest have 6 or more lanes.
Mexico was one of the first Latin American countries to promote railway development, and the network covers 30,952 km (19,233 mi). The Secretary of Communications and Transport of Mexico proposed a high-speed rail link that will transport its passengers from Mexico City to Guadalajara, Jalisco. The train, which will travel at 300 kilometres per hour (190 miles per hour), will allow passengers to travel from Mexico City to Guadalajara in just 2 hours. The whole project was projected to cost 240 billion pesos, or about 25 billion US$ and is being paid for jointly by the Mexican government and the local private sector including the wealthiest man in the world, Mexico's billionaire business tycoon Carlos Slim. The government of the state of Yucatán is also funding the construction of a high speed line connecting the cities of Cozumel to Mérida and Chichen Itza and Cancún.
Mexico has 233 airports with paved runways; of these, 35 carry 97% of the passenger traffic. The Mexico City International Airport remains the largest in Latin America and the 44th largest in the world transporting 21 million passengers a year.




Among the achievements is a significant increase in access to piped water supply in urban areas (88% to 93%) as well as in rural areas (50% to 74%) between 1990 and 2010. Additionally, a strong nationwide increase in access to improved sanitation (64% to 85%) was observed in the same period. Other achievements include the existence of a functioning national system to finance water and sanitation infrastructure with a National Water Commission as its apex institution; and the existence of a few well-performing utilities such as Aguas y Drenaje de Monterrey.
The challenges include water scarcity in the northern and central parts of the country; inadequate water service quality (drinking water quality; 55% of Mexicans receiving water only intermittently according to results of the 2000 census); poor technical and commercial efficiency of most utilities (with an average level of non-revenue water of 51% in 2003); an insufficient share of wastewater receiving treatment (36% in 2006); and still inadequate access in rural areas. In addition to on-going investments to expand access, the government has embarked on a large investment program to improve wastewater treatment.




The 2010 Census showed a population of 112,336,538, making it the most populous Spanish-speaking country in the world. Between 2005 and 2010, the Mexican population grew at an average of 1.70% per year, up from 1.16% per year between 2000 and 2005.
Prior to 2015, the Mexican government did not ask for the ethnicity nor race of its citizens (last doing so in 1921). The number of indígenas (indigenous peoples) was defined narrowly to speakers of one of Mexico's 62 indigenous languages or connections to established indigenous communities. As a result, the 2010 census found that 14.86% of the population was indigenous. However, beginning with the 2015 inter-census estimate, the government now asks whether an individual self-identifies as indigenous (21.5% of the population) and/or Afro-Mexican (1.2% of the population). These categories are not exclusionary and an individual can indicate both indigenous and afro-descendant heritage. No other groups (such as mestizos, whites or Asian-descendants) are quantified by the government.
As of 2015, the foreign-born population was 1,007,063. The majority of these individuals were born in the United States and Mexico is home to the largest number of U.S. citizens abroad. After Americans the largest immigrant groups are Guatemalans, Spaniards and Colombians. Besides the Spanish, large immigrant-descended groups are the French, Germans, Lebanese and Chinese. Mexico is the largest source of immigration to the United States. Some 11.6 million residents of the United States have Mexican citizenship as of 2014.




Mexico is ethnically diverse; the various indigenous peoples, whites, afro-descendants and mestizos are united under a single national identity. The core part of Mexican national identity is formed on the basis of a synthesis of cultures, primarily European culture and indigenous cultures, in a process known as mestizaje, alluding to the mixed biological origins of the majority of Mexicans. Mexican politicians and reformers such as José Vasconcelos (promoter of the cosmic race) and Manuel Gamio (promoter of indigenismo) were instrumental in building a Mexican national identity on the concept of mestizaje.
In 1810, towards the end of the Colonial era, the population of Mexico was estimated to be about 6 million (based on the 1793 Revillagigedo Census and the 1803 estimate by geographer Alexander Humboldt and 1810 estimate by royal accountant Francisco Navarro y Noriega). From these population estimates, anthropologist Gonzalo Aguirre Beltrán approximated the following in regards to race and ethnicity: there were some 15,000 peninsulares (expelled after independence), less than 10,000 Africans (mostly enslaved, legally freed in 1829), more than a million "Euromestizos" (criollos and individuals of primarily European descent, such as castizos), some 700,000 "Indomestizos" (individuals of significant indigenous descent), some 600,000 "Afromestizos" (individuals of significant African descent, such as mulatos) and some 3.7 million indigenous peoples. Mexico does not ask about race in its census in part because it eliminated the legal basis of the Colonial caste system (based on race and birth) after independence.
A large majority of Mexicans have been classified as "Mestizos" (between 50% to 67% according to the Encyclopædia Britannica). In modern Mexico, the term mestizo is primarily a cultural identity rather than the racial identity it was during the colonial era, resulting in individuals with varying phenotypes being classified under the same identity. The term is not in wide use in Mexican society, although often used in literature about Mexican social identities. Since the term carries a variety of socio-cultural, economic, racial and biological meanings, it was deemed too imprecise to be used for ethnic classification and was abandoned in Mexican censuses. Various genetic studies have found that Mexico's population is not uniform in its genetic composition, with there being significant regional variation. According to the country's Instituto Nacional de Medicina Genómica, on average mestizos of primarily European ancestry predominate in the north while mestizos from the southern region have predominately indigenous ancestry. Mestizos from the center of the country have a more even contribution from Europeans and Natives, while the highest African contribution was found in the southwest and Veracruz. In the Yucatán peninsula the word Mestizo is even used about Maya-speaking populations living in traditional communities, because during the Caste War of the late 19th century those Maya who did not join the rebellion were classified as mestizos. In Chiapas the word "Ladino" is used instead of mestizo.
Estimates of the number of whites range from 10% to 20% according to the Encyclopædia Britannica. The numbers very since the criteria to define mestizo might be different from study to study and in Mexico some number of white people have been historically classified as mestizos since the Mexican government defined ethnicity on cultural standards as opposed to racial ones. During the colonial and independent era, most of the European migration into Mexico was Spanish. However, in the 19th and 20th centuries a substantial number of non-Spanish Europeans immigrated to the country. However, at its height, the total immigrant population in Mexico never exceeded two percent of the total. Some of these immigrants, along with non-European immigrants, were forced out of the country as a result of the Mexican Revolution. Mexico's northern regions have the greatest European population and admixture. According to the last racial census Mexico took, which was in 1921, there were no states in Mexico that had a majority "white" population, and in virtually every state in the north Mestizos were the largest population group. The only state where "whites" outnumbered Mestizos was Sonora, in which "whites" composed 41.85% of the population, and Mestizos 40.38%.
The absolute indigenous population of Mexico (26,694,928 individuals as of 2015) is growing, but at a slower rate than the rest of the population so that the percentage of indigenous peoples in regards to total population is nonetheless falling. The majority of the indigenous population is concentrated in the central and southern states, primarily in rural areas. Some indigenous communities have a degree of autonomy under the legislation of "usos y costumbres", which allows them to regulate some internal issues under customary law. According to the National Commission for the Development of Indigenous Peoples, the states with the greatest proportion of indigenous residents are: Yucatán at 59%, Quintana Roo 39% and Campeche 27%, chiefly Maya; Oaxaca with 48% of the population, the most numerous groups being the Mixtec and Zapotec peoples; Chiapas at 28%, the majority being Tzeltal and Tzotzil Maya; Hidalgo 24%, the majority being Otomi; Puebla 19%, and Guerrero 17%, mostly Nahua peoples and the states of San Luis Potosí and Veracruz both home to a population that is 15% indigenous, mostly from the Totonac, Nahua and Teenek (Huastec) groups. All of the indices of social development for the indigenous population are considerably lower than the national average. In all states indigenous people have higher infant mortality, in some states almost double of the non-indigenous populations. Literacy rates are also much lower, with 27% of indigenous children between 6 and 14 being illiterate compared to a national average of 12%. The indigenous population participate in the workforce longer than the national average, starting earlier and continuing longer. However, 55% of the indigenous population receive less than a minimum salary, compared to 20% for the national average. Many practice subsistence agriculture and receive no salaries. Indigenous people also have less access to health care and a lower quality of housing.
The Afro-Mexican population (1,381,853 individuals as of 2015) is an ethnic group made up of descendants of Colonial-era slaves and recent immigrants of sub-Saharan African descent. Mexico had an active slave trade during the colonial period and some 200,000 Africans were taken there, primarily in the 17th century. The creation of a national Mexican identity, especially after the Mexican Revolution, emphasized Mexico's indigenous and European past; it passively eliminated the African ancestors and contributions. Most of the African-descended population was absorbed into the surrounding Mestizo (mixed European/indigenous) and indigenous populations through unions among the groups. Evidence of this long history of intermarriage with Mestizo and indigenous Mexicans is also expressed in the fact that in the 2015 inter-census, 64.9% (896,829) of Afro-Mexicans also identified as indigenous. It was also reported that 9.3% of Afro-Mexicans speak an indigenous language. The states with the highest self-report of Afro-Mexicans were Guerrero (6.5% of the population), Oaxaca (4.95%) and Veracruz (3.28%). Afro-Mexican culture is strongest in the communities of the Costa Chica of Oaxaca and Costa Chica of Guerrero.
Smaller ethnic groups in Mexico include South and East Asians, present since the colonial era. During the colonial era Asians were termed Chino (regardless of ethnicity), and arrived as merchants, artisans and slaves. The largest group were Filipinos and some 200,000 Mexicans can trace Filipino ancestry. Modern Asian immigration began in the late 19th century and at one point in the early 20th century, the Chinese were the second largest immigrant group. During the early 20th century, a substantial number of Arabs (mostly Christians) began arriving from the crumbling Ottoman Empire. The largest group were the Lebanese and an estimated 400,000 Mexicans have some Lebanese ancestry.




The country has the largest Spanish-speaking population in the world with almost a third of all Spanish native speakers.
Almost all of the Mexican population speaks Spanish, 99.3% according to the latest census, nonetheless around 5.4% still speaks an indigenous language besides Spanish. The indigenous languages with most speakers are Nahuatl, spoken by approximately 1.45 million people, Yukatek Maya spoken by some 750,000 people and the Mixtec and Zapotec languages each spoken by more than 400,000 people.
The National Institute of Indigenous Languages INALI recognizes 68 linguistic groups and some 364 different specific varieties of indigenous languages. Since the promulgation of the Law of Indigenous Linguistic Rights in 2003, these languages have had status as national languages, with equal validity with Spanish in all the areas and contexts in which they are spoken.
In addition to the indigenous languages, other minority languages are spoken by immigrant populations, such as the 80,000 German-speaking Mennonites in Mexico, and 5,000 the Chipilo dialect of the Venetian language spoken in Chipilo, Puebla.
English is the most common foreign language in Mexico.




Here are the 20 largest urban areas in Mexico.




The 2010 census by the Instituto Nacional de Estadística y Geografía (National Institute of Statistics and Geography) gave Roman Catholicism as the main religion, with 83% of the population, while 10% (10,924,103) belong to other Christian denominations, including Evangelicals (5%); Pentecostals (1.6%); other Protestant or Reformed (0.7%); Jehovah's Witnesses (1.4%); Seventh-day Adventists (0.6%); and members of The Church of Jesus Christ of Latter-day Saints (0.3%). 172,891 (or less than 0.2% of the total) belonged to other, non-Christian religions; 4.7% declared having no religion; 2.7% were unspecified.
The 92,924,489 Catholics of Mexico constitute in absolute terms the second largest Catholic community in the world, after Brazil's. 47% percent of them attend church services weekly. The feast day of Our Lady of Guadalupe, the patron saint of Mexico, is celebrated on December 12 and is regarded by many Mexicans as the most important religious holiday of their country.
The 2010 census reported 314,932 members of The Church of Jesus Christ of Latter-day Saints, though the church in 2009 claimed to have over one million registered members. About 25% of registered members attend a weekly sacrament service although this can fluctuate up and down.
The presence of Jews in Mexico dates back to 1521, when Hernán Cortés conquered the Aztecs, accompanied by several Conversos. According to the 2010 census, there are 67,476 Jews in Mexico. Islam in Mexico is practiced by a small population in the city of Torreón, Coahuila, and there are an estimated 300 Muslims in the San Cristóbal de las Casas area in Chiapas. In the 2010 census 18,185 Mexicans reported belonging to an Eastern religion, a category which includes a tiny Buddhist population.




Until the twentieth century, Mexico was an overwhelmingly rural country, with rural women's status defined within the context of the family and local community. With urbanization beginning in the sixteenth century, following the Spanish conquest of the Aztec empire, cities have provided economic and social opportunities not possible within rural villages. Roman Catholicism in Mexico has shaped societal attitudes about women's social role, emphasizing the role of women as nurturers of the family, with the Virgin Mary as a model. Marianismo has been an ideal, with women's role as being within the family under the authority of men. In the twentieth century, Mexican women made great strides toward toward a more equal legal and social status. In 1953, women in Mexico were granted the vote in national elections.
Mexican women face discrimination and at times harassment from the machismo population. Although women in Mexico are making big advancements they are faced with the traditional expectations of being the head of the household. Researcher Margarita Valdés noted that while there are few inequalities enforced by law or policy in Mexico, there are gender inequalities perpetuated by social structures and Mexican cultural expectations that limit the capabilities of Mexican women.
As of 2014, Mexico has the 16th highest rate of homicides committed against women in the world  The prevalence of domestic violence against women in Mexican marital relationships varies at between 30 and 60 percent of relationships. The remains of the victims were frequently mutilated. According to a 1997 study, domestic abuse in Mexican culture "is embedded in gender and marital relations fostered in Mexican women's dependence on their spouses for subsistence and for self-esteem, sustained by ideologies of romantic love, by family structure and residential arrangements." The perpetrators are often the boyfriend, father-in-law, ex-husbands or husbands but only 1.6% of the murder cases led to an arrest and sentencing.




Mexican culture reflects the complexity of the country's history through the blending of indigenous cultures and the culture of Spain, imparted during Spain's 300-year colonization of Mexico. Exogenous cultural elements have been incorporated into Mexican culture as time has passed.
The Porfirian era (el Porfiriato), in the last quarter of the 19th century and the first decade of the 20th century, was marked by economic progress and peace. After four decades of civil unrest and war, Mexico saw the development of philosophy and the arts, promoted by President Díaz himself. Since that time, as accentuated during the Mexican Revolution, cultural identity has had its foundation in the mestizaje, of which the indigenous (i.e. Amerindian) element is the core. In light of the various ethnicities that formed the Mexican people, José Vasconcelos in his publication La Raza Cósmica (The Cosmic Race) (1925) defined Mexico to be the melting pot of all races (thus extending the definition of the mestizo) not only biologically but culturally as well.




Mexican literature has its antecedents in the literatures of the indigenous settlements of Mesoamerica. The most well known prehispanic poet is Nezahualcoyotl. Modern Mexican literature was influenced by the concepts of the Spanish colonialization of Mesoamerica. Outstanding colonial writers and poets include Juan Ruiz de Alarcón and Juana Inés de la Cruz.
Other writers include Alfonso Reyes, José Joaquín Fernández de Lizardi, Ignacio Manuel Altamirano, Carlos Fuentes, Octavio Paz (Nobel Laureate), Renato Leduc, Carlos Monsiváis, Elena Poniatowska, Mariano Azuela ("Los de abajo") and Juan Rulfo ("Pedro Páramo"). Bruno Traven wrote "Canasta de cuentos mexicanos" (Mexican tales basket), "El tesoro de la Sierra Madre" (Treasure of the Sierra Madre).




Post-revolutionary art in Mexico had its expression in the works of renowned artists such as David Alfaro Siqueiros, Federico Cantú Garza, Frida Kahlo, Juan O'Gorman, José Clemente Orozco, Diego Rivera, and Rufino Tamayo. Diego Rivera, the most well-known figure of Mexican muralism, painted the Man at the Crossroads at the Rockefeller Center in New York City, a huge mural that was destroyed the next year because of the inclusion of a portrait of Russian communist leader Lenin. Some of Rivera's murals are displayed at the Mexican National Palace and the Palace of Fine Arts.
Mesoamerican architecture is mostly noted for its pyramids which are the largest such structures outside of Ancient Egypt. Spanish Colonial architecture is marked by the contrast between the simple, solid construction demanded by the new environment and the Baroque ornamentation exported from Spain. Mexico, as the center of New Spain has some of the most renowned buildings built in this style.




Mexican films from the Golden Age in the 1940s and 1950s are the greatest examples of Latin American cinema, with a huge industry comparable to the Hollywood of those years. Mexican films were exported and exhibited in all of Latin America and Europe. Maria Candelaria (1943) by Emilio Fernández, was one of the first films awarded a Palme d'Or at the Cannes Film Festival in 1946, the first time the event was held after World War II. The famous Spanish-born director Luis Buñuel realized in Mexico, between 1947 and 1965 some of him master pieces like Los Olvidados (1949) and Viridiana (1961). Famous actors and actresses from this period include María Félix, Pedro Infante, Dolores del Río, Jorge Negrete and the comedian Cantinflas.
More recently, films such as Como agua para chocolate (1992), Cronos (1993), Y tu mamá también (2001), and Pan's Labyrinth (2006) have been successful in creating universal stories about contemporary subjects, and were internationally recognized, as in the prestigious Cannes Film Festival. Mexican directors Alejandro González Iñárritu (Amores perros, Babel, Birdman, The Revenant), Alfonso Cuarón (Children of Men, Harry Potter and the Prisoner of Azkaban, Gravity), Guillermo del Toro, Carlos Carrera (The Crime of Father Amaro), screenwriter Guillermo Arriaga and photographer Emmanuel Lubezki are some of the most known present-day film makers.
Some Mexican actors have achieved recognition as Hollywood stars. These include Ramon Novarro, Dolores del Río, Lupe Vélez, Gilbert Roland, Anthony Quinn, Katy Jurado, Ricardo Montalbán and Salma Hayek




There are two major television companies in Mexico that own the four primary networks that broadcast to 75% of the population. They are Televisa, which owns the Canal de las Estrellas and Canal 5 networks, and TV Azteca, which owns the Azteca 7 and Azteca Trece networks. Televisa is also the largest producer of Spanish-language content in the world and also the world's largest Spanish-language media network. Grupo Multimedios is another media conglomerate with Spanish-language broadcasting in Mexico, Spain, and the United States. The telenovelas are very traditional in Mexico and are translated to many languages and seen all over the world with renowned names like Verónica Castro, Lucía Méndez and Thalía.




Mexican society enjoys a vast array of music genres, showing the diversity of Mexican culture. Traditional music includes mariachi, banda, norteño, ranchera and corridos; on an everyday basis most Mexicans listen to contemporary music such as pop, rock, etc. in both English and Spanish. Mexico has the largest media industry in Latin America, producing Mexican artists who are famous in Central and South America and parts of Europe, especially Spain.
Some well-known Mexican singers are Thalía, Luis Miguel, Juan Gabriel, Alejandro Fernández, Julieta Venegas, Gloria Trevi and Paulina Rubio. Mexican singers of traditional music are: Lila Downs, Susana Harp, Jaramar, GEO Meneses and Alejandra Robles. Popular groups are Café Tacuba, Caifanes, Molotov and Maná, among others. Since the early years of the 2000s (decade), Mexican rock has seen widespread growth both domestically and internationally.
According to the Sistema Nacional de Fomento Musical, there are between 120 and 140 youth orchestras affiliated to this federal agency from all federal states. Some states, through their state agencies in charge of culture and the arts—Ministry or Secretary or Institute or Council of Culture, or in some cases the Secretary of Education or the State University—sponsor the activities of a professional symphony orchestra or philharmonic crchestra so all citizens can have access to this artistic expression from the field of classical music. Mexico City is the most intense hub of this activity, hosting 12 professional orchestras sponsored by different agencies such as the National Institute of Fine Arts, the Secretary of Culture of the Federal District, The National University, the National Polytechnic Institute, a Delegación Política (Coyoacán) and private ventures.




Mexican cuisine is known for its intense and varied flavors, colorful decoration, and variety of spices. Most of today's Mexican food is based on pre-Columbian traditions, including Aztec and Maya, combined with culinary trends introduced by Spanish colonists.
The conquistadores eventually combined their imported diet of rice, beef, pork, chicken, wine, garlic and onions with the native pre-Columbian food, including maize, tomato, vanilla, avocado, guava, papaya, pineapple, chili pepper, beans, squash, sweet potato, peanut, and turkey.
Mexican food varies by region, because of local climate and geography and ethnic differences among the indigenous inhabitants and because these different populations were influenced by the Spaniards in varying degrees. The north of Mexico is known for its beef, goat and ostrich production and meat dishes, in particular the well-known Arrachera cut.
Central Mexico's cuisine is largely made up of influences from the rest of the country, but also has its authentics, such as barbacoa, pozole, menudo, tamales, and carnitas.
Southeastern Mexico, on the other hand, is known for its spicy vegetable and chicken-based dishes. The cuisine of Southeastern Mexico also has quite a bit of Caribbean influence, given its geographical location. Veal is common in the Yucatan. Seafood is commonly prepared in the states that border the Pacific Ocean or the Gulf of Mexico, the latter having a famous reputation for its fish dishes, in particular à la veracruzana.
In modern times, other cuisines of the world have become very popular in Mexico, thus adopting a Mexican fusion. For example, sushi in Mexico is often made with a variety of sauces based on mango or tamarind, and very often served with serrano-chili-blended soy sauce, or complemented with vinegar, habanero and chipotle peppers
The most internationally recognized dishes include chocolate, tacos, quesadillas, enchiladas, burritos, tamales and mole among others. Regional dishes include mole poblano, chiles en nogada and chalupas from Puebla; cabrito and machaca from Monterrey, cochinita pibil from Yucatán, Tlayudas from Oaxaca, as well as barbacoa, chilaquiles, milanesas, and many others.




Mexico City hosted the XIX Olympic Games in 1968, making it the first Latin American city to do so. The country has also hosted the FIFA World Cup twice, in 1970 and 1986.
Mexico's most popular sport is association football. It is commonly believed that football was introduced in Mexico by Cornish miners at the end of the 19th century. By 1902 a five-team league had emerged with a strong British influence. Mexico's top clubs are América with 12 championships, Guadalajara with 11, and Toluca with 10. Antonio Carbajal was the first player to appear in five World Cups, and Hugo Sánchez was named best CONCACAF player of the 20th century by IFFHS.

The Mexican professional baseball league is named the Liga Mexicana de Beisbol. While usually not as strong as the United States, the Caribbean countries and Japan, Mexico has nonetheless achieved several international baseball titles. Mexican teams have won the Caribbean Series nine times. Mexico has had several players signed by Major League teams, the most famous of them being Dodgers pitcher Fernando Valenzuela.
In 2013, Mexico's basketball team won the Americas Basketball Championship and qualified for the 2014 Basketball World Cup where it reached the playoffs. Because of these achievements the country earned the hosting rights for the 2015 FIBA Americas Championship.
Bullfighting is a popular sport in the country, and almost all large cities have bullrings. Plaza México in Mexico City, is the largest bullring in the world, which seats 55,000 people. Professional wrestling (or Lucha libre in Spanish) is a major crowd draw with national promotions such as AAA, LLL, CMLL and others.
Mexico is an international power in professional boxing (at the amateur level, several Olympic boxing medals have also been won by Mexico). Vicente Saldivar, Rubén Olivares, Salvador Sánchez, Julio César Chávez, Ricardo Lopez and Erik Morales are but a few Mexican fighters who have been ranked among the best of all time.
Notable Mexican athletes include golfer Lorena Ochoa, who was ranked first in the LPGA world rankings prior to her retirement, Ana Guevara, former world champion of the 400 metres (1,300 ft) and Olympic subchampion in Athens 2004, Fernando Platas, four-time Olympic medal winning diver, and taekwondo fighter María Espinoza, most decorated Mexican female Olympian.




Since the early 1990s, Mexico entered a transitional stage in the health of its population and some indicators such as mortality patterns are identical to those found in highly developed countries like Germany or Japan. Mexico's medical infrastructure is highly rated for the most part and is usually excellent in major cities, but rural communities still lack equipment for advanced medical procedures, forcing patients in those locations to travel to the closest urban areas to get specialized medical care. Social determinants of health can be used to evaluate the state of health in Mexico.
State-funded institutions such as Mexican Social Security Institute (IMSS) and the Institute for Social Security and Services for State Workers (ISSSTE) play a major role in health and social security. Private health services are also very important and account for 13% of all medical units in the country.
Medical training is done mostly at public universities with much specializations done in vocational or internship settings. Some public universities in Mexico, such as the University of Guadalajara, have signed agreements with the U.S. to receive and train American students in Medicine. Health care costs in private institutions and prescription drugs in Mexico are on average lower than that of its North American economic partners.




In 2004, the literacy rate was at 97% for youth under the age of 14 and 91% for people over 15, placing Mexico at the 24th place in the world rank according to UNESCO.
The National Autonomous University of Mexico ranks 190th place in the Top 200 World University Ranking published by The Times Higher Education Supplement in 2009. Private business schools also stand out in international rankings. IPADE and EGADE, the business schools of Universidad Panamericana and of Monterrey Institute of Technology and Higher Education respectively, were ranked in the top 10 in a survey conducted by The Wall Street Journal among recruiters outside the United States.




Index of Mexico-related articles
Outline of Mexico
 Mexico – Wikipedia book




Navarrete Linares, Federico (2008). Los pueblos indígenas de México (PDF online facsimile). Pueblos Indígenas del México Contemporáneo series (in Spanish). México, D.F.: Comisión Nacional para el Desarrollo de los Pueblos Indígenas. ISBN 978-970-753-157-4. OCLC 319215886. 
Satish Kumar*, Claire Bellis, Mark Zlojutro, Phillip E Melton, John Blangero and Joanne E Curran (2011). Large scale mitochondrial sequencing in Mexican Americans suggests a reappraisal of Native American origins (PDF).  CS1 maint: Multiple names: authors list (link)






Government
The Presidency of Mexico
Mexico Tourism Official Website |VisitMexico
General information
"Mexico". The World Factbook. Central Intelligence Agency. 
Mexico from UCB Libraries GovPubs
Mexico at DMOZ
Mexico from the BBC News
Mexico at Encyclopædia Britannica
 Wikimedia Atlas of Mexico
 Geographic data related to Mexico at OpenStreetMap
Key Development Forecasts for Mexico from International Futures
Mexico by World Painters.Canada (/ˈkænədə/; French: [ka.na.dɑ]) is a country in the northern half of North America. Its ten provinces and three territories extend from the Atlantic to the Pacific and northward into the Arctic Ocean, covering 9.98 million square kilometres (3.85 million square miles), making it the world's second-largest country by total area and the fourth-largest country by land area. Canada's border with the United States is the world's longest land border. The majority of the country has a cold or severely cold winter climate, but southerly areas are warm in summer. Canada is sparsely populated, the majority of its land territory being dominated by forest and tundra and the Rocky Mountains. About four-fifths of the country's population of 36 million people is urbanized and live near the southern border. Its capital is Ottawa, its largest metropolis is Toronto; other major urban areas include Montreal, Vancouver, Calgary, Edmonton, Quebec City, Winnipeg and Hamilton.
Canada has been inhabited for millennia by various Aboriginal peoples. Beginning in the 16th century, British and French claims were made on the area, with the colony of Canada first being established by the French in 1537. As a consequence of various conflicts, the United Kingdom gained and lost territories within British North America until it was left, in the late 18th century, with what mostly geographically comprises Canada today. Pursuant to the British North America Act, on July 1, 1867, the colonies of Canada, New Brunswick, and Nova Scotia joined to form the semi-autonomous federal Dominion of Canada. This began an accretion of provinces and territories to the mostly self-governing Dominion to the present ten provinces and three territories forming modern Canada.
In 1931, Canada achieved near total independence from the United Kingdom with the Statute of Westminster 1931, and full sovereignty was attained when the Canada Act 1982 removed the last remaining ties of legal dependence on the Parliament of the United Kingdom. Canada is a federal parliamentary democracy and a constitutional monarchy, with Queen Elizabeth II being the head of state. The country is officially bilingual at the federal level. It is one of the world's most ethnically diverse and multicultural nations, the product of large-scale immigration from many other countries. Its advanced economy is the eleventh largest in the world, relying chiefly upon its abundant natural resources and well-developed international trade networks. Canada's long and complex relationship with the United States has had a significant impact on its economy and culture.
Canada is a developed country and has the tenth highest nominal per capita income globally as well as the ninth highest ranking in the Human Development Index. It ranks among the highest in international measurements of government transparency, civil liberties, quality of life, economic freedom, and education. Canada is a Commonwealth realm member of the Commonwealth of Nations, a member of the Francophonie, and part of several major international and intergovernmental institutions or groupings including the United Nations, the North Atlantic Treaty Organization, the G8, the Group of Ten, the G20, the North American Free Trade Agreement and the Asia-Pacific Economic Cooperation forum.




While a variety of theories have been postulated for the etymological origins of Canada, the name is now accepted as coming from the St. Lawrence Iroquoian word kanata, meaning "village" or "settlement". In 1535, indigenous inhabitants of the present-day Quebec City region used the word to direct French explorer Jacques Cartier to the village of Stadacona. Cartier later used the word Canada to refer not only to that particular village, but the entire area subject to Donnacona (the chief at Stadacona); by 1545, European books and maps had begun referring to this small region along the St Lawrence River as Canada.
From the 16th to the early 18th century "Canada" referred to the part of New France that lay along the St. Lawrence River. In 1791, the area became two British colonies called Upper Canada and Lower Canada collectively named The Canadas; until their union as the British Province of Canada in 1841. Upon Confederation in 1867, Canada was adopted as the legal name for the new country, and the word Dominion was conferred as the country's title. The transition away from the use of Dominion was formally reflected in 1982 with the passage of the Canada Act, which refers only to Canada. Later that year, the name of national holiday was changed from Dominion Day to Canada Day. The term Dominion is also used to distinguish the federal government from the provinces, though after the Second World War the term federal had replaced dominion.







Aboriginal peoples in present-day Canada include the First Nations, Inuit, and Métis, the latter being a mixed-blood people who originated in the mid-17th century when First Nations and Inuit people married European settlers. The first inhabitants of North America migrated from Siberia by way of the Bering land bridge and arrived at least 15,000 years ago, though increasing evidence suggests an even earlier arrival. The Paleo-Indian archeological sites at Old Crow Flats and Bluefish Caves are two of the oldest sites of human habitation in Canada. The characteristics of Canadian Aboriginal societies included permanent settlements, agriculture, complex societal hierarchies, and trading networks. Some of these cultures had collapsed by the time European explorers arrived in the late 15th and early 16th centuries and have only been discovered through archeological investigations.
The Aboriginal population at the time of the first European settlements is estimated to have been between 200,000 and two million, with a figure of 500,000 accepted by Canada's Royal Commission on Aboriginal Peoples. As a consequence of contact with European diseases, Canada's Aboriginal peoples suffered from repeated outbreaks of newly introduced infectious diseases, such as influenza, measles, and smallpox (to which they had no natural immunity), resulting in a forty to eighty percent population decrease in the centuries after the European arrival.
Although not without conflict, European Canadians' early interactions with First Nations and Inuit populations were relatively peaceful. The Crown and Aboriginal peoples began interactions during the European colonialization period, though, the Inuit, in general, had more limited interaction with European settlers. From the late 18th century, European Canadians encouraged Aboriginals to assimilate into their own culture. These attempts reached a climax in the late 19th and early 20th centuries with forced integration and relocations. A period of redress is underway, which started with the appointment of the Truth and Reconciliation Commission of Canada by the Canadian government.



The first known attempt at European colonization began when Norsemen settled briefly at L'Anse aux Meadows in Newfoundland around 1000 AD. No further European exploration occurred until 1497, when Italian seafarer John Cabot explored and claimed Canada's Atlantic coast in the name of King Henry VII of England. Then Basque and Portuguese mariners established seasonal whaling and fishing outposts along the Atlantic coast in the early 16th century. In 1534, French explorer Jacques Cartier explored the Saint Lawrence River, where, on July 24, he planted a 10-metre (33 ft) cross bearing the words "Long Live the King of France" and took possession of the territory (known as the colony of Canada) in the name of King Francis I. In general the settlements appear to have been short-lived, possibly due to the similarity of outputs producible in Scandinavia and northern Canada and the problems of navigating trade routes at that time.
In 1583, Sir Humphrey Gilbert, by the royal prerogative of Queen Elizabeth I, founded St. John's, Newfoundland, as the first North American English colony. French explorer Samuel de Champlain arrived in 1603 and established the first permanent European settlements at Port Royal (in 1605) and Quebec City (in 1608). Among the colonists of New France, Canadiens extensively settled the Saint Lawrence River valley and Acadians settled the present-day Maritimes, while fur traders and Catholic missionaries explored the Great Lakes, Hudson Bay, and the Mississippi watershed to Louisiana. The Beaver Wars broke out in the mid-17th century over control of the North American fur trade.

The English established additional colonies in Cupids and Ferryland, Newfoundland, beginning in 1610. The Thirteen Colonies to the south were founded soon after. A series of four wars erupted in colonial North America between 1689 and 1763; the later wars of the period constituted the North American theatre of the Seven Years' War. Mainland Nova Scotia came under British rule with the 1713 Treaty of Utrecht and the 1763 Treaty of Paris ceded Canada and most of New France to Britain after the Seven Years' War.
The Royal Proclamation of 1763 created the Province of Quebec out of New France, and annexed Cape Breton Island to Nova Scotia. St. John's Island (now Prince Edward Island) became a separate colony in 1769. To avert conflict in Quebec, the British parliament passed the Quebec Act of 1774, expanding Quebec's territory to the Great Lakes and Ohio Valley. It re-established the French language, Catholic faith, and French civil law there. This angered many residents of the Thirteen Colonies, fuelling anti-British sentiment in the years prior to the 1775 outbreak of the American Revolution.
The 1783 Treaty of Paris recognized American independence and ceded the newly added territories south (but not north) of the Great Lakes to the new United States. New Brunswick was split from Nova Scotia as part of a reorganization of Loyalist settlements in the Maritimes. To accommodate English-speaking Loyalists in Quebec, the Constitutional Act of 1791 divided the province into French-speaking Lower Canada (later Quebec) and English-speaking Upper Canada (later Ontario), granting each its own elected legislative assembly.

The Canadas were the main front in the War of 1812 between the United States and Britain. Peace came in 1815; no boundaries were changed. Immigration now resumed at a higher level, with over 960,000 arrivals from Britain 1815–50. New arrivals included Irish refugees escaping the Great Irish Famine as well as Gaelic-speaking Scots displaced by the Highland Clearances. Infectious diseases killed between 25 and 33 per cent of Europeans who immigrated to Canada before 1891.
The desire for responsible government resulted in the abortive Rebellions of 1837. The Durham Report subsequently recommended responsible government and the assimilation of French Canadians into English culture. The Act of Union 1840 merged the Canadas into a united Province of Canada and responsible government was established for all provinces of British North America by 1849. The signing of the Oregon Treaty by Britain and the United States in 1846 ended the Oregon boundary dispute, extending the border westward along the 49th parallel. This paved the way for British colonies on Vancouver Island (1849) and in British Columbia (1858).




Following several constitutional conferences, the 1867 Constitution Act officially proclaimed Canadian Confederation on July 1, 1867, initially with four provinces: Ontario, Quebec, Nova Scotia, and New Brunswick. Canada assumed control of Rupert's Land and the North-Western Territory to form the Northwest Territories, where the Métis' grievances ignited the Red River Rebellion and the creation of the province of Manitoba in July 1870. British Columbia and Vancouver Island (which had been united in 1866) joined the confederation in 1871, while Prince Edward Island joined in 1873.
The Canadian parliament passed a bill introduced by the Conservative Cabinet that established a National Policy of tariffs to protect the nascent Canadian manufacturing industries. To open the West, parliament also approved sponsoring the construction of three transcontinental railways (including the Canadian Pacific Railway), opening the prairies to settlement with the Dominion Lands Act, and establishing the North-West Mounted Police to assert its authority over this territory. In 1898, during the Klondike Gold Rush in the Northwest Territories, parliament created the Yukon Territory. The Cabinet of Liberal Prime Minister Wilfrid Laurier fostered continental European immigrants settling the prairies and Alberta and Saskatchewan became provinces in 1905.




Because Britain still maintained control of Canada's foreign affairs under the Confederation Act, its declaration of war in 1914 automatically brought Canada into World War I. Volunteers sent to the Western Front later became part of the Canadian Corps, which played a substantial role in the Battle of Vimy Ridge and other major engagements of the war. Out of approximately 625,000 Canadians who served in World War I, some 60,000 were killed and another 172,000 were wounded. The Conscription Crisis of 1917 erupted when the Unionist Cabinet's proposal to augment the military's dwindling number of active members with conscription was met with vehement objections from French-speaking Quebecers. The Military Service Act brought in compulsory military service, though it, coupled with disputes over French language schools outside Quebec, deeply alienated Francophone Canadians and temporarily split the Liberal Party. In 1919, Canada joined the League of Nations independently of Britain, and the 1931 Statute of Westminster affirmed Canada's independence.

The Great Depression in Canada during the early 1930s saw an economic downturn, leading to hardship across the country. In response to the downturn, the Co-operative Commonwealth Federation (CCF) in Saskatchewan introduced many elements of a welfare state (as pioneered by Tommy Douglas) in the 1940s and 1950s. On the advice of Prime Minister William Lyon Mackenzie King, war with Germany was declared effective September 10, 1939 by King George VI, seven days after the United Kingdom. The delay underscored Canada's independence.
The first Canadian Army units arrived in Britain in December 1939. In all, over a million Canadians served in the armed forces during World War II and approximately 42,000 were killed and another 55,000 were wounded. Canadian troops played important roles in many key battles of the war, including the failed 1942 Dieppe Raid, the Allied invasion of Italy, the Normandy landings, the Battle of Normandy, and the Battle of the Scheldt in 1944. Canada provided asylum for the Dutch monarchy while that country was occupied and is credited by the Netherlands for major contributions to its liberation from Nazi Germany. The Canadian economy boomed during the war as its industries manufactured military materiel for Canada, Britain, China, and the Soviet Union. Despite another Conscription Crisis in Quebec in 1944, Canada finished the war with a large army and strong economy.



The financial crisis of the great depression had led the Dominion of Newfoundland to relinquish responsible government in 1934 and become a crown colony ruled by a British governor. After two bitter referendums, Newfoundlanders voted to join Canada in 1949 as a province.

Canada's post-war economic growth, combined with the policies of successive Liberal governments, led to the emergence of a new Canadian identity, marked by the adoption of the current Maple Leaf Flag in 1965, the implementation of official bilingualism (English and French) in 1969, and the institution of official multiculturalism in 1971. Socially democratic programs were also instituted, such as Medicare, the Canada Pension Plan, and Canada Student Loans, though provincial governments, particularly Quebec and Alberta, opposed many of these as incursions into their jurisdictions. Finally, another series of constitutional conferences resulted in the 1982 patriation of Canada's constitution from the United Kingdom, concurrent with the creation of the Canadian Charter of Rights and Freedoms. In 1999, Nunavut became Canada's third territory after a series of negotiations with the federal government.
At the same time, Quebec underwent profound social and economic changes through the Quiet Revolution of the 1960s, giving birth to a modern nationalist movement. The radical Front de libération du Québec (FLQ) ignited the October Crisis with a series of bombings and kidnappings in 1970 and the sovereignist Parti Québécois was elected in 1976, organizing an unsuccessful referendum on sovereignty-association in 1980. Attempts to accommodate Quebec nationalism constitutionally through the Meech Lake Accord failed in 1990. This led to the formation of the Bloc Québécois in Quebec and the invigoration of the Reform Party of Canada in the West. A second referendum followed in 1995, in which sovereignty was rejected by a slimmer margin of 50.6 to 49.4 percent. In 1997, the Supreme Court ruled that unilateral secession by a province would be unconstitutional and the Clarity Act was passed by parliament, outlining the terms of a negotiated departure from Confederation.
In addition to the issues of Quebec sovereignty, a number of crises shook Canadian society in the late 1980s and early 1990s. These included the explosion of Air India Flight 182 in 1985, the largest mass murder in Canadian history; the École Polytechnique massacre in 1989, a university shooting targeting female students; and the Oka Crisis of 1990, the first of a number of violent confrontations between the government and Aboriginal groups. Canada also joined the Gulf War in 1990 as part of a US-led coalition force and was active in several peacekeeping missions in the 1990s, including the UNPROFOR mission in the former Yugoslavia.
Canada sent troops to Afghanistan in 2001, but declined to join the US-led invasion of Iraq in 2003. In 2009, Canada's economy suffered in the worldwide Great Recession, but it has since largely rebounded. In 2011, Canadian forces participated in the NATO-led intervention into the Libyan civil war, and also became involved in battling the Islamic State insurgency in Iraq in the mid-2010s.




Canada occupies much of the continent of North America, sharing land borders with the contiguous United States to the south, and the US state of Alaska to the northwest. Canada stretches from the Atlantic Ocean in the east to the Pacific Ocean in the west; to the north lies the Arctic Ocean. Greenland is to the northeast. By total area (including its waters), Canada is the second-largest country in the world, after Russia. By land area alone, however, Canada ranks fourth, the difference being due to it having the world's largest proportion of fresh water lakes.

Since 1925, Canada has claimed the portion of the Arctic between 60° and 141°W longitude, but this claim is not universally recognized. Canada is home to the world's northernmost settlement, Canadian Forces Station Alert, on the northern tip of Ellesmere Island – latitude 82.5°N – which lies 817 kilometres (508 mi) from the North Pole. Much of the Canadian Arctic is covered by ice and permafrost. Canada has the longest coastline in the world, with a total length of 243,042 kilometres (151,019 mi); additionally, its border with the United States is the world's longest land border, stretching 8,891 kilometres (5,525 mi).
Since the end of the last glacial period, Canada has consisted of eight distinct forest regions, including extensive boreal forest on the Canadian Shield. Canada has over 2,000,000 lakes (563 greater than 100 km2 (39 sq mi)), more than any other country, containing much of the world's fresh water. There are also fresh-water glaciers in the Canadian Rockies and the Coast Mountains.
Canada is geologically active, having many earthquakes and potentially active volcanoes, notably Mount Meager, Mount Garibaldi, Mount Cayley, and the Mount Edziza volcanic complex. The volcanic eruption of the Tseax Cone in 1775 was among Canada's worst natural disasters, killing 2,000 Nisga'a people and destroying their village in the Nass River valley of northern British Columbia. The eruption produced a 22.5-kilometre (14.0 mi) lava flow, and, according to Nisga'a legend, blocked the flow of the Nass River. Canada's population density, at 3.3 inhabitants per square kilometre (8.5/sq mi), is among the lowest in the world. The most densely populated part of the country is the Quebec City – Windsor Corridor, situated in Southern Quebec and Southern Ontario along the Great Lakes and the St. Lawrence River.
Average winter and summer high temperatures across Canada vary from region to region. Winters can be harsh in many parts of the country, particularly in the interior and Prairie provinces, which experience a continental climate, where daily average temperatures are near −15 °C (5 °F), but can drop below −40 °C (−40 °F) with severe wind chills. In noncoastal regions, snow can cover the ground for almost six months of the year, while in parts of the north snow can persist year-round. Coastal British Columbia has a temperate climate, with a mild and rainy winter. On the east and west coasts, average high temperatures are generally in the low 20s °C (70s °F), while between the coasts, the average summer high temperature ranges from 25 to 30 °C (77 to 86 °F), with temperatures in some interior locations occasionally exceeding 40 °C (104 °F).




Canada has a parliamentary system within the context of a constitutional monarchy, the monarchy of Canada being the foundation of the executive, legislative, and judicial branches. The sovereign is Queen Elizabeth II, who is also monarch of 15 other Commonwealth countries and each of Canada's 10 provinces. As such, the Queen's representative, the Governor General of Canada (at present David Johnston), carries out most of the federal royal duties in Canada.
The direct participation of the royal and viceroyal figures in areas of governance is limited. In practice, their use of the executive powers is directed by the Cabinet, a committee of ministers of the Crown responsible to the elected House of Commons and chosen and headed by the Prime Minister of Canada (at present Justin Trudeau), the head of government. The governor general or monarch may, though, in certain crisis situations exercise their power without ministerial advice. To ensure the stability of government, the governor general will usually appoint as prime minister the person who is the current leader of the political party that can obtain the confidence of a plurality in the House of Commons. The Prime Minister's Office (PMO) is thus one of the most powerful institutions in government, initiating most legislation for parliamentary approval and selecting for appointment by the Crown, besides the aforementioned, the governor general, lieutenant governors, senators, federal court judges, and heads of Crown corporations and government agencies. The leader of the party with the second-most seats usually becomes the Leader of Her Majesty's Loyal Opposition and is part of an adversarial parliamentary system intended to keep the government in check.

Each of the 338 members of parliament in the House of Commons is elected by simple plurality in an electoral district or riding. General elections must be called by the governor general, either on the advice of the prime minister, or if the government loses a confidence vote in the House. Constitutionally, an election may be held no more than five years after the preceding election, although the Canada Elections Act currently limits this to four years with a fixed election date in October. The 105 members of the Senate, whose seats are apportioned on a regional basis, serve until age 75. Five parties had representatives elected to the federal parliament in the 2015 election: the Liberal Party of Canada who currently form the government, the Conservative Party of Canada who are the Official Opposition, the New Democratic Party, the Bloc Québécois, and the Green Party of Canada. The list of historical parties with elected representation is substantial.
Canada's federal structure divides government responsibilities between the federal government and the ten provinces. Provincial legislatures are unicameral and operate in parliamentary fashion similar to the House of Commons. Canada's three territories also have legislatures, but these are not sovereign and have fewer constitutional responsibilities than the provinces. The territorial legislatures also differ structurally from their provincial counterparts.
The Bank of Canada is the central bank of the country. In addition, the Minister of Finance and Minister of Industry utilize the Statistics Canada agency for financial planning and economic policy development. The Bank of Canada is the sole authority authorized to issue currency in the form of Canadian bank notes. The bank does not issue Canadian coins; they are issued by the Royal Canadian Mint.




The Constitution of Canada is the supreme law of the country, and consists of written text and unwritten conventions. The Constitution Act, 1867 (known as the British North America Act prior to 1982), affirmed governance based on parliamentary precedent and divided powers between the federal and provincial governments. The Statute of Westminster 1931 granted full autonomy and the Constitution Act, 1982, ended all legislative ties to the UK, as well as adding a constitutional amending formula and the Canadian Charter of Rights and Freedoms. The Charter guarantees basic rights and freedoms that usually cannot be over-ridden by any government—though a notwithstanding clause allows the federal parliament and provincial legislatures to override certain sections of the Charter for a period of five years.

The Indian Act, various treaties and case laws were established to mediate relations between Europeans and native peoples. Most notably, a series of eleven treaties known as the Numbered Treaties were signed between Aboriginals in Canada and the reigning Monarch of Canada between 1871 and 1921. These treaties are agreements with the Canadian Crown-in-Council, administered by Canadian Aboriginal law, and overseen by the Minister of Aboriginal Affairs and Northern Development. The role of the treaties and the rights they support were reaffirmed by Section Thirty-five of the Constitution Act, 1982. These rights may include provision of services, such as health care, and exemption from taxation. The legal and policy framework within which Canada and First Nations operate was further formalized in 2005, through the First Nations–Federal Crown Political Accord.

Canada's judiciary plays an important role in interpreting laws and has the power to strike down Acts of Parliament that violate the constitution. The Supreme Court of Canada is the highest court and final arbiter and has been led since 2000 by the Chief Justice Beverley McLachlin (the first female Chief Justice). Its nine members are appointed by the governor general on the advice of the prime minister and minister of justice. All judges at the superior and appellate levels are appointed after consultation with nongovernmental legal bodies. The federal Cabinet also appoints justices to superior courts in the provincial and territorial jurisdictions.
Common law prevails everywhere except in Quebec, where civil law predominates. Criminal law is solely a federal responsibility and is uniform throughout Canada. Law enforcement, including criminal courts, is officially a provincial responsibility, conducted by provincial and municipal police forces. However, in most rural areas and some urban areas, policing responsibilities are contracted to the federal Royal Canadian Mounted Police.




Canada is recognized as a middle power for its role in international affairs with a tendency to pursue multilateral solutions. Canada's foreign policy based on international peacekeeping and security is carried out through coalitions and international organizations, and through the work of numerous federal institutions. Canada's peacekeeping role during the 20th century has played a major role in its global image. The strategy of the Canadian government's foreign aid policy reflects an emphasis to meet the Millennium Development Goals, while also providing assistance in response to foreign humanitarian crises.
Canada was a founding member of the United Nations and has membership in the World Trade Organization, the G20 and the Organisation for Economic Co-operation and Development (OECD). Canada is also a member of various other international and regional organizations and forums for economic and cultural affairs. Canada acceded to the International Covenant on Civil and Political Rights in 1976. Canada joined the Organization of American States (OAS) in 1990 and hosted the OAS General Assembly in 2000 and the 3rd Summit of the Americas in 2001. Canada seeks to expand its ties to Pacific Rim economies through membership in the Asia-Pacific Economic Cooperation forum (APEC).
Canada and the United States share the world's longest undefended border, co-operate on military campaigns and exercises, and are each other's largest trading partner. Canada nevertheless has an independent foreign policy, most notably maintaining full relations with Cuba since, and declining to officially participate in the 2003 invasion of Iraq. Canada also maintains historic ties to the United Kingdom and France and to other former British and French colonies through Canada's membership in the Commonwealth of Nations and the Francophonie. Canada is noted for having a positive relationship with the Netherlands, owing, in part, to its contribution to the Dutch liberation during World War II.

Canada's strong attachment to the British Empire and Commonwealth led to major participation in British military efforts in the Second Boer War, World War I and World War II. Since then, Canada has been an advocate for multilateralism, making efforts to resolve global issues in collaboration with other nations. During the Cold War, Canada was a major contributor to UN forces in the Korean War and founded the North American Aerospace Defense Command (NORAD) in co-operation with the United States to defend against potential aerial attacks from the Soviet Union.
During the Suez Crisis of 1956, future Prime Minister Lester B. Pearson eased tensions by proposing the inception of the United Nations Peacekeeping Force, for which he was awarded the 1957 Nobel Peace Prize. As this was the first UN peacekeeping mission, Pearson is often credited as the inventor of the concept. Canada has since served in over 50 peacekeeping missions, including every UN peacekeeping effort until 1989, and has since maintained forces in international missions in Rwanda, the former Yugoslavia, and elsewhere; Canada has sometimes faced controversy over its involvement in foreign countries, notably in the 1993 Somalia Affair.

In 2001, Canada deployed troops to Afghanistan as part of the US stabilization force and the UN-authorized, NATO-led International Security Assistance Force. In February 2007, Canada, Italy, the United Kingdom, Norway, and Russia announced their joint commitment to a $1.5-billion project to help develop vaccines for developing nations, and called on other countries to join them. In August 2007, Canada's territorial claims in the Arctic were challenged after a Russian underwater expedition to the North Pole; Canada has considered that area to be sovereign territory since 1925.
Canada currently employs a professional, volunteer military force of 92,000 active personnel and approximately 51,000 reserve personnel. The unified Canadian Forces (CF) comprise the Canadian Army, Royal Canadian Navy, and Royal Canadian Air Force. In 2013, Canada's military expenditure totalled approximately C$19 billion, or around 1% of the country's GDP.




Canada is a federation composed of ten provinces and three territories. In turn, these may be grouped into four main regions: Western Canada, Central Canada, Atlantic Canada, and Northern Canada (Eastern Canada refers to Central Canada and Atlantic Canada together). Provinces have more autonomy than territories, having responsibility for social programs such as health care, education, and welfare. Together, the provinces collect more revenue than the federal government, an almost unique structure among federations in the world. Using its spending powers, the federal government can initiate national policies in provincial areas, such as the Canada Health Act; the provinces can opt out of these, but rarely do so in practice. Equalization payments are made by the federal government to ensure that reasonably uniform standards of services and taxation are kept between the richer and poorer provinces.




Canada is the world's eleventh-largest economy as of 2015, with a nominal GDP of approximately US$1.79 trillion. It is a member of the Organisation for Economic Co-operation and Development (OECD) and the Group of Eight (G8), and is one of the world's top ten trading nations, with a highly globalized economy. Canada is a mixed economy, ranking above the US and most western European nations on the Heritage Foundation's index of economic freedom, and experiencing a relatively low level of income disparity. The country's average household disposable income per capita is over US$23,900, higher than the OECD average. Furthermore, the Toronto Stock Exchange is the seventh largest stock exchange in the world by market capitalization, listing over 1,500 companies with a combined market capitalization of over US$2 trillion as of 2015.
In 2014, Canada's exports totalled over C$528 billion, while its imported goods were worth over $523 billion, of which approximately $349 billion originated from the United States, $49 billion from the European Union, and $35 billion from China. The country's 2014 trade surplus totalled C$5.1 billion, compared with a C$46.9 billion surplus in 2008.
Since the early 20th century, the growth of Canada's manufacturing, mining, and service sectors has transformed the nation from a largely rural economy to an urbanized, industrial one. Like many other developed nations, the Canadian economy is dominated by the service industry, which employs about three-quarters of the country's workforce. However, Canada is unusual among developed countries in the importance of its primary sector, in which the forestry and petroleum industries are two of the most prominent components.

Canada is one of the few developed nations that are net exporters of energy. Atlantic Canada possesses vast offshore deposits of natural gas, and Alberta also hosts large oil and gas resources. The vastness of the Athabasca oil sands and other assets results in Canada having a 13% share of global oil reserves, comprising the world's third-largest share after Venezuela and Saudi Arabia. Canada is additionally one of the world's largest suppliers of agricultural products; the Canadian Prairies are one of the most important global producers of wheat, canola, and other grains. Canada's Ministry of Natural Resources provides statistics regarding its major exports; the country is a leading exporter of zinc, uranium, gold, nickel, aluminum, steel, iron ore, coking coal and lead. Many towns in northern Canada, where agriculture is difficult, are sustainable because of nearby mines or sources of timber. Canada also has a sizeable manufacturing sector centred in southern Ontario and Quebec, with automobiles and aeronautics representing particularly important industries.
Canada's economic integration with the United States has increased significantly since World War II. The Automotive Products Trade Agreement of 1965 opened Canada's borders to trade in the automobile manufacturing industry. In the 1970s, concerns over energy self-sufficiency and foreign ownership in the manufacturing sectors prompted Prime Minister Pierre Trudeau's Liberal government to enact the National Energy Program (NEP) and the Foreign Investment Review Agency (FIRA). In the 1980s, Prime Minister Brian Mulroney's Progressive Conservatives abolished the NEP and changed the name of FIRA to Investment Canada, to encourage foreign investment. The Canada – United States Free Trade Agreement (FTA) of 1988 eliminated tariffs between the two countries, while the North American Free Trade Agreement (NAFTA) expanded the free-trade zone to include Mexico in 1994. In the mid-1990s, Jean Chrétien's Liberal government began to post annual budgetary surpluses, and steadily paid down the national debt.
The global financial crisis of 2008 caused a major recession, which led to a significant rise in unemployment in Canada. By October 2009, Canada's national unemployment rate had reached 8.6 percent, with provincial unemployment rates varying from a low of 5.8 percent in Manitoba to a high of 17 percent in Newfoundland and Labrador. Between October 2008 and October 2010, the Canadian labour market lost 162,000 full-time jobs and a total of 224,000 permanent jobs. Canada's federal debt was estimated to total $566.7 billion for the fiscal year 2010–11, up from $463.7 billion in 2008–09. In addition, Canada's net foreign debt rose by $41 billion to $194 billion in the first quarter of 2010. However, Canada's regulated banking sector (comparatively conservative among G8 nations), the federal government's pre-crisis budgetary surpluses, and its long-term policies of lowering the national debt, resulted in a less severe recession compared to other G8 nations. As of 2015, the Canadian economy has largely stabilized and has seen a modest return to growth, although the country remains troubled by volatile oil prices, sensitivity to the Eurozone crisis and higher-than-normal unemployment rates. The federal government and many Canadian industries have also started to expand trade with emerging Asian markets, in an attempt to diversify exports; Asia is now Canada's second-largest export market after the United States. Widely debated oil pipeline proposals, in particular, are hoped to increase exports of Canadian oil reserves to China.




In 2012, Canada spent approximately C$31.3 billion on domestic research and development, of which around $7 billion was provided by the federal and provincial governments. As of 2015, the country has produced thirteen Nobel laureates in physics, chemistry, and medicine, and was ranked fourth worldwide for scientific research quality in a major 2012 survey of international scientists. It is furthermore home to the headquarters of a number of global technology firms. Canada has one of the highest levels of Internet access in the world, with over 33 million users, equivalent to around 94 percent of its total 2014 population.
The Canadian Space Agency operates a highly active space program, conducting deep-space, planetary, and aviation research, and developing rockets and satellites. Canada was the third country to launch a satellite into space after the USSR and the United States, with the 1962 Alouette 1 launch. In 1984, Marc Garneau became Canada's first male astronaut. Canada is a participant in the International Space Station (ISS), and is a pioneer in space robotics, having constructed the Canadarm, Canadarm2 and Dextre robotic manipulators for the ISS and NASA's Space Shuttle. Since the 1960s, Canada's aerospace industry has designed and built numerous marques of satellite, including Radarsat-1 and 2, ISIS and MOST. Canada has also produced one of the world's most successful and widely used sounding rockets, the Black Brant; over 1,000 Black Brants have been launched since the rocket's introduction in 1961.




The 2011 Canadian census counted a total population of 33,476,688, an increase of around 5.9 percent over the 2006 figure. By December 2012, Statistics Canada reported a population of over 35 million, signifying the fastest growth rate of any G8 nation. Between 1990 and 2008, the population increased by 5.6 million, equivalent to 20.4 percent overall growth. The main drivers of population growth are immigration and, to a lesser extent, natural growth. Canada has one of the highest per-capita immigration rates in the world, driven mainly by economic policy and, to a lesser extent family reunification. The Canadian public as-well as the major political parties support the current level of immigration. In 2010, a record 280,636 people immigrated to Canada. The Canadian government anticipated between 280,000 and 305,000 new permanent residents in 2016, a similar number of immigrants as in recent years. New immigrants settle mostly in major urban areas such as Toronto, Montreal and Vancouver. Canada also accepts large numbers of refugees, accounting for over 10 percent of annual global refugee resettlements.

About four-fifths of the population lives within 150 kilometres (93 mi) of the contiguous United States border. Approximately 50 percent of Canadians live in urban areas concentrated along the Quebec City–Windsor Corridor, with an additional 30 percent living along the British Columbia Lower Mainland, and the Calgary–Edmonton Corridor in Alberta. Canada spans latitudinally from the 83rd parallel north to the 41st parallel north, and approximately 95% of the population is found below the 55th parallel north. In common with many other developed countries, Canada is experiencing a demographic shift towards an older population, with more retirees and fewer people of working age. In 2006, the average age was 39.5 years; by 2011, it had risen to approximately 39.9 years. As of 2013, the average life expectancy for Canadians is 81 years. The majority of Canadians (69.9%) live in family households, 26.8% report living alone, and those living with unrelated persons reported at 3.7%. The average size of a household in 2006 was 2.5 people.




According to a 2012 report by the Organisation for Economic Co-operation and Development (OECD), Canada is the most educated country in the world; the country ranks first worldwide in the number of adults having tertiary education, with 51 percent of Canadian adults having attained at least an undergraduate college or university degree. Canada spends about 5.3% of its GDP on education. The country invests heavily in tertiary education (more than 20 000 USD per student). As of 2014, 89 percent of adults aged 25 to 64 have earned the equivalent of a high-school degree, compared to an OECD average of 75 percent.
Since the adoption of section 23 of the Constitution Act, 1982, education in both English and French has been available in most places across Canada. Canadian provinces and territories are responsible for education provision. The mandatory school age ranges between 5–7 to 16–18 years, contributing to an adult literacy rate of 99 percent. In 2002, 43 percent of Canadians aged 25 to 64 possessed a post-secondary education; for those aged 25 to 34, the rate of post-secondary education reached 51 percent. The Programme for International Student Assessment indicates that Canadian students perform well above the OECD average, particularly in mathematics, science, and reading.




According to the 2006 census, the country's largest self-reported ethnic origin is Canadian (accounting for 32% of the population), followed by English (21%), French (15.8%), Scottish (15.1%), Irish (13.9%), German (10.2%), Italian (4.6%), Chinese (4.3%), First Nations (4.0%), Ukrainian (3.9%), and Dutch (3.3%). There are 600 recognized First Nations governments or bands, encompassing a total of 1,172,790 people. Canada's Aboriginal population is growing at almost twice the national rate, and four percent of Canada's population claimed Aboriginal identity in 2006. Another 16.2 percent of the population belonged to a non-Aboriginal visible minority. In 2006, the largest visible minority groups were South Asian (4.0%), Chinese (3.9%) and Black (2.5%). Between 2001 and 2006, the visible minority population rose by 27.2 percent. In 1961, less than two percent of Canada's population (about 300,000 people) were members of visible minority groups. By 2007, almost one in five (19.8%) were foreign-born, with nearly 60 percent of new immigrants coming from Asia (including the Middle East). The leading sources of immigrants to Canada were China, the Philippines and India. According to Statistics Canada, visible minority groups could account for a third of the Canadian population by 2031.




Canada is religiously diverse, encompassing a wide range of beliefs and customs. Canada has no official church, and the government is officially committed to religious pluralism. Freedom of religion in Canada is a constitutionally protected right, allowing individuals to assemble and worship without limitation or interference. The practice of religion is now generally considered a private matter throughout society and the state. With Christianity in decline after having once been central and integral to Canadian culture and daily life, Canada has become a post-Christian, secular state. The majority of Canadians consider religion to be unimportant in their daily lives, but still believe in God. According to the 2011 census, 67.3% of Canadians identify as Christian; of these, Roman Catholics make up the largest group, accounting for 38.7% of the population. The largest Protestant denomination is the United Church of Canada (accounting for 6.1% of Canadians), followed by Anglicans (5.0%), and Baptists (1.9%). Secularization has been growing since the 1960s. In 2011, 23.9% declared no religious affiliation, compared to 16.5% in 2001. The remaining 8.8% are affiliated with non-Christian religions, the largest of which are Islam (3.2%) and Hinduism (1.5%).




A multitude of languages are used by Canadians, with English and French (the official languages) being the mother tongues of approximately 60% and 20% of Canadians respectively. Nearly 6.8 million Canadians listed a non-official language as their mother tongue. Some of the most common non-official first languages include Chinese (mainly Cantonese; 1,072,555 first-language speakers), Punjabi (430,705), Spanish (410,670), German (409,200), and Italian (407,490). Canada's federal government practices official bilingualism, which is applied by the Commissioner of Official Languages in consonance with Section 16 of the Canadian Charter of Rights and Freedoms and the Federal Official Languages Act English and French have equal status in federal courts, parliament, and in all federal institutions. Citizens have the right, where there is sufficient demand, to receive federal government services in either English or French and official-language minorities are guaranteed their own schools in all provinces and territories.
The 1977 Charter of the French Language established French as the official language of Quebec. Although more than 85 percent of French-speaking Canadians live in Quebec, there are substantial Francophone populations in New Brunswick, Alberta, and Manitoba; Ontario has the largest French-speaking population outside Quebec. New Brunswick, the only officially bilingual province, has a French-speaking Acadian minority constituting 33 percent of the population. There are also clusters of Acadians in southwestern Nova Scotia, on Cape Breton Island, and through central and western Prince Edward Island.
Other provinces have no official languages as such, but French is used as a language of instruction, in courts, and for other government services, in addition to English. Manitoba, Ontario, and Quebec allow for both English and French to be spoken in the provincial legislatures, and laws are enacted in both languages. In Ontario, French has some legal status, but is not fully co-official. There are 11 Aboriginal language groups, composed of more than 65 distinct languages and dialects. Of these, only the Cree, Inuktitut and Ojibway languages have a large enough population of fluent speakers to be considered viable to survive in the long term. Several Aboriginal languages have official status in the Northwest Territories. Inuktitut is the majority language in Nunavut, and is one of three official languages in the territory.
Additionally, Canada is home to many sign languages, two of which are Indigenous. American Sign Language (ASL), is spoken across the country due to the prevalence of ASL in primary and secondary schools. Quebec Sign Language (LSQ), is spoken primarily in Quebec, however there are sizeable population centres in francophone communities in New Brunswick, Ontario and Manitoba; due to its historical relation to the francophone culture. Plains Sign Talk was the most widespread language in North America prior to colonization, spoken across the Prairies by a number of First Nations. Inuit Uukturausingit is used by Inuit in Nunavut. Maritime Sign Language was used in Nova Scotia, New Brunswick, and Prince Edward Island before ASL became available in the mid-20th century.




Canada's culture draws influences from its broad range of constituent nationalities, and policies that promote a "just society" are constitutionally protected. Canada has placed emphasis on equality and inclusiveness for all its people. Multiculturalism is often cited as one of Canada's significant accomplishments, and a key distinguishing element of Canadian identity. In Quebec, cultural identity is strong, and many commentators speak of a culture of Quebec that is distinct from English Canadian culture. However, as a whole, Canada is in theory a cultural mosaic—a collection of several regional, Aboriginal, and ethnic subcultures.
Canada's approach to governance emphasizing multiculturalism, which is based on selective immigration, social integration, and suppression of far right politics, has wide public support. Government policies such as publicly funded health care, higher taxation to redistribute wealth, the outlawing of capital punishment, strong efforts to eliminate poverty, strict gun control, and the legalization of same-sex marriage are further social indicators of Canada's political and cultural values. Canadians also identify with the country's health care institutions, peacekeeping, the National park system and the Canadian Charter of Rights and Freedoms.
Historically, Canada has been influenced by British, French, and Aboriginal cultures and traditions. Through their language, art and music, Aboriginal peoples continue to influence the Canadian identity. During the 20th century Canadians with African, Caribbean and Asian nationalities have added to the Canadian identity and its culture. Canadian humour is an integral part of the Canadian Identity and is reflected in its folklore, literature, music, art and media. The primary characteristics of Canadian humour are irony, parody, and satire. Many Canadian comedians have archived international success in the American TV and film industries and are amongst the most recognized in the world.
Canada has a well-developed media sector, but its cultural output; particularly in English films, television shows, and magazines, is often overshadowed by imports from the United States. As a result, the preservation of a distinctly Canadian culture is supported by federal government programs, laws, and institutions such as the Canadian Broadcasting Corporation (CBC), the National Film Board of Canada (NFB), and the Canadian Radio-television and Telecommunications Commission (CRTC).




Canada's national symbols are influenced by natural, historical, and Aboriginal sources. The use of the maple leaf as a Canadian symbol dates to the early 18th century. The maple leaf is depicted on Canada's current and previous flags, and on the Arms of Canada. The Arms of Canada is closely modelled after the royal coat of arms of the United Kingdom with French and distinctive Canadian elements replacing or added to those derived from the British version. The Great Seal of Canada is a governmental seal used for purposes of state, being set on letters patent, proclamations and commissions, for representatives of the Queen and for the appointment of cabinet ministers, lieutenant governors, senators, and judges. Other prominent symbols include the beaver, Canada goose, common loon, the Crown, the Royal Canadian Mounted Police, and more recently, the totem pole and Inuksuk. Canadian coins feature many of these symbols: the loon on the $1 coin, the Arms of Canada on the 50¢ piece, the beaver on the nickel. The penny, removed from circulation in 2013, featured the maple leaf. The Queen' s image appears on $20 bank notes, and on the obverse of all current Canadian coins.




Canadian literature is often divided into French- and English-language literatures, which are rooted in the literary traditions of France and Britain, respectively. There are four major themes that can be found within historical Canadian literature; nature, frontier life, Canada's position within the world, all three of which tie into the garrison mentality. By the 1990s, Canadian literature was viewed as some of the world's best. Canada's ethnic and cultural diversity are reflected in its literature, with many of its most prominent modern writers focusing on ethnic life. Arguably, the best-known living Canadian writer internationally (especially since the deaths of Robertson Davies and Mordecai Richler) is Margaret Atwood, a prolific novelist, poet, and literary critic. Numerous other Canadian authors have accumulated international literary awards; including Nobel Laureate Alice Munro, who has been called the best living writer of short stories in English; and Booker Prize recipient Michael Ondaatje, who is perhaps best known for the novel The English Patient, which was adapted as a film of the same name that won the Academy Award for Best Picture.




Canadian visual art has been dominated by figures such as Tom Thomson – the country's most famous painter – and by the Group of Seven. Thomson's career painting Canadian landscapes spanned a decade up to his death in 1917 at age 39. The Group were painters with a nationalistic and idealistic focus, who first exhibited their distinctive works in May 1920. Though referred to as having seven members, five artists—Lawren Harris, A. Y. Jackson, Arthur Lismer, J. E. H. MacDonald, and Frederick Varley—were responsible for articulating the Group's ideas. They were joined briefly by Frank Johnston, and by commercial artist Franklin Carmichael. A. J. Casson became part of the Group in 1926. Associated with the Group was another prominent Canadian artist, Emily Carr, known for her landscapes and portrayals of the Indigenous peoples of the Pacific Northwest Coast. Since the 1950s, works of Inuit art have been given as gifts to foreign dignitaries by the Canadian government.




The Canadian music industry is the sixth largest in the world producing internationally renowned composers, musicians and ensembles. Music broadcasting in the country is regulated by the CRTC. The Canadian Academy of Recording Arts and Sciences presents Canada's music industry awards, the Juno Awards, which were first awarded in 1970. The Canadian Music Hall of Fame established in 1976 honours Canadian musicians for their lifetime achievements. Patriotic music in Canada dates back over 200 years as a distinct category from British patriotism, preceding the first legal steps to independence by over 50 years. The earliest, The Bold Canadian, was written in 1812. The national anthem of Canada, "O Canada", was originally commissioned by the Lieutenant Governor of Quebec, the Honourable Théodore Robitaille, for the 1880 St. Jean-Baptiste Day ceremony, and was officially adopted in 1980. Calixa Lavallée wrote the music, which was a setting of a patriotic poem composed by the poet and judge Sir Adolphe-Basile Routhier. The text was originally only in French, before it was translated to English in 1906.




The roots of organized sports in Canada date back to the 1770s. Canada's official national sports are ice hockey and lacrosse. Seven of Canada's eight largest metropolitan areas – Toronto, Montreal, Vancouver, Ottawa, Calgary, Edmonton and Winnipeg – have franchises in the National Hockey League (NHL) while Quebec City had the Quebec Nordiques until they relocated to Colorado in 1995. Canada does have one Major League Baseball team, the Toronto Blue Jays, one professional basketball team, the Toronto Raptors, three Major League Soccer teams and four National Lacrosse League teams. Canada has participated in almost every Olympic Games since its Olympic debut in 1900, and has hosted several high-profile international sporting events, including the 1976 Summer Olympics in Montreal, the 1988 Winter Olympics in Calgary, the 1994 Basketball World Championship, the 2007 FIFA U-20 World Cup, the 2010 Winter Olympics in Vancouver and Whistler, British Columbia and the 2015 FIFA Women's World Cup. Other popular and professional spectator sports in Canada include curling, Canadian football and rugby league; the latter is played professionally in the Canadian Football League (CFL) and League 1 (Toronto Wolfpack). Golf, tennis, baseball, skiing, cricket, volleyball, rugby union, Australian Rules Football, soccer and basketball are widely played at youth and amateur levels, but professional leagues and franchises are not widespread.




Index of Canada-related articles
Outline of Canada
Topics by provinces and territories
 Canada – Wikipedia book










Overviews
Canada from UCB Libraries GovPubs
Canada at DMOZ
Canada from BBC News
Canada from CIA World Factbook
Canada profile from the OECD
Canadiana: The National Bibliography of Canada from Library and Archives Canada
Key Development Forecasts for Canada from International Futures
Government
Official website of the Government of Canada
Official website of the Governor General of Canada
Official website of the Prime Ministers of Canada
Travel'
Canada's official website for travel and tourism
Official website of Destination Canada
Studies
A Guide to the Sources from International Council for Canadian StudiesFrance (French: [fʁɑ̃s]), officially the French Republic (République française [ʁepyblik fʁɑ̃sɛz]), is a country with territory in western Europe and several overseas regions and territories. The European, or metropolitan, area of France extends from the Mediterranean Sea to the English Channel and the North Sea, and from the Rhine to the Atlantic Ocean. Overseas France include French Guiana on the South American continent and several island territories in the Atlantic, Pacific and Indian oceans. France spans 643,801 square kilometres (248,573 sq mi) and had a total population of almost 67 million people as of January 2017. It is a unitary semi-presidential republic with the capital in Paris, the country's largest city and main cultural and commercial centre. Other major urban centres include Marseille-Aix-en-Provence, Lyon, Lille, Nice, Toulouse and Bordeaux.
During the Iron Age, what is now metropolitan France was inhabited by the Gauls, a Celtic people. The area was annexed in 51 BC by Rome, which held Gaul until 486, when the Germanic Franks conquered the region and formed the Kingdom of France. France emerged as a major European power in the Late Middle Ages, with its victory in the Hundred Years' War (1337 to 1453) strengthening state-building and political centralisation. During the Renaissance, French culture flourished and a global colonial empire was established, which by the 20th century would be the second largest in the world. The 16th century was dominated by religious civil wars between Catholics and Protestants (Huguenots). France became Europe's dominant cultural, political, and military power under Louis XIV. In the late 18th century, the French Revolution overthrew the absolute monarchy, established one of modern history's earliest republics, and saw the drafting of the Declaration of the Rights of Man and of the Citizen, which expresses the nation's ideals to this day.
In the 19th century Napoleon took power and established the First French Empire, whose subsequent Napoleonic Wars shaped the course of continental Europe. Following the collapse of the Empire, France endured a tumultuous succession of governments culminating with the establishment of the French Third Republic in 1870. France was a major participant in the First World War, from which it emerged victorious, and was one of the Allied Powers in the Second World War, but came under occupation by the Axis Powers in 1940. Following liberation in 1944, a Fourth Republic was established and later dissolved in the course of the Algerian War. The Fifth Republic, led by Charles de Gaulle, was formed in 1958 and remains to this day. Algeria and nearly all the other colonies became independent in the 1960s with minimal controversy and typically retained close economic and military connections with France.
France has long been a global centre of art, science, and philosophy. It hosts Europe's fourth-largest number of cultural UNESCO World Heritage Sites and receives around 83 million foreign tourists annually, the most of any country in the world. France is a developed country with the world's sixth-largest economy by nominal GDP and ninth-largest by purchasing power parity. In terms of aggregate household wealth, it ranks fourth in the world. France performs well in international rankings of education, health care, life expectancy, and human development. France remains a great power in the world, being a founding member of the United Nations, where it serves as one of the five permanent members of the UN Security Council, and a founding and leading member state of the European Union and the Eurozone. It is also a member of the Group of 7, North Atlantic Treaty Organization (NATO), Organisation for Economic Co-operation and Development (OECD), the World Trade Organization (WTO), and La Francophonie.




Originally applied to the whole Frankish Empire, the name "France" comes from the Latin Francia, or "country of the Franks". Modern France is still named today Francia in Italian and Spanish, Frankreich in German and Frankrijk in Dutch, all of which have the same historical meaning.
There are various theories as to the origin of the name Frank. Following the precedents of Edward Gibbon and Jacob Grimm, the name of the Franks has been linked with the word frank (free) in English. It has been suggested that the meaning of "free" was adopted because, after the conquest of Gaul, only Franks were free of taxation. Another theory is that it is derived from the Proto-Germanic word frankon, which translates as javelin or lance as the throwing axe of the Franks was known as a francisca. However, it has been determined that these weapons were named because of their use by the Franks, not the other way around.







The oldest traces of human life in what is now France date from approximately 1.8 million years ago. Humans were then confronted by a harsh and variable climate, marked by several glacial eras. Early homonids led a nomadic hunter-gatherer life. France has a large number of decorated caves from the upper Palaeolithic era, including one of the most famous and best preserved: Lascaux (approximately 18,000 BC).
At the end of the last glacial period (10,000 BC), the climate became milder; from approximately 7,000 BC, this part of Western Europe entered the Neolithic era and its inhabitants became sedentary. After strong demographic and agricultural development between the 4th and 3rd millennia, metallurgy appeared at the end of the 3rd millennium, initially working gold, copper and bronze, and later iron. France has numerous megalithic sites from the Neolithic period, including the exceptionally dense Carnac stones site (approximately 3,300 BC).




In 600 BC, Ionian Greeks, originating from Phocaea, founded the colony of Massalia (present-day Marseille), on the shores of the Mediterranean Sea. This makes it France's oldest city. At the same time, some Gallic Celtic tribes penetrated parts of the current territory of France, and this occupation spread to the rest of France between the 5th and 3rd century BC.

The concept of Gaul emerged at that time; it corresponds to the territories of Celtic settlement ranging between the Rhine, the Atlantic Ocean, the Pyrenees and the Mediterranean. The borders of modern France are roughly the same as those of ancient Gaul, which was inhabited by Celtic Gauls. Gaul was then a prosperous country, of which the southernmost part was heavily subject to Greek and Roman cultural and economic influences.
Around 390 BC the Gallic chieftain Brennus and his troops made their way to Italy through the Alps, defeated the Romans in the Battle of the Allia, and besieged and ransomed Rome. The Gallic invasion left Rome weakened, and the Gauls continued to harass the region until 345 BC when they entered into a formal peace treaty with Rome. But the Romans and the Gauls would remain adversaries for the next several centuries, and the Gauls would continue to be a threat in Italia.
Around 125 BC, the south of Gaul was conquered by the Romans, who called this region Provincia Nostra ("Our Province"), which over time evolved into the name Provence in French. Julius Caesar conquered the remainder of Gaul and overcame a revolt carried out by the Gallic chieftain Vercingetorix in 52 BC. Gaul was divided by Augustus into Roman provinces. Many cities were founded during the Gallo-Roman period, including Lugdunum (present-day Lyon), which is considered the capital of the Gauls. These cities were built in traditional Roman style, with a forum, a theatre, a circus, an amphitheatre and thermal baths. The Gauls mixed with Roman settlers and eventually adopted Roman culture and Roman speech (Latin, from which the French language evolved). The Roman polytheism merged with the Gallic paganism into the same syncretism.
From the 250s to the 280s AD, Roman Gaul suffered a serious crisis with its fortified borders being attacked on several occasions by barbarians. Nevertheless, the situation improved in the first half of the 4th century, which was a period of revival and prosperity for Roman Gaul. In 312, the emperor Constantin I converted to Christianity. Subsequently, Christians, who had been persecuted until then, increased rapidly across the entire Roman Empire. But, from the beginning of the 5th century, the Barbarian Invasions resumed, and Germanic tribes, such as the Vandals, Suebi and Alans crossed the Rhine and settled in Gaul, Spain and other parts of the collapsing Roman Empire.




At the end of the Antiquity period, ancient Gaul was divided into several Germanic kingdoms and a remaining Gallo-Roman territory, known as the Kingdom of Syagrius. Simultaneously, Celtic Britons, fleeing the Anglo-Saxon settlement of Britain, settled the western part of Armorica. As a result, the Armorican peninsula was renamed Brittany, Celtic culture was revived and independent petty kingdoms arose in this region.

The pagan Franks, from whom the ancient name of "Francie" was derived, originally settled the north part of Gaul, but under Clovis I conquered most of the other kingdoms in northern and central Gaul. In 498, Clovis I was the first Germanic conqueror after the fall of the Roman Empire to convert to Catholic Christianity, rather than Arianism; thus France was given the title "Eldest daughter of the Church" (French: La fille aînée de l'Église) by the papacy, and French kings would be called "the Most Christian Kings of France" (Rex Christianissimus).
The Franks embraced the Christian Gallo-Roman culture and ancient Gaul was eventually renamed Francia ("Land of the Franks"). The Germanic Franks adopted Romanic languages, except in northern Gaul where Roman settlements were less dense and where Germanic languages emerged. Clovis made Paris his capital and established the Merovingian dynasty, but his kingdom would not survive his death. The Franks treated land purely as a private possession and divided it among their heirs, so four kingdoms emerged from Clovis's: Paris, Orléans, Soissons, and Rheims. The last Merovingian kings lost power to their mayors of the palace (head of household). One mayor of the palace, Charles Martel, defeated an Islamic invasion of Gaul at the Battle of Tours (732) and earned respect and power within the Frankish kingdoms. His son, Pepin the Short, seized the crown of Francia from the weakened Merovingians and founded the Carolingian dynasty. Pepin's son, Charlemagne, reunited the Frankish kingdoms and built a vast empire across Western and Central Europe.
Proclaimed Holy Roman Emperor by Pope Leo III and thus establishing in earnest the French government's longtime historical association with the Catholic Church, Charlemagne tried to revive the Western Roman Empire and its cultural grandeur. Charlemagne's son, Louis I (emperor 814–840), kept the empire united; however, this Carolingian Empire would not survive his death. In 843, under the Treaty of Verdun, the empire was divided between Louis' three sons, with East Francia going to Louis the German, Middle Francia to Lothair I, and West Francia to Charles the Bald. West Francia approximated the area occupied by, and was the precursor, to modern France.
During the 9th and 10th centuries, continually threatened by Viking invasions, France became a very decentralised state: the nobility's titles and lands became hereditary, and the authority of the king became more religious than secular and thus was less effective and constantly challenged by powerful noblemen. Thus was established feudalism in France. Over time, some of the king's vassals would grow so powerful that they often posed a threat to the king. For example, after the Battle of Hastings in 1066, William the Conqueror added "King of England" to his titles, becoming both the vassal to (as Duke of Normandy) and the equal of (as king of England) the king of France, creating recurring tensions.




The Carolingian dynasty ruled France until 987, when Hugh Capet, Duke of France and Count of Paris, was crowned King of the Franks. His descendants—the Capetians, the House of Valois, and the House of Bourbon—progressively unified the country through wars and dynastic inheritance into the Kingdom of France, which was fully declared in 1190 by Philip II Augustus. The French nobility played a prominent role in most Crusades in order to restore Christian access to the Holy Land. French knights made up the bulk of the steady flow of reinforcements throughout the two-hundred-year span of the Crusades, in such a fashion that the Arabs uniformly referred to the crusaders as Franj caring little whether they really came from France. The French Crusaders also imported the French language into the Levant, making French the base of the lingua franca (litt. "Frankish language") of the Crusader states. French knights also comprised the majority in both the Hospital and the Temple orders. The latter, in particular, held numerous properties throughout France and by the 13th century were the principal bankers for the French crown, until Philip IV annihilated the order in 1307. The Albigensian Crusade was launched in 1209 to eliminate the heretical Cathars in the southwestern area of modern-day France. In the end, the Cathars were exterminated and the autonomous County of Toulouse was annexed into the kingdom of France. Later kings expanded their domain to cover over half of modern continental France, including most of the north, centre and west of France. Meanwhile, the royal authority became more and more assertive, centred on a hierarchically conceived society distinguishing nobility, clergy, and commoners.
Charles IV the Fair died without an heir in 1328. Under the rules of the Salic law the crown of France could not pass to a woman nor could the line of kingship pass through the female line. Accordingly, the crown passed to Philip of Valois, a cousin of Charles, rather than through the female line to Charles' nephew, Edward, who would soon become Edward III of England. During the reign of Philip of Valois, the French monarchy reached the height of its medieval power. Philip's seat on the throne was contested by Edward III of England and in 1337, on the eve of the first wave of the Black Death, England and France went to war in what would become known as the Hundred Years' War. The exact boundaries changed greatly with time, but French landholdings of the English Kings remained extensive for decades. With charismatic leaders, such as Joan of Arc and La Hire, strong French counterattacks won back English continental territories. Like the rest of Europe, France was struck by the Black Death; half of the 17 million population of France died.




The French Renaissance saw a spectacular cultural development and the first standardisation of the French language, which would become the official language of France and the language of Europe's aristocracy. It also saw a long set of wars, known as the Italian Wars, between the Kingdom of France and the powerful Holy Roman Empire. French explorers, such as Jacques Cartier or Samuel de Champlain, claimed lands in the Americas for France, paving the way for the expansion of the First French colonial empire. The rise of Protestantism in Europe led France to a civil war known as the French Wars of Religion, where, in the most notorious incident, thousands of Huguenots were murdered in the St. Bartholomew's Day massacre of 1572. The Wars of Religion were ended by Henry IV's Edict of Nantes, which granted some freedom of religion to the Huguenots.
Under Louis XIII, the energetic Cardinal Richelieu reinforced the centralisation of the state, royal power and French dominance in Europe, foreshadowing the reign of Louis XIV. During Louis XIV's minority and the regency of Queen Anne and Cardinal Mazarin, a period of trouble known as the Fronde occurred in France, which was at that time at war with Spain. This rebellion was driven by the great feudal lords and sovereign courts as a reaction to the rise of royal power in France.

The monarchy reached its peak during the 17th century and the reign of Louis XIV. By turning powerful feudal lords into courtiers at the Palace of Versailles, Louis XIV's personal power became unchallenged. Remembered for his numerous wars, he made France the leading European power. France became the most populous country in Europe and had tremendous influence over European politics, economy, and culture. French became the most-used language in diplomacy, science, literature and international affairs, and remained so until the 20th century. France obtained many overseas possessions in the Americas, Africa and Asia. Louis XIV also revoked the Edict of Nantes, forcing thousands of Huguenots into exile.
Under Louis XV, Louis XIV's grandson, France lost New France and most of its Indian possessions after its defeat in the Seven Years' War, which ended in 1763. Its European territory kept growing, however, with notable acquisitions such as Lorraine (1766) and Corsica (1770). An unpopular king, Louis XV's weak rule, his ill-advised financial, political and military decisions – as well as the debauchery of his court– discredited the monarchy, which arguably paved the way for the French Revolution 15 years after his death.
Louis XVI, Louis XV's grandson, actively supported the Americans, who were seeking their independence from Great Britain (realised in the Treaty of Paris (1783)). The financial crisis that followed France's involvement in the American Revolutionary War was one of many contributing factors to the French Revolution. Much of the Enlightenment occurred in French intellectual circles, and major scientific breakthroughs and inventions, such as the discovery of oxygen (1778) and the first hot air balloon carrying passengers (1783), were achieved by French scientists. French explorers, such as Bougainville and Lapérouse, took part in the voyages of scientific exploration through maritime expeditions around the globe. The Enlightenment philosophy, in which reason is advocated as the primary source for legitimacy and authority, undermined the power of and support for the monarchy and helped pave the way for the French Revolution.




Facing financial troubles, Louis XVI summoned the Estates-General (gathering the three Estates of the realm) in May 1789 to propose solutions to his government. As it came to an impasse, the representatives of the Third Estate formed into a National Assembly, signalling the outbreak of the French Revolution. Fearing that the king would suppress the newly created National Assembly, insurgents stormed the Bastille on 14 July 1789, a date which would become France's National Day.
The absolute monarchy was subsequently replaced by a constitutional monarchy. Through the Declaration of the Rights of Man and of the Citizen, France established fundamental rights for men. The Declaration affirms "the natural and imprescriptible rights of man" to "liberty, property, security and resistance to oppression". Freedom of speech and press were declared, and arbitrary arrests outlawed. It called for the destruction of aristocratic privileges and proclaimed freedom and equal rights for all men, as well as access to public office based on talent rather than birth. While Louis XVI, as a constitutional king, enjoyed popularity among the population, his disastrous flight to Varennes seemed to justify rumours he had tied his hopes of political salvation to the prospects of foreign invasion. His credibility was so deeply undermined that the abolition of the monarchy and establishment of a republic became an increasing possibility.
European monarchies gathered against the new régime, to restore the French absolute monarchy. The foreign threat exacerbated France's political turmoil and deepened the sense of urgency among the various factions and war was declared against Austria on 20 April 1792. Mob violence occurred during the insurrection of 10 August 1792 and the following month. As a result of this violence and the political instability of the constitutional monarchy, the Republic was proclaimed on 22 September 1792.

Louis XVI was convicted of treason and guillotined in 1793. Facing increasing pressure from European monarchies, internal guerrilla wars and counterrevolutions (such as the War in the Vendée or the Chouannerie), the young Republic fell into the Reign of Terror. Between 1793 and 1794, between 16,000 and 40,000 people were executed. In Western France, the civil war between the Bleus ("Blues", supporters of the Revolution) and the Blancs ("Whites", supporters of the Monarchy) lasted from 1793 to 1796 and led to the loss of between 200,000 and 450,000 lives. Both foreign armies and French counter-revolutionaries were crushed and the French Republic survived. Furthermore, it extended greatly its boundaries and established "Sister Republics" in the surrounding countries. As the threat of a foreign invasion receded and France became mostly pacified, the Thermidorian Reaction put an end to Robespierre's rule and to the Terror. The abolition of slavery and male universal suffrage, enacted during this radical phase of the revolution, were cancelled by subsequent governments.
After a short-lived governmental scheme, Napoleon Bonaparte seized control of the Republic in 1799 becoming First Consul and later Emperor of the French Empire (1804–1814/1815). As a continuation of the wars sparked by the European monarchies against the French Republic, changing sets of European Coalitions declared wars on Napoleon's Empire. His armies conquered most of continental Europe with swift victories such as the battles of Jena-Auerstadt or Austerlitz. He redrew the European political map, while members of the Bonaparte family were appointed as monarchs in some of the newly established kingdoms. These victories led to the worldwide expansion of French revolutionary ideals and reforms, such as the Metric system, the Napoleonic Code and the Declaration of the Rights of Man. After the catastrophic Russian campaign, and the ensuing uprising of European monarchies against his rule, Napoleon was defeated and the Bourbon monarchy restored. About a million Frenchmen died during the Napoleonic Wars.

After his brief return from exile, Napoleon was finally defeated in 1815 at the Battle of Waterloo, the monarchy was re-established (1815–1830), with new constitutional limitations. The discredited Bourbon dynasty was overthrown by the July Revolution of 1830, which established the constitutional July Monarchy, which lasted until 1848, when the French Second Republic was proclaimed, in the wake of the European Revolutions of 1848. The abolition of slavery and male universal suffrage, both briefly enacted during the French Revolution were re-enacted in 1848. In 1852, the president of the French Republic, Louis-Napoléon Bonaparte, Napoleon I's nephew, was proclaimed emperor of the second Empire, as Napoleon III. He multiplied French interventions abroad, especially in Crimea, in Mexico and Italy which resulted in the annexation of the duchy of Savoy and the county of Nice, then part of the Kingdom of Sardinia. Napoleon III was unseated following defeat in the Franco-Prussian War of 1870 and his regime was replaced by the Third Republic. France had colonial possessions, in various forms, since the beginning of the 17th century, but in the 19th and 20th centuries, its global overseas colonial empire extended greatly and became the second largest in the world behind the British Empire. Including metropolitan France, the total area of land under French sovereignty almost reached 13 million square kilometres in the 1920s and 1930s, 8.6% of the world's land. Known as the Belle Époque, the turn of the century was a period characterised by optimism, regional peace, economic prosperity and technological, scientific and cultural innovations. In 1905, state secularism was officially established.




France was a member of the Triple Entente when World War I broke out. A small part of Northern France was occupied, but France and its allies emerged victorious against the Central Powers at a tremendous human and material cost. World War I left 1.4 million French soldiers dead, 4% of its population. Between 27 and 30% of soldiers conscripted from 1912–1915 were killed. The interbellum years were marked by intense international tensions and a variety of social reforms introduced by the Popular Front government (annual leave, eight-hour workdays, women in government, etc...).

In 1940 France was invaded and occupied by Nazi Germany. Metropolitan France was divided into a German occupation zone in the north and Vichy France, a newly established authoritarian regime collaborating with Germany, in the south, while Free France, the government-in-exile led by Charles de Gaulle, was set up in London. From 1942 to 1944, about 160,000 French citizens, including around 75,000 Jews, were deported to death camps and concentration camps in Germany and Poland. On 6 June 1944 the Allies invaded Normandy and in August they invaded Provence. Over the following year the Allies and the French Resistance emerged victorious over the Axis powers and French sovereignty was restored with the establishment of the Provisional Government of the French Republic (GPRF). This interim government, established by de Gaulle, aimed to continue to wage war against Germany and to purge collaborators from office. It also made several important reforms (suffrage extended to women, creation of a social security system).
The GPRF laid the groundwork for a new constitutional order that resulted in the Fourth Republic, which saw spectacular economic growth (les Trente Glorieuses). France was one of the founding members of NATO (1949). France attempted to regain control of French Indochina but was defeated by the Viet Minh in 1954 at the climactic Battle of Dien Bien Phu. Only months later, France faced another anti-colonialist conflict in Algeria. Torture and illegal executions were perpetrated by both sides and the debate over whether or not to keep control of Algeria, then home to over one million European settlers, wracked the country and nearly led to a coup and civil war.
In 1958, the weak and unstable Fourth Republic gave way to the Fifth Republic, which included a strengthened Presidency. In the latter role, Charles de Gaulle managed to keep the country together while taking steps to end the war. The Algerian War was concluded with the Évian Accords in 1962 that led to Algerian independence. A vestige of the colonial empire are the French overseas departments and territories.
In the context of the Cold War, de Gaulle pursued a policy of "national independence" towards the Western and Eastern blocs. To this end, he withdrew from NATO's military integrated command, he launched a nuclear development programme and made France the fourth nuclear power. He restored cordial Franco-German relations in order to create a European counterweight between the American and Soviet spheres of influence. However, he opposed any development of a supranational Europe, favouring a Europe of sovereign Nations. In the wake of the series of worldwide protests of 1968, the revolt of May 1968 had an enormous social impact. In France, it is considered to be the watershed moment when a conservative moral ideal (religion, patriotism, respect for authority) shifted towards a more liberal moral ideal (secularism, individualism, sexual revolution). Although the revolt was a political failure (as the Gaullist party emerged even stronger than before) it announced a split between the French people and de Gaulle who resigned shortly after.

In the post-Gaullist era, France remained one of the most developed economies in the World, but faced several economic crises that resulted in high unemployment rates and increasing public debt. In the late 20th and early 21st centuries France has been at the forefront of the development of a supranational European Union, notably by signing the Maastricht Treaty (which created the European Union) in 1992, establishing the Eurozone in 1999, and signing the Lisbon Treaty in 2007. France has also gradually but fully reintegrated into NATO and has since participated in most NATO sponsored wars.
Since the 19th century France has received many immigrants. These have been mostly male foreign workers from European Catholic countries who generally returned home when not employed. During the 1970s France faced economic crisis and allowed new immigrants (mostly from the Maghreb) to permanently settle in France with their families and to acquire French citizenship. It resulted in hundreds of thousands of Muslims (especially in the larger cities) living in subsidised public housing and suffering from very high unemployment rates. Simultaneously France renounced the assimilation of immigrants, where they were expected to adhere to French traditional values and cultural norms. They were encouraged to retain their distinctive cultures and traditions and required merely to integrate.
Since the 1995 Paris Métro and RER bombings, France has been sporadically targeted by Islamist organisations, notably the Charlie Hebdo attack in January 2015 which provoked the largest public rallies in French history, gathering 4.4 million people, the November 2015 Paris attacks which resulted in 130 deaths, the deadliest attack on French soil since World War II, and the deadliest in the European Union since the Madrid train bombings in 2004 and the 2016 Nice attack which caused 87 deaths during Bastille Day celebrations.







The European part of France is called Metropolitan France and it is located in one of the occidental ends of Europe. It is bordered by the North Sea in the north, the English Channel in the northwest, the Atlantic Ocean in the west and the Mediterranean sea in the southeast. It borders Belgium and Luxembourg in the northeast, Germany and Switzerland in the east, Italy and Monaco in the southeast, and Spain and Andorra in the south and southwest. The borders in the south and in the east of the country are mountain ranges: the Pyrenees, the Alps and the Jura, the border in the east is from the Rhine river, while the border in the north and the northeast melts in no natural elements. Due to its shape, it is often referred to in French as l'Hexagone ("The Hexagon"). Metropolitan France includes various islands: Corsica and coastal islands. Metropolitan France is situated mostly between latitudes 41° and 51° N, and longitudes 6° W and 10° E, on the western edge of Europe, and thus lies within the northern temperate zone. Its continental part covers about 1000 km from north to south and from east to west.
France has Overseas regions across the world. These territories have varying statuses in the territorial administration of France and are located:
In South America: French Guiana.
In the Atlantic Ocean: Saint Pierre and Miquelon and, in the Antilles: Guadeloupe, Martinique, Saint Martin and Saint Barthélemy.
In the Pacific Ocean: French Polynesia, the special collectivity of New Caledonia, Wallis and Futuna and Clipperton Island.
In the Indian Ocean: Réunion island, Mayotte, Scattered Islands in the Indian Ocean, Crozet Islands, St. Paul and Amsterdam islands.
In the Indian Ocean: Kerguelen Islands.
In the Antarctic: Adélie Land.
France has land borders with Brazil and Suriname in French Guiana and with the Kingdom of the Netherlands through the French part of Saint Martin.
The European territory of France covers 551,500 square kilometres (212,935 sq mi), the largest among European Union members. France's total land area, with its overseas departments and territories (excluding Adélie Land), is 643,801 km2 (248,573 sq mi), 0.45% of the total land area on Earth. France possesses a wide variety of landscapes, from coastal plains in the north and west to mountain ranges of the Alps in the southeast, the Massif Central in the south central and Pyrenees in the southwest.
Due to its numerous overseas departments and territories scattered across the planet, France possesses the second-largest Exclusive economic zone (EEZ) in the world, covering 11,035,000 km2 (4,260,000 mi2), just behind the EEZ of the United States (11,351,000 km2 / 4,383,000 mi2), but ahead of the EEZ of Australia (8,148,250 km2 / 4,111,312 mi2). Its EEZ covers approximately 8% of the total surface of all the EEZs of the world.
At 4,810.45 metres (15,782 ft) above sea level, the highest point in Western Europe, Mont Blanc, is situated in the Alps on the French and Italian border. France also has extensive river systems such as the Seine, the Loire, the Garonne, and the Rhone, which divides the Massif Central from the Alps and flows into the Mediterranean Sea at the Camargue. Corsica lies off the Mediterranean coast.



Most of the low-lying areas of metropolitan France excluding Corsica are located in the oceanic climate zone, Cfb, Cwb and Cfc in the Köppen classification. A small part of the territory bordering the mediterranean basin lies in the Csa and Csb zones. As the French metropolitan territory is relatively large, the climate is not uniform, giving rise to the following climate nuances:
The west of France has strictly oceanic climate – it extends from Flanders to the Basque Country in a coastal strip several tens of kilometres wide, narrower to the north and south but wider in Brittany, which is almost entirely in this climate zone.
The climate of the South is also oceanic but warmer.
The climate of the Northwest is oceanic but cooler and windier.

Away from the coast, the climate is oceanic throughout but its characteristics change somewhat. The Paris sedimentary basin and, more so, the basins protected by mountain chains show a stronger seasonal temperature variability and less rainfall during autumn and winter. Therefore, most of the territory has a semi-oceanic climate and forms a transition zone between strictly oceanic climate near the coasts and the semi-continental climate of the north and centre-east (Alsace, plains of the Saône, the middle part of the Rhône, Dauphiné, Auvergne and Savoy).
The Mediterranean and the lower Rhône valley experience a Mediterranean climate due to the effect of mountain chains isolating them from the rest of the country and the resulting Mistral and Tramontana winds.
The mountain (or alpine) climate is confined to the Alps, the Pyrenees and the summits of the Massif Central, the Jura and the Vosges.
In the overseas regions, there are three broad types of climate:
A tropical climate in most overseas regions: high constant temperature throughout the year with a dry and a wet season.
An equatorial climate in French Guiana: high constant temperature with even precipitation throughout the year.
A subpolar climate in Saint Pierre and Miquelon and in most of the French Southern and Antarctic Lands: short mild summers and long very cold winters.




France was one of the first countries to create an environment ministry, in 1971. Although it is one of the most industrialised countries in the world, France is ranked only 17th by carbon dioxide emissions, behind less populous nations such as Canada or Australia. This is because France decided to invest in nuclear power following the 1973 oil crisis, which now accounts for 75% of its electricity production and results in less pollution.
Like all European Union members, France agreed to cut carbon emissions by at least 20% of 1990 levels by the year 2020, compared to the U.S. plan to reduce emissions by 4% of 1990 levels. As of 2009, French carbon dioxide emissions per capita were lower than that of China's. The country was set to impose a carbon tax in 2009 at 17 euros per tonne of carbon emitted, which would have raised 4 billion euros of revenue annually. However, the plan was abandoned due to fears of burdening French businesses.
Forests account for 28% of France's land area, and are some of the most diverse in Europe, comprising more than 140 species of trees. There are nine national parks and 46 natural parks in France, with the government planning to convert 20% of its Exclusive Economic Zone into a Marine Protected Area by 2020. A regional nature park (French: parc naturel régional or PNR) is a public establishment in France between local authorities and the French national government covering an inhabited rural area of outstanding beauty, in order to protect the scenery and heritage as well as setting up sustainable economic development in the area. A PNR sets goals and guidelines for managed human habitation, sustainable economic development, and protection of the natural environment based on each park's unique landscape and heritage. The parks foster ecological research programmes and public education in the natural sciences. As of 2014 there are 49 PNRs in France.
According to the 2012 Environmental Performance Index conducted by Yale and Columbia, France was the sixth-most environmentally conscious country in the world, one place higher than the previous report in 2010.



Metropolitan France has a wide variety of topographical sets and natural landscapes. Large parts of the current territory of France were raised during several tectonic episodes like the Hercynian uplift in the Paleozoic Era, during which the Armorican Massif, the Massif Central, the Morvan massif, the Vosges and Ardennes ranges and the island of Corsica were formed. These massifs delineate several sedimentary basins such as the Aquitaine basin in the southwest and the Paris basin in the north, the latter including several areas of particularly fertile ground such as the silt beds of Beauce and Brie. Various routes of natural passage, such as the Rhône valley, allow easy communications. The Alpine, Pyrenean and Jura mountains are much younger and have less eroded forms. The highest peak of the Alps is Montblanc at 4809 metres above sea level. Although 60% of municipalities are classified as having seismic risks, these risks remain moderate. The coastlines offer contrasting landscapes: mountain ranges along the French Riviera, coastal cliffs such as the Côte d'Albâtre, and wide sandy plains in the Languedoc. The river system of France comprises the four major rivers Loire, Seine, Garonne and Rhône and their tributaries, whose combined catchment includes over 62% of the metropolitan territory. Other water courses drain towards the Meuse and Rhine along the north-eastern borders. France has 11 million square kilometres of marine waters within three oceans under its jurisdiction, of which 97% are overseas.




The French republic is divided into 18 regions (located in Europe and overseas), 5 overseas collectivities, 1 overseas territory, 1 special collectivity : New Caledonia and 1 uninhabited island directly under the authority of the Minister of Overseas France : Clipperton.
Regions

Since 2016 France is mainly divided into 18 administrative regions: 13 regions in metropolitan France (including the territorial collectivity of Corsica), and five located overseas. The regions are further subdivided into 101 departments, which are numbered mainly alphabetically. This number is used in postal codes and was formerly used on vehicle number plates. Among the 101 departments of France, five (French Guiana, Guadeloupe, Martinique, Mayotte, and Réunion) are in overseas regions (ROMs) that are also simultaneously overseas departments (DOMs), enjoy exactly the same status as metropolitan departments and are an integral part of the European Union.
The 101 departments are subdivided into 335 arrondissements, which are, in turn, subdivided into 2,054 cantons. These cantons are then divided into 36,658 communes, which are municipalities with an elected municipal council. Three communes—Paris, Lyon and Marseille—are subdivided into 45 municipal arrondissements.
The regions, departments and communes are all known as territorial collectivities, meaning they possess local assemblies as well as an executive. Arrondissements and cantons are merely administrative divisions. However, this was not always the case. Until 1940, the arrondissements were territorial collectivities with an elected assembly, but these were suspended by the Vichy regime and definitely abolished by the Fourth Republic in 1946.
Overseas territories and collectivities
In addition to the 18 regions and 101 departments, the French Republic has five overseas collectivities (French Polynesia, Saint Barthélemy, Saint Martin, Saint Pierre and Miquelon, and Wallis and Futuna), one sui generis collectivity (New Caledonia), one overseas territory (French Southern and Antarctic Lands), and one island possession in the Pacific Ocean (Clipperton Island).

Overseas collectivities and territories form part of the French Republic, but do not form part of the European Union or its fiscal area (with the exception of St. Bartelemy, which seceded from Guadeloupe in 2007). The Pacific Collectivities (COMs) of French Polynesia, Wallis and Futuna, and New Caledonia continue to use the CFP franc whose value is strictly linked to that of the euro. In contrast, the five overseas regions used the French franc and now use the euro.







The French Republic is a unitary semi-presidential representative democratic republic with strong democratic traditions. The constitution of the Fifth Republic was approved by referendum on 28 September 1958. It greatly strengthened the authority of the executive in relation to parliament. The executive branch itself has two leaders: the President of the Republic, currently François Hollande, who is head of state and is elected directly by universal adult suffrage for a 5-year term (formerly 7 years), and the Government, led by the president-appointed Prime Minister, currently Bernard Cazeneuve.
The French parliament is a bicameral legislature comprising a National Assembly (Assemblée Nationale) and a Senate. The National Assembly deputies represent local constituencies and are directly elected for 5-year terms. The Assembly has the power to dismiss the government, and thus the majority in the Assembly determines the choice of government. Senators are chosen by an electoral college for 6-year terms (originally 9-year terms), and one half of the seats are submitted to election every 3 years starting in September 2008.
The Senate's legislative powers are limited; in the event of disagreement between the two chambers, the National Assembly has the final say. The Government has a strong influence in shaping the agenda of Parliament.
French politics are characterised by two politically opposed groupings: one left-wing, centred on the French Socialist Party, and the other right-wing, centred previously around the Rassemblement pour la République (RPR), then its successor the UMP Union for a Popular Movement (UMP), which in 2015 was renamed Les Républicains. Since the 2012 elections, the executive branch is currently composed mostly of the Socialist Party.




France uses a civil legal system; that is, law arises primarily from written statutes; judges are not to make law, but merely to interpret it (though the amount of judicial interpretation in certain areas makes it equivalent to case law). Basic principles of the rule of law were laid in the Napoleonic Code (which was, in turn, largely based on the royal law codified under Louis XIV). In agreement with the principles of the Declaration of the Rights of Man and of the Citizen, law should only prohibit actions detrimental to society. As Guy Canivet, first president of the Court of Cassation, wrote about the management of prisons: Freedom is the rule, and its restriction is the exception; any restriction of Freedom must be provided for by Law and must follow the principles of necessity and proportionality. That is, Law should lay out prohibitions only if they are needed, and if the inconveniences caused by this restriction do not exceed the inconveniences that the prohibition is supposed to remedy.

French law is divided into two principal areas: private law and public law. Private law includes, in particular, civil law and criminal law. Public law includes, in particular, administrative law and constitutional law. However, in practical terms, French law comprises three principal areas of law: civil law, criminal law, and administrative law. Criminal laws can only address the future and not the past (criminal ex post facto laws are prohibited). While administrative law is often a subcategory of civil law in many countries, it is completely separated in France and each body of law is headed by a specific supreme court: ordinary courts (which handle criminal and civil litigation) are headed by the Court of Cassation and administrative courts are headed by the Council of State.
To be applicable, every law must be officially published in the Journal officiel de la République française.
France does not recognise religious law as a motivation for the enactment of prohibitions. France has long had neither blasphemy laws nor sodomy laws (the latter being abolished in 1791). However, "offences against public decency" (contraires aux bonnes mœurs) or disturbing public order (trouble à l'ordre public) have been used to repress public expressions of homosexuality or street prostitution. Since 1999, civil unions for homosexual couples are permitted, and since May 2013, same-sex marriage and LGBT adoption are legal in France. Laws prohibiting discriminatory speech in the press are as old as 1881. Some consider however that hate speech laws in France are too broad or severe and damage freedom of speech. France has laws against racism and antisemitism. Since 1990, the Gayssot Act prohibits Holocaust denial.
Freedom of religion is constitutionally guaranteed by the 1789 Declaration of the Rights of Man and of the Citizen. The 1905 French law on the Separation of the Churches and the State is the basis for laïcité (state secularism): the state does not formally recognize any religion, except in Alsace-Moselle. Nonetheless, it does recognize religious associations. The Parliament has listed many religious movements as dangerous cults since 1995, and has banned wearing conspicuous religious symbols in schools since 2004. In 2010, it banned the wearing of face-covering Islamic veils in public; human rights groups such as Amnesty International and Human Rights Watch described the law as discriminatory towards Muslims. However, it is supported by most of the population.




France is a founding member of the United Nations and serves as one of the permanent members of the UN Security Council with veto rights. In 2015, France was described as being "the best networked state in the world", because it is a country that "is member of more multi-lateral organisations than any other country".
France is a member of the G8, World Trade Organization (WTO), the Secretariat of the Pacific Community (SPC) and the Indian Ocean Commission (COI). It is an associate member of the Association of Caribbean States (ACS) and a leading member of the International Francophone Organisation (OIF) of fifty-one fully or partly French-speaking countries.
As a significant hub for international relations, France hosts the second largest assembly of diplomatic missions in the world and the headquarters of international organisations including the OECD, UNESCO, Interpol, the International Bureau of Weights and Measures, and la Francophonie.
Postwar French foreign policy has been largely shaped by membership of the European Union, of which it was a founding member. Since the 1960s, France has developed close ties with reunified Germany to become the most influential driving force of the EU. In the 1960s, France sought to exclude the British from the European unification process, seeking to build its own standing in continental Europe. However, since 1904, France has maintained an "Entente cordiale" with the United Kingdom, and there has been a strengthening of links between the countries, especially militarily.

France is a member of the North Atlantic Treaty Organisation (NATO), but under President de Gaulle, it excluded itself from the joint military command to protest the special relationship between the United States and Britain and to preserve the independence of French foreign and security policies. France vigorously opposed the 2003 invasion of Iraq, straining bilateral relations with the US and the UK. However, as a result of Nicolas Sarkozy's pro-American politics (much criticised in France by the leftists and by a part of the right), France rejoined the NATO joint military command on 4 April 2009.
In the early 1990s, the country drew considerable criticism from other nations for its underground nuclear tests in French Polynesia.
France retains strong political and economic influence in its former African colonies (Françafrique) and has supplied economic aid and troops for peace-keeping missions in Ivory Coast and Chad. Recently, after the unilateral declaration of independence of northern Mali by the Tuareg MNLA and the subsequent regional Northern Mali conflict with several Islamist groups including Ansar Dine and MOJWA, France and other African states intervened to help the Malian Army to retake control.
In 2013, France was the fourth largest (in absolute terms) donor of development aid in the world, behind the US, the UK and Germany. This represents 0.36% of its GDP, in this regard rating France as twelfth largest donor on the list. The organisation managing the French help is the French Development Agency, which finances primarily humanitarian projects in sub-Saharan Africa. The main goals of this support are "developing infrastructure, access to health care and education, the implementation of appropriate economic policies and the consolidation of the rule of law and democracy".




The French Armed Forces (Forces armées françaises) are the military and paramilitary forces of France, under the president as supreme commander. They consist of the French Army (Armée de Terre), French Navy (Marine Nationale, formerly called Armée de Mer), the French Air Force (Armée de l'Air), the French Strategic Nuclear Force (Force Nucléaire Stratégique, nicknamed Force de Frappe or "Strike Force") and the Military Police called National Gendarmerie (Gendarmerie nationale), which also fulfils civil police duties in the rural areas of France. Together they are among the largest armed forces in the world and the largest in the EU.
While the Gendarmerie is an integral part of the French armed forces (gendarmes are career soldiers), and therefore under the purview of the Ministry of Defence, it is operationally attached to the Ministry of the Interior as far as its civil police duties are concerned.
When acting as general purpose police force, the Gendarmerie encompasses the counter terrorist units of the Parachute Intervention Squadron of the National Gendarmerie (Escadron Parachutiste d'Intervention de la Gendarmerie Nationale), the National Gendarmerie Intervention Group (Groupe d'Intervention de la Gendarmerie Nationale), the Search Sections of the National Gendarmerie (Sections de Recherche de la Gendarmerie Nationale), responsible for criminal enquiries, and the Mobile Brigades of the National Gendarmerie (Brigades mobiles de la Gendarmerie Nationale, or in short Gendarmerie mobile) which have the task to maintain public order.
The following special units are also part of the Gendarmerie: The Republican Guard (Garde républicaine) which protects public buildings hosting major French institutions, the Maritime Gendarmerie (Gendarmerie maritime) serving as Coast Guard, the Provost Service (Prévôté), acting as the Military Police branch of the Gendarmerie.
As far as the French intelligence units are concerned, the Directorate-General for External Security (Direction générale de la sécurité extérieure) is considered to be a component of the Armed Forces under the authority of the Ministry of Defence. The other, the Central Directorate for Interior Intelligence (Direction centrale du renseignement intérieur) is a division of the National Police Force (Direction générale de la Police Nationale), and therefore reports directly to the Ministry of the Interior. There has been no national conscription since 1997.
France has a special military corps, the French Foreign Legion, founded in 1830, which consists of foreign nationals from over 140 countries who are willing to serve in the French Armed Forces and become French citizens after the end of their service period. The only other countries having similar units are Spain (the Spanish Foreign Legion, called Tercio, was founded in 1920) and Luxembourg (foreigners can serve in the National Army provided they speak Luxembourgish).
France is a permanent member of the Security Council of the UN, and a recognised nuclear state since 1960. France has signed and ratified the Comprehensive Nuclear-Test-Ban Treaty (CTBT) and acceded to the Nuclear Non-Proliferation Treaty. France's annual military expenditure in 2011 was US$62.5 billion, or 2.3%, of its GDP making it the fifth biggest military spender in the world after the United States, China, Russia, and the United Kingdom.
French nuclear deterrence, (formerly known as "Force de Frappe"), relies on complete independence. The current French nuclear force consists of four Triomphant class submarines equipped with submarine-launched ballistic missiles. In addition to the submarine fleet, it is estimated that France has about 60 ASMP medium-range air-to-ground missiles with nuclear warheads, of which around 50 are deployed by the Air Force using the Mirage 2000N long-range nuclear strike aircraft, while around 10 are deployed by the French Navy's Super Étendard Modernisé (SEM) attack aircraft, which operate from the nuclear-powered aircraft carrier Charles de Gaulle. The new Rafale F3 aircraft will gradually replace all Mirage 2000N and SEM in the nuclear strike role with the improved ASMP-A missile with a nuclear warhead.
France has major military industries with one of the largest aerospace industries in the world. Its industries have produced such equipment as the Rafale fighter, the Charles de Gaulle aircraft carrier, the Exocet missile and the Leclerc tank among others. Despite withdrawing from the Eurofighter project, France is actively investing in European joint projects such as the Eurocopter Tiger, multipurpose frigates, the UCAV demonstrator nEUROn and the Airbus A400M. France is a major arms seller, with most of its arsenal's designs available for the export market with the notable exception of nuclear-powered devices.
The military parade held in Paris each 14 July for France's national day, called Bastille Day in English-speaking countries (but not in France), is the oldest and largest regular military parade in Europe.




In April and May 2012, France held a presidential election in which the winner, François Hollande, had opposed austerity measures, promising to eliminate France's budget deficit by 2017. The new government stated that it aimed to cancel recently enacted tax cuts and exemptions for the wealthy, raising the top tax bracket rate to 75% on incomes over a million euros, restoring the retirement age to 60 with a full pension for those who have worked 42 years, restoring 60,000 jobs recently cut from public education, regulating rent increases; and building additional public housing for the poor.
In June, Hollande's Socialist Party won a supermajority in legislative elections capable of amending the French Constitution and enabling the immediate enactment of the promised reforms. French government bond interest rates fell 30% to record lows, less than 50 basis points above German government bond rates.
Under European Union rules, member states are supposed to limit their debt to 60% of output or be reducing the ratio structurally towards this ceiling, and run public deficits of no more than 3% of GDP. The French government has run a budget deficit each year since the early 1970s. In 2012, French government debt levels reached 1.8 trillion euros, the equivalent of 90% of French GDP.
In late 2012, credit rating agencies warned that growing French government debt levels risked France's AAA credit rating, raising the possibility of a future downgrade and subsequent higher borrowing costs for the French government.




A member of the Group of 7 (formerly G8) leading industrialised countries, as of 2014, it is ranked as the world's ninth largest and the EU's second largest economy by purchasing power parity. With 31 of the 500 biggest companies in the world in 2015, France ranks fourth in the Fortune Global 500, ahead of Germany and the UK. France joined 11 other EU members to launch the euro in 1999, with euro coins and banknotes completely replacing the French franc (₣) in 2002.

France has a mixed economy that combines extensive private enterprise with substantial state enterprise and government intervention. The government retains considerable influence over key segments of infrastructure sectors, with majority ownership of railway, electricity, aircraft, nuclear power and telecommunications. It has been relaxing its control over these sectors since the early 1990s. The government is slowly corporatising the state sector and selling off holdings in France Télécom, Air France, as well as in the insurance, banking, and defence industries. France has an important aerospace industry led by the European consortium Airbus, and has its own national spaceport, the Centre Spatial Guyanais.
According to the World Trade Organization (WTO), in 2009 France was the world's sixth largest exporter and the fourth largest importer of manufactured goods. In 2008, France was the third largest recipient of foreign direct investment among OECD countries at $118 billion, ranking behind Luxembourg (where foreign direct investment was essentially monetary transfers to banks located there) and the US ($316 billion), but above the UK ($96.9 billion), Germany ($25 billion), or Japan ($24 billion).
In the same year, French companies invested $220 billion outside France, ranking France as the second largest outward direct investor in the OECD, behind the US ($311 billion), and ahead of the UK ($111 billion), Japan ($128 billion) and Germany ($157 billion).
Financial services, banking and the insurance sector are an important part of the economy. The Paris stock exchange (French: La Bourse de Paris) is an old institution, created by Louis XV in 1724. In 2000, the stock exchanges of Paris, Amsterdam and Bruxelles merged into Euronext. In 2007, Euronext merged with the New York stock exchange to form NYSE Euronext, the world's largest stock exchange. Euronext Paris, the French branch of the NYSE Euronext group is Europe's 2nd largest stock exchange market, behind the London Stock Exchange.
France is part of the European single market which represents more than 500 million consumers. Several domestic commercial policies are determined by agreements among European Union (EU) members and by EU legislation. France introduced the common European currency, the Euro in 2002. It is a member of the Eurozone which represents around 330 million citizens.
French companies have maintained key positions in the insurance and banking industries: AXA is the world's largest insurance company. The leading French banks are BNP Paribas and the Crédit Agricole, ranking as the world's first and sixth largest banks in 2010 (by assets), while the Société Générale group was ranked the world's eighth largest in 2009.




France has historically been a large producer of agricultural products. Extensive tracts of fertile land, the application of modern technology, and EU subsidies have combined to make France the leading agricultural producer and exporter in Europe (representing 20% of the EU's agricultural production) and the world's third biggest exporter of agricultural products.
Wheat, poultry, dairy, beef, and pork, as well as internationally recognised processed foods are the primary French agricultural exports. Rosé wines are primarily consumed within the country, but Champagne and Bordeaux wines are major exports, being known worldwide. EU agriculture subsidies to France have decreased in recent years but still amounted to $8 billion in 2007. That same year, France sold 33.4 billion euros of transformed agricultural products.
Agriculture is an important sector of France's economy: 3.8% of the active population is employed in agriculture, whereas the total agri-food industry made up 4.2% of French GDP in 2005.




With 83 million foreign tourists in 2012, France is ranked as the first tourist destination in the world, ahead of the US (67 million) and China (58 million). This 83 million figure excludes people staying less than 24 hours, such as North Europeans crossing France on their way to Spain or Italy. It is third in income from tourism due to shorter duration of visits. France has 37 sites inscribed in UNESCO's World Heritage List and features cities of high cultural interest, beaches and seaside resorts, ski resorts, and rural regions that many enjoy for their beauty and tranquillity (green tourism). Small and picturesque French villages are promoted through the association Les Plus Beaux Villages de France (litt. "The Most Beautiful Villages of France"). The "Remarkable Gardens" label is a list of the over 200 gardens classified by the French Ministry of Culture. This label is intended to protect and promote remarkable gardens and parks. France attracts many religious pilgrims on their way to St. James, or to Lourdes, a town in the Hautes-Pyrénées that hosts several million visitors a year.

France, especially Paris, has some of the world's largest and renowned museums, including the Louvre, which is the most visited art museum in the world, the Musée d'Orsay, mostly devoted to impressionism, and Beaubourg, dedicated to Contemporary art. Disneyland Paris is Europe's most popular theme park, with 15 million combined visitors to the resort's Disneyland Park and Walt Disney Studios Park in 2009.
With more than 10 millions tourists a year, the French Riviera (or Côte d'Azur), in south-east France, is the second leading tourist destination in the country, after the Paris region. It benefits from 300 days of sunshine per year, 115 kilometres (71 mi) of coastline and beaches, 18 golf courses, 14 ski resorts and 3,000 restaurants. Each year the Côte d'Azur hosts 50% of the world's superyacht fleet.
Another major destination are the Châteaux of the Loire Valley, this World Heritage Site is noteworthy for its architectural heritage, in its historic towns but in particular its castles (châteaux), such as the Châteaux d'Amboise, de Chambord, d'Ussé, de Villandry and Chenonceau. The most popular tourist sites include: (according to a 2003 ranking visitors per year): Eiffel Tower (6.2 million), Louvre Museum (5.7 million), Palace of Versailles (2.8 million), Musée d'Orsay (2.1 million), Arc de Triomphe (1.2 million), Centre Pompidou (1.2 million), Mont Saint-Michel (1 million), Château de Chambord (711,000), Sainte-Chapelle (683,000), Château du Haut-Kœnigsbourg (549,000), Puy de Dôme (500,000), Musée Picasso (441,000), Carcassonne (362,000).




Électricité de France (EDF), the main electricity generation and distribution company in France, is also one of the world's largest producers of electricity. In 2003, it produced 22% of the European Union's electricity, primarily from nuclear power. France is the smallest emitter of carbon dioxide among the G8, due to its heavy investment in nuclear power. As a result of large investments in nuclear technology, most electricity produced by France is generated by 59 nuclear power plants (75% in 2012). In this context, renewable energies are having difficulty taking off. France also uses hydroelectric dams to produce electricity, such as the Eguzon dam, Étang de Soulcem, and Lac de Vouglans.




The railway network of France, which as of 2008 stretches 29,473 kilometres (18,314 mi) is the second most extensive in Western Europe after that of Germany. It is operated by the SNCF, and high-speed trains include the Thalys, the Eurostar and TGV, which travels at 320 km/h (199 mph) in commercial use. The Eurostar, along with the Eurotunnel Shuttle, connects with the United Kingdom through the Channel Tunnel. Rail connections exist to all other neighbouring countries in Europe, except Andorra. Intra-urban connections are also well developed with both underground services (Paris, Lyon, Lille, Marseille, Toulouse, Rennes) and tramway services (Nantes, Strasbourg, Bordeaux, Grenoble, Montpellier...) complementing bus services.

There are approximately 1,027,183 kilometres (638,262 mi) of serviceable roadway in France, ranking it the most extensive network of the European continent. The Paris region is enveloped with the most dense network of roads and highways that connect it with virtually all parts of the country. French roads also handle substantial international traffic, connecting with cities in neighbouring Belgium, Luxembourg, Germany, Switzerland, Italy, Spain, Andorra and Monaco. There is no annual registration fee or road tax; however, usage of the mostly privately owned motorways is through tolls except in the vicinity of large communes. The new car market is dominated by domestic brands such as Renault (27% of cars sold in France in 2003), Peugeot (20.1%) and Citroën (13.5%). Over 70% of new cars sold in 2004 had diesel engines, far more than contained petrol or LPG engines. France possesses the Millau Viaduct, the world's tallest bridge, and has built many important bridges such as the Pont de Normandie.
There are 464 airports in France. Charles de Gaulle Airport, located in the vicinity of Paris, is the largest and busiest airport in the country, handling the vast majority of popular and commercial traffic and connecting Paris with virtually all major cities across the world. Air France is the national carrier airline, although numerous private airline companies provide domestic and international travel services. There are ten major ports in France, the largest of which is in Marseille, which also is the largest bordering the Mediterranean Sea. 12,261 kilometres (7,619 mi) of waterways traverse France including the Canal du Midi, which connects the Mediterranean Sea to the Atlantic Ocean through the Garonne river.




Since the Middle Ages, France has been a major contributor to scientific and technological achievement. Around the beginning of the 11th century Pope Sylvester II, born Gerbert d'Aurillac, reintroduced the abacus and armillary sphere, and introduced Arabic numerals and clocks to northern and western Europe. The University of Paris, founded in the mid-12th century, is still one of the most important universities in the Western world. In the 17th century, mathematician René Descartes defined a method for the acquisition of scientific knowledge, while Blaise Pascal became famous for his work on probability and fluid mechanics. They were both key figures of the Scientific revolution, which blossomed in Europe during this period. The Academy of Sciences was founded by Louis XIV to encourage and protect the spirit of French scientific research. It was at the forefront of scientific developments in Europe in the 17th and 18th centuries. It is one of the earliest academies of sciences.
The Age of Enlightenment was marked by the work of biologist Buffon and chemist Lavoisier, who discovered the role of oxygen in combustion, while Diderot and D'Alembert published the Encyclopédie, which aimed to give access to "useful knowledge" to the people, a knowledge that they can apply to their everyday life. With the Industrial Revolution, the 19th century saw spectacular scientific developments in France with scientists such as Augustin Fresnel, founder of modern optics, Sadi Carnot who laid the foundations of thermodynamics, and Louis Pasteur, a pioneer of microbiology. Other eminent French scientists of the 19th century have their names inscribed on the Eiffel Tower.
Famous French scientists of the 20th century include the mathematician and physicist Henri Poincaré, physicists Henri Becquerel, Pierre and Marie Curie, remained famous for their work on radioactivity, the physicist Paul Langevin and virologist Luc Montagnier, co-discoverer of HIV AIDS. Hand transplantation was developed on 23 September 1998 in Lyon by a team assembled from different countries around the world including Jean-Michel Dubernard who, shortly thereafter, performed the first successful double hand transplant. Telesurgery was developed by Jacques Marescaux and his team on 7 September 2001 across the Atlantic Ocean (New-York-Strasbourg, Lindbergh Operation). A face transplant was first done on 27 November 2005 by Dr Bernard Devauchelle.
France was the fourth country to achieve nuclear capability and has the third largest nuclear weapons arsenal in the world. It is also a leader in civilian nuclear technology. France was the third nation, after the former USSR and the United States, to launch its own space satellite and remains the biggest contributor to the European Space Agency (ESA). The European Airbus Group, formed from the French group Aérospatiale along with DaimlerChrysler Aerospace AG (DASA) and Construcciones Aeronáuticas SA (CASA), designs and develops civil and military aircraft as well as communications systems, missiles, space rockets, helicopters, satellites, and related systems. From 1970 SNCF, the French national railroad company, has developed the TGV, a high speed train which holds a series of world speed records. The TGV has been the fastest wheeled train in commercial use since reaching a speed of 574.8 km/h (357.2 mph) on 3 April 2007. Western Europe is now serviced by a network of TGV lines.
As of 2016, 68 French people have been awarded a Nobel Prize and 12 have received the Fields Medal.




With an estimated total population of around 67 million people as of January 2017, with 64.8 million in metropolitan France, France is the 20th most populous country in the world and the third-most populous in Europe. France is also second most populous country in the European Union after Germany.
France is an outlier among developed countries in general, and European countries in particular, in having a fairly high rate of natural population growth: by birth rates alone, France was responsible for almost all natural population growth in the European Union in 2006, with the natural growth rate (excess of births over deaths) rising to 300,000 and with the immigration the population grew with almost 400,000 people, although in the late 2010s it fell to 200,000. This was the highest rate since the end of the baby boom in 1973, and coincides with the rise of the total fertility rate from a nadir of 1.7 in 1994 to 2.0 in 2010. As of January 2017 the fertility rate was 1.93.
From 2006 to 2011 population growth was on average +0.6% per year. Immigrants are also major contributors to this trend; in 2010, 27% of newborns in metropolitan France had at least one foreign-born parent and 24% had at least one parent born outside of Europe (parents born in overseas territories are considered as born in France).



Most French people are of Celtic (Gauls) origin, with an admixture of Latin (Romans) and Germanic (Franks) groups. Different regions reflect this diverse heritage, with notable Breton elements in western France, Aquitanian in the southwest, Scandinavian in the northwest, Alemannic in the northeast and Ligurian influence in the southeast.
Large-scale immigration over the last century and a half has led to a more multicultural society. In 2004, the Institut Montaigne estimated that within Metropolitan France, 51 million people were White (85% of the population), 6 million were North African (10%), 2 million were Black (3.3%), and 1 million were Asian (1.7%).
A law originating from the 1789 revolution and reaffirmed in the 1958 French Constitution makes it illegal for the French state to collect data on ethnicity and ancestry. In 2008, the TeO ("Trajectories and origins") poll conducted jointly by INED and the French National Institute of Statistics estimated that 5 million people were of Italian ancestry (the largest immigrant community), followed by 3 million to 6 million people of North African ancestry, 2.5 million people of Sub-Saharan African origin, and 200,000 people of Turkish ancestry. There are over 500,000 ethnic Armenians in France. There are also sizeable minorities of other European ethnic groups, namely Spanish, Portuguese, Polish, and Greek.
France has a significant Gypsy (Gitan) population, numbering around 400,000. Famous French Gypsies (Gitans) include Django Reinhardt, Gipsy Kings and Kendji Girac. Gypsies inspired the French novel Hunchback of Notre Dame. However, many Romani people get deported, expelled and kicked back to Bulgaria and Romania frequently.
It is currently estimated that 40% of the French population is descended at least partially from the different waves of immigration the country has received since the early 20th century; between 1921 and 1935 alone, about 1.1 million net immigrants came to France. The next largest wave came in the 1960s, when around 1.6 million pieds noirs returned to France following the independence of its North African possessions, Algeria and Morocco. They were joined by numerous former colonial subjects from North and West Africa, as well as numerous immigrants from Spain and Portugal.
France remains a major destination for immigrants, accepting about 200,000 legal immigrants annually. It is also Western Europe's leading recipient of asylum seekers, with an estimated 50,000 applications in 2005 (a 15% decrease from 2004). The European Union allows free movement between the member states, although France established controls to curb Eastern European migration, and immigration remains a contentious political issue.
In 2008, the INSEE estimated that the total number of foreign-born immigrants was around 5 million (8% of the population), while their French-born descendants numbered 6.5 million, or 11% of the population. Thus, nearly a fifth of the country's population were either first or second-generation immigrants, of which more than 5 million where of European origin and 4 million of Maghrebi ancestry. In 2008, France granted citizenship to 137,000 persons, mostly to people from Morocco, Algeria and Turkey.
In 2014 The National Institute of Statistics (INSEE, for its acronym in French) published a study which reported doubling of the number of Spanish immigrants, Portuguese and Italians in France between 2009 and 2012. According to the French Institute, this increase resulting from the financial crisis that hit several European countries in that period, has pushed up the number of Europeans installed in France. Statistics on Spanish immigrants in France show a growth of 107 percent between 2009 and 2012, i.e. in this period went from 5300 to 11,000 people. Of the total of 229,000 foreigners who were in France in 2012, nearly 8% were Portuguese, 5% British, 5% Spanish, 4% Italians, 4% Germans, 3% Romanians, and 3% Belgians.



France is a highly urbanized country, with its largest cities (in terms of metropolitan area population in 2013) being Paris (12,405,426 inh.), Lyon (2,237,676), Marseille (1,734,277), Toulouse (1,291,517), Bordeaux (1,178,335), Lille (1,175,828), Nice (1,004,826), Nantes (908,815), Strasbourg (773,447) and Rennes (700,675). (Note: There are significant differences between the metropolitan population figures just cited and those in the following table, which only include the core population). Rural flight was a perennial political issue throughout most of the 20th century.







According to Article 2 of the Constitution, the official language of France is French, a Romance language derived from Latin. Since 1635, the Académie française has been France's official authority on the French language, although its recommendations carry no legal power.
The French government does not regulate the choice of language in publications by individuals but the use of French is required by law in commercial and workplace communications. In addition to mandating the use of French in the territory of the Republic, the French government tries to promote French in the European Union and globally through institutions such as La Francophonie. The perceived threat from anglicisation has prompted efforts to safeguard the position of the French language in France. Besides French, there exist 77 vernacular minority languages of France, eight spoken in French metropolitan territory and 69 in the French overseas territories.
From the 17th to the mid-20th century, French served as the pre-eminent international language of diplomacy and international affairs as well as a lingua franca among the educated classes of Europe. The dominant position of French language in international affairs was overtaken by English, since the emergence of the US as a major power.
For most of the time in which French served as an international lingua franca, it was not the native language of most Frenchmen: a report in 1794 conducted by Henri Grégoire found that of the country's 25 million people, only three million spoke French natively; the rest spoke one of the country's many regional languages, such as Alsatian, Breton or Occitan. Through the expansion of public education, in which French was the sole language of instruction, as well as other factors such as increased urbanisation and the rise of mass communication, French gradually came to be adopted by virtually the entire population, a process not completed until the 20th century.
As a result of France's extensive colonial ambitions between the 17th and 20th centuries, French was introduced to the Americas, Africa, Polynesia, South-East Asia, and the Caribbean. French is the second most studied foreign language in the world after English, and is a lingua franca in some regions, notably in Africa. The legacy of French as a living language outside Europe is mixed: it is nearly extinct in some former French colonies (The Levant, South and Southeast Asia), while creoles and pidgins based on French have emerged in the French departments in the West Indies and the South Pacific (French Polynesia). On the other hand, many former French colonies have adopted French as an official language, and the total number of French speakers is increasing, especially in Africa.
It is estimated that between 300 million and 500 million people worldwide can speak French, either as a mother tongue or a second language.




France is a secular country, and freedom of religion is a constitutional right. French religious policy is based on the concept of laïcité, a strict separation of church and state under which public life is kept completely secular.
Catholicism has been the predominant religion in France for more than a millennium, though it is not as actively practised today as it was. Among the 47,000 religious buildings in France, 94% are Roman Catholic. While in 1965, 81% of the French declared themselves to be Catholics, in 2009 this proportion was 64%. Moreover, while 27% of the French went to Mass once a week or more in 1952, only 5% did so in 2006. The same survey found that Protestants accounted for 3% of the population, an increase from previous surveys, and 5% adhered to other religions, with the remaining 28% stating they had no religion. Evangelicalism may be the fastest growing religious category in France.
During the French Revolution, activists conducted a brutal campaign of de-Christianisation, ending the established state status of the Catholic Church. In some cases clergy and churches were attacked, with iconoclasm stripping the churches of statues and ornament. After the back and forth of Catholic royal and secular republican governments during the 19th century, France established laïcité by passage of the 1905 law on the Separation of the Churches and the State.
According to a survey in January 2007, only 5% of the French population attended church regularly (among the respondents who identified as Catholic, 10% attend church services regularly). The poll showed that 51% of citizens identified as being Catholic, 31% identified as agnostic or atheist (another poll sets the proportion of atheists equal to 27%), 10% identified as being from other religions or being without opinion, 4% identified as Muslim, 3% identified as Protestant, 1% identified as Buddhist, and 1% identified as Jewish. Meanwhile, an independent estimate by the political scientist Pierre Bréchon in 2009 concluded that the proportion of Catholics had fallen to 42%, while the number of atheists and agnostics had risen to 50%.
According to Eurobarometer poll in 2012, Christianity is the largest religion in France, accounting for 60% of French citizens. Catholics are the largest Christian group in France, accounting for 50% of French citizens, while Protestants make up 8%, and other Christians make up 2%. Non believer/Agnostic account for 20%, Atheist 13%, and Muslim 6%.
Estimates of the number of Muslims in France vary widely. In 2003, the French Ministry of the Interior estimated the total number of people of Muslim background to be between 5 and 6 million (8–10%). According to the Pewforum, "In France, proponents of a 2004 law banning the wearing of religious symbols in schools say it protects Muslim girls from being forced to wear a headscarf, but the law also restricts those who want to wear headscarves – or any other "conspicuous" religious symbol, including large Christian crosses and Sikh turbans – as an expression of their faith."
The current Jewish community in France numbers around 600,000, according to the World Jewish Congress, and is the largest in Europe. It is the third-largest in the world, after those in Israel and the United States.
Since 1905 the French government has followed the principle of laïcité, in which it is prohibited from recognising any specific right to a religious community (except for legacy statutes like those of military chaplains and the local law in Alsace-Moselle). It recognises religious organisations, according to formal legal criteria that do not address religious doctrine. Conversely, religious organisations are expected to refrain from intervening in policy-making. Certain groups, such as Scientology, Children of God, the Unification Church, or the Order of the Solar Temple, are considered cults ("sectes" in French), and therefore do not have the same status as recognised religions in France. Secte is considered a pejorative term in France.




The French health care system is one of universal health care largely financed by government national health insurance. In its 2000 assessment of world health care systems, the World Health Organization found that France provided the "close to best overall health care" in the world. The French healthcare system was ranked first worldwide by the World Health Organization in 1997. In 2011, France spent 11.6% of GDP on health care, or US$4,086 per capita, a figure much higher than the average spent by countries in Europe but less than in the US. Approximately 77% of health expenditures are covered by government funded agencies.
Care is generally free for people affected by chronic diseases (affections de longues durées) such as cancer, AIDS or cystic fibrosis. Average life expectancy at birth is 78 years for men and 85 years for women, one of the highest of the European Union. There are 3.22 physicians for every 1000 inhabitants in France, and average health care spending per capita was US$4,719 in 2008. As of 2007, approximately 140,000 inhabitants (0.4%) of France are living with HIV/AIDS.
Even if the French have the reputation of being one of the thinnest people in developed countries, France—like other rich countries—faces an increasing and recent epidemic of obesity, due mostly to the replacement in French eating habits of traditional healthy French cuisine by junk food. The French obesity rate is still far below that of the USA (for instance, obesity rate in France is the same as the US had in the 1970s), and is still the lowest of Europe. Authorities now regard obesity as one of the main public health issues and fight it fiercely. Rates of childhood obesity are slowing in France, while continuing to grow in other countries.




In 1802, Napoleon created the lycée. Nevertheless, it is Jules Ferry who is considered to be the father of the French modern school, which is free, secular, and compulsory until the age of 13 since 1882 (school attendance in France is now compulsory until the age of 16).
Nowadays, the schooling system in France is centralised, and is composed of three stages, primary education, secondary education, and higher education. The Programme for International Student Assessment, coordinated by the OECD, currently ranks France's education as the 25th best in the world, being neither significantly higher nor lower than the OECD average. Primary and secondary education are predominantly public, run by the Ministry of National Education. In France, education is compulsory from six to sixteen years old, and the public school is secular and free. While training and remuneration of teachers and the curriculum are the responsibility of the state centrally, the management of primary and secondary schools is overseen by local authorities. Primary education comprises two phases, nursery school (école maternelle) and elementary school (école élémentaire). Nursery school aims to stimulate the minds of very young children and promote their socialisation and development of a basic grasp of language and number. Around the age of six, children transfer to elementary school, whose primary objectives are learning about writing, arithmetic and citizenship. Secondary education also consists of two phases. The first is delivered through colleges (collège) and leads to the national certificate (Diplôme national du brevet). The second is offered in high schools (lycée) and finishes in national exams leading to a baccalaureate (baccalauréat, available in professional, technical or general flavours) or certificate of professional competence (certificat d'aptitude professionelle).
Higher education is divided between public universities and the prestigious and selective Grandes écoles, such as Sciences Po Paris for Political studies, HEC Paris for Economics, Polytechnique and the École nationale supérieure des mines de Paris that produce high-profile engineers, or the École nationale d'administration for careers in the Grands Corps of the state. The Grandes écoles have been criticised for alleged elitism; they have produced many if not most of France's high-ranking civil servants, CEOs, and politicians.
Since higher education is funded by the state, the fees are very low; tuition fees vary from €150 to €700 depending on the university and the different levels of education (licence, master, doctorate). One can therefore get a master's degree (in 5 years) for about €750–3,500. The tuition fees in public engineering schools are comparable to universities, albeit a little higher (around €700). However they can reach €7000 a year for private engineering schools, while business schools, which are all private or partially private, charge up to €15000 a year. Health insurance for students is free until the age of 20.




France has been a centre of Western cultural development for centuries. Many French artists have been among the most renowned of their time, and France is still recognised in the world for its rich cultural tradition.
The successive political regimes have always promoted artistic creation, and the creation of the Ministry of Culture in 1959 helped preserve the cultural heritage of the country and make it available to the public. The Ministry of Culture has been very active since its creation, granting subsidies to artists, promoting French culture in the world, supporting festivals and cultural events, protecting historical monuments. The French government also succeeded in maintaining a cultural exception to defend audiovisual products made in the country.
France receives the highest number of tourists per year, largely thanks to the numerous cultural establishments and historical buildings implanted all over the territory. It counts 1,200 museums welcoming more than 50 million people annually. The most important cultural sites are run by the government, for instance through the public agency Centre des monuments nationaux, which is responsible for approximately 85 national historical monuments.
The 43,180 buildings protected as historical monuments include mainly residences (many castles, or châteaux in French) and religious buildings (cathedrals, basilicas, churches, etc.), but also statutes, memorials and gardens. The UNESCO inscribed 41 sites in France on the World Heritage List.




The origins of French art were very much influenced by Flemish art and by Italian art at the time of the Renaissance. Jean Fouquet, the most famous medieval French painter, is said to have been the first to travel to Italy and experience the Early Renaissance at first hand. The Renaissance painting School of Fontainebleau was directly inspired by Italian painters such as Primaticcio and Rosso Fiorentino, who both worked in France. Two of the most famous French artists of the time of Baroque era, Nicolas Poussin and Claude Lorrain, lived in Italy.
The 17th century was the period when French painting became prominent and individualised itself through classicism. Louis XIV's prime minister Jean-Baptiste Colbert founded in 1648 the Royal Academy of Painting and Sculpture to protect these artists, and in 1666 he created the still-active French Academy in Rome to have direct relations with Italian artists.

French artists developed the rococo style in the 18th century, as a more intimate imitation of old baroque style, the works of the court-endorsed artists Antoine Watteau, François Boucher and Jean-Honoré Fragonard being the most representative in the country. The French Revolution brought great changes, as Napoleon favoured artists of neoclassic style such as Jacques-Louis David and the highly influential Académie des Beaux-Arts defined the style known as Academism. At this time France had become a centre of artistic creation, the first half of the 19th century being dominated by two successive movements, at first Romanticism with Théodore Géricault and Eugène Delacroix, and Realism with Camille Corot, Gustave Courbet and Jean-François Millet, a style that eventually evolved into Naturalism.
In the second part of the 19th century, France's influence over painting became even more important, with the development of new styles of painting such as Impressionism and Symbolism. The most famous impressionist painters of the period were Camille Pissarro, Édouard Manet, Edgar Degas, Claude Monet and Auguste Renoir. The second generation of impressionist-style painters, Paul Cézanne, Paul Gauguin, Toulouse-Lautrec and Georges Seurat, were also at the avant-garde of artistic evolutions, as well as the fauvist artists Henri Matisse, André Derain and Maurice de Vlaminck.
At the beginning of the 20th century, Cubism was developed by Georges Braque and the Spanish painter Pablo Picasso, living in Paris. Other foreign artists also settled and worked in or near Paris, such as Vincent van Gogh, Marc Chagall, Amedeo Modigliani and Wassily Kandinsky.
Many museums in France are entirely or partly devoted to sculptures and painting works. A huge collection of old masterpieces created before or during the 18th century are displayed in the state-owned Musée du Louvre, such as Mona Lisa, also known as La Joconde. While the Louvre Palace has been for a long time a museum, the Musée d'Orsay was inaugurated in 1986 in the old railway station Gare d'Orsay, in a major reorganisation of national art collections, to gather French paintings from the second part of the 19th century (mainly Impressionism and Fauvism movements).
Modern works are presented in the Musée National d'Art Moderne, which moved in 1976 to the Centre Georges Pompidou. These three state-owned museums welcome close to 17 million people a year. Other national museums hosting paintings include the Grand Palais (1.3 million visitors in 2008), but there are also many museums owned by cities, the most visited being the Musée d'Art Moderne de la Ville de Paris (0.8 million entries in 2008), which hosts contemporary works.
Outside Paris, all the large cities have a Museum of Fine Arts with a section dedicated to European and French painting. Some of the finest collections are in Lyon, Lille, Rouen, Dijon, Rennes and Grenoble.




During the Middle Ages, many fortified castles were built by feudal nobles to mark their powers. Some French castles that survived are Chinon, Château d'Angers, the massive Château de Vincennes and the so-called Cathar castles. During this era, France had been using Romanesque architecture like most of Western Europe. Some of the greatest examples of Romanesque churches in France are the Saint Sernin Basilica in Toulouse, the largest romanesque church in Europe, and the remains of the Cluniac Abbey.
The Gothic architecture, originally named Opus Francigenum meaning "French work", was born in Île-de-France and was the first French style of architecture to be copied in all Europe. Northern France is the home of some of the most important Gothic cathedrals and basilicas, the first of these being the Saint Denis Basilica (used as the royal necropolis); other important French Gothic cathedrals are Notre-Dame de Chartres and Notre-Dame d'Amiens. The kings were crowned in another important Gothic church: Notre-Dame de Reims. Aside from churches, Gothic Architecture had been used for many religious palaces, the most important one being the Palais des Papes in Avignon.

The final victory in the Hundred Years' War marked an important stage in the evolution of French architecture. It was the time of the French Renaissance and several artists from Italy were invited to the French court; many residential palaces were built in the Loire Valley. Such residential castles were the Château de Chambord, the Château de Chenonceau, or the Château d'Amboise.
Following the renaissance and the end of the Middle Ages, Baroque architecture replaced the traditional Gothic style. However, in France, baroque architecture found a greater success in the secular domain than in a religious one. In the secular domain, the Palace of Versailles has many baroque features. Jules Hardouin Mansart, who designed the extensions to Versailles, was one of the most influential French architect of the baroque era; he is famous for his dome at Les Invalides. Some of the most impressive provincial baroque architecture is found in places that were not yet French such as the Place Stanislas in Nancy. On the military architectural side, Vauban designed some of the most efficient fortresses in Europe and became an influential military architect; as a result, imitations of his works can be found all over Europe, the Americas, Russia and Turkey.

After the Revolution, the Republicans favoured Neoclassicism although neoclassicism was introduced in France prior to the revolution with such building as the Parisian Pantheon or the Capitole de Toulouse. Built during the first French Empire, the Arc de Triomphe and Sainte Marie-Madeleine represent the best example of Empire style architecture.
Under Napoleon III, a new wave of urbanism and architecture was given birth; extravagant buildings such as the neo-baroque Palais Garnier were built. The urban planning of the time was very organised and rigorous; for example, Haussmann's renovation of Paris. The architecture associated to this era is named Second Empire in English, the term being taken from the Second French Empire. At this time there was a strong Gothic resurgence across Europe and in France; the associated architect was Eugène Viollet-le-Duc. In the late 19th century, Gustave Eiffel designed many bridges, such as Garabit viaduct, and remains one of the most influential bridge designers of his time, although he is best remembered for the iconic Eiffel Tower.
In the 20th century, French-Swiss architect Le Corbusier designed several buildings in France. More recently, French architects have combined both modern and old architectural styles. The Louvre Pyramid is an example of modern architecture added to an older building. The most difficult buildings to integrate within French cities are skyscrapers, as they are visible from afar. For instance, in Paris, since 1977, new buildings had to be under 37 meters (121 feet). France's largest financial district is La Defense, where a significant number of skyscrapers are located. Other massive buildings that are a challenge to integrate into their environment are large bridges; an example of the way this has been done is the Millau Viaduct. Some famous modern French architects include Jean Nouvel, Dominique Perrault, Christian de Portzamparc or Paul Andreu.




The earliest French literature dates from the Middle Ages, when what is now known as modern France did not have a single, uniform language. There were several languages and dialects and writers used their own spelling and grammar. Some authors of French mediaeval texts are unknown, such as Tristan and Iseult and Lancelot-Grail. Other authors are known, for example Chrétien de Troyes and Duke William IX of Aquitaine, who wrote in Occitan.
Much mediaeval French poetry and literature were inspired by the legends of the Matter of France, such as The Song of Roland and the various chansons de geste. The Roman de Renart, written in 1175 by Perrout de Saint Cloude, tells the story of the mediaeval character Reynard ('the Fox') and is another example of early French writing.
An important 16th-century writer was François Rabelais, whose novel Gargantua and Pantagruel has remained famous and appreciated until now. Michel de Montaigne was the other major figure of the French literature during that century. His most famous work, Essais, created the literary genre of the essay. French poetry during that century was embodied by Pierre de Ronsard and Joachim du Bellay. Both writers founded the La Pléiade literary movement.
During the 17th century, Madame de La Fayette published anonymously La Princesse de Clèves, a novel that is considered to be one of the very first psychological novels of all times. Jean de La Fontaine is one of the most famous fabulist of that time, as he wrote hundreds of fables, some being far more famous than others, such as The Ant and the Grasshopper. Generations of French pupils had to learn his fables, that were seen as helping teaching wisdom and common sense to the young people. Some of his verses have entered the popular language to become proverbs.

Jean Racine, whose incredible mastery of the alexandrine and of the French language has been praised for centuries, created plays such as Phèdre or Britannicus. He is, along with Pierre Corneille (Le Cid) and Molière, considered as one of the three great dramatists of the France's golden age. Molière, who is deemed to be one of the greatest masters of comedy of the Western literature, wrote dozens of plays, including Le Misanthrope, L'Avare, Le Malade imaginaire, and Le Bourgeois Gentilhomme. His plays have been so popular around the world that French language is sometimes dubbed as "the language of Molière" (la langue de Molière), just like English is considered as "the language of Shakespeare".
French literature and poetry flourished even more in the 18th and 19th centuries. Denis Diderot's best-known works are Jacques the Fatalist and Rameau's Nephew. He is however best known for being the main redactor of the Encyclopédie, whose aim was to sum up all the knowledge of his century (in fields such as arts, sciences, languages, philosophy) and to present them to the people, in order to fight ignorance and obscurantism. During that same century, Charles Perrault was a prolific writer of famous children's fairy tales including Puss in Boots, Cinderella, Sleeping Beauty and Bluebeard. At the start of the 19th century, symbolist poetry was an important movement in French literature, with poets such as Charles Baudelaire, Paul Verlaine and Stéphane Mallarmé.
The 19th century saw the writings of many renowned French authors. Victor Hugo is sometimes seen as "the greatest French writer of all times" for excelling in all literary genres. The preface of his play Cromwell is considered to be the manifesto of the Romantic movement. Les Contemplations and La Légende des siècles are considered as "poetic masterpieces", Hugo's verse having been compared to that of Shakespeare, Dante and Homer. His novel Les Misérables is widely seen as one of the greatest novel ever written and The Hunchback of Notre Dame has remained immensely popular.
Other major authors of that century include Alexandre Dumas (The Three Musketeers and The Count of Monte-Cristo), Jules Verne (Twenty Thousand Leagues Under the Sea), Émile Zola (Les Rougon-Macquart), Honoré de Balzac (La Comédie humaine), Guy de Maupassant, Théophile Gautier and Stendhal (The Red and the Black, The Charterhouse of Parma), whose works are among the most well known in France and the world.
The Prix Goncourt is a French literary prize first awarded in 1903. Important writers of the 20th century include Marcel Proust, Louis-Ferdinand Céline, Albert Camus, and Jean-Paul Sartre. Antoine de Saint Exupéry wrote Little Prince, which has remained popular for decades with children and adults around the world. As of 2014, French authors had more Literature Nobel Prizes than those of any other nation. The first Nobel Prize in Literature was a French author, while France's latest Nobel prize in literature is Patrick Modiano, who was awarded the prize in 2014. Jean-Paul Sartre was also the first nominee in the committee's history to refuse the prize in 1964.




Medieval philosophy was dominated by Scholasticism until the emergence of Humanism in the Renaissance. Modern philosophy began in France in the 17th century with the philosophy of René Descartes, Blaise Pascal, and Nicolas Malebranche. Descartes revitalised Western philosophy, which had been declined after the Greek and Roman eras. His Meditations on First Philosophy changed the primary object of philosophical thought and raised some of the most fundamental problems for foreigners such as Spinoza, Leibniz, Hume, Berkeley, and Kant.

During the 18th century, French philosophers produced one of the most important works of the Age of Enlightenment. In The Spirit of the Laws, Baron de Montesquieu theorised the principle of separation of powers, which has been implemented in all liberal democracies since it was first applied in the United States. In The Social Contract, Jean-Jacques Rousseau openly criticised the European divine right monarchies and strongly affirmed the principle of the sovereignty of the people. Voltaire came to embody the Enlightenment with his defence of civil liberties, such as the right to a free trial and freedom of religion.
19th-century French thought was targeted at responding to the social malaise following the French Revolution. Rationalist philosophers such as Victor Cousin and Auguste Comte, who called for a new social doctrine, were opposed by reactionary thinkers such as Joseph de Maistre, Louis de Bonald and Lamennais, who blamed the rationalist rejection of traditional order. De Maistre is considered, together with the Englishman Edmund Burke, one of the founders of European conservatism, while Comte is regarded as the founder of positivism and sociology.
In the early 20th century, French spiritualist thinkers such as Maine de Biran, Henri Bergson and Louis Lavelle influenced Anglo-Saxon thought, including the Americans Charles Sanders Peirce and William James, and the Englishman Alfred North Whitehead. In the late 20th century, partly influenced by German phenomenology and existentialism, postmodern philosophy began in France, with notable post-structuralist thinkers including Jean-François Lyotard, Jean Baudrillard, Jacques Derrida, Jacques Lacan, Michel Foucault and Gilles Deleuze.




France has a long and varied musical history. It experienced a golden age in the 17th century thanks to Louis XIV, who employed a number of talented musicians and composers in the royal court. The most renowned composers of this period include Marc-Antoine Charpentier, François Couperin, Michel-Richard Delalande, Jean-Baptiste Lully and Marin Marais, all of them composers at the court. After the death of the "Roi Soleil", French musical creation lost dynamism, but in the next century the music of Jean-Philippe Rameau reached some prestige, and today he is still one of the most renowned French composers. Rameau became the dominant composer of French opera and the leading French composer for the harpsichord.
French composers played an important role during the music of the 19th and early 20th century, which is considered to be the Romantic music era. Romantic music emphasised a surrender to nature, a fascination with the past and the supernatural, the exploration of unusual, strange and surprising sounds, and a focus on national identity. This period was also a golden age for operas. French composers from the Romantic era included: Hector Berlioz (best known for his Symphonie fantastique), Georges Bizet (best known for Carmen, which has become one of the most popular and frequently performed operas), Gabriel Fauré (best known for his Pavane, Requiem, and nocturnes), Charles Gounod (best known for his Ave Maria and his opera Faust), Jacques Offenbach (best known for his 100 operettas of the 1850s–1870s and his uncompleted opera The Tales of Hoffmann), Édouard Lalo (best known for his Symphonie espagnole for violin and orchestra and his Cello Concerto in D minor), Jules Massenet (best known for his operas, of which he wrote more than thirty, the most frequently staged are Manon (1884) and Werther (1892)) and Camille Saint-Saëns (he has many frequently-performed works, including The Carnival of the Animals, Danse macabre, Samson and Delilah (Opera), Introduction and Rondo Capriccioso, and his Symphony No. 3 (Organ Symphony)).
Later came precursors of modern classical music. Érik Satie was a key member of the early 20th century Parisian avant-garde, best known for his Gymnopédies. Francis Poulenc's best known works are his piano suite Trois mouvements perpétuels (1919), the ballet Les biches (1923), the Concert champêtre (1928) for harpsichord and orchestra, the opera Dialogues des Carmélites (1957), and the Gloria (1959) for soprano, choir and orchestra. Maurice Ravel and Claude Debussy are the most prominent figures associated with Impressionist music. Debussy was among the most influential composers of the late 19th and early 20th centuries, and his use of non-traditional scales and chromaticism influenced many composers who followed. Debussy's music is noted for its sensory content and frequent usage of atonality. The two composers invented new musical forms and new sounds. Ravel's piano compositions, such as Jeux d'eau, Miroirs, Le tombeau de Couperin and Gaspard de la nuit, demand considerable virtuosity. His mastery of orchestration is evident in the Rapsodie espagnole, Daphnis et Chloé, his arrangement of Modest Mussorgsky's Pictures at an Exhibition and his orchestral work Boléro (1928).
More recently, at the middle of the 20th century, Maurice Ohana, Pierre Schaeffer and Pierre Boulez contributed to the evolutions of contemporary classical music.

French music then followed the rapid emergence of pop and rock music at the middle of the 20th century. Although English-speaking creations achieved popularity in the country, French pop music, known as chanson française, has also remained very popular. Among the most important French artists of the century are Édith Piaf, Georges Brassens, Léo Ferré, Charles Aznavour and Serge Gainsbourg. Although there are very few rock bands in France compared to English-speaking countries, bands such as Noir Désir, Mano Negra, Niagara, Les Rita Mitsouko and more recently Superbus, Phoenix and Gojira have reached worldwide popularity.
Other French artists with international careers have been popular in several countries, for example female singers Dalida, Mireille Mathieu, Mylène Farmer and Nolwenn Leroy, electronic music pioneers Jean-Michel Jarre, Laurent Garnier and Bob Sinclar, and later Martin Solveig and David Guetta. In the 1990s and 2000s (decade), electronic duos Daft Punk, Justice and Air also reached worldwide popularity and contributed to the reputation of modern electronic music in the world.
Among current musical events and institutions in France, many are dedicated to classical music and operas. The most prestigious institutions are the state-owned Paris National Opera (with its two sites Palais Garnier and Opéra Bastille), the Opéra National de Lyon, the Théâtre du Châtelet in Paris, the Théâtre du Capitole in Toulouse and the Grand Théâtre de Bordeaux. As for music festivals, there are several events organised, the most popular being the Eurockéennes and Rock en Seine. The Fête de la Musique, imitated by many foreign cities, was first launched by the French government in 1982. Major music halls and venues in France include Le Zénith sites present in many cities and other places in Paris (Paris Olympia, Théâtre Mogador, Élysée Montmartre, etc.).




France has historical and strong links with cinema, with two Frenchmen, Auguste and Louis Lumière (known as the Lumière Brothers) having created cinema in 1895. Several important cinematic movements, including the late 1950s and 1960s Nouvelle Vague, began in the country. It is noted for having a particularly strong film industry, due in part to protections afforded by the French government. France remains a leader in filmmaking, as of 2006 producing more films than any other European country. The nation also hosts the Cannes Festival, one of the most important and famous film festivals in the world.
Apart from its strong and innovative film tradition, France has also been a gathering spot for artists from across Europe and the world. For this reason, French cinema is sometimes intertwined with the cinema of foreign nations. Directors from nations such as Poland (Roman Polanski, Krzysztof Kieślowski, and Andrzej Żuławski), Argentina (Gaspar Noé and Edgardo Cozarinsky), Russia (Alexandre Alexeieff, Anatole Litvak), Austria (Michael Haneke), and Georgia (Géla Babluani, Otar Iosseliani) are prominent in the ranks of French cinema. Conversely, French directors have had prolific and influential careers in other countries, such as Luc Besson, Jacques Tourneur, or Francis Veber in the United States.
Although the French film market is dominated by Hollywood, France is the only nation in the world where American films make up the smallest share of total film revenues, at 50%, compared with 77% in Germany and 69% in Japan. French films account for 35% of the total film revenues of France, which is the highest percentage of national film revenues in the developed world outside the United States, compared to 14% in Spain and 8% in the UK. France is in 2013 the 2nd exporter of films in the world after the United States.
Until recently, France had for centuries been the cultural center of the world, although its dominant position has been surpassed by the United States. Subsequently, France takes steps in protecting and promoting its culture, becoming a leading advocate of the cultural exception. The nation succeeded in convincing all EU members to refuse to include culture and audiovisuals in the list of liberalised sectors of the WTO in 1993. Moreover, this decision was confirmed in a voting in the UNESCO in 2005, and the principle of "cultural exception" won an overwhelming victory: 198 countries voted for it, only 2 countries, the U.S and Israel, voted against it.




Fashion has been an important industry and cultural export of France since the 17th century, and modern "haute couture" originated in Paris in the 1860s. Today, Paris, along with London, Milan, and New York City, is considered one of the world's fashion capitals, and the city is home or headquarters to many of the premier fashion houses. The expression Haute couture is, in France, a legally protected name, guaranteeing certain quality standards.
The association of France with fashion and style (French: la mode) dates largely to the reign of Louis XIV when the luxury goods industries in France came increasingly under royal control and the French royal court became, arguably, the arbiter of taste and style in Europe. But France renewed its dominance of the high fashion (French: couture or haute couture) industry in the years 1860–1960 through the establishing of the great couturier houses such as Chanel, Dior, and Givenchy. The French perfume industry is world leader in its sector and is centered on the town of Grasse.
In the 1960s, the elitist "Haute couture" came under criticism from France's youth culture. In 1966, the designer Yves Saint Laurent broke with established Haute Couture norms by launching a prêt-à-porter ("ready to wear") line and expanding French fashion into mass manufacturing. With a greater focus on marketing and manufacturing, new trends were established by Sonia Rykiel, Thierry Mugler, Claude Montana, Jean-Paul Gaultier and Christian Lacroix in the 1970s and 1980s. The 1990s saw a conglomeration of many French couture houses under luxury giants and multinationals such as LVMH.




Compared to other developed countries, the French do not spend much time reading newspapers, due to the popularity of broadcast media. Best-selling daily national newspapers in France are Le Parisien Aujourd'hui en France (with 460,000 sold daily), Le Monde and Le Figaro, with around 300,000 copies sold daily, but also L'Équipe, dedicated to sports coverage. In the past years, free dailies made a breakthrough, with Metro, 20 Minutes and Direct Plus distributed at more than 650,000 copies respectively. However, the widest circulations are reached by regional daily Ouest France with more than 750,000 copies sold, and the 50 other regional papers have also high sales. The sector of weekly magazines is stronger and diversified with more than 400 specialised weekly magazines published in the country.
The most influential news magazines are the left-wing Le Nouvel Observateur, centrist L'Express and right-wing Le Point (more than 400.000 copies), but the highest circulation for weeklies is reached by TV magazines and by women's magazines, among them Marie Claire and ELLE, which have foreign versions. Influential weeklies also include investigative and satirical papers Le Canard Enchaîné and Charlie Hebdo, as well as Paris Match. Like in most industrialised nations, the print media have been affected by a severe crisis in the past decade. In 2008, the government launched a major initiative to help the sector reform and become financially independent, but in 2009 it had to give 600,000 euros to help the print media cope with the economic crisis, in addition to existing subsidies.
In 1974, after years of centralised monopoly on radio and television, the governmental agency ORTF was split into several national institutions, but the three already-existing TV channels and four national radio stations remained under state-control. It was only in 1981 that the government allowed free broadcasting in the territory, ending state monopoly on radio. French television was partly liberalised in the next two decade with the creation of several commercial channels, mainly thanks to cable and satellite television. In 2005 the national service Télévision Numérique Terrestre introduced digital television all over the territory, allowing the creation of other channels.
The four existing national channels are now owned by state-owned consortium France Télévisions, while public broadcasting group Radio France run five national radio stations. Among these public media are Radio France Internationale, which broadcasts programmes in French all over the world, and Franco-German TV channel TV5 Monde. In 2006, the government created global news channel France 24. Long-established TV channels TF1 (privatised in 1987), France 2 and France 3 have the highest shares, while radio stations RTL, Europe 1 and state-owned France Inter are the least listened to.




According to a BBC poll in 2010, based on 29,977 responses in 28 countries, France is globally seen as a positive influence in the world's affairs: 49% have a positive view of the country's influence, whereas 19% have a negative view. The Nation Brand Index of 2008 suggested that France has the second best international reputation, only behind Germany.

According to a poll in 2011, the French were found to have the highest level of religious tolerance and to be the country where the highest proportion of the population defines its identity primarily in term of nationality and not religion. 69% of French have a favourable view of the US, making France one of the most pro-American countries in the world.
In January 2010, the magazine International Living ranked France as "best country to live in", ahead of 193 other countries, for the fifth year running.
The French Revolution continues to permeate the country's collective memory. The tricolour flag, the anthem "La Marseillaise", and the motto Liberté, egalité, fraternité, defined in Title 1 of the Constitution as national symbols, all emerged during the cultural ferment of the early revolution, along with Marianne, a common national personification. In addition, Bastille Day, the national holiday, commemorates the storming of the Bastille on 14 July 1789.
A common and traditional symbol of the French people is the Gallic rooster. Its origins date back to Antiquity, since the Latin word Gallus meant both "rooster" and "inhabitant of Gaul". Then this figure gradually became the most widely shared representation of the French, used by French monarchs, then by the Revolution and under the successive republican regimes as representation of the national identity, used for some stamps and coins.




French cuisine is renowned for being one of the finest in the world. According to the regions, traditional recipes are different, the North of the country prefers to use butter as the preferred fat for cooking, whereas olive oil is more commonly used in the South. Moreover, each region of France has iconic traditional specialities: Cassoulet in the Southwest, Choucroute in Alsace, Quiche in the Lorraine region, Beef bourguignon in the Bourgogne, provençal Tapenade, etc. France's most renowned products are wines, including Champagne, Bordeaux, Bourgogne, and Beaujolais as well as a large variety of different cheeses, such as Camembert, Roquefort and Brie. There are more than 400 different varieties.
A meal often consists of three courses, hors d'œuvre or entrée (introductory course, sometimes soup), plat principal (main course), fromage (cheese course) and/or dessert, sometimes with a salad offered before the cheese or dessert. Hors d'œuvres include terrine de saumon au basilic, lobster bisque, foie gras, French onion soup or a croque monsieur. The plat principal could include a pot au feu or steak frites. The dessert could be mille-feuille pastry, a macaron, an éclair, crème brûlée, mousse au chocolat, crêpes, or Café liégeois.

French cuisine is also regarded as a key element of the quality of life and the attractiveness of France. A French publication, the Michelin guide, awards Michelin stars for excellence to a select few establishments. The acquisition or loss of a star can have dramatic effects on the success of a restaurant. By 2006, the Michelin Guide had awarded 620 stars to French restaurants, at that time more than any other country, although the guide also inspects more restaurants in France than in any other country (by 2010, Japan was awarded as many Michelin stars as France, despite having half the number of Michelin inspectors working there).
In addition to it wine tradition, France is also a major producer of beer. The three main French brewing regions are Alsace (60% of national production), the Nord-Pas-de-Calais and Lorraine. A meal often consists of three courses, hors d'œuvre or entrée (introductory course, sometimes soup), plat principal (main course), fromage (cheese course) or dessert, sometimes with a salad offered before the cheese or dessert.




Popular sports played in France include football, judo, tennis, rugby and pétanque. France has hosted events such as the 1938 and 1998 FIFA World Cups, and the 2007 Rugby World Cup. The country also hosted UEFA Euro 2016. The Stade de France in Saint-Denis is France's largest stadium and was the venue for the 1998 FIFA World Cup and 2007 Rugby World Cup finals. Since 1903, France hosts the annual Tour de France, the most famous road bicycle race in the world. France is famous for its 24 Hours of Le Mans sports car endurance race. Several major tennis tournaments take place in France, including the Paris Masters and the French Open, one of the four Grand Slam tournaments. French martial arts include Savate and Fencing.
France has a close association with the Modern Olympic Games; it was a French aristocrat, Baron Pierre de Coubertin, who suggested the Games' revival, at the end of the 19th century. After Athens was awarded the first Games, in reference to the Olympics' Greek origins, Paris hosted the second Games in 1900. Paris was the first home of the International Olympic Committee, before it moved to Lausanne. Since 1900, France has hosted the Olympics on 4 further occasions: the 1924 Summer Olympics, again in Paris and three Winter Games (1924 in Chamonix, 1968 in Grenoble and 1992 in Albertville).

Both the national football team and the national rugby union team are nicknamed "Les Bleus" in reference to the team's shirt colour as well as the national French tricolour flag. Football is the most popular sport in France, with over 1,800,000 registered players, and over 18,000 registered clubs. The football team is among the most successful in the world, particularly at the start of the 21st century, with one FIFA World Cup victory in 1998, one FIFA World Cup second place in 2006, and two UEFA European Championships in 1984 and 2000. The top national football club competition is Ligue 1. France has produced some of the greatest players in the world, including three time FIFA World Player of the Year Zinedine Zidane, three time Ballon d'Or recipient Michel Platini, record holder for most goals scored at a World Cup Just Fontaine, first football player to receive the Légion d'honneur Raymond Kopa, and the all-time leading goalscorer for the French national team Thierry Henry.
Rugby union is popular, particularly in Paris and the southwest of France. The national rugby union team has competed at every Rugby World Cup, and takes part in the annual Six Nations Championship. Stemming from a strong domestic league, the French rugby team has won 16 Six Nations Championships, including 8 grand slams; and has reached the semi-final of the Rugby World Cup 6 times and the final 3 times.
Rugby league in France is a sport that is most popular in the south, in cities such as Perpignan and Toulouse. The Catalans Dragons currently play in the Super League, which is the top tier rugby league competition in Europe. The Elite One Championship is the professional competition for rugby league clubs in France.
In recent decades, France has produced world-elite basketball players, most notably Tony Parker. The French National Basketball Team won gold at the FIBA EuroBasket 2013. The national team has won two Olympic Silver Medals: in 2000 and 1948.



Outline of France
France in the 1920s










"France". The World Factbook. Central Intelligence Agency. 
France from the BBC News
France at UCB Libraries GovPubs
France at DMOZ
France Encyclopædia Britannica entry
France at the EU
 Wikimedia Atlas of France
 Geographic data related to France at OpenStreetMap
Key Development Forecasts for France from International Futures
Economy

INSEE
OECD France statistics
Government
France.fr (in English) Official French tourism website
(French) Official Site of the Government
Official site of the French public service – Links to various administrations and institutions
Official site of the National Assembly
Culture
Contemporary French Civilization, journal, University of Illinois.
FranceGuide – Official website of the French Government Tourist OfficeBelgium (/ˈbɛldʒəm/; Dutch: België [ˈbɛlɣijə]; French: Belgique [bɛlʒik]; German: Belgien [ˈbɛlɡiən̩]), officially the Kingdom of Belgium, is a sovereign state in Western Europe bordered by France, the Netherlands, Germany, Luxembourg, and the North Sea. It is a small, densely populated country which covers an area of 30,528 square kilometres (11,787 sq mi) and has a population of about 11 million people. Straddling the cultural boundary between Germanic and Latin Europe, Belgium is home to two main linguistic groups: the Dutch-speaking, mostly Flemish community, which constitutes about 59% of the population, and the French-speaking, mostly Walloon population, which comprises 41% of all Belgians. Additionally, there is a small group of German-speakers who live in the East Cantons located around the High Fens area, and bordering Germany.
Historically, Belgium, the Netherlands and Luxembourg (along with parts of Northern France and Western Germany) were known as the Low Countries; it once covered a somewhat larger area than the current Benelux group of states. The region was called Belgica in Latin, after the Roman province of Gallia Belgica. From the end of the Middle Ages until the 17th century, the area of Belgium was a prosperous and cosmopolitan centre of commerce and culture. From the 16th century until the Belgian Revolution in 1830, when Belgium seceded from the Netherlands, the area of Belgium served as the battleground between many European powers, causing it to be dubbed the "Battlefield of Europe," a reputation strengthened by both world wars.
Today, Belgium is a federal constitutional monarchy with a parliamentary system of governance. It is divided into three regions and three communities, that exist next to each other. Its two largest regions are the Dutch-speaking region of Flanders in the north and the French-speaking southern region of Wallonia. The Brussels-Capital Region is an officially bilingual (French and Dutch) enclave within the Flemish Region. A German-speaking Community exists in eastern Wallonia. Belgium's linguistic diversity and related political conflicts are reflected in its political history and complex system of governance, made up of six different governments.
Upon its independence, declared in 1830, Belgium participated in the Industrial Revolution and, during the course of the 20th century, possessed a number of colonies in Africa. The second half of the 20th century was marked by rising tensions between the Dutch-speaking and the French-speaking citizens fueled by differences in language and culture and the unequal economic development of Flanders and Wallonia. This continuing antagonism has led to several far-reaching reforms, resulting in a transition from a unitary to a federal arrangement during the period from 1970 to 1993. Despite the reforms, tensions between the groups have remained, if not increased; there is significant separatism particularly among the Flemish; controversial language laws exist such as the municipalities with language facilities; and the formation of a coalition government took 18 months following the June 2010 federal election, a world record. Belgium is one of the six founding countries of the European Union and hosts the official seats of the European Commission, Council of the European Union, and European Council, as well as a seat of the European Parliament in the country's capital, Brussels. Belgium is also a founding member of the Eurozone, NATO, OECD and WTO, and a part of the trilateral Benelux Union. Its capital, Brussels, hosts several of the EU's official seats as well as the headquarters of many major international organizations such as NATO. Belgium is also a part of the Schengen Area. Belgium is a developed country, with an advanced high-income economy and is categorized as "very high" in the Human Development Index.




The name 'Belgium' is derived from Gallia Belgica, a Roman province in the northernmost part of Gaul that before Roman invasion in 100 BC, was inhabited by the Belgae, a mix of Celtic and Germanic peoples. A gradual immigration by Germanic Frankish tribes during the 5th century brought the area under the rule of the Merovingian kings. A gradual shift of power during the 8th century led the kingdom of the Franks to evolve into the Carolingian Empire.
The Treaty of Verdun in 843 divided the region into Middle and West Francia and therefore into a set of more or less independent fiefdoms which, during the Middle Ages, were vassals either of the King of France or of the Holy Roman Emperor.
Many of these fiefdoms were united in the Burgundian Netherlands of the 14th and 15th centuries. Emperor Charles V extended the personal union of the Seventeen Provinces in the 1540s, making it far more than a personal union by the Pragmatic Sanction of 1549 and increased his influence over the Prince-Bishopric of Liège.
The Eighty Years' War (1568–1648) divided the Low Countries into the northern United Provinces (Belgica Foederata in Latin, the "Federated Netherlands") and the Southern Netherlands (Belgica Regia, the "Royal Netherlands"). The latter were ruled successively by the Spanish (Spanish Netherlands) and the Austrian Habsburgs (Austrian Netherlands) and comprised most of modern Belgium. This was the theatre of most Franco-Spanish and Franco-Austrian wars during the 17th and 18th centuries.
Following the campaigns of 1794 in the French Revolutionary Wars, the Low Countries—including territories that were never nominally under Habsburg rule, such as the Prince-Bishopric of Liège—were annexed by the French First Republic, ending Austrian rule in the region. The reunification of the Low Countries as the United Kingdom of the Netherlands occurred at the dissolution of the First French Empire in 1815, after the defeat of Napoleon.

In 1830, the Belgian Revolution led to the separation of the Southern Provinces from the Netherlands and to the establishment of a Catholic and bourgeois, officially French-speaking and neutral, independent Belgium under a provisional government and a national congress. Since the installation of Leopold I as king on 21 July 1831, now celebrated as Belgium's National Day, Belgium has been a constitutional monarchy and parliamentary democracy, with a laicist constitution based on the Napoleonic code. Although the franchise was initially restricted, universal suffrage for men was introduced after the general strike of 1893 (with plural voting until 1919) and for women in 1949.
The main political parties of the 19th century were the Catholic Party and the Liberal Party, with the Belgian Labour Party emerging towards the end of the 19th century. French was originally the single official language adopted by the nobility and the bourgeoisie. It progressively lost its overall importance as Dutch became recognized as well. This recognition became official in 1898 and in 1967 the parliament accepted a Dutch version of the Constitution.
The Berlin Conference of 1885 ceded control of the Congo Free State to King Leopold II as his private possession. From around 1900 there was growing international concern for the extreme and savage treatment of the Congolese population (millions of whom are thought to have died) under Leopold II, for whom the Congo was primarily a source of revenue from ivory and rubber production. Many Congolese were killed by Leopold's agents for failing to meet production quotas for ivory and rubber. It is estimated that nearly 10 million were killed during the Leopold period. In 1908 this outcry led the Belgian state to assume responsibility for the government of the colony, henceforth called the Belgian Congo. A Belgian commission in 1919 estimated that Congo's population was half what it was in 1879.
Germany invaded Belgium in August 1914 as part of the Schlieffen Plan to attack France and much of the Western Front fighting of World War I occurred in western parts of the country. The opening months of the war were known as the Rape of Belgium due to German excesses. Belgium assumed control of the German colonies of Ruanda-Urundi (modern-day Rwanda and Burundi) during the war, and in 1924 the League of Nations mandated them to Belgium. In the aftermath of the First World War, the Prussian districts of Eupen and Malmedy were annexed by Belgium in 1925, thereby causing the presence of a German-speaking minority.
German forces again invaded the country in May 1940 and 40,690 Belgians, over half of them Jews, were killed during the subsequent occupation and The Holocaust. From September 1944 to February 1945 Belgium was liberated by the Allies. After World War II, a general strike forced King Leopold III, who many Belgians felt had collaborated with Germany during the war, to abdicate in 1951. The Belgian Congo gained independence in 1960 during the Congo Crisis; Ruanda-Urundi followed with its independence two years later. Belgium joined NATO as a founding member and formed the Benelux group of nations with the Netherlands and Luxembourg.
Belgium became one of the six founding members of the European Coal and Steel Community in 1951 and of the European Atomic Energy Community and European Economic Community, established in 1957. The latter has now become the European Union, for which Belgium hosts major administrations and institutions, including the European Commission, the Council of the European Union and the extraordinary and committee sessions of the European Parliament.




Belgium shares borders with France (620 km), Germany (167 km), Luxembourg (148 km) and the Netherlands (450 km). Its total area, including surface water area, is 30,528 square kilometres; land area alone is 30,278 km2. It lies between latitudes 49°30 and 51°30 N, and longitudes 2°33 and 6°24 E.
Belgium has three main geographical regions: the coastal plain in the north-west and the central plateau both belong to the Anglo-Belgian Basin; the Ardennes uplands in the south-east are part of the Hercynian orogenic belt. The Paris Basin reaches a small fourth area at Belgium's southernmost tip, Belgian Lorraine.

The coastal plain consists mainly of sand dunes and polders. Further inland lies a smooth, slowly rising landscape irrigated by numerous waterways, with fertile valleys and the northeastern sandy plain of the Campine (Kempen). The thickly forested hills and plateaux of the Ardennes are more rugged and rocky with caves and small gorges. Extending westward into France, this area is eastwardly connected to the Eifel in Germany by the High Fens plateau, on which the Signal de Botrange forms the country's highest point at 694 metres (2,277 ft).
The climate is maritime temperate with significant precipitation in all seasons (Köppen climate classification: Cfb), like most of northwest Europe. The average temperature is lowest in January at 3 °C (37.4 °F) and highest in July at 18 °C (64.4 °F). The average precipitation per month varies between 54 millimetres (2.1 in) for February or April, to 78 mm (3.1 in) for July. Averages for the years 2000 to 2006 show daily temperature minimums of 7 °C (44.6 °F) and maximums of 14 °C (57.2 °F) and monthly rainfall of 74 mm (2.9 in); these are about 1 °C and nearly 10 millimetres above last century's normal values, respectively.
Phytogeographically, Belgium is shared between the Atlantic European and Central European provinces of the Circumboreal Region within the Boreal Kingdom. According to the World Wide Fund for Nature, the territory of Belgium belongs to the ecoregion of Atlantic mixed forests. Because of its high population density, industrialization and its location in the centre of Western Europe, Belgium still faces some environmental problems. However, due to consistent efforts by the various levels of government in Belgium, the state of the environment in Belgium is gradually improving. This led to Belgium being ranked as one of the top 10 countries (9 out of 132) in terms of environmental protection trends, and to Belgium being ranked in 2012 as the 24th country out of 132 for environmental protection. Belgium moreover has one of Europe's highest waste recycling rates. In particular, the Flemish region of Belgium has the highest waste diversion rate in Europe. Almost 75% of the residential waste produced there is reused, recycled, or composted.




The territory of Belgium is divided into three Regions, two of which, the Flemish Region and Walloon Region, are in turn subdivided into provinces; the third Region, the Brussels Capital Region, is neither a province nor a part of a province.




Belgium is a constitutional, popular monarchy and a federal parliamentary democracy. The bicameral federal parliament is composed of a Senate and a Chamber of Representatives. The former is made up of 50 senators appointed by the parliaments of the communities and regions and 10 co-opted senators. Prior to 2014, most of the Senate's members were directly elected. The Chamber's 150 representatives are elected under a proportional voting system from 11 electoral districts. Belgium has compulsory voting and thus maintains one of the highest rates of voter turnout in the world.
The King (currently Philippe) is the head of state, though with limited prerogatives. He appoints ministers, including a Prime Minister, that have the confidence of the Chamber of Representatives to form the federal government. The Council of Ministers is composed of no more than fifteen members. With the possible exception of the Prime Minister, the Council of Ministers is composed of an equal number of Dutch-speaking members and French-speaking members. The judicial system is based on civil law and originates from the Napoleonic code. The Court of Cassation is the court of last resort, with the Court of Appeal one level below.



Belgium's political institutions are complex; most political power is organized around the need to represent the main cultural communities. Since about 1970, the significant national Belgian political parties have split into distinct components that mainly represent the political and linguistic interests of these communities. The major parties in each community, though close to the political centre, belong to three main groups: Christian Democrats, Liberals, and Social Democrats. Further notable parties came into being well after the middle of last century, mainly around linguistic, nationalist, or environmental themes and recently smaller ones of some specific liberal nature.

A string of Christian Democrat coalition governments from 1958 was broken in 1999 after the first dioxin crisis, a major food contamination scandal. A "rainbow coalition" emerged from six parties: the Flemish and the French-speaking Liberals, Social Democrats and Greens. Later, a "purple coalition" of Liberals and Social Democrats formed after the Greens lost most of their seats in the 2003 election.
The government led by Prime Minister Guy Verhofstadt from 1999 to 2007 achieved a balanced budget, some tax reforms, a labour-market reform, scheduled nuclear phase-out and instigated legislation allowing more stringent war crime and more lenient soft drug usage prosecution. Restrictions on withholding euthanasia were reduced and same-sex marriage legalized. The government promoted active diplomacy in Africa and opposed the invasion of Iraq. It is the only country that does not have age restrictions on euthanasia.
Verhofstadt's coalition fared badly in the June 2007 elections. For more than a year, the country experienced a political crisis. This crisis was such that many observers speculated on a possible partition of Belgium. From 21 December 2007 until 20 March 2008 the temporary Verhofstadt III Government was in office. This coalition of the Flemish and Francophone Christian Democrats, the Flemish and Francophone Liberals together with the Francophone Social Democrats was an interim government until 20 March 2008.
On that day a new government, led by Flemish Christian Democrat Yves Leterme, the actual winner of the federal elections of June 2007, was sworn in by the king. On 15 July 2008 Leterme announced the resignation of the cabinet to the king, as no progress in constitutional reforms had been made. In December 2008 he once more offered his resignation to the king after a crisis surrounding the sale of Fortis to BNP Paribas. At this juncture, his resignation was accepted and Christian Democratic and Flemish Herman Van Rompuy was sworn in as Prime Minister on 30 December 2008.
After Herman Van Rompuy was designated the first permanent President of the European Council on 19 November 2009, he offered the resignation of his government to King Albert II on 25 November 2009. A few hours later, the new government under Prime Minister Yves Leterme was sworn in. On 22 April 2010, Leterme again offered the resignation of his cabinet to the king after one of the coalition partners, the OpenVLD, withdrew from the government, and on 26 April 2010 King Albert officially accepted the resignation.
The Parliamentary elections in Belgium on 13 June 2010 saw the Flemish nationalist N-VA become the largest party in Flanders, and the Socialist Party PS the largest party in Wallonia. Until December 2011, Belgium was governed by Leterme's caretaker government awaiting the end of the deadlocked negotiations for formation of a new government. By 30 March 2011 this set a new world record for the elapsed time without an official government, previously held by war-torn Iraq. Finally, in December 2011 the Di Rupo Government led by Walloon socialist Prime Minister Elio Di Rupo was sworn in.
The 2014 federal election (coinciding with the regional elections) resulted in a further electoral gain for the Flemish nationalist N-VA, although the incumbent coalition (composed of Flemish and French-speaking Social Democrats, Liberals, and Christian Democrats) maintains a solid majority in Parliament and in all electoral constituencies. On 22 July 2014, King Philippe nominated Charles Michel (MR) and Kris Peeters (CD&V) to lead the formation of a new federal cabinet composed of the Flemish parties N-VA, CD&V, Open Vld and the French-speaking MR, which resulted in the Michel Government. It is the first time N-VA is part of the federal cabinet, while the French-speaking side is represented only by the MR, which achieved a minority of the public votes in Wallonia.




Following a usage which can be traced back to the Burgundian and Habsburgian courts, in the 19th century it was necessary to speak French to belong to the governing upper class, and those who could only speak Dutch were effectively second-class citizens. Late that century, and continuing into the 20th century, Flemish movements evolved to counter this situation.
While the people in Southern Belgium spoke French or dialects of French, and most Brusselers adopted French as their first language, the Flemings refused to do so and succeeded progressively in making Dutch an equal language in the education system. Following World War II, Belgian politics became increasingly dominated by the autonomy of its two main linguistic communities. Intercommunal tensions rose and the constitution was amended to minimise the potential for conflict.
Based on the four language areas defined in 1962–63 (the Dutch, bilingual, French and German language areas), consecutive revisions of the country's constitution in 1970, 1980, 1988 and 1993 established a unique form of a federal state with segregated political power into three levels:
The federal government, based in Brussels.
The three language communities:
the Flemish Community (Dutch-speaking);
the French Community (French-speaking);
the German-speaking Community.

The three regions:
the Flemish Region, subdivided into five provinces;
the Walloon Region, subdivided into five provinces;
the Brussels-Capital Region.

The constitutional language areas determine the official languages in their municipalities, as well as the geographical limits of the empowered institutions for specific matters. Although this would allow for seven parliaments and governments, when the Communities and Regions were created in 1980, Flemish politicians decided to merge both. Thus the Flemings just have one single institutional body of parliament and government is empowered for all except federal and specific municipal matters.
The overlapping boundaries of the Regions and Communities have created two notable peculiarities: the territory of the Brussels-Capital Region (which came into existence nearly a decade after the other regions) is included in both the Flemish and French Communities, and the territory of the German-speaking Community lies wholly within the Walloon Region. Conflicts about jurisdiction between the bodies are resolved by the Constitutional Court of Belgium. The structure is intended as a compromise to allow different cultures to live together peacefully.



The Federal State's authority includes justice, defence, federal police, social security, nuclear energy, monetary policy and public debt, and other aspects of public finances. State-owned companies include the Belgian Post Group and Belgian Railways. The Federal Government is responsible for the obligations of Belgium and its federalized institutions towards the European Union and NATO. It controls substantial parts of public health, home affairs and foreign affairs. The budget—without the debt—controlled by the federal government amounts to about 50% of the national fiscal income. The federal government employs around 12% of the civil servants.
Communities exercise their authority only within linguistically determined geographical boundaries, originally oriented towards the individuals of a Community's language: culture (including audiovisual media), education and the use of the relevant language. Extensions to personal matters less directly connected with language comprise health policy (curative and preventive medicine) and assistance to individuals (protection of youth, social welfare, aid to families, immigrant assistance services, and so on.).
Regions have authority in fields that can be broadly associated with their territory. These include economy, employment, agriculture, water policy, housing, public works, energy, transport, the environment, town and country planning, nature conservation, credit and foreign trade. They supervise the provinces, municipalities and intercommunal utility companies.
In several fields, the different levels each have their own say on specifics. With education, for instance, the autonomy of the Communities neither includes decisions about the compulsory aspect nor allows for setting minimum requirements for awarding qualifications, which remain federal matters. Each level of government can be involved in scientific research and international relations associated with its powers. The treaty-making power of the Regions' and Communities' Governments is the broadest of all the Federating units of all the Federations all over the world.




Because of its location at the crossroads of Western Europe, Belgium has historically been the route of invading armies from its larger neighbours. With virtually defenceless borders, Belgium has traditionally sought to avoid domination by the more powerful nations which surround it through a policy of mediation. The Belgians have been strong advocates of European integration. Both the European Union and NATO are headquartered in Belgium.




The Belgian Armed Forces have about 47,000 active troops. In 2010, Belgium's defence budget totaled €3.95 billion (representing 1.12% of its GDP). They are organized into one unified structure which consists of four main components: Land Component, or the Army; Air Component, or the Air Force; Naval Component, or the Navy; Medical Component. The operational commands of the four components are subordinate to the Staff Department for Operations and Training of the Ministry of Defence, which is headed by the Assistant Chief of Staff Operations and Training, and to the Chief of Defence.
The effects of the Second World War made collective security a priority for Belgian foreign policy. In March 1948 Belgium signed the Treaty of Brussels, and then joined NATO in 1948. However the integration of the armed forces into NATO did not begin until after the Korean War. The Belgians, along with the Luxembourg government, sent a detachment of battalion strength to fight in Korea known as the Belgian United Nations Command. This mission was the first in a long line of UN missions which the Belgians supported. Currently, the Belgian Naval Component is working closely together with the Dutch Navy under the command of the Admiral Benelux.




Belgium's strongly globalized economy and its transport infrastructure are integrated with the rest of Europe. Its location at the heart of a highly industrialized region helped make it the world's 15th largest trading nation in 2007. The economy is characterized by a highly productive work force, high GNP and high exports per capita. Belgium's main imports are raw materials, machinery and equipment, chemicals, raw diamonds, pharmaceuticals, foodstuffs, transportation equipment, and oil products. Its main exports are machinery and equipment, chemicals, finished diamonds, metals and metal products, and foodstuffs.
The Belgian economy is heavily service-oriented and shows a dual nature: a dynamic Flemish economy and a Walloon economy that lags behind. One of the founding members of the European Union, Belgium strongly supports an open economy and the extension of the powers of EU institutions to integrate member economies. Since 1922, through the Belgium-Luxembourg Economic Union, Belgium and Luxembourg have been a single trade market with customs and currency union.

Belgium was the first continental European country to undergo the Industrial Revolution, in the early 19th century. Liège and Charleroi rapidly developed mining and steelmaking, which flourished until the mid-20th century in the Sambre and Meuse valley and made Belgium among one of the three most industrialized nations in the world from 1830 to 1910. However, by the 1840s the textile industry of Flanders was in severe crisis, and the region experienced famine from 1846 to 1850.
After World War II, Ghent and Antwerp experienced a rapid expansion of the chemical and petroleum industries. The 1973 and 1979 oil crises sent the economy into a recession; it was particularly prolonged in Wallonia, where the steel industry had become less competitive and experienced serious decline. In the 1980s and 1990s, the economic centre of the country continued to shift northwards and is now concentrated in the populous Flemish Diamond area.
By the end of the 1980s, Belgian macroeconomic policies had resulted in a cumulative government debt of about 120% of GDP. As of 2006, the budget was balanced and public debt was equal to 90.30% of GDP. In 2005 and 2006, real GDP growth rates of 1.5% and 3.0%, respectively, were slightly above the average for the Euro area. Unemployment rates of 8.4% in 2005 and 8.2% in 2006 were close to the area average. By October 2010, this had grown to 8.5% compared to an average rate of 9.6% for the European Union as a whole (EU 27). From 1832 until 2002, Belgium's currency was the Belgian franc. Belgium switched to the euro in 2002, with the first sets of euro coins being minted in 1999. The standard Belgian euro coins designated for circulation show the portrait of the monarch (first King Albert II, since 2013 King Philippe).
Despite an 18% decrease observed from 1970 to 1999, Belgium still had in 1999 the highest rail network density within the European Union with 113.8 km/1 000 km2. On the other hand, the same period of time, 1970–1999, has seen a huge growth (+56%) of the motorway network. In 1999, the density of km motorways per 1000 km2 and 1000 inhabitants amounted to 55.1 and 16.5 respectively and were significantly superior to the EU's means of 13.7 and 15.9.
Belgium experiences some of the most congested traffic in Europe. In 2010, commuters to the cities of Brussels and Antwerp spent respectively 65 and 64 hours a year in traffic jams. Like in most small European countries, more than 80% of the airways traffic is handled by a single airport, the Brussels Airport. The ports of Antwerp and Zeebrugge share more than 80% of Belgian maritime traffic, Antwerp being the second European harbour with a gross weight of goods handled of 115 988 000 t in 2000 after a growth of 10.9% over the preceding five years.
There is a large economic gap between Flanders and Wallonia. Wallonia was historically wealthy compared to Flanders, mostly due to its heavy industries, but the decline of the steel industry post-World War II led to the region's rapid decline, whereas Flanders rose swiftly. Since then, Flanders has been prosperous, among the wealthiest regions in Europe, whereas Wallonia has been languishing. As of 2007, the unemployment rate of Wallonia is over double that of Flanders. The divide has played a key part in the tensions between the Flemish and Walloons in addition to the already-existing language divide. Pro-independence movements have gained high popularity in Flanders as a consequence. The separatist New Flemish Alliance (N-VA) party for instance is the largest party in Flanders.




Contributions to the development of science and technology have appeared throughout the country's history. The 16th century Early Modern flourishing of Western Europe included cartographer Gerardus Mercator, anatomist Andreas Vesalius, herbalist Rembert Dodoens and mathematician Simon Stevin among the most influential scientists.
Chemist Ernest Solvay and engineer Zenobe Gramme (École Industrielle de Liège) gave their names to the Solvay process and the Gramme dynamo, respectively, in the 1860s. Bakelite was developed in 1907–1909 by Leo Baekeland. Ernest Solvay also acted as a major philanthropist and gave its name to the Solvay Institute of Sociology, the Solvay Brussels School of Economics and Management and the International Solvay Institutes for Physics and Chemistry which are now part of the Université libre de Bruxelles. In 1911, he started a series of conferences, the Solvay Conferences on Physics and Chemistry, which have had a deep impact on the evolution of quantum physics and chemistry. A major contribution to fundamental science was also due to a Belgian, Monsignor Georges Lemaître (Catholic University of Leuven), who is credited with proposing the Big Bang theory of the origin of the universe in 1927.
Three Nobel Prizes in Physiology or Medicine were awarded to Belgians: Jules Bordet (Université libre de Bruxelles) in 1919, Corneille Heymans (University of Ghent) in 1938 and Albert Claude (Université Libre de Bruxelles) together with Christian de Duve (Université Catholique de Louvain) in 1974. François Englert (Université Libre de Bruxelles) was awarded the Nobel Prize in Physics in 2013. Ilya Prigogine (Université Libre de Bruxelles) was awarded the Nobel Prize in Chemistry in 1977. Two Belgian mathematicians have been awarded the Fields Medal: Pierre Deligne in 1978 and Jean Bourgain in 1994.




As of 1 January 2015, the total population of Belgium according to its population register was 11,190,845. Almost all of the population is urban, at 97% in 2004. The population density of Belgium is 365 per square kilometre (952 per square mile) as of March 2013. The most densely inhabited area is Flanders. The Ardennes have the lowest density. As of 1 January 2015, the Flemish Region had a population of 6,437,680, its most populous cities being Antwerp (511,771), Ghent (252,274) and Bruges (117,787). Wallonia had 3,585,214 with Charleroi (202,021), Liège (194,937) and Namur (110,447), its most populous cities. Brussels has 1,167,951 inhabitants in the Capital Region's 19 municipalities, three of which have over 100,000 residents.
As of 2007, nearly 92% of the population had Belgian citizenship, and other European Union member citizens account for around 6%. The prevalent foreign nationals were Italian (171,918), French (125,061), Dutch (116,970), Moroccan (80,579), Portuguese (43,509), Spanish (42,765), Turkish (39,419) and German (37,621). In 2007, there were 1.38 million foreign-born residents in Belgium, corresponding to 12.9% of the total population. Of these, 685,000 (6.4%) were born outside the EU and 695,000 (6.5%) were born in another EU Member State.
At the beginning of 2012, people of foreign background and their descendants were estimated to have formed around 25% of the total population i.e. 2.8 million new Belgians. Of these new Belgians, 1,200,000 are of European ancestry and 1,350,000 are from non-Western countries (most of them from Morocco, Turkey, and the DR Congo). Since the modification of the Belgian nationality law in 1984 more than 1.3 million migrants have acquired Belgian citizenship. The largest group of immigrants and their descendants in Belgium are Moroccans, with more than 450,000 people. The Turks are the third largest group, and the second largest Muslim ethnic group, numbering 220,000. 89.2% of inhabitants of Turkish origin have been naturalized, as have 88.4% of people of Moroccan background, 75.4% of Italians, 56.2% of the French and 47.8% of Dutch people.







Belgium has three official languages: Dutch, French and German. A number of non-official minority languages are spoken as well. As no census exists, there are no official statistical data regarding the distribution or usage of Belgium's three official languages or their dialects. However, various criteria, including the language(s) of parents, of education, or the second-language status of foreign born, may provide suggested figures. An estimated 60% of the Belgian population speaks Dutch (often referred to as Flemish), and 40% of the population speaks French. French-speaking Belgians are often referred to as Walloons, although the French speakers in Brussels are not Walloons.
Total Dutch speakers are 6.23 million, concentrated in the northern Flanders region, while French speakers number 3.32 million in Wallonia and an estimated 870,000 (or 85%) in the officially bilingual Brussels-Capital Region. The German-speaking Community is made up of 73,000 people in the east of the Walloon Region; around 10,000 German and 60,000 Belgian nationals are speakers of German. Roughly 23,000 more German speakers live in municipalities near the official Community.
Both Belgian Dutch and Belgian French have minor differences in vocabulary and semantic nuances from the varieties spoken respectively in the Netherlands and France. Many Flemish people still speak dialects of Dutch in their local environment. Walloon, considered either as a dialect of French or a distinct Romance language, is now only understood and spoken occasionally, mostly by elderly people. Walloon is the name collectively given to four French dialects spoken in Belgium. Wallonia's dialects, along with those of Picard, are not used in public life and have been replaced by French.
A very small group with a fourth language can be found in Vresse-sur-Semois, one municipality where they speak Champenois, the local dialect in the Champagne area. The language is recognised by France and the Walloon part of Belgium.




Since the country's independence, Roman Catholicism, counterbalanced by strong freethought movements, has had an important role in Belgium's politics. However Belgium is largely a secular country as the laicist constitution provides for freedom of religion, and the government generally respects this right in practice. During the reigns of Albert I and Baudouin, the monarchy had a reputation of deeply rooted Catholicism.
Roman Catholicism has traditionally been Belgium's majority religion; being especially strong in Flanders. However, by 2009 Sunday church attendance was 5% for Belgium in total; 3% in Brussels, and 5.4% in Flanders. Church attendance in 2009 in Belgium was roughly half of the Sunday church attendance in 1998 (11% for the total of Belgium in 1998). Despite the drop in church attendance, Catholic identity nevertheless remains an important part of Belgium's culture.
According to the most recent Eurobarometer Poll 2010, 37% of Belgian citizens responded that they believe there is a God. 31% answered that they believe there is some sort of spirit or life-force. 27% answered that they do not believe there is any sort of spirit, God, or life-force. 5% did not respond.
Symbolically and materially, the Roman Catholic Church remains in a favourable position. Belgium has three officially recognized religions: Christianity (Catholic, Protestantism, Orthodoxy and Anglicanism), Islam and Judaism.
In the early 2000s there were approximately 42,000 Jews in Belgium. The Jewish Community of Antwerp (numbering some 18,000) is one of the largest in Europe, and one of the last places in the world where Yiddish is the primary language of a large Jewish community (mirroring certain Orthodox and Hasidic communities in New York and Israel). In addition most Jewish children in Antwerp receive a Jewish education. There are several Jewish newspapers and more than 45 active synagogues (30 of which are in Antwerp) in the country.
A 2006 inquiry in Flanders, considered to be a more religious region than Wallonia, showed that 55% considered themselves religious and that 36% believed that God created the universe. On the other hand, Wallonia has become one of Europe's most secular/least religious regions. Most of the French-speaking region's population does not consider religion an important part of their lives, and as much as 45% of the population identifies as irreligious. This is particularly the case in eastern Wallonia and areas along the French border.
A 2008 estimate found that approximately 6% of the Belgian population (628,751 people) is Muslim. Muslims constitute 23.6% of the population of Brussels, 4.9% of Wallonia and 5.1% of Flanders. The majority of Belgian Muslims live in the major cities, such as Antwerp, Brussels and Charleroi. The largest group of immigrants in Belgium are Moroccans, with 400,000 people. The Turks are the third largest group, and the second largest Muslim ethnic group, numbering 220,000.
According to new polls about religiosity in the European Union in 2012 by Eurobarometer found that Christianity is the largest religion in Belgium accounting 65% of Belgians. Catholics are the largest Christian group in Belgium, accounting for 58% of Belgium citizens, while Protestants make up 2%, and Other Christian make up 5%. Non believer/agnostics account for 20%, atheists 7%, and Muslims 5%.




The Belgians enjoy good health. According to 2012 estimates, the average life expectancy is 79.65 years. Since 1960, life expectancy has, in line with the European average, grown by two months per year. Death in Belgium is mainly due to heart and vascular disorders, neoplasms, disorders of the respiratory system and unnatural causes of death (accidents, suicide). Non-natural causes of death and cancer are the most common causes of death for females up to age 24 and males up to age 44.
Healthcare in Belgium is financed through both social security contributions and taxation. Health insurance is compulsory. Health care is delivered by a mixed public and private system of independent medical practitioners and public, university and semi-private hospitals. Health care service are payable by the patient and reimbursed later by health insurance institutions, but for ineligible categories (of patients and services) so-called 3rd party payment systems exist. The Belgian health care system is supervised and financed by the federal government, the Flemish and Walloon Regional governments; and the German Community also has (indirect) oversight and responsibilities.
For the first time in Belgium history, the first child was euthanized following the 2 year mark of the removal of the euthanization age restrictions. The child had been euthanized due to an incurable disease that was inflicted upon the child. Although there may have been some support for the euthanization there is a possibility of controversy due to the issue revolving around the subject of assisted suicide.




Education is compulsory from 6 to 18 years of age for Belgians. Among OECD countries in 2002, Belgium had the third highest proportion of 18- to 21-year-olds enrolled in postsecondary education, at 42%. Though an estimated 99% of the adult population is literate, concern is rising over functional illiteracy. The Programme for International Student Assessment (PISA), coordinated by the OECD, currently ranks Belgium's education as the 19th best in the world, being significantly higher than the OECD average. Education being organized separately by each, the Flemish Community scores noticeably above the French and German-speaking Communities.
Mirroring the dual structure of the 19th-century Belgian political landscape, characterized by the Liberal and the Catholic parties, the educational system is segregated within a secular and a religious segment. The secular branch of schooling is controlled by the communities, the provinces, or the municipalities, while religious, mainly Catholic branch education, is organized by religious authorities, although subsidized and supervised by the communities.




Despite its political and linguistic divisions, the region corresponding to today's Belgium has seen the flourishing of major artistic movements that have had tremendous influence on European art and culture. Nowadays, to a certain extent, cultural life is concentrated within each language Community, and a variety of barriers have made a shared cultural sphere less pronounced. Since the 1970s, there are no bilingual universities or colleges in the country except the Royal Military Academy and the Antwerp Maritime Academy, no common media and no single large cultural or scientific organization in which both main communities are represented.




Contributions to painting and architecture have been especially rich. The Mosan art, the Early Netherlandish, the Flemish Renaissance and Baroque painting and major examples of Romanesque, Gothic, Renaissance and Baroque architecture are milestones in the history of art. While the 15th century's art in the Low Countries is dominated by the religious paintings of Jan van Eyck and Rogier van der Weyden, the 16th century is characterized by a broader panel of styles such as Peter Breughel's landscape paintings and Lambert Lombard's representation of the antique. Though the Baroque style of Peter Paul Rubens and Anthony van Dyck flourished in the early 17th century in the Southern Netherlands, it gradually declined thereafter.
During the 19th and 20th centuries many original romantic, expressionist and surrealist Belgian painters emerged, including James Ensor and other artists belonging to the Les XX group, Constant Permeke, Paul Delvaux and René Magritte. The avant-garde CoBrA movement appeared in the 1950s, while the sculptor Panamarenko remains a remarkable figure in contemporary art. Multidisciplinary artists Jan Fabre, Wim Delvoye and the painters Guy Huygens and Luc Tuymans are other internationally renowned figures on the contemporary art scene.
Belgian contributions to architecture also continued into the 19th and 20th centuries, including the work of Victor Horta and Henry van de Velde, who were major initiators of the Art Nouveau style.

The vocal music of the Franco-Flemish School developed in the southern part of the Low Countries and was an important contribution to Renaissance culture. In the 19th and 20th centuries, there was an emergence of major violinists, such as Henri Vieuxtemps, Eugène Ysaÿe and Arthur Grumiaux, while Adolphe Sax invented the saxophone in 1846. The composer César Franck was born in Liège in 1822. Contemporary popular music in Belgium is also of repute. Jazz musician Toots Thielemans and singer Jacques Brel have achieved global fame. Nowadays, singer Stromae has been a musical revelation in Europe and beyond, having great success. In rock/pop music, Telex, Front 242, K's Choice, Hooverphonic, Zap Mama, Soulwax and dEUS are well known. In the heavy metal scene, bands like Machiavel, Channel Zero and Enthroned have a worldwide fan-base.
Belgium has produced several well-known authors, including the poets Emile Verhaeren, Robert Goffin and novelists Hendrik Conscience, Georges Simenon, Suzanne Lilar, Hugo Claus, Joseph Weterings and Amélie Nothomb. The poet and playwright Maurice Maeterlinck won the Nobel Prize in literature in 1911. The Adventures of Tintin by Hergé is the best known of Franco-Belgian comics, but many other major authors, including Peyo (The Smurfs), André Franquin (Gaston Lagaffe), Dupa (Cubitus), Morris (Lucky Luke), Greg (Achille Talon), Lambil (Les Tuniques Bleues), Edgar P. Jacobs and Willy Vandersteen brought the Belgian cartoon strip industry a worldwide fame.
Belgian cinema has brought a number of mainly Flemish novels to life on-screen. Other Belgian directors include André Delvaux, Stijn Coninx, Luc and Jean-Pierre Dardenne; well-known actors include Jean-Claude Van Damme, Jan Decleir and Marie Gillain; and successful films include Bullhead, Man Bites Dog and The Alzheimer Affair. In the 1980s, Antwerp's Royal Academy of Fine Arts produced important fashion trendsetters, known as the Antwerp Six.




Folklore plays a major role in Belgium's cultural life: the country has a comparatively high number of processions, cavalcades, parades, 'ommegangs' and 'ducasses', 'kermesse' and other local festivals, nearly always with an originally religious or mythological background. The Carnival of Binche with its famous Gilles and the 'Processional Giants and Dragons' of Ath, Brussels, Dendermonde, Mechelen and Mons are recognized by UNESCO as Masterpieces of the Oral and Intangible Heritage of Humanity.
Other examples are the Carnival of Aalst; the still very religious processions of the Holy Blood in Bruges, Virga Jesse Basilica in Hasselt and Basilica of Our Lady of Hanswijk in Mechelen; 15 August festival in Liège; and the Walloon festival in Namur. Originated in 1832 and revived in the 1960s, the Gentse Feesten have become a modern tradition. A major non-official holiday is the Saint Nicholas Day, a festivity for children and, in Liège, for students.




Many highly ranked Belgian restaurants can be found in the most influential restaurant guides, such as the Michelin Guide. Belgium is famous for beer, chocolate, waffles and french fries with mayonnaise. Contrary to their name, french fries are claimed to have originated in Belgium, although their exact place of origin is uncertain. The national dishes are "steak and fries with salad", and "mussels with fries".
Brands of Belgian chocolate and pralines, like Côte d'Or, Neuhaus, Leonidas and Godiva are famous, as well as independent producers such as Burie and Del Rey in Antwerp and Mary's in Brussels. Belgium produces over 1100 varieties of beer. The Trappist beer of the Abbey of Westvleteren has repeatedly been rated the world's best beer. The biggest brewer in the world by volume is Anheuser-Busch InBev, based in Leuven.




Since the 1970s, sports clubs and federations are organized separately within each language community. Association football is the most popular sport in both parts of Belgium; also very popular are cycling, tennis, swimming, judo and basketball.

Belgians hold the most Tour de France victories of any country except France. They have also the most victories on the UCI Road World Championships. Philippe Gilbert is the 2012 world champion. Another modern well-known Belgian cyclist is Tom Boonen. With five victories in the Tour de France and numerous other cycling records, Belgian cyclist Eddy Merckx is regarded as one of the greatest cyclists of all time. Jean-Marie Pfaff, a former Belgian goalkeeper, is considered one of the greatest in the history of association football.
Belgium hosted the 1972 European Football Championships, and co-hosted the 2000 European Championships with the Netherlands. The Belgium national football team reached first place in the FIFA World Rankings for the first time in November 2015.
Kim Clijsters and Justine Henin both were Player of the Year in the Women's Tennis Association as they were ranked the number one female tennis player. The Spa-Francorchamps motor-racing circuit hosts the Formula One World Championship Belgian Grand Prix. The Belgian driver, Jacky Ickx, won eight Grands Prix and six 24 Hours of Le Mans and finished twice as runner-up in the Formula One World Championship. Belgium also has a strong reputation in, motocross with the rider Stefan Everts. Sporting events annually held in Belgium include the Memorial Van Damme athletics competition, the Belgian Grand Prix Formula One, and a number of classic cycle races such as the Tour of Flanders and Liège–Bastogne–Liège. The 1920 Summer Olympics were held in Antwerp. The 1977 European Basketball Championship was held in Liège and Ostend.




Index of Belgium-related articles
Outline of Belgium







Online sources

Bibliography




Government
Official site of Belgian monarchy
Official site of the Belgian federal government
General
"Belgium". The World Factbook. Central Intelligence Agency. 
Belgium at UCB Libraries GovPubs
Belgium information from the United States Department of State
Belgium at DMOZ
Portals to the World from the United States Library of Congress
Belgium profile from the BBC News
FAO Country Profiles: Belgium
Statistical Profile of Belgium at the Association of Religion Data Archives
 Wikimedia Atlas of Belgium
Key Development Forecasts for Belgium from International Futures
Official Site of the Belgian Tourist Office in the Americas and GlobeScopeDenmark (/ˈdɛnmɑːrk/; Danish: Danmark [ˈd̥ænmɑɡ̊]) is a Scandinavian country in Europe. The southernmost and smallest of the Nordic countries, it is south-west of Sweden and south of Norway, and bordered to the south by Germany. The Kingdom of Denmark is a sovereign state that comprises Denmark proper and two autonomous constituent countries in the North Atlantic Ocean: the Faroe Islands and Greenland. Denmark has a total area of 42,924 square kilometres (16,573 sq mi), and a population of 5.7 million. The country consists of a peninsula, Jutland, and an archipelago of 443 named islands, with the largest being Zealand and Funen. The islands are characterised by flat, arable land and sandy coasts, low elevation and a temperate climate.
The unified kingdom of Denmark emerged in the 10th century as a proficient seafaring nation in the struggle for control of the Baltic Sea. Denmark, Sweden and Norway were ruled together under the Kalmar Union, established in 1397 and ending with Swedish secession in 1523. Denmark and Norway remained under the same monarch until outside forces dissolved the union in 1814. The union with Norway made it possible for Denmark to inherit the Faroe Islands, Iceland, and Greenland. Beginning in the 17th century, there were several cessions of territory to Sweden. In the 19th century there was a surge of nationalist movements, which were defeated in the 1864 Second Schleswig War. Denmark remained neutral during World War I. In April 1940, a German invasion saw brief military skirmishes while the Danish resistance movement was active from 1943 until the German surrender in May 1945. An industrialised exporter of agricultural produce in the second half of the 19th century, Denmark introduced social and labour-market reforms in the early 20th century that created the basis for the present welfare state model with a highly developed mixed economy.
The Constitution of Denmark was signed on 5 June 1849, ending the absolute monarchy which had begun in 1660. It establishes a constitutional monarchy organised as a parliamentary democracy. The government and national parliament are seated in Copenhagen, the nation's capital, largest city and main commercial centre. Denmark exercises hegemonic influence in the Danish Realm, devolving powers to handle internal affairs. Home rule was established in the Faroe Islands in 1948; in Greenland home rule was established in 1979 and further autonomy in 2009. Denmark became a member of the European Economic Community (now the EU) in 1973, maintaining certain opt-outs; it retains its own currency, the krone. It is among the founding members of NATO, the Nordic Council, the OECD, OSCE, and the United Nations; it is also part of the Schengen Area.
Danes enjoy a high standard of living and the country ranks highly in some metrics of national performance, including education, health care, protection of civil liberties, democratic governance, prosperity and human development. The country ranks as having the world's highest social mobility, a high level of income equality, is the country with the lowest perceived level of corruption in the world, has one of the world's highest per capita incomes, and one of the world's highest personal income tax rates.




The etymology of the word Denmark, and especially the relationship between Danes and Denmark and the unifying of Denmark as a single kingdom, is a subject which attracts debate. This is centred primarily on the prefix "Dan" and whether it refers to the Dani or a historical person Dan and the exact meaning of the -"mark" ending.
Most handbooks derive the first part of the word, and the name of the people, from a word meaning "flat land", related to German Tenne "threshing floor", English den "cave". The -mark is believed to mean woodland or borderland (see marches), with probable references to the border forests in south Schleswig.
The first recorded use of the word Danmark within Denmark itself is found on the two Jelling stones, which are runestones believed to have been erected by Gorm the Old (c. 955) and Harald Bluetooth (c. 965). The larger stone of the two is popularly cited as Denmark's baptismal certificate (dåbsattest), though both use the word "Denmark", in the form of accusative ᛏᛅᚾᛘᛅᚢᚱᚴ "tanmaurk" ([danmɒrk]) on the large stone, and genitive ᛏᛅᚾᛘᛅᚱᚴᛅᚱ "tanmarkar" (pronounced [danmarkaɽ]) on the small stone. The inhabitants of Denmark are there called "tani" ([danɪ]), or "Danes", in the accusative.







The earliest archaeological findings in Denmark date back to the Eem interglacial period from 130,000–110,000 BC. Denmark has been inhabited since around 12,500 BC and agriculture has been evident since 3900 BC. The Nordic Bronze Age (1800–600 BC) in Denmark was marked by burial mounds, which left an abundance of findings including lurs and the Sun Chariot.
During the Pre-Roman Iron Age (500 BC – AD 1), native groups began migrating south, and the first tribal Danes came to the country between the Pre-Roman and the Germanic Iron Age, in the Roman Iron Age (AD 1–400). The Roman provinces maintained trade routes and relations with native tribes in Denmark, and Roman coins have been found in Denmark. Evidence of strong Celtic cultural influence dates from this period in Denmark and much of North-West Europe and is among other things reflected in the finding of the Gundestrup cauldron.
The tribal Danes came from the east Danish islands (Zealand) and Scania and spoke an early form of North Germanic. Historians believe that before their arrival, most of Jutland and the nearest islands were settled by tribal Jutes. The Jutes migrated to Great Britain eventually, some as mercenaries by Brythonic King Vortigern, and were granted the south-eastern territories of Kent, the Isle of Wight and other areas, where they settled. They were later absorbed or ethnically cleansed by the invading Angles and Saxons, who formed the Anglo-Saxons. The remaining Jutish population in Jutland assimilated in with the settling Danes.
A short note about the Dani in "Getica" by the historian Jordanes is believed to be an early mention of the Danes, one of the ethnic groups from whom modern Danes are descended. The Danevirke defence structures were built in phases from the 3rd century forward and the sheer size of the construction efforts in AD 737 are attributed to the emergence of a Danish king. A new runic alphabet was first used around the same time and Ribe, the oldest town of Denmark, was founded about AD 700.




From the 8th to the 10th century the wider Scandinavian region was the source of Vikings. They colonised, raided, and traded in all parts of Europe. The Danish Vikings were most active in the eastern and southern British Isles and Western Europe. They conquered and settled parts of England (known as the Danelaw) under King Sweyn Forkbeard in 1013, and France where Danes and Norwegians founded Normandy with Rollo as head of state. More Anglo-Saxon pence of this period have been found in Denmark than in England.

Denmark was largely consolidated by the late 8th century and its rulers are consistently referred to in Frankish sources as kings (reges). Under the reign of Gudfred in 804 the Danish kingdom may have included all the lands of Jutland, Scania and the Danish islands, excluding Bornholm. The extant Danish monarchy traces its roots back to Gorm the Old, who established his reign in the early 10th century. As attested by the Jelling stones, the Danes were Christianised around 965 by Harald Bluetooth, the son of Gorm. It is believed that Denmark became Christian for political reasons so as not to get invaded by the rising Christian power in Europe, the Holy Roman Empire, which was an important trading area for the Danes. In that case, Harald built six fortresses around Denmark called Trelleborg and built a further Danevirke. In the early 11th century, Canute the Great won and united Denmark, England, and Norway for almost 30 years with a Scandinavian army.
Throughout the High and Late Middle Ages, Denmark also included Skåneland (the areas of Scania, Halland, and Blekinge in present-day south Sweden) and Danish kings ruled Danish Estonia, as well as the duchies of Schleswig and Holstein. Most of the latter two now form the state of Schleswig-Holstein in northern Germany.
In 1397, Denmark entered into a personal union with Norway and Sweden, united under Queen Margaret I. The three countries were to be treated as equals in the union. However, even from the start, Margaret may not have been so idealistic—treating Denmark as the clear "senior" partner of the union. Thus, much of the next 125 years of Scandinavian history revolves around this union, with Sweden breaking off and being re-conquered repeatedly. The issue was for practical purposes resolved on 17 June 1523, as Swedish King Gustav Vasa conquered the city of Stockholm. The Protestant Reformation spread to Scandinavia in the 1530s, and following the Count's Feud civil war, Denmark converted to Lutheranism in 1536. Later that year, Denmark entered into a union with Norway.




After Sweden permanently broke away from the personal union, Denmark tried on several occasions to reassert control over its neighbour. King Christian IV attacked Sweden in the 1611–1613 Kalmar War but failed to accomplish his main objective of forcing it to return to the union. The war led to no territorial changes, but Sweden was forced to pay a war indemnity of 1 million silver riksdaler to Denmark, an amount known as the Älvsborg ransom. King Christian used this money to found several towns and fortresses, most notably Glückstadt (founded as a rival to Hamburg) and Christiania. Inspired by the Dutch East India Company, he founded a similar Danish company and planned to claim Ceylon as a colony, but the company only managed to acquire Tranquebar on India's Coromandel Coast. Denmark's large colonial aspirations were limited to a few key trading posts in Africa and India. The empire was sustained by trade with other major powers, and plantations – ultimately a lack of resources led to its stagnation.
In the Thirty Years' War, Christian tried to become the leader of the Lutheran states in Germany but suffered a crushing defeat at the Battle of Lutter. The result was that the Catholic army under Albrecht von Wallenstein was able to invade, occupy, and pillage Jutland, forcing Denmark to withdraw from the war. Denmark managed to avoid territorial concessions, but King Gustavus Adolphus' intervention in Germany was seen as a sign that the military power of Sweden was on the rise while Denmark's influence in the region was declining. In 1643, Swedish armies invaded Jutland and claimed Scania in 1644.
In the 1645 Treaty of Brømsebro, Denmark surrendered Halland, Gotland, the last parts of Danish Estonia, and several provinces in Norway. In 1657, King Frederick III declared war on Sweden and marched on Bremen-Verden. This led to a massive Danish defeat and the armies of King Charles X Gustav of Sweden conquered both Jutland, Funen, and much of Zealand before signing the Peace of Roskilde in February 1658 which gave Sweden control of Scania, Blekinge, Trøndelag, and the island of Bornholm. Charles X Gustav quickly regretted not having wrecked Denmark and in August 1658, he began a two-year-long siege of Copenhagen but failed to take the capital. In the following peace settlement, Denmark managed to maintain its independence and regain control of Trøndelag and Bornholm.

Denmark tried to regain control of Scania in the Scanian War (1675–1679) but it ended in failure. Following the Great Northern War (1700–21), Denmark managed to restore control of the parts of Schleswig and Holstein ruled by the house of Holstein-Gottorp in the 1720 Treaty of Frederiksborg and the 1773 Treaty of Tsarskoye Selo, respectively. Denmark prospered greatly in the last decades of the eighteenth century due to its neutral status allowing it to trade with both sides in the many contemporary wars. In the Napoleonic Wars, Denmark traded with both France and the United Kingdom and joined the League of Armed Neutrality with Russia, Sweden, and Prussia. The British considered this a hostile act and attacked Copenhagen in both 1801 and 1807, in one case carrying off the Danish fleet, in the other, burning large parts of the Danish capital. This led to the so-called Danish-British Gunboat War. British control over the waterways between Denmark and Norway proved disastrous to the union's economy and in 1813 Denmark–Norway went bankrupt.
The Danish-Norwegian union was dissolved by the Treaty of Kiel in 1814; the Danish monarchy "irrevocably and forever" renounced claims to the Kingdom of Norway in favour of the Swedish king. After the dissolution of the union with Norway, Denmark kept the possessions of Iceland (which retained the Danish monarchy until 1944), the Faroe Islands and Greenland, all of which had been governed by Norway for centuries. Apart from the Nordic colonies, Denmark continued to rule over Danish India from 1620 to 1869, the Danish Gold Coast (Ghana) from 1658 to 1850, and the Danish West Indies from 1671 to 1917.




A nascent Danish liberal and national movement gained momentum in the 1830s; after the European Revolutions of 1848, Denmark peacefully became a constitutional monarchy on 5 June 1849. A new constitution established a two-chamber parliament. Denmark faced war against both Prussia and Habsburg Austria in what became known as the Second Schleswig War, lasting from February to October 1864. Denmark was defeated and obliged to cede Schleswig and Holstein to Prussia. This loss came as the latest in the long series of defeats and territorial loss that had begun in the 17th century. After these events, Denmark pursued a policy of neutrality in Europe.
Industrialisation came to Denmark in the second half of the 19th century. The nation's first railroads were constructed in the 1850s, and improved communications and overseas trade allowed industry to develop in spite of Denmark's lack of natural resources. Trade unions developed starting in the 1870s. There was a considerable migration of people from the countryside to the cities, and Danish agriculture became centred on the export of dairy and meat products.
Denmark maintained its neutral stance during World War I. After the defeat of Germany, the Versailles powers offered to return the region of Schleswig-Holstein to Denmark. Fearing German irredentism, Denmark refused to consider the return of the area without a plebiscite; the two Schleswig Plebiscites took place on 10 February and 14 March 1920, respectively. On 10 July 1920, Northern Schleswig was recovered by Denmark, thereby adding some 163,600 inhabitants and 3,984 square kilometres (1,538 sq mi).
In 1939 Denmark signed a 10-year non-aggression pact with Nazi Germany but Germany invaded Denmark on 9 April 1940 and the Danish government quickly surrendered. World War II in Denmark was characterised by economic co-operation with Germany until 1943, when the Danish government refused further co-operation and its navy scuttled most of its ships and sent many of its officers to Sweden, which was neutral. The Danish resistance performed a rescue operation that managed to evacuate several thousand Jews and their families to safety in Sweden before the Germans could send them to death camps. Some Danes supported Nazism by joining the Danish Nazi Party or volunteering to fight with Germany as part of the Frikorps Danmark. Iceland severed ties to Denmark and became an independent republic in 1944; Germany surrendered in May 1945; in 1948, the Faroe Islands gained home rule; in 1949, Denmark became a founding member of NATO.

Denmark was a founding member of European Free Trade Association (EFTA). During the 1960s, the EFTA countries were often referred to as the Outer Seven, as opposed to the Inner Six of what was then the European Economic Community (EEC). In 1973, along with Britain and Ireland, Denmark joined the European Economic Community (now the European Union) after a public referendum. The Maastricht Treaty, which involved further European integration, was rejected by the Danish people in 1992; it was only accepted after a second referendum in 1993, which provided for four opt-outs from policies. The Danes rejected the euro as the national currency in a referendum in 2000. Greenland gained home rule in 1979 and was awarded self-determination in 2009. Neither the Faroe Islands nor Greenland are members of the Union, the Faroese having declined membership of the EEC in 1973 and Greenland in 1986, in both cases because of fisheries policies.
Constitutional change in 1953 led to a single-chamber parliament elected by proportional representation, female accession to the Danish throne, and Greenland becoming an integral part of Denmark. The centre-left Social Democrats led a string of coalition governments for most of the second half of the 20th century, introducing the Nordic welfare model. The Liberal Party and the Conservative People's Party have also led centre-right governments. In recent years the right-wing populist Danish People's Party has emerged as a major party—becoming the second-largest following the 2015 general election—during which time immigration and integration have become major issues of public debate.




Located in Northern Europe, Denmark consists of the peninsula of Jutland and 443 named islands (1,419 islands above 100 square metres (1,100 sq ft) in total). Of these, 74 are inhabited (January 2015), with the largest being Zealand, the North Jutlandic Island, and Funen. The island of Bornholm is located east of the rest of the country, in the Baltic Sea. Many of the larger islands are connected by bridges; the Øresund Bridge connects Zealand with Sweden; the Great Belt Bridge connects Funen with Zealand; and the Little Belt Bridge connects Jutland with Funen. Ferries or small aircraft connect to the smaller islands. The largest cities with populations over 100,000 are the capital Copenhagen on Zealand; Aarhus and Aalborg in Jutland; and Odense on Funen.

The country occupies a total area of 42,924 square kilometres (16,573 sq mi) The area of inland water is 700 km2 (270 sq mi), variously stated as from 500 – 700 km2 (193–270 sq m). Lake Arresø northwest of Copenhagen is the largest lake. The size of the land area cannot be stated exactly since the ocean constantly erodes and adds material to the coastline, and because of human land reclamation projects (to counter erosion). Post-glacial rebound raises the land by a bit less than 1 cm (0.4 in) per year in the north and east, extending the coast. A circle enclosing the same area as Denmark would be 234 kilometres (145 miles) in diameter with a circumference of 742 km (461 mi). It shares a border of 68 kilometres (42 mi) with Germany to the south and is otherwise surrounded by 8,750 km (5,437 mi) of tidal shoreline (including small bays and inlets). No location in Denmark is further from the coast than 52 km (32 mi). On the south-west coast of Jutland, the tide is between 1 and 2 m (3.28 and 6.56 ft), and the tideline moves outward and inward on a 10 km (6.2 mi) stretch. Denmark's territorial waters total 105,000 square kilometres (40,541 square miles).
Denmark's northernmost point is Skagen's point (the north beach of the Skaw) at 57° 45' 7" northern latitude; the southernmost is Gedser point (the southern tip of Falster) at 54° 33' 35" northern latitude; the westernmost point is Blåvandshuk at 8° 4' 22" eastern longitude; and the easternmost point is Østerskær at 15° 11' 55" eastern longitude. This is in the archipelago Ertholmene 18 kilometres (11 mi) north-east of Bornholm. The distance from east to west is 452 kilometres (281 mi), from north to south 368 kilometres (229 mi).
The country is flat with little elevation; having an average height above sea level of 31 metres (102 ft). The highest natural point is Møllehøj, at 170.86 metres (560.56 ft). A sizeable portion of Denmark's terrain consists of rolling plains whilst the coastline is sandy, with large dunes in northern Jutland. Although once extensively forested, today Denmark largely consists of arable land. It is drained by a dozen or so rivers, and the most significant include the Gudenå, Odense, Skjern, Suså and Vidå—a river that flows along its southern border with Germany.
The Kingdom of Denmark includes two overseas territories, both well to the west of Denmark: Greenland, the world's largest island, and the Faroe Islands in the North Atlantic Ocean. These territories are self-governing and form part of the Danish Realm.



Denmark has a temperate climate, characterised by mild winters, with mean temperatures in January of 1.5 °C (34.7 °F), and cool summers, with a mean temperature in August of 17.2 °C (63.0 °F). The most extreme temperatures recorded in Denmark, since 1874 when recordings began, was 36.4 °C (97.5 °F) in 1975 and −31.02 °C (−23.84 °F) in 1982. Denmark has an average of 179 days per year with precipitation, on average receiving a total of 765 millimetres (30 in) per year; autumn is the wettest season and spring the driest. The position between a continent and an ocean means that weather often changes.
Because of Denmark's northern location, there are large seasonal variations in daylight. There are short days during the winter with sunrise coming around 8:45 am and sunset 3:45 pm (standard time), as well as long summer days with sunrise at 4:30 am and sunset at 10 pm (daylight saving time).




Denmark belongs to the Boreal Kingdom and can be subdivided into two ecoregions: the Atlantic mixed forests and Baltic mixed forests. Almost all of Denmark's primeval temperate forests have been destroyed or fragmented, chiefly for agricultural purposes during the last millennia. The deforestation has created large swaths of heathland and devastating sand drifts. In spite of this, there are several larger second growth woodlands in the country and, in total, 12.9% of the land is now forested.
Roe deer occupy the countryside in growing numbers, and large-antlered red deer can be found in the sparse woodlands of Jutland. Denmark is also home to smaller mammals, such as polecats, hares and hedgehogs. Approximately 400 bird species inhabit Denmark and about 160 of those breed in the country. Large marine mammals include healthy populations of Harbour porpoise, growing numbers of pinnipeds and occasional visits of large whales, including blue whales and orcas. Cod, herring and plaice are abundant fish in Danish waters and form the basis for a large fishing industry.



Land and water pollution are two of Denmark's most significant environmental issues, although much of the country's household and industrial waste is now increasingly filtered and sometimes recycled. The country has historically taken a progressive stance on environmental preservation; in 1971 Denmark established a Ministry of Environment and was the first country in the world to implement an environmental law in 1973. To mitigate environmental degradation and global warming the Danish Government has signed the Climate Change-Kyoto Protocol. However, the national ecological footprint is 8.26 global hectares per person, which is very high compared to a world average of 1.7 in 2010. Contributing factors to this value are an exceptional high value for cropland but also a relatively high value for grazing land, which may be explained by the substantially high meat production in Denmark (115.8 kilograms (255 lb) meat annually per capita) and the large economic role of the meat and dairy industries. In December 2014, the Climate Change Performance Index for 2015 placed Denmark at the top of the table, explaining that although emissions are still quite high, the country was able to implement effective climate protection policies.
Denmark has an outstanding performance in the global Environmental Performance Index (EPI) with an overall ranking of 4 out of 180 countries in 2016. This recent and significant increase in ranking and performance is mostly due to remarkable achievements in energy efficiency and reductions in CO2 emission levels. A future implementation of air quality improvements are expected. The EPI was established in 2001 by the World Economic Forum as a global gauge to measure how well individual countries perform in implementing the United Nations' Sustainable Development Goals. The environmental areas where Denmark performs best (i.e. lowest ranking) are sanitation (12), water resource management (13) and health impacts of environmental issues (14), followed closely by the area of biodiversity and habitat. The latter are due to the many protection laws and protected areas of significance within the country even though the EPI is not considering how well these laws and regulations are affecting the current biodiversity and habitats in reality; one of many weaknesses in the EPI. Denmark performs worst (i.e. highest ranking) in the areas of environmental effects of fisheries (128) and forest management (96). The very poor ranking in the fisheries area are due to alarmingly low and continually rapidly declining fish stocks, placing Denmark among the worst performing countries of the world.




Denmark, with a total area of 43,094 square kilometres (16,639 sq mi), is divided into five administrative regions (Danish: regioner). The regions are further subdivided into 98 municipalities (kommuner). The easternmost land in Denmark, the Ertholmene archipelago, with an area of 39 hectares (0.16 sq m), is neither part of a municipality nor a region but belongs to the Ministry of Defence.
The regions were created on 1 January 2007 to replace the sixteen former counties. At the same time, smaller municipalities were merged into larger units, reducing the number from 270. Most municipalities have a population of at least 20,000 to give them financial and professional sustainability, although a few exceptions were made to this rule. The administrative divisions are led by directly elected councils, elected proportionally every four years; the most recent Danish local elections were held on 19 November 2013. Other regional structures use the municipal boundaries as a layout, including the police districts, the court districts and the electoral wards.



The governing bodies of the regions are the regional councils with forty-one members elected for four-year terms. The head of the council is the regional council chairman (regionsrådsformand), who is elected by the council. The areas of responsibility for the regional councils are the national health service, social services and regional development. Unlike the counties they replaced, the regions are not allowed to levy taxes and the health service is partly financed by a national health care contribution until 2018 (sundhedsbidrag), partly by funds from both government and municipalities. From 1 January 2019 this contribution will be abolished.
The area and populations of the regions vary widely; for example, the Capital Region, which encompasses the Copenhagen metropolitan area with the exception of the subtracted province East Zeeland but includes the Baltic Sea island of Bornholm, has a population three times larger than that of North Denmark Region, which covers the more sparsely populated area of northern Jutland. Under the county system certain densely populated municipalities, such as Copenhagen Municipality and Frederiksberg, had been given a status equivalent to that of counties, making them first-level administrative divisions. These sui generis municipalities were incorporated into the new regions under the 2007 reforms.



The Kingdom of Denmark is a unitary state that comprises, in addition to Denmark proper, two autonomous constituent countries in the North Atlantic Ocean: Greenland and the Faroe Islands. They have been integrated parts of the Danish Realm since the 18th century; however, due to their separate historical and cultural identities, these parts of the Realm have extensive political powers and have assumed legislative and administrative responsibility in a substantial number of fields. The Faroe Islands gained home rule in 1948 and Greenland in 1979, having previously had the status of counties.
The two territories have their own home governments and parliaments and are effectively self-governing in regards to domestic affairs. High Commissioners (Rigsombudsmand) act as representatives of the Danish government in the Faroese Løgting and in the Greenlandic Parliament, but they cannot vote. The Faroese home government is defined to be an equal partner with the Danish national government, while the Greenlandic people are defined as a separate people with the right to self-determination.




Politics in Denmark operates under a framework laid out in the Constitution of Denmark. First written in 1849, it establishes a sovereign state in the form of a constitutional monarchy, with a representative parliamentary system. The Monarch officially retains executive power and presides over the Council of State (privy council). In practice, the duties of the Monarch are strictly representative and ceremonial, such as the formal appointment and dismissal of the Prime Minister and other Government ministers. The Monarch is not answerable for his or her actions, and their person is sacrosanct. Hereditary monarch Queen Margrethe II has been head of state since 14 January 1972.




The Danish Parliament is called the Folketing (Danish: Folketinget). It is the legislature of the Kingdom of Denmark, passing Acts that apply in Denmark and, in limited cases, Greenland and the Faroe Islands. The Folketing is also responsible for adopting the state's budgets, approving the state's accounts, appointing and exercising control of the Government, and taking part in international co-operation. Bills may be initiated by the Government or by members of parliament. All bills passed must be presented before the Council of State to receive Royal Assent within thirty days in order to become law.

Denmark is a representative democracy with universal suffrage. Membership of the Folketing is based on proportional representation of political parties, with a 2% electoral threshold. Danes elect 175 members to the Folketing, with Greenland and the Faroe Islands electing an additional two members each—179 members in total. Parliamentary elections are held at least every four years, but it is within the powers of the Prime Minister to ask the Monarch to call for an election before the term has elapsed. On a vote of no confidence, the Folketing may force a single minister or the entire government to resign.
The Government of Denmark operates as a cabinet government, where executive authority is exercised—formally on behalf of the Monarch—by Prime Minister and other cabinet ministers, who head ministries. As the executive branch, the Cabinet is responsible for proposing bills and a budget, executing the laws, and guiding the foreign and internal policies of Denmark. The position of prime minister belongs to the person most likely to command the confidence of a majority in the Folketing; this is usually the current leader of the largest political party or, more effectively, through a coalition of parties. A single party generally does not have sufficient political power in terms of the number of seats to form a cabinet on its own; Denmark has often been ruled by coalition governments, themselves sometimes minority governments dependent on non-government parties.
Following a general election defeat, in June 2015 Helle Thorning-Schmidt, leader of the Social Democrats (Socialdemokraterne), resigned as Prime Minister. She was succeeded by Lars Løkke Rasmussen, the leader of the Liberal Party (Venstre). Rasmussen became the leader of a cabinet which, unusually, consisted entirely of ministers from his own party. In the next cabinet, created November 2016, there are several political parties represented.




Denmark has a civil law system with some references to Germanic law. Denmark resembles Norway and Sweden in never having developed a case-law like that of England and the United States nor comprehensive codes like those of France and Germany. Much of its law is customary.
The judicial system of Denmark is divided between courts with regular civil and criminal jurisdiction and administrative courts with jurisdiction over litigation between individuals and the public administration. Articles sixty-two and sixty-four of the Constitution ensure judicial independence from government and Parliament by providing that judges shall only be guided by the law, including acts, statutes and practice. The Kingdom of Denmark does not have a single unified judicial system – Denmark has one system, Greenland another, and the Faroe Islands a third. However, decisions by the highest courts in Greenland and the Faroe Islands may be appealed to the Danish High Courts. The Danish Supreme Court is the highest civil and criminal court responsible for the administration of justice in the Kingdom.




Denmark wields considerable influence in Northern Europe and is a middle power in international affairs. In recent years, Greenland and the Faroe Islands have been guaranteed a say in foreign policy issues such as fishing, whaling, and geopolitical concerns. The foreign policy of Denmark is substantially influenced by its membership of the European Union (EU); Denmark joined the European Economic Community (EEC), the EU's predecessor, in 1973. Denmark held the Presidency of the Council of the European Union on seven occasions, most recently from January to June 2012. Following World War II, Denmark ended its two-hundred-year-long policy of neutrality. It has been a founding member of the North Atlantic Treaty Organization (NATO) since 1949, and membership remains highly popular.
As a member of Development Assistance Committee (DAC), Denmark has for a long time been among the countries of the world contributing the largest percentage of gross national income to development aid. In 2015, Denmark contributed 0.85% of its gross national income (GNI) to foreign aid and was one of only six countries meeting the longstanding UN target of 0.7% of GNI. The country participates in both bilateral and multilateral aid, with the aid usually administered by the Ministry of Foreign Affairs. The organisational name of Danish International Development Agency (DANIDA) is often used, in particular when operating bilateral aid.




Denmark's armed forces are known as the Danish Defence (Danish: Forsvaret). The Minister of Defence is commander-in-chief of the Danish Defence, and serves as chief diplomatic official abroad. During peacetime, the Ministry of Defence employs around 33,000 in total. The main military branches employ almost 27,000: 15,460 in the Royal Danish Army, 5,300 in the Royal Danish Navy and 6,050 in the Royal Danish Air Force (all including conscripts). The Danish Emergency Management Agency employs 2,000 (including conscripts), and about 4,000 are in non-branch-specific services like the Danish Defence Command and the Danish Defence Intelligence Service. Furthermore, around 55,000 serve as volunteers in the Danish Home Guard.
Denmark is a long-time supporter of international peacekeeping, but since the NATO bombing of Yugoslavia in 1999 and the War in Afghanistan in 2001, Denmark has also found a new role as a warring nation, participating actively in several wars and invasions. This relatively new situation has stirred some internal critique, but the Danish population has generally been very supportive, in particular of the War in Afghanistan. The Danish Defence has around 1,400 staff in international missions, not including standing contributions to NATO SNMCMG1. Danish forces were heavily engaged in the former Yugoslavia in the UN Protection Force (UNPROFOR), with IFOR, and now SFOR. Between 2003 and 2007, there were approximately 450 Danish soldiers in Iraq. Denmark also strongly supported American operations in Afghanistan and has contributed both monetarily and materially to the ISAF. These initiatives are often described by the authorities as part of a new "active foreign policy" of Denmark.




Denmark has a developed mixed economy that is classed as a high-income economy by the World Bank. It ranks 18th in the world in terms of GDP (PPP) per capita and 6th in nominal GDP per capita. Denmark's economy stands out as one of the most free in the Index of Economic Freedom and the Economic Freedom of the World. It is the 13th most competitive economy in the world, and 8th in Europe, according to the World Economic Forum in its Global Competitiveness Report 2014–2015.
Denmark has the fourth highest ratio of tertiary degree holders in the world. The country ranks highest in the world for workers' rights. GDP per hour worked was the 13th highest in 2009. The country has a market income inequality close to the OECD average, but after public cash transfers the income inequality is very low. According to the International Monetary Fund, Denmark has the world's highest minimum wage. As Denmark has no minimum wage legislation, the high wage floor has been attributed to the power of trade unions. For example, as the result of a collective bargaining agreement between the 3F trade union and the employers group Horesta, workers at McDonald's and other fast food chains make the equivalent of US$20 an hour, which is more than double what their counterparts earn in the United States, and have access to five weeks' paid vacation, parental leave and a pension plan.

Once a predominantly agricultural country on account of its arable landscape, since 1945 Denmark has greatly expanded its industrial base so that by 2006 industry contributed about 25% of GDP and agriculture less than 2%. Major industries include iron, steel, chemicals, food processing, pharmaceuticals, shipbuilding and construction. The country's main exports are: industrial production/manufactured goods 73.3% (of which machinery and instruments were 21.4%, and fuels (oil, natural gas), chemicals, etc. 26%); agricultural products and others for consumption 18.7% (in 2009 meat and meat products were 5.5% of total export; fish and fish products 2.9%). Denmark is a net exporter of food and energy and has for a number of years had a balance of payments surplus while battling an equivalent of approximately 39% of GNP foreign debt or more than DKK 300 billion.

A liberalisation of import tariffs in 1797 marked the end of mercantilism and further liberalisation in the 19th and the beginning of the 20th century established the Danish liberal tradition in international trade that was only to be broken by the 1930s. Even when other countries, such as Germany and France, raised protection for their agricultural sector because of increased American competition resulting in much lower agricultural prices after 1870, Denmark retained its free trade policies, as the country profited from the cheap imports of cereals (used as feedstuffs for their cattle and pigs) and could increase their exports of butter and meat of which the prices were more stable. Today, Denmark is part of the European Union's internal market, which represents more than 508 million consumers. Several domestic commercial policies are determined by agreements among European Union (EU) members and by EU legislation. Support for free trade is high among the Danish public; in a 2007 poll 76% responded that globalisation is a good thing. 70% of trade flows are inside the European Union. As of 2014, Denmark's largest export partners are Germany, Sweden, the United Kingdom and Norway.
Denmark's currency, the krone (DKK), is pegged at approximately 7.46 kroner per euro through the ERM. Although a September 2000 referendum rejected adopting the euro, the country follows the policies set forth in the Economic and Monetary Union of the European Union and meets the economic convergence criteria needed to adopt the euro. The majority of the political parties in the Folketing support adopting the euro, but as yet a new referendum has not been held, despite plans; scepticism of the EU among Danish voters has historically been strong.
Denmark is home to many multinational companies, among them: A.P. Møller-Mærsk, (international shipping), Arla Foods (dairy), Lego Group (toys), Danfoss (industrial services), Carlsberg Group (beer), Vestas (wind turbines), and the pharmaceutical companies Leo Pharma and Novo Nordisk.




Denmark has a long tradition of scientific and technological invention and engagement, and has been involved internationally from the very start of the scientific revolution. In current times, Denmark is participating in many high-profile international science and technology projects, including CERN, ITER, ESA, ISS and E-ELT.
In the 20th century, Danes have also been innovative in several fields of the technology sector. Danish companies have been influential in the shipping industry with the design of the largest and most energy efficient container ships in the world, the Maersk Triple E class, and Danish engineers have contributed to the design of MAN Diesel engines. In the software and electronic field, Denmark contributed to design and manufacturing of Nordic Mobile Telephones, and the now-defunct Danish company DanCall was among the first to develop GSM mobile phones.
Life science is a key sector with extensive research and development activities. Danish engineers are world-leading in providing diabetes care equipment and medication products from Novo Nordisk and, since 2000, the Danish biotech company Novozymes, the world market leader in enzymes for first generation starch based bioethanol, has pioneered development of enzymes for converting waste to cellulosic ethanol. Medicon Valley, spanning the Øresund Region between Zealand and Sweden, is one of Europe's largest life science clusters, containing a large number of life science companies and research institutions located within a very small geographical area.
Danish-born computer scientists and software engineers have taken leading roles in some of the world's programming languages: Anders Hejlsberg (Turbo Pascal, Delphi, C#); Rasmus Lerdorf (PHP); Bjarne Stroustrup (C++); David Heinemeier Hansson (Ruby on Rails); Lars Bak, a pioneer in virtual machines (V8, Java VM, Dart). Physicist Lene Vestergaard Hau is the first person to stop light, leading to advances in quantum computing, nanoscale engineering and linear optics.




Danes enjoy a high standard of living and the Danish economy is characterised by extensive government welfare provisions. Like other Nordic countries, Denmark has adopted the Nordic Model which combines free market capitalism with a comprehensive welfare state and strong worker protection. As a result of its acclaimed "flexicurity" model, Denmark has the most free labour market in Europe, according to the World Bank. Employers can hire and fire whenever they want (flexibility), and between jobs, unemployment compensation is very high (security). Establishing a business can be done in a matter of hours and at very low costs. No restrictions apply regarding overtime work, which allows companies to operate 24 hours a day, 365 days a year. Denmark has a competitive corporate tax rate of 24.5% and a special time-limited tax regime for expatriates. The Danish taxation system is broad based, with a 25% value-added tax, in addition to excise taxes, income taxes and other fees. The overall level of taxation (sum of all taxes, as a percentage of GDP) is estimated to be 46% in 2011.
As of 2014, 6% of the population was reported to live below the poverty line, when adjusted for taxes and transfers. Denmark has the 2nd lowest relative poverty rate in the OECD, below the 11.3% OECD average. The share of the population reporting that they feel that they cannot afford to buy sufficient food in Denmark is less than half of the OECD average. With an employment rate of 72.8%, Denmark ranks 7th highest among the OECD countries, and above the OECD average of 66.2%. The number of unemployed people is forecast to be 65,000 in 2015. The number of people in the working age group, less disability pensioners etc., will grow by 10,000 to 2,860,000, and jobs by 70,000 to 2,790,000; part-time jobs are included. Because of the present high demand and short supply of skilled labour, for instance for factory and service jobs, including hospital nurses and physicians, the annual average working hours have risen, especially compared with the recession 1987–1993. Increasingly, service workers of all kinds are in demand, i.e. in the postal services and as bus drivers, and academics.
The level of unemployment benefits is dependent on former employment (the maximum benefit is at 90% of the wage) and at times also on membership of an unemployment fund, which is almost always—but need not be—administered by a trade union, and the previous payment of contributions. However, the largest share of the financing is still carried by the central government and is financed by general taxation, and only to a minor degree from earmarked contributions. There is no taxation, however, on proceeds gained from selling one's home (provided there was any home equity (friværdi)), as the marginal tax rate on capital income from housing savings is around 0%.




Denmark has considerably large deposits of oil and natural gas in the North Sea and ranks as number 32 in the world among net exporters of crude oil and was producing 259,980 barrels of crude oil a day in 2009. Denmark is a long-time leader in wind power: In 2015 wind turbines provided 42.1% of the total electricity power consumption. in May 2011 Denmark derived 3.1% of its gross domestic product from renewable (clean) energy technology and energy efficiency, or around €6.5 billion ($9.4 billion). Denmark is connected by electric transmission lines to other European countries. On 6 September 2012, Denmark launched the biggest wind turbine in the world, and will add four more over the next four years.Denmark's electricity sector has integrated energy sources such as wind power into the national grid. Denmark now aims to focus on intelligent battery systems (V2G) and plug-in vehicles in the transport sector. The country is a member nation of the International Renewable Energy Agency (IRENA).




Significant investment has been made in building road and rail links between regions in Denmark, most notably the Great Belt Fixed Link, which connects Zealand and Funen. It is now possible to drive from Frederikshavn in northern Jutland to Copenhagen on eastern Zealand without leaving the motorway. The main railway operator is DSB for passenger services and DB Schenker Rail for freight trains. The railway tracks are maintained by Banedanmark. The North Sea and the Baltic Sea are intertwined by various, international ferry links. Construction of the Fehmarn Belt Fixed Link, connecting Denmark and Germany with a second link, will start in 2015. Copenhagen has a rapid transit system, the Copenhagen Metro, and an extensive electrified suburban railway network, the S-train. In the four largest cities – Copenhagen, Aarhus, Odense, Aalborg – light rail systems are planned to be in operation around 2020.

With Norway and Sweden, Denmark is part of the Scandinavian Airlines flag carrier. Copenhagen Airport is Scandinavia's busiest passenger airport, handling over 25 million passengers in 2014. Other notable airports are Billund Airport, Aalborg Airport, and Aarhus Airport.
Cycling in Denmark is a common form of transport, particularly for the young and for city dwellers. With a network of bicycle routes extending more than 12,000 km and an estimated 7,000 km of segregated dedicated bicycle paths and lanes, Denmark has a solid bicycle infrastructure.
Private vehicles are increasingly used as a means of transport. Because of the high registration tax (150%), VAT (25%), and one of the world's highest income tax rates, new cars are very expensive. The purpose of the tax is to discourage car ownership. In 2007, an attempt was made by the government to favour environmentally friendly cars by slightly reducing taxes on high mileage vehicles. However, this has had little effect, and in 2008 Denmark experienced an increase in the import of fuel inefficient old cars, as the cost for older cars—including taxes—keeps them within the budget of many Danes. As of 2011, the average car age is 9.2 years.




The population of Denmark, as defined by Statistics Denmark, was estimated in January 2016 to be 5,707,251. The median age is 41.4 years, with 0.97 males per female. The total fertility rate is 1.73 children born per woman; despite the low birth rate, the population is still growing at an average annual rate of 0.22%. Notably, very few Down Syndrome children are born in Denmark, with 98% of DS pregnancies aborted in 2014. The World Happiness Report frequently ranks Denmark's population as the happiest in the world. This has been attributed to the country's highly regarded education and health care systems, and its low level of income inequality.
Denmark is an historically homogeneous nation. However, as with its Scandinavian neighbours, Denmark has recently transformed from a nation of net emigration, up until World War II, to a nation of net immigration. Today, immigration to Denmark consists particularly of asylum seekers and persons who arrive as family dependants. In addition, Denmark annually receives a number of citizens from Western countries, notably Nordic countries, the EU, and North America, who seek residency to work or study for a definite period of time. Recently, substantial numbers of workers—several tens of thousands—from the new EU accession countries, especially Poland and the Baltic nations, have arrived to perform menial labour in construction, agriculture, consumer industries, and cleaning. Overall, the net migration rate in 2015 was 2.2 migrant(s)/1,000 population, comparable to the United Kingdom and well below other North European countries, except the Baltic states.
There are no official statistics on ethnic groups, but according to 2016 figures from Statistics Denmark, approximately 87.7% of the population was of Danish descent, defined as having at least one parent who was born in Denmark and has Danish citizenship. The remaining 12.3% were of a foreign background, defined as immigrants or descendants of recent immigrants. With the same definition, the most common countries of origin were Poland, Turkey, Germany, Iraq, Romania, Syria, Somalia, Iran, Afghanistan, and Yugoslavia and its successor states.




Danish is the de facto national language of Denmark. Faroese and Greenlandic are the official languages of the Faroe Islands and Greenland respectively. German is a recognised minority language in the area of the former South Jutland County (now part of the Region of Southern Denmark), which was part of the German Empire prior to the Treaty of Versailles. Danish and Faroese belong to the North Germanic (Nordic) branch of the Indo-European languages, along with Icelandic, Norwegian and Swedish. The languages are so closely related that it is possible for Danish, Norwegian and Swedish speakers to understand each other with relatively little effort. Danish is more distantly related to German, which is a West Germanic language. Greenlandic or "Kalaallisut" belongs to the Eskimo–Aleut languages; it is closely related to the Inuit languages in Canada, such as Inuktitut, and entirely unrelated to Danish.
A large majority (86%) of Danes speak English as a second language, generally with a high level of proficiency. German is the second-most spoken foreign language, with 47% reporting a conversational level of proficiency. Denmark had 25,900 native speakers of German in 2007 (mostly in the South Jutland area).




Christianity is the dominant religion in Denmark that has a state religion that represents nearly all Christians in Denmark. In January 2016, 76.9% of the population of Denmark were members of the Church of Denmark (Den Danske Folkekirke), the officially established church, which is Lutheran in tradition. This is down 0.9% compared to the year earlier and 1.5% down compared to two years earlier. Despite the high membership figures, only 3% of the population regularly attend Sunday services and only 19% of Danes consider religion to be an important part of their life.

The Constitution states that a member of the Royal Family must be a member of the Church of Denmark, though the rest of the population is free to adhere to other faiths. In 1682 the state granted limited recognition to three religious groups dissenting from the Established Church: Roman Catholicism, the Reformed Church and Judaism, although conversion to these groups from the Church of Denmark remained illegal initially. Until the 1970s, the state formally recognised "religious societies" by royal decree. Today, religious groups do not need official government recognition, they can be granted the right to perform weddings and other ceremonies without this recognition. Denmark's Muslims make up approximately 3.7% of the population and form the country's second largest religious community and largest minority religion. The Danish Foreign Ministry estimates that other religious groups comprise less than 1% of the population individually and approximately 2% when taken all together.
According to a 2010 Eurobarometer Poll, 28% of Danish citizens polled responded that they "believe there is a God", 47% responded that they "believe there is some sort of spirit or life force" and 24% responded that they "do not believe there is any sort of spirit, God or life force". Another poll, carried out in 2009, found that 25% of Danes believe Jesus is the son of God, and 18% believe he is the saviour of the world.




All educational programmes in Denmark are regulated by the Ministry of Education and administered by local municipalities. Folkeskole covers the entire period of compulsory education, encompassing primary and lower secondary education. Most children attend folkeskole for 10 years, from the ages of 6 to 16. There are no final examinations, but pupils can choose to go to a test when finishing ninth grade (14–15 years old). The test is obligatory if further education is to be attended. Pupils can alternatively attend an independent school (friskole), or a private school (privatskole), such as Christian schools or Waldorf schools.
Following graduation from compulsory education, there are several continuing educational opportunities; the Gymnasium (STX) attaches importance in teaching a mix of humanities and science, Higher Technical Examination Programme (HTX) focuses on scientific subjects and the Higher Commercial Examination Programme emphasises on subjects in economics. Higher Preparatory Examination (HF) is similar to Gymnasium (STX), but is one year shorter. For specific professions, there is vocational education, training young people for work in specific trades by a combination of teaching and apprenticeship.
The government records upper secondary school completion rates of 95% and tertiary enrolment and completion rates of 60%. All university and college (tertiary) education in Denmark is free of charges; there are no tuition fees to enrol in courses. Students aged 18 or above may apply for state educational support grants, known as Statens Uddannelsesstøtte (SU) which provides fixed financial support, disbursed monthly. Danish universities offer international students a range of opportunities for obtaining an internationally recognised qualification in Denmark. Many programmes may be taught in the English language, the academic lingua franca, in bachelor's degrees, master's degrees, doctorates and student exchange programmes.




As of 2012, Denmark has a life expectancy of 79.5 years at birth (77 for men, 82 for women), up from 75 years in 1990. This ranks it 37th among 193 nations, behind the other Nordic countries. The National Institute of Public Health of the University of Southern Denmark has calculated 19 major risk factors among Danes that contribute to a lowering of the life expectancy; this includes smoking, alcohol, drug abuse and physical inactivity. The large number of Danes becoming overweight is an increasing problem and results in an annual additional consumption in the health care system of DKK 1,625 million. In a 2012 study, Denmark had the highest cancer rate of all countries listed by the World Cancer Research Fund International; researchers suggest the reasons are better reporting, but also lifestyle factors like heavy alcohol consumption, smoking and physical inactivity.
Denmark has a universal health care system, characterised by being publicly financed through taxes and, for most of the services, run directly by the regional authorities. One of the sources of income is a national health care contribution (sundhedsbidrag) (2007–11:8%; '12:7%; '13:6%; '14:5%; '15:4%; '16:3%; '17:2%; '18:1%; '19:0%) but it is being phased out and will be gone from January 2019, with the income taxes in the lower brackets being raised gradually each year instead. Another source comes from the municipalities which had their income taxes raised by 3 percentage points from 1 January 2007, a contribution confiscated from the former county tax to be used from 1 January 2007 for health purposes by the municipalities instead. This means that most health care provision is free at the point of delivery for all residents. Additionally, roughly two in five have complementary private insurance to cover services not fully covered by the state, such as physiotherapy. As of 2012, Denmark spends 11.2% of its GDP on health care; this is up from 9.8% in 2007 (US$3,512 per capita). This places Denmark above the OECD average and above the other Nordic countries.




Denmark shares strong cultural and historic ties with its Scandinavian neighbours Sweden and Norway. It has historically been one of the most socially progressive cultures in the world. In 1969, Denmark was the first country to legalise pornography, and in 2012, Denmark replaced its "registered partnership" laws, which it had been the first country to introduce in 1989, with gender-neutral marriage. Modesty and social equality are important parts of Danish culture, so much so that, 'success' or what may be seen as a deliberate attempt to distinguish oneself from others may be viewed with hostility. This characteristic is called Janteloven or Law of Jante by Danes.

The astronomical discoveries of Tycho Brahe (1546–1601), Ludwig A. Colding's (1815–88) neglected articulation of the principle of conservation of energy, and the contributions to atomic physics of Niels Bohr (1885–1962) indicate the range of Danish scientific achievement. The fairy tales of Hans Christian Andersen (1805–1875), the philosophical essays of Søren Kierkegaard (1813–55), the short stories of Karen Blixen (penname Isak Dinesen), (1885–1962), the plays of Ludvig Holberg (1684–1754), and the dense, aphoristic poetry of Piet Hein (1905–96), have earned international recognition, as have the symphonies of Carl Nielsen (1865–1931). From the mid-1990s, Danish films have attracted international attention, especially those associated with Dogme 95 like those of Lars von Trier.
A major feature of Danish culture is Jul (Danish Christmas). The holiday is celebrated throughout December, starting either at the beginning of Advent or on 1 December with a variety of traditions, culminating with the Christmas Eve meal.
There are five Danish heritage sites inscribed on the UNESCO World Heritage list in Northern Europe: Christiansfeld, a Moravian Church Settlement, the Jelling Mounds (Runic Stones and Church), Kronborg Castle, Roskilde Cathedral, and The par force hunting landscape in North Zealand.




Danish mass media date back to the 1540s, when handwritten fly sheets reported on the news. In 1666, Anders Bording, the father of Danish journalism, began a state paper. In 1834, the first liberal, factual newspaper appeared, and the 1849 Constitution established lasting freedom of the press in Denmark. Newspapers flourished in the second half of the 19th century, usually tied to one or another political party or trade union. Modernisation, bringing in new features and mechanical techniques, appeared after 1900. The total circulation was 500,000 daily in 1901, more than doubling to 1.2 million in 1925. The German occupation during World War II brought informal censorship; some offending newspaper buildings were simply blown up by the Nazis. During the war, the underground produced 550 newspapers—small, surreptitiously printed sheets that encouraged sabotage and resistance.

Danish cinema dates back to 1897 and since the 1980s has maintained a steady stream of product due largely to funding by the state-supported Danish Film Institute. There have been three big internationally important waves of Danish cinema: erotic melodrama of the silent era; the increasingly explicit sex films of the 1960s and 1970s; and lastly, the Dogme 95 movement of the late 1990s, where directors often used hand-held cameras to dynamic effect in a conscious reaction against big-budget studios. Danish films have been noted for their realism, religious and moral themes, sexual frankness and technical innovation. The Danish filmmaker Carl Th. Dreyer (1889–1968) is considered one of the greatest directors of early cinema.
Other Danish filmmakers of note include Erik Balling, the creator of the popular Olsen-banden films; Gabriel Axel, an Oscar-winner for Babette's Feast in 1987; and Bille August, the Oscar-, Palme d'Or- and Golden Globe-winner for Pelle the Conqueror in 1988. In the modern era, notable filmmakers in Denmark include Lars von Trier, who co-created the Dogme movement, and multiple award-winners Susanne Bier and Nicolas Winding Refn. Mads Mikkelsen is a world-renowned Danish actor, having starred in films such as King Arthur, Casino Royale, the Danish film The Hunt, and the American TV series Hannibal. Another renowned Danish actor Nikolaj Coster-Waldau is internationally known for playing the role of Jaime Lannister in the critically acclaimed HBO series Game of Thrones.
Danish mass media and news programming are dominated by a few large corporations. In printed media JP/Politikens Hus and Berlingske Media, between them, control the largest newspapers Politiken, Berlingske Tidende and Jyllands-Posten and major tabloids B.T. and Ekstra Bladet. In television, publicly owned stations DR and TV 2 have large shares of the viewers. DR in particular is famous for its high quality TV-series often sold to foreign broadcasters and often with strong leading female characters like internationally known actresses Sidse Babett Knudsen and Sofie Gråbøl. In radio, DR has a near monopoly, currently broadcasting on all four nationally available FM channels, competing only with local stations.




Copenhagen and its multiple outlying islands have a wide range of folk traditions. The Royal Danish Orchestra is among the world's oldest orchestras. Denmark's most famous classical composer is Carl Nielsen, especially remembered for his six symphonies and his Wind Quintet, while the Royal Danish Ballet specialises in the work of the Danish choreographer August Bournonville. Danes have distinguished themselves as jazz musicians, and the Copenhagen Jazz Festival has acquired an international reputation. The modern pop and rock scene has produced a few names of note internationally, including MØ, Aqua, Lukas Graham, D-A-D, Oh Land, The Raveonettes, Michael Learns to Rock, King Diamond, Alphabeat, Kashmir, Mew and Volbeat, among others. All together, Lars Ulrich, the drummer of the band Metallica, has become the first Danish musician to be inducted into the Rock and Roll Hall of Fame.
Roskilde Festival near Copenhagen is the largest music festival in Northern Europe since 1971 and Denmark has many recurring music festivals of all genres throughout, including Aarhus International Jazz Festival, Skanderborg Festival, The Blue Festival in Aalborg, Esbjerg International Chamber Music Festiva] and Skagen Festival among many others.




Denmark's architecture became firmly established in the Middle Ages when first Romanesque, then Gothic churches and cathedrals sprang up throughout the country. From the 16th century, Dutch and Flemish designers were brought to Denmark, initially to improve the country's fortifications, but increasingly to build magnificent royal castles and palaces in the Renaissance style. During the 17th century, many impressive buildings were built in the Baroque style, both in the capital and the provinces. Neoclassicism from France was slowly adopted by native Danish architects who increasingly participated in defining architectural style. A productive period of Historicism ultimately merged into the 19th-century National Romantic style.
The 20th century brought along new architectural styles; including expressionism, best exemplified by the designs of architect Peder Vilhelm Jensen-Klint, which relied heavily on Scandinavian brick Gothic traditions; and Nordic Classicism, which enjoyed brief popularity in the early decades of the century. It was in the 1960s that Danish architects such as Arne Jacobsen entered the world scene with their highly successful Functionalist architecture. This, in turn, has evolved into more recent world-class masterpieces including Jørn Utzon's Sydney Opera House and Johan Otto von Spreckelsen's Grande Arche de la Défense in Paris, paving the way for a number of contemporary Danish designers such as Bjarke Ingels to be rewarded for excellence both at home and abroad.
Danish design is a term often used to describe a style of functionalistic design and architecture that was developed in the mid-20th century, originating in Denmark. Danish design is typically applied to industrial design, furniture and household objects, which have won many international awards. The Royal Porcelain Factory is famous for the quality of its ceramics and export products worldwide. Danish design is also a well-known brand, often associated with world-famous, 20th-century designers and architects such as Børge Mogensen, Finn Juhl, Hans Wegner, Arne Jacobsen, Poul Henningsen and Verner Panton. Other designers of note include Kristian Solmer Vedel (1923–2003) in the area of industrial design, Jens Quistgaard (1919–2008) for kitchen furniture and implements and Ole Wanscher (1903–1985) who had a classical approach to furniture design.




The first known Danish literature is myths and folklore from the 10th and 11th century. Saxo Grammaticus, normally considered the first Danish writer, worked for bishop Absalon on a chronicle of Danish history (Gesta Danorum). Very little is known of other Danish literature from the Middle Ages. With the Age of Enlightenment came Ludvig Holberg whose comedy plays are still being performed.
In the late 19th century, literature was seen as a way to influence society. Known as the Modern Breakthrough, this movement was championed by Georg Brandes, Henrik Pontoppidan (awarded the Nobel Prize in Literature) and J. P. Jacobsen. Romanticism influenced the renowned writer and poet Hans Christian Andersen, known for his stories and fairy tales, e.g. The Ugly Duckling, The Little Mermaid and The Snow Queen. In recent history Johannes Vilhelm Jensen was also awarded the Nobel Prize for Literature. Karen Blixen is famous for her novels and short stories. Other Danish writers of importance are Herman Bang, Gustav Wied, William Heinesen, Martin Andersen Nexø, Piet Hein, Hans Scherfig, Klaus Rifbjerg, Dan Turèll, Tove Ditlevsen, Inger Christensen and Peter Høeg.
Danish philosophy has a long tradition as part of Western philosophy. Perhaps the most influential Danish philosopher was Søren Kierkegaard, the creator of Christian existentialism. Kierkegaard had a few Danish followers, including Harald Høffding, who later in his life moved on to join the movement of positivism. Among Kierkegaard's other followers include Jean-Paul Sartre who was impressed with Kierkegaard's views on the individual, and Rollo May, who helped create humanistic psychology. Another Danish philosopher of note is Grundtvig, whose philosophy gave rise to a new form of non-aggressive nationalism in Denmark, and who is also influential for his theological and historical works.




While Danish art was influenced over the centuries by trends in Germany and the Netherlands, the 15th- and 16th-century church frescos, which can be seen in many of the country's older churches, are of particular interest as they were painted in a style typical of native Danish painters.
The Danish Golden Age, which began in the first half of the 19th century, was inspired by a new feeling of nationalism and romanticism, typified in the later previous century by history painter Nicolai Abildgaard. Christoffer Wilhelm Eckersberg was not only a productive artist in his own right but taught at the Royal Danish Academy of Fine Arts where his students included notable painters such as Wilhelm Bendz, Christen Købke, Martinus Rørbye, Constantin Hansen, and Wilhelm Marstrand.
In 1871, Holger Drachmann and Karl Madsen visited Skagen in the far north of Jutland where they quickly built up one of Scandinavia's most successful artists' colonies specialising in Naturalism and Realism rather than in the traditional approach favoured by the Academy. Hosted by Michael and his wife Anna, they were soon joined by P.S. Krøyer, Carl Locher and Laurits Tuxen. All participated in painting the natural surroundings and local people. Similar trends developed on Funen with the Fynboerne who included Johannes Larsen, Fritz Syberg and Peter Hansen, and on the island of Bornholm with the Bornholm school of painters including Niels Lergaard, Kræsten Iversen and Oluf Høst.
Painting has continued to be a prominent form of artistic expression in Danish culture, inspired by and also influencing major international trends in this area. These include impressionism and the modernist styles of expressionism, abstract painting and surrealism. While international co-operation and activity has almost always been essential to the Danish artistic community, influential art collectives with a firm Danish base includes De Tretten (1909–1912), Linien (1930s and 1940s), COBRA (1948–51), Fluxus (1960s and 1970s), De Unge Vilde (1980s) and more recently Superflex (founded in 1993). Most Danish painters of modern times have also been very active with other forms of artistic expressions, such as sculpting, ceramics, art installations, activism, film and experimental architecture. Notable Danish painters from modern times representing various art movements include Theodor Philipsen (1840–1920, impressionism and naturalism), Anna Klindt Sørensen (1899–1985, expressionism), Franciska Clausen (1899–1986, Neue Sachlichkeit, cubism, surrealism and others), Henry Heerup (1907–1993, naivism), Robert Jacobsen (1912–1993, abstract painting), Carl Henning Pedersen (1913–2007, abstract painting), Asger Jorn (1914–1973, Situationist, abstract painting), Bjørn Wiinblad (1918–2006, art deco, orientalism), Per Kirkeby (b. 1938, neo-expressionism, abstract painting), Per Arnoldi (b. 1941, pop art), Michael Kvium (b. 1955, neo-surrealism) and Simone Aaberg Kærn (b. 1969, superrealism).
Danish photography has developed from strong participation and interest in the very beginnings of the art of photography in 1839 to the success of a considerable number of Danes in the world of photography today. Pioneers such as Mads Alstrup and Georg Emil Hansen paved the way for a rapidly growing profession during the last half of the 19th century. Today Danish photographers such as Astrid Kruse Jensen and Jacob Aue Sobol are active both at home and abroad, participating in key exhibitions around the world.




The traditional cuisine of Denmark, like that of the other Nordic countries and of Northern Germany, consists mainly of meat, fish and potatoes. Danish dishes are highly seasonal, stemming from the country's agricultural past, its geography, and its climate of long, cold winters.
The open sandwiches, known as smørrebrød, which in their basic form are the usual fare for lunch, can be considered a national speciality when prepared and decorated with a variety of fine ingredients. Hot meals traditionally consist of ground meats, such as frikadeller (meat balls of veal and pork) and hakkebøf (minced beef patties), or of more substantial meat and fish dishes such as flæskesteg (roast pork with crackling) and kogt torsk (poached cod) with mustard sauce and trimmings. Denmark is known for its Carlsberg and Tuborg beers and for its akvavit and bitters.
Since around 1970, chefs and restaurants across Denmark have introduced gourmet cooking, largely influenced by French cuisine. Also inspired by continental practices, Danish chefs have recently developed a new innovative cuisine and a series of gourmet dishes based on high-quality local produce known as New Danish cuisine. As a result of these developments, Denmark now have a considerable number of internationally acclaimed restaurants of which several have been awarded Michelin stars. This includes Geranium and Noma in Copenhagen.




Sports are popular in Denmark, and its citizens participate in and watch a wide variety. The national sport is football (soccer), with over 320,000 players in more than 1600 clubs. Denmark qualified six times consecutively for the European Championships between 1984 and 2004, and were crowned European champions in 1992; other significant achievements include winning the Confederations Cup in 1995 and reaching the quarter-final of the 1998 World Cup. Notable Danish footballers include Allan Simonsen, named the best player in Europe in 1977, Peter Schmeichel, named the "World's Best Goalkeeper" in 1992 and 1993, and Michael Laudrup, named the best Danish player of all time by the Danish Football Association.
There is much focus on handball, too. The women's national team celebrated great successes during the 1990s. On the men's side, Denmark has won eight medals—two gold (in 2008 and 2012), three silver (in 2011, 2013 and 2014) and three bronze (in 2002, 2004 and 2006)—the most that have been won by any team in European Handball Championship history.
In recent years, Denmark has made a mark as a strong cycling nation, with Michael Rasmussen reaching King of the Mountains status in the Tour de France in 2005 and 2006. Other popular sports include golf—which is mostly popular among those in the older demographic; tennis—in which Denmark is successful on a professional level; basketball—Denmark joined the international governing body FIBA in 1951; rugby—the Danish Rugby Union dates back to 1950; hockey— often competing in the top division in the Men's World Championships; rowing—Denmark specialise in lightweight rowing and are particularly known for their lightweight coxless four, having won six gold and two silver World Championship medals and three gold and two bronze Olympic medals; and several indoor sports—especially badminton, table tennis and gymnastics, in each of which Denmark holds World Championships and Olympic medals. Denmark's numerous beaches and resorts are popular locations for fishing, canoeing, kayaking, and many other water-themed sports.



Index of Denmark-related articles
Denmark gives its name to the Danian Age of the Paleocene Epoch of geological time
Outline of Denmark







Bibliography
Stone, Andrew; Bain, Carolyn; Booth, Michael; Parnell, Fran (2008). Denmark (5th ed.). Footscray, Victoria: Lonely Planet. p. 31. ISBN 9781741046694. 
(Danish) Busck, Steen and Poulsen, Henning (ed.), "Danmarks historie  – i grundtræk", Aarhus Universitetsforlag, 2002, ISBN 87-7288-941-1
Gammelgaard, Frederik; Sørensen, Niels (1998). Danmark – en demokratisk stat (in Danish). Alinea. ISBN 87-23-00280-8. 
Jørgensen, Gitte (1995). Sådan styres Danmark (in Danish). Flachs. ISBN 87-7826-031-0. 
(Danish) Michaelsen, Karsten Kjer, "Politikens bog om Danmarks oldtid", Politikens Forlag (1. bogklubudgave), 2002, ISBN 87-00-69328-6
(Swedish) Nationalencyklopedin, vol. 4, Bokförlaget Bra Böcker, 2000, ISBN 91-7024-619-X.




Denmark.dk
"Denmark". The World Factbook. Central Intelligence Agency. 
Denmark entry at Encyclopædia Britannica.
A guide to Danish Culture at Denmark.net.
Denmark at UCB Libraries GovPubs.
Denmark at DMOZ
Denmark profile from the BBC News.
Tourism portal at VisitDenmark.
Key Development Forecasts for Denmark from International Futures.
Government
Ministry of Foreign Affairs of Denmark
Summary vital statistics about Denmark from Statistikbanken.
Maps
 Wikimedia Atlas of Denmark
 Geographic data related to Denmark at OpenStreetMap
Satellite image of Denmark at the NASA Earth Observatory.
Trade
World Bank Summary Trade Statistics Denmark
News and media
Google news Denmark
History of Denmark: Primary Documents
(Danish) Krak printable mapsearch
(Swedish) (English) Ministry of the Environment National Survey and Cadastre
Old Denmark in Cyberspace – Information about Denmark – the Danes at the Wayback Machine (archived 8 February 2006)
Other
Vifanord.de – library of scientific information on the Nordic and Baltic countries.Japan (Japanese: 日本 Nippon [nip̚põ̞ɴ] or Nihon [nihõ̞ɴ]; formally 日本国  Nippon-koku or Nihon-koku, "State of Japan") is an island nation in East Asia. Located in the Pacific Ocean, it lies to the east of the Sea of Japan, the East China Sea, China, Korea and Russia, stretching from the Sea of Okhotsk in the north to the East China Sea and Taiwan in the southwest. The kanji that make up Japan's name mean "sun origin", and it is often called the "Land of the Rising Sun".
Japan is a stratovolcanic archipelago of 6,852 islands. The four largest are Honshu, Hokkaido, Kyushu and Shikoku, which make up about ninety-seven percent of Japan's land area. The country is divided into 47 prefectures in eight regions. The population of 127 million is the world's tenth largest. Japanese people make up 98.5% of Japan's total population. Approximately 9.1 million people live in the core city of Tokyo, the capital of Japan.
Archaeological research indicates that Japan was inhabited as early as the Upper Paleolithic period. The first written mention of Japan is in Chinese history texts from the 1st century AD. Influence from other regions, mainly China, followed by periods of isolation, particularly from Western Europe, has characterized Japan's history. From the 12th century until 1868, Japan was ruled by successive feudal military shoguns who ruled in the name of the Emperor.
Japan entered into a long period of isolation in the early 17th century, which was ended in 1853 when a United States fleet pressured Japan to open to the West. After nearly two decades of internal conflict and insurrection, the Imperial Court regained its political power in 1868 through the help of several clans from Chōshū and Satsuma, and the Empire of Japan was established. In the late 19th and early 20th centuries, victories in the First Sino-Japanese War, the Russo-Japanese War and World War I allowed Japan to expand its empire during a period of increasing militarism.
The Second Sino-Japanese War of 1937 expanded into part of World War II in 1941, which came to an end in 1945 following the atomic bombings of Hiroshima and Nagasaki. Since adopting its revised constitution in 1947, Japan has maintained a unitary constitutional monarchy with an Emperor and an elected legislature called the National Diet.
Japan is a member of the UN, the G7, the G8, and the G20 and is considered a great power. The country has the world's third-largest economy by nominal GDP and the world's fourth-largest economy by purchasing power parity. It is also the world's fourth-largest exporter and fourth-largest importer. The country benefits from a highly skilled workforce and is among the most highly educated countries in the world with one of the highest percentages of its citizens holding a tertiary education degree.
Although Japan has officially renounced its right to declare war, it maintains a modern military with the world's eighth largest military budget, used for self-defense and peacekeeping roles. Japan is a developed country with a very high standard of living and Human Development Index whose population enjoys the highest life expectancy and the third lowest infant mortality in the world.




In ancient Chinese geography, Japan was called 倭 (pronounced wa in Japanese). An ancient Chinese history book from the Tang dynasty refers to Japan as "Wa". Mention of wa also occurs in China's 'Sangoku-shi' (三国志) in the section commonly referred to as Gi-shi Wajin-den, which the 'Romance of the Three Kingdoms' is based on. This character means obedient, gentle, or meek. The ancient Japanese, however, hated the name because it resembled another character, 矮, meaning 'dwarf'. The first "Wa" kanji was later replaced with 和 (wa) meaning "harmony".
The Japanese word for Japan is 日本, which is pronounced Nippon or Nihon. The earliest record of the name "Nihon" appears in the Chinese historical records of the Tang dynasty, the Old Book of Tang, kutōjo () in Japanese. At the start of the seventh century, a delegation from Japan introduced their country as Nihon. Prince Shotoku, the Regent of Japan, sent a mission to China with a letter in which he called himself 'the Emperor of the Land in which the Sun rises'. Thus Nihon might have originated in this period. The reading of the message in Japanese is:

Hi iduru tokoro no Tenshi, Sho wo Hi bossuru tokoro no Tenshi ni itasu. Tsutsuga nakiya?

which means

"The Emperor of the land where Sun rises sends a letter to the Emperor of the land where Sun sets. Are you healthy?"

This letter was sent in the early period of the 7th century, either 605, 608 or 612. The message is recorded in the official history book of the Sui dynasty.
The English word Japan possibly derives from the historical Chinese pronunciation of 日本. The Old Mandarin or possibly early Wu Chinese pronunciation of Japan was recorded by Marco Polo as Cipangu. In modern Shanghainese, a Wu dialect, the pronunciation of characters 日本 Japan is Zeppen [zəʔpən]. The old Malay word for Japan, Jepang, was borrowed from a southern coastal Chinese dialect, probably Fukienese or Ningpo, and this Malay word was encountered by Portuguese traders in Malacca in the 16th century. Early Portuguese traders then brought the word to Europe. An early record of the word in English is in a 1565 letter, spelled Giapan.
From the Meiji Restoration until the end of World War II, the full title of Japan was Dai Nippon Teikoku (大日本帝國), meaning "the Empire of Great Japan". Today the name Nippon-koku / Nihon-koku (日本国) is used as a formal modern-day equivalent simply meaning "the State of Japan"; countries like Japan whose long form does not contain a descriptive designation are generally given a name appended by the character koku (国), meaning "country", "nation" or "state".
The character nichi (日) means "sun" or "day"; hon (本) means "base" or "origin". The compound means "origin of the sun" or "sunrise", and is the source of the popular Western epithet "Land of the Rising Sun". The reason Japan refers to itself in this way is that Japan is east of China, and from China the sun rises from Japan.







A Paleolithic culture around 30,000 BC constitutes the first known habitation of the Japanese archipelago. This was followed from around 14,000 BC (the start of the Jōmon period) by a Mesolithic to Neolithic semi-sedentary hunter-gatherer culture, who include ancestors of both the contemporary Ainu people and Yamato people, characterized by pit dwelling and rudimentary agriculture. Decorated clay vessels from this period are some of the oldest surviving examples of pottery in the world. Around 300 BC, the Yayoi people began to enter the Japanese islands, intermingling with the Jōmon. The Yayoi period, starting around 500 BC, saw the introduction of practices like wet-rice farming, a new style of pottery, and metallurgy, introduced from China and Korea.
Japan first appears in written history in the Chinese Book of Han. According to the Records of the Three Kingdoms, the most powerful kingdom on the archipelago during the 3rd century was called Yamataikoku. Buddhism was first introduced to Japan from Baekje, Korea and was promoted by Prince Shōtoku, but the subsequent development of Japanese Buddhism was primarily influenced by China. Despite early resistance, Buddhism was promoted by the ruling class and gained widespread acceptance beginning in the Asuka period (592–710).
The Nara period (710–784) of the 8th century marked an emergence of the centralized Japanese state centered on the Imperial Court in Heijō-kyō (modern Nara). The Nara period is characterized by the appearance of a nascent literature as well as the development of Buddhist-inspired art and architecture. The smallpox epidemic of 735–737 is believed to have killed as much as one-third of Japan's population. In 784, Emperor Kanmu moved the capital from Nara to Nagaoka-kyō before relocating it to Heian-kyō (modern Kyoto) in 794.
This marked the beginning of the Heian period (794–1185), during which a distinctly indigenous Japanese culture emerged, noted for its art, poetry and prose. Murasaki Shikibu's The Tale of Genji and the lyrics of Japan's national anthem "Kimigayo" were written during this time.
Buddhism began to spread during the Heian era chiefly through two major sects, Tendai by Saichō, and Shingon by Kūkai. Pure Land Buddhism (Jōdo-shū, Jōdo Shinshū) became greatly popular in the latter half of the 11th century.




Japan's feudal era was characterized by the emergence and dominance of a ruling class of warriors, the samurai. In 1185, following the defeat of the Taira clan in the Genpei War, sung in the epic Tale of Heike, samurai Minamoto no Yoritomo was appointed shogun by Emperor Go-Toba, and he established a base of power in Kamakura. After his death, the Hōjō clan came to power as regents for the shoguns. The Zen school of Buddhism was introduced from China in the Kamakura period (1185–1333) and became popular among the samurai class. The Kamakura shogunate repelled Mongol invasions in 1274 and 1281, but was eventually overthrown by Emperor Go-Daigo. Emperor Go-Daigo was himself defeated by Ashikaga Takauji in 1336.

Ashikaga Takauji established the shogunate in Muromachi, Kyoto. This was the start of the Muromachi period (1336–1573). The Ashikaga shogunate achieved glory in the age of Ashikaga Yoshimitsu, and the culture based on Zen Buddhism (art of Miyabi) prospered. This evolved to Higashiyama Culture, and prospered until the 16th century. On the other hand, the succeeding Ashikaga shogunate failed to control the feudal warlords (daimyōs), and a civil war (the Ōnin War) began in 1467, opening the century-long Sengoku period ("Warring States").
During the 16th century, traders and Jesuit missionaries from Portugal reached Japan for the first time, initiating direct commercial and cultural exchange between Japan and the West. This allowed Oda Nobunaga to obtain European technology and firearms, which he used to conquer many other daimyōs. His consolidation of power began what was known as the Azuchi–Momoyama period (1573–1603). After he was assassinated in 1582, his successor Toyotomi Hideyoshi unified the nation in 1590 and launched two unsuccessful invasions of Korea in 1592 and 1597.
Tokugawa Ieyasu served as regent for Hideyoshi's son and used his position to gain political and military support. When open war broke out, he defeated rival clans in the Battle of Sekigahara in 1600. Tokugawa Ieyasu was appointed shogun by Emperor Go-Yōzei in 1603, and he established the Tokugawa shogunate in Edo (modern Tokyo). The Tokugawa shogunate enacted measures including buke shohatto, as a code of conduct to control the autonomous daimyōs; and in 1639, the isolationist sakoku ("closed country") policy that spanned the two and a half centuries of tenuous political unity known as the Edo period (1603–1868). The study of Western sciences, known as rangaku, continued through contact with the Dutch enclave at Dejima in Nagasaki. The Edo period also gave rise to kokugaku ("national studies"), the study of Japan by the Japanese.




On March 31, 1854, Commodore Matthew Perry and the "Black Ships" of the United States Navy forced the opening of Japan to the outside world with the Convention of Kanagawa. Subsequent similar treaties with Western countries in the Bakumatsu period brought economic and political crises. The resignation of the shogun led to the Boshin War and the establishment of a centralized state nominally unified under the Emperor (the Meiji Restoration).
Adopting Western political, judicial and military institutions, the Cabinet organized the Privy Council, introduced the Meiji Constitution, and assembled the Imperial Diet. The Meiji Restoration transformed the Empire of Japan into an industrialized world power that pursued military conflict to expand its sphere of influence. After victories in the First Sino-Japanese War (1894–1895) and the Russo-Japanese War (1904–1905), Japan gained control of Taiwan, Korea, and the southern half of Sakhalin. Japan's population grew from 35 million in 1873 to 70 million in 1935.

World War I enabled Japan, on the side of the victorious Allies, to widen its influence and territorial holdings in Asia. The early 20th century saw a brief period of "Taishō democracy (1912–1926)" but the 1920s saw a fragile democracy buckle under a political shift towards fascism, the passing of laws against political dissent and a series of attempted coups. The subsequent "Shōwa period" initially saw the power of the military increased and brought about Japanese expansionism and militarization along with the totalitarianism and ultranationalism that are a part of fascist ideology. In 1931 Japan invaded and occupied Manchuria and following international condemnation of this occupation, Japan resigned from the League of Nations in 1933. In 1936, Japan signed the Anti-Comintern Pact with Germany, and the 1940 Tripartite Pact made it one of the Axis Powers. In 1941, following its defeat in the brief Soviet–Japanese Border War, Japan negotiated the Soviet–Japanese Neutrality Pact, which lasted until 1945 with the Soviet invasion of Manchuria.

The Empire of Japan invaded other parts of China in 1937, precipitating the Second Sino-Japanese War (1937–1945). The Imperial Japanese Army swiftly captured the capital Nanjing and conducted the Nanking Massacre. In 1940, the Empire then invaded French Indochina, after which the United States placed an oil embargo on Japan. On December 7–8, 1941, Japanese forces carried out surprise attacks on Pearl Harbor, British forces in Malaya, Singapore, and Hong Kong and declared war on the United States and the British Empire, bringing the US and the UK into World War II in the Pacific. After Allied victories across the Pacific during the next four years, which culminated in the Soviet invasion of Manchuria and the atomic bombings of Hiroshima and Nagasaki in 1945, Japan agreed to an unconditional surrender on August 15. The war cost Japan, its colonies, China and the war's other combatants tens of millions of lives and left much of Japan's industry and infrastructure destroyed. The Allies (led by the US) repatriated millions of ethnic Japanese from colonies and military camps throughout Asia, largely eliminating the Japanese empire and restoring the independence of its conquered territories. The Allies also convened the International Military Tribunal for the Far East on May 3, 1946 to prosecute some Japanese leaders for war crimes. However, the bacteriological research units and members of the imperial family involved in the war were exonerated from criminal prosecutions by the Supreme Commander for the Allied Powers despite calls for the trial of both groups.
In 1947, Japan adopted a new constitution emphasizing liberal democratic practices. The Allied occupation ended with the Treaty of San Francisco in 1952 and Japan was granted membership in the United Nations in 1956. Japan later achieved rapid growth to become the second-largest economy in the world, until surpassed by China in 2010. This ended in the mid-1990s when Japan suffered a major recession. In the beginning of the 21st century, positive growth has signaled a gradual economic recovery. On March 11, 2011, Japan suffered the strongest earthquake in its recorded history; this triggered the Fukushima Daiichi nuclear disaster, one of the worst disasters in the history of nuclear power.




Japan has a total of 6,852 islands extending along the Pacific coast of East Asia. The country, including all of the islands it controls, lies between latitudes 24° and 46°N, and longitudes 122° and 146°E. The main islands, from north to south, are Hokkaido, Honshu, Shikoku and Kyushu. The Ryukyu Islands, which include Okinawa, are a chain to the south of Kyushu. Together they are often known as the Japanese archipelago.
About 73 percent of Japan is forested, mountainous, and unsuitable for agricultural, industrial, or residential use. As a result, the habitable zones, mainly located in coastal areas, have extremely high population densities. Japan is one of the most densely populated countries in the world.
The islands of Japan are located in a volcanic zone on the Pacific Ring of Fire. They are primarily the result of large oceanic movements occurring over hundreds of millions of years from the mid-Silurian to the Pleistocene as a result of the subduction of the Philippine Sea Plate beneath the continental Amurian Plate and Okinawa Plate to the south, and subduction of the Pacific Plate under the Okhotsk Plate to the north. The Boso Triple Junction off the coast of Japan is a triple junction where the North American Plate, the Pacific Plate and the Philippine Sea Plate meets. Japan was originally attached to the eastern coast of the Eurasian continent. The subducting plates pulled Japan eastward, opening the Sea of Japan around 15 million years ago.
Japan has 108 active volcanoes. During the twentieth century several new volcanoes emerged, including Shōwa-shinzan on Hokkaido and Myōjin-shō off the Bayonnaise Rocks in the Pacific. Destructive earthquakes, often resulting in tsunami, occur several times each century. The 1923 Tokyo earthquake killed over 140,000 people. More recent major quakes are the 1995 Great Hanshin earthquake and the 2011 Tōhoku earthquake, a 9.0-magnitude quake which hit Japan on March 11, 2011, and triggered a large tsunami. Japan is substantially prone to earthquakes, tsunami and volcanoes due to its location along the Pacific Ring of Fire. It has the 15th highest natural disaster risk as measured in the 2013 World Risk Index.




The climate of Japan is predominantly temperate, but varies greatly from north to south. Japan's geographical features divide it into six principal climatic zones: Hokkaido, Sea of Japan, Central Highland, Seto Inland Sea, Pacific Ocean, and Ryukyu Islands. The northernmost zone, Hokkaido, has a humid continental climate with long, cold winters and very warm to cool summers. Precipitation is not heavy, but the islands usually develop deep snowbanks in the winter.
In the Sea of Japan zone on Honshu's west coast, northwest winter winds bring heavy snowfall. In the summer, the region is cooler than the Pacific area, though it sometimes experiences extremely hot temperatures because of the foehn. The Central Highland has a typical inland humid continental climate, with large temperature differences between summer and winter seasons, as well as large diurnal variation; precipitation is light, though winters are usually snowy. The mountains of the Chūgoku and Shikoku regions shelter the Seto Inland Sea from seasonal winds, bringing mild weather year-round.
The Pacific coast features a humid subtropical climate that experiences milder winters with occasional snowfall and hot, humid summers because of the southeast seasonal wind. The Ryukyu Islands have a subtropical climate, with warm winters and hot summers. Precipitation is very heavy, especially during the rainy season.
The average winter temperature in Japan is 5.1 °C (41.2 °F) and the average summer temperature is 25.2 °C (77.4 °F). The highest temperature ever measured in Japan 40.9 °C (105.6 °F) was recorded on August 16, 2007. The main rainy season begins in early May in Okinawa, and the rain front gradually moves north until reaching Hokkaido in late July. In most of Honshu, the rainy season begins before the middle of June and lasts about six weeks. In late summer and early autumn, typhoons often bring heavy rain.




Japan has nine forest ecoregions which reflect the climate and geography of the islands. They range from subtropical moist broadleaf forests in the Ryūkyū and Bonin Islands, to temperate broadleaf and mixed forests in the mild climate regions of the main islands, to temperate coniferous forests in the cold, winter portions of the northern islands. Japan has over 90,000 species of wildlife, including the brown bear, the Japanese macaque, the Japanese raccoon dog, the Large Japanese Field Mouse, and the Japanese giant salamander. A large network of national parks has been established to protect important areas of flora and fauna as well as thirty-seven Ramsar wetland sites. Four sites have been inscribed on the UNESCO World Heritage List for their outstanding natural value.




In the period of rapid economic growth after World War II, environmental policies were downplayed by the government and industrial corporations; as a result, environmental pollution was widespread in the 1950s and 1960s. Responding to rising concern about the problem, the government introduced several environmental protection laws in 1970. The oil crisis in 1973 also encouraged the efficient use of energy because of Japan's lack of natural resources. Current environmental issues include urban air pollution (NOx, suspended particulate matter, and toxics), waste management, water eutrophication, nature conservation, climate change, chemical management and international co-operation for conservation.
As of June 2015, more than 40 coal-fired power plants are planned or under construction in Japan. The NGO Climate Action Network announced Japan as the winner of its "Fossil of the Day" award for "doing the most to block progress on climate action."
Japan ranks 39th in the 2016 Environmental Performance Index, which measures a nation's commitment to environmental sustainability. As a signatory of the Kyoto Protocol, and host of the 1997 conference that created it, Japan is under treaty obligation to reduce its carbon dioxide emissions and to take other steps to curb climate change.







Japan is a constitutional monarchy whereby the power of the Emperor is very limited. As a ceremonial figurehead, he is defined by the constitution as "the symbol of the State and of the unity of the people." Power is held chiefly by the Prime Minister and other elected members of the Diet, while sovereignty is vested in the Japanese people. Akihito is the current Emperor of Japan; Naruhito, Crown Prince of Japan, stands as next in line to the Chrysanthemum Throne.

Japan's legislative organ is the National Diet, seated in Chiyoda, Tokyo. The Diet is a bicameral body, consisting of a House of Representatives with 480 seats, elected by popular vote every four years or when dissolved, and a House of Councillors of 242 seats, whose popularly elected members serve six-year terms. There is universal suffrage for adults over 18 years of age, with a secret ballot for all elected offices. The Diet is dominated by the social liberal Democratic Party of Japan and the conservative Liberal Democratic Party (LDP). The LDP has enjoyed near continuous electoral success since 1955, except for a brief 11-month period between 1993 and 1994, and from 2009 to 2012. As of September 2016, it holds 291 seats in the lower house and 122 seats in the upper house.
The Prime Minister of Japan is the head of government and is appointed by the Emperor after being designated by the Diet from among its members. The Prime Minister is the head of the Cabinet, and he appoints and dismisses the Ministers of State. Following the LDP's landslide victory in the 2012 general election, Shinzō Abe replaced Yoshihiko Noda as the Prime Minister on December 26, 2012 and became the country's sixth prime minister to be sworn in during a span of six years. Although the Prime Minister is formally appointed by the Emperor, the Constitution of Japan explicitly requires the Emperor to appoint whoever is designated by the Diet.
Historically influenced by Chinese law, the Japanese legal system developed independently during the Edo period through texts such as Kujikata Osadamegaki. However, since the late 19th century the judicial system has been largely based on the civil law of Europe, notably Germany. For example, in 1896, the Japanese government established a civil code based on a draft of the German Bürgerliches Gesetzbuch; with the code remaining in effect with post–World War II modifications. Statutory law originates in Japan's legislature and has the rubber stamp of the Emperor. The Constitution requires that the Emperor promulgate legislation passed by the Diet, without specifically giving him the power to oppose legislation. Japan's court system is divided into four basic tiers: the Supreme Court and three levels of lower courts. The main body of Japanese statutory law is called the Six Codes.




Japan consists of 47 prefectures, each overseen by an elected governor, legislature and administrative bureaucracy. Each prefecture is further divided into cities, towns and villages. The nation is currently undergoing administrative reorganization by merging many of the cities, towns and villages with each other. This process will reduce the number of sub-prefecture administrative regions and is expected to cut administrative costs.




Japan has diplomatic relations with nearly all independent nations and has been an active member of the UN since December 1956. Japan is a member of the G8, APEC, and "ASEAN Plus Three", and is a participant in the East Asia Summit. Japan signed a security pact with Australia in March 2007 and with India in October 2008. It is the world's fifth largest donor of official development assistance, donating US$9.2 billion in 2014.
Japan has close ties to the United States. Since Japan's defeat by the United States in World War II, the two countries have maintained close economic and defense relations. The United States is a major market for Japanese exports and the primary source of Japanese imports, and is committed to defending the country, having military bases in Japan for that purpose.
Japan contests Russia's control of the Southern Kuril Islands (including Etorofu, Kunashiri, Shikotan, and the Habomai group) which were occupied by the Soviet Union in 1945. South Korea's assertions concerning Liancourt Rocks (Japanese: "Takeshima", Korean: "Dokdo") are acknowledged, but not accepted by Japan. Japan has strained relations with the People's Republic of China (PRC) and the Republic of China (Taiwan) over the Senkaku Islands; and with the People's Republic of China over the status of Okinotorishima.
Japan's relationship with South Korea has been strained due to Japan's treatment of Koreans during Japanese colonial rule, particularly over the issue of comfort women. However, in December 2015, Japan and South Korea agreed to settle the issue with Japan issuing a formal apology and taking responsibility for the issue and paying money to the surviving comfort women.




Japan maintains one of the largest military budgets of any country in the world. The country's military (the Japan Self-Defense Forces) is restricted by Article 9 of the Japanese Constitution, which renounces Japan's right to declare war or use military force in international disputes. Accordingly, Japan's Self-Defence force is an unusual military that has never fired shots outside Japan. Japan is the highest-ranked Asian country in the Global Peace Index. The military is governed by the Ministry of Defense, and primarily consists of the Japan Ground Self-Defense Force (JGSDF), the Japan Maritime Self-Defense Force (JMSDF) and the Japan Air Self-Defense Force (JASDF). The Japan Maritime Self-Defense Force (JMSDF) is a regular participant in RIMPAC maritime exercises. The forces have been recently used in peacekeeping operations; the deployment of troops to Iraq marked the first overseas use of Japan's military since World War II. Japan Business Federation has called on the government to lift the ban on arms exports so that Japan can join multinational projects such as the Joint Strike Fighter.
The 21st century is witnessing a rapid change in global power balance along with globalization. The security environment around Japan has become increasingly severe as represented by nuclear and missile development by North Korea. Transnational threats grounded on technological progress including international terrorism and cyber attacks are also increasing their significance. Japan, including its Self Defense Forces, has contributed to the maximum extent possible to the efforts to maintain and restore international peace and security, such as UN peacekeeping operations. Building on the ongoing efforts as a peaceful state, the Government of Japan has been making various efforts on its security policy which include: the establishment of the National Security Council (NSC), the adoption of the National Security Strategy (NSS), and the National Defense Program Guidelines (NDPG). These efforts are made based on the belief that Japan, as a "Proactive Contributor to Peace", needs to contribute more actively to the peace and stability of the region and the international community, while coordinating with other countries including its ally, the United States.
Japan has close economic and military relations with the United States; the US-Japan security alliance acts as the cornerstone of the nation's foreign policy. A member state of the United Nations since 1956, Japan has served as a non-permanent Security Council member for a total of 20 years, most recently for 2009 and 2010. It is one of the G4 nations seeking permanent membership in the Security Council.
In May 2014, Prime Minister Shinzō Abe said Japan wanted to shed the passiveness it has maintained since the end of World War II and take more responsibility for regional security. He said Japan wanted to play a key role and offered neighboring countries Japan's support. In recent years, they have been engaged in international peacekeeping operations including the UN peacekeeping. Recent tensions, particularly with North Korea, have reignited the debate over the status of the JSDF and its relation to Japanese society. New military guidelines, announced in December 2010, will direct the JSDF away from its Cold War focus on the former Soviet Union to a focus on China, especially regarding the territorial dispute over the Senkaku Islands.




Japan is the third largest national economy in the world, after the United States and China, in terms of nominal GDP, and the fourth largest national economy in the world, after the United States, China and India, in terms of purchasing power parity. As of 2014, Japan's public debt was estimated at more than 200 percent of its annual gross domestic product, the largest of any nation in the world. In August 2011, Moody's rating has cut Japan's long-term sovereign debt rating one notch from Aa3 to Aa2 inline with the size of the country's deficit and borrowing level. The large budget deficits and government debt since the 2009 global recession and followed by the earthquake and tsunami in March 2011 caused the rating downgrade. The service sector accounts for three quarters of the gross domestic product.
Japan has a large industrial capacity, and is home to some of the largest and most technologically advanced producers of motor vehicles, electronics, machine tools, steel and nonferrous metals, ships, chemical substances, textiles, and processed foods. Agricultural businesses in Japan cultivate 13 percent of Japan's land, and Japan accounts for nearly 15 percent of the global fish catch, second only to China. As of 2010, Japan's labor force consisted of some 65.9 million workers. Japan has a low unemployment rate of around four percent. Some 20 million people, around 17 per cent of the population, were below the poverty line in 2007. Housing in Japan is characterized by limited land supply in urban areas.
Japan's exports amounted to US$4,210 per capita in 2005. As of 2012, Japan's main export markets were China (18.1 percent), the United States (17.8 percent), South Korea (7.7 percent), Thailand (5.5 percent) and Hong Kong (5.1 percent). Its main exports are transportation equipment, motor vehicles, iron and steel products, semiconductors and auto parts. Japan's main import markets as of 2012 were China (21.3 percent), the US (8.8 percent), Australia (6.4 percent), Saudi Arabia (6.2 percent), United Arab Emirates (5.0 percent), South Korea (4.6 percent) and Qatar (4.0 percent).
Japan's main imports are machinery and equipment, fossil fuels, foodstuffs (in particular beef), chemicals, textiles and raw materials for its industries. By market share measures, domestic markets are the least open of any OECD country. Junichirō Koizumi's administration began some pro-competition reforms, and foreign investment in Japan has soared.
Japan ranks 27th of 189 countries in the 2014 Ease of doing business index and has one of the smallest tax revenues of the developed world. The Japanese variant of capitalism has many distinct features: keiretsu enterprises are influential, and lifetime employment and seniority-based career advancement are relatively common in the Japanese work environment. Japanese companies are known for management methods like "The Toyota Way", and shareholder activism is rare.



Modern Japan's economic growth began in the Edo period. Some of the surviving elements of the Edo period are roads and water transportation routes, as well as financial instruments such as futures contracts, banking and insurance of the Osaka rice brokers. During the Meiji period from 1868, Japan expanded economically with the embrace of the market economy. Many of today's enterprises were founded at the time, and Japan emerged as the most developed nation in Asia. The period of overall real economic growth from the 1960s to the 1980s has been called the Japanese post-war economic miracle: it averaged 7.5 percent in the 1960s and 1970s, and 3.2 percent in the 1980s and early 1990s.
Growth slowed in the 1990s during the "Lost Decade" due to after-effects of the Japanese asset price bubble and government policies intended to wring speculative excesses from the stock and real estate markets. Efforts to revive economic growth were unsuccessful and further hampered by the global slowdown in 2000. The economy recovered after 2005; GDP growth for that year was 2.8 percent, surpassing the growth rates of the US and European Union during the same period.
Today Japan ranks highly for competitiveness and economic freedom. It is ranked sixth in the Global Competitiveness Report for 2015–2016.




The Japanese agricultural sector accounts for about 1.4% of the total country's GDP. Only 12% of Japan's land is suitable for cultivation. Due to this lack of arable land, a system of terraces is used to farm in small areas. This results in one of the world's highest levels of crop yields per unit area, with an overall agricultural self-sufficiency rate of about 50% on fewer than 56,000 square kilometres (14,000,000 acres) cultivated.
Japan's small agricultural sector, however, is also highly subsidized and protected, with government regulations that favor small-scale cultivation instead of large-scale agriculture as practiced in North America. There has been a growing concern about farming as the current farmers are aging with a difficult time finding successors.
Rice accounts for almost all of Japan's cereal production. Japan is the second-largest agricultural product importer in the world. Rice, the most protected crop, is subject to tariffs of 777.7%.
In 1996, Japan ranked fourth in the world in tonnage of fish caught. Japan captured 4,074,580 metric tons of fish in 2005, down from 4,987,703 tons in 2000, 9,558,615 tons in 1990, 9,864,422 tons in 1980, 8,520,397 tons in 1970, 5,583,796 tons in 1960 and 2,881,855 tons in 1950. In 2003, the total aquaculture production was predicted at 1,301,437 tonnes. In 2010, Japan's total fisheries production was 4,762,469 fish. Offshore fisheries accounted for an average of 50% of the nation's total fish catches in the late 1980s although they experienced repeated ups and downs during that period.
Today, Japan maintains one of the world's largest fishing fleets and accounts for nearly 15% of the global catch, prompting some claims that Japan's fishing is leading to depletion in fish stocks such as tuna. Japan has also sparked controversy by supporting quasi-commercial whaling.




Japan's industrial sector makes up approximately 27.5% of its GDP. Japan's major industries are motor vehicles, electronics, machine tools, metals, ships, chemicals and processed foods; some major Japanese industrial companies include Toyota, Canon Inc., Toshiba and Nippon Steel.
Japan is the third largest automobile producer in the world, and is home to Toyota, the world's largest automobile company. The Japanese consumer electronics industry, once considered the strongest in the world, is currently in a state of decline as competition arises in countries like South Korea, the United States and China. However, despite also facing similar competition from South Korea and China, the Japanese shipbuilding industry is expected to remain strong thanks to an increased focus on specialized, high-tech designs.




Japan's service sector accounts for about three-quarters of its total economic output. Banking, insurance, real estate, retailing, transportation, and telecommunications are all major industries, with companies such as Mitsubishi UFJ, Mizuho, NTT, TEPCO, Nomura, Mitsubishi Estate, ÆON, Mitsui Sumitomo, Softbank, JR East, Seven & I, KDDI and Japan Airlines listed as some of the largest in the world. Four of the five most circulated newspapers in the world are Japanese newspapers. Japan Post, one of the country's largest providers of savings and insurance services, was slated for privatization by 2015. The six major keiretsus are the Mitsubishi, Sumitomo, Fuyo, Mitsui, Dai-Ichi Kangyo and Sanwa Groups.




Japan attracted 19.73 million international tourists in 2015. Japan has 19 World Heritage Sites, including Himeji Castle, Historic Monuments of Ancient Kyoto and Nara. Popular tourist attractions include Tokyo and Hiroshima, Mount Fuji, ski resorts such as Niseko in Hokkaido, Okinawa, riding the shinkansen and taking advantage of Japan's hotel and hotspring network.
In inbound tourism, Japan was ranked 16th in the world in 2015. In 2009, the Yomiuri Shimbun published a modern list of famous sights under the name Heisei Hyakkei (the Hundred Views of the Heisei period). The Travel and Tourism Competitiveness Report 2015 ranks Japan 9th out of 141 countries overall, which was the best in Asia. Japan gained relatively high scores in almost all aspects, especially health and hygiene, safety and security, cultural resources and business travel. In 2015, 19,737,409 foreign tourists visited Japan.
Neighbouring South Korea is Japan's most important source of foreign tourists. In 2010, the 2.4 million arrivals made up 27% of the tourists visiting Japan. Chinese travelers are the highest spenders in Japan by country, spending an estimated 196.4 billion yen (US$2.4 billion) in 2011, or almost a quarter of total expenditure by foreign visitors, according to data from the Japan Tourism Agency.
The Japanese government hopes to receive 40 million foreign tourists every year by 2020.




Japan is a leading nation in scientific research, particularly in fields related to the natural sciences and engineering. The country ranks second among the most innovative countries in the Bloomberg Innovation Index. Nearly 700,000 researchers share a US$130 billion research and development budget. The amount spent on research and development relative to its gross domestic product third highest in the world. The country is a world leader in fundamental scientific research, having produced twenty-two Nobel laureates in either physics, chemistry or medicine, and three Fields medalists.
Japanese scientists and engineers have contributed to the advancement of agricultural sciences, electronics, industrial robotics, optics, chemicals, semiconductors, life sciences and various fields of engineering. Japan leads the world in robotics production and use, possessing more than 20% (300,000 of 1.3 million) of the world's industrial robots as of 2013—though its share was historically even higher, representing one-half of all industrial robots worldwide in 2000. Japan boasts the third highest number of scientists, technicians, and engineers per capita in the world with 83 scientists, technicians, and engineers per 10,000 employees.




The Japanese electronics and automotive manufacturing industry is well known throughout the world, and the country's electronic and automotive products account for a large share in the global market, compared to a majority of other countries. Brands such as Fujifilm, Sony, Panasonic, Toyota, Nissan, and Honda are internationally famous. It's estimated that 16% of the world's gold and 22% of the world's silver is contained in electronic technology in Japan.
Japan has started a project to build the world's fastest supercomputer by the end of 2017.




The Japan Aerospace Exploration Agency (JAXA) is Japan's space agency; it conducts space, planetary, and aviation research, and leads development of rockets and satellites. It is a participant in the International Space Station: the Japanese Experiment Module (Kibo) was added to the station during Space Shuttle assembly flights in 2008. Japan's plans in space exploration include: launching a space probe to Venus, Akatsuki; developing the Mercury Magnetospheric Orbiter to be launched in 2016; and building a moon base by 2030.
On September 14, 2007, it launched lunar explorer SELENE (Selenological and Engineering Explorer) on an H-IIA (Model H2A2022) carrier rocket from Tanegashima Space Center. SELENE is also known as Kaguya, after the lunar princess of The Tale of the Bamboo Cutter. Kaguya is the largest lunar mission since the Apollo program. Its purpose is to gather data on the moon's origin and evolution. It entered a lunar orbit on October 4, flying at an altitude of about 100 km (62 mi). The probe's mission was ended when it was deliberately crashed by JAXA into the Moon on June 11, 2009.




Japan has received the most science Nobel prizes in Asia and ranked 8th in the world. Japanese researchers have won several Nobel prizes. Hideki Yukawa, educated at Kyoto University, was awarded the prize for physics in 1949. Sin-Itiro Tomonaga followed in 1965. Solid-state physicist Leo Esaki, educated at the University of Tokyo, received the prize in 1973. Kenichi Fukui of Kyoto University shared the 1981 chemistry prize, and Susumu Tonegawa, also educated at Kyoto University, became Japan's first (and, as of 2007, only) laureate in physiology or medicine in 1987. Japanese chemists took prizes in 2000 and 2001: first Hideki Shirakawa (Tokyo Institute of Technology) and then Ryōji Noyori (Kyoto University). Masatoshi Koshiba (University of Tokyo) and Koichi Tanaka (Tohoku University) won in physics and chemistry, respectively, in 2002. Makoto Kobayashi, Toshihide Masukawa, and Yoichiro Nambu who is an American citizen when awarded, shared the physics prize and Osamu Shimomura also won the chemistry prize in 2008. Isamu Akasaki, Hiroshi Amano, Shuji Nakamura, who is an American citizen when awarded, shared the physics prize in 2014, and the Nobel Prize in Physiology or Medicine was awarded to Yoshinori Ohsumi in 2016.







Japan's road spending has been extensive. Its 1.2 million kilometres (0.75 million miles) of paved road are the main means of transportation. As of April 2012 Japan has approximately 1,215,000 kilometres (134,000 miles) of roads made up of 1,022,000 kilometres (14,000 miles) of city, town and village roads, 129,000 kilometres (80,000 miles) of prefectural roads, 55,000 kilometres (34,000 miles) of general national highways and 8,050 kilometres (5,000 miles) of national expressways. The Foreign Press Center/Japan cites a total length of expressways at 7,641 kilometres (4,748 miles) (fiscal 2008). A single network of high-speed, divided, limited-access toll roads connects major cities on Honshu, Shikoku and Kyushu. Hokkaido has a separate network, and Okinawa Island has a highway of this type. A single network of high-speed, divided, limited-access toll roads connects major cities and is operated by toll-collecting enterprises. New and used cars are inexpensive; car ownership fees and fuel levies are used to promote energy efficiency. However, at just 50 percent of all distance traveled, car usage is the lowest of all G8 countries.
Since privatisation in 1987, dozens of Japanese railway companies compete in regional and local passenger transportation markets; major companies include seven JR enterprises, Kintetsu, Seibu Railway and Keio Corporation. Some 250 high-speed Shinkansen trains connect major cities and Japanese trains are known for their safety and punctuality. Proposals for a new Maglev route between Tokyo and Osaka are at an advanced stage.
There are 175 airports in Japan; the largest domestic airport, Haneda Airport, is Asia's second-busiest airport. The largest international gateways are Narita International Airport, Kansai International Airport and Chūbu Centrair International Airport. Nagoya Port is the country's largest and busiest port, accounting for 10 percent of Japan's trade value.




As of 2011, 46.1% of energy in Japan was produced from petroleum, 21.3% from coal, 21.4% from natural gas, 4.0% from nuclear power, and 3.3% from hydropower. Nuclear power produced 9.2 percent of Japan's electricity, as of 2011, down from 24.9 percent the previous year. However, by May 2012 all of the country's nuclear power plants had been taken offline because of ongoing public opposition following the Fukushima Daiichi nuclear disaster in March 2011, though government officials continued to try to sway public opinion in favor of returning at least some of Japan's 50 nuclear reactors to service. As of November 2014, two reactors at Sendai are likely to restart in early 2015. Japan lacks significant domestic reserves and so has a heavy dependence on imported energy. Japan has therefore aimed to diversify its sources and maintain high levels of energy efficiency.




The government took responsibility for regulating the water and sanitation sector is shared between the Ministry of Health, Labor and Welfare in charge of water supply for domestic use; the Ministry of Land, Infrastructure, Transport and Tourism in charge of water resources development as well as sanitation; the Ministry of the Environment in charge of ambient water quality and environmental preservation; and the Ministry of Internal Affairs and Communications in charge of performance benchmarking of utilities.
Access to an improved water source is universal in Japan. 97% of the population receives piped water supply from public utilities and 3% receive water from their own wells or unregulated small systems, mainly in rural areas.
Access to improved sanitation is also universal, either through sewers or on-site sanitation. All collected waste water is treated at secondary-level treatment plants. All effluents discharged to closed or semi-closed water bodies, such as Tokyo Bay, Osaka Bay, or Lake Biwa, are further treated to tertiary level. This applies to about 15% of waste water. The effluent quality is remarkably good at 3–10 mg/l of BOD for secondary-level treatment, well below the national effluent standard of 20 mg/l.
Water supply and sanitation in Japan is facing some challenges, such as a decreasing population, declining investment, fiscal constraints, ageing facilities, an ageing workforce, a fragmentation of service provision among thousands of municipal utilities, and the vulnerability of parts of the country to droughts that are expected to become more frequent due to climate change.






Japan's population is estimated at around 127 million, with 80% of the population living on Honshū. Japanese society is linguistically and culturally homogeneous, composed of 98.5% ethnic Japanese, with small populations of foreign workers. Zainichi Koreans, Chinese, Filipinos, Brazilians mostly of Japanese descent, Peruvians mostly of Japanese descent and Americans are among the small minority groups in Japan. In 2003, there were about 134,700 non-Latin American Western (not including more than 33,000 American military personnel and their dependents stationed throughout the country) and 345,500 Latin American expatriates, 274,700 of whom were Brazilians (said to be primarily Japanese descendants, or nikkeijin, along with their spouses), the largest community of Westerners.
The most dominant native ethnic group is the Yamato people; primary minority groups include the indigenous Ainu and Ryukyuan peoples, as well as social minority groups like the burakumin. There are persons of mixed ancestry incorporated among the Yamato, such as those from Ogasawara Archipelago. In 2014, foreign-born non-naturalized workers made up only 1.5% of the total population. Japan is widely regarded as ethnically homogeneous, and does not compile ethnicity or race statistics for Japanese nationals; however, at least one analysis describes Japan as a multiethnic society. Most Japanese continue to see Japan as a monocultural society. Former Japanese Prime Minister and current Finance Minister Tarō Asō described Japan as being a nation of "one race, one civilization, one language and one culture", which drew criticism from representatives of ethnic minorities such as the Ainu.
Japan has the second longest overall life expectancy at birth of any country in the world: 83.5 years for persons born in the period 2010–2015. The Japanese population is rapidly aging as a result of a post–World War II baby boom followed by a decrease in birth rates. In 2012, about 24.1 percent of the population was over 65, and the proportion is projected to rise to almost 40 percent by 2050.




Japan has full religious freedom based on Article 20 of its Constitution. Upper estimates suggest that 84–96 percent of the Japanese population subscribe to Shinto as its indigenous religion (50% to 80% of which considering degrees of syncretism with Buddhism, shinbutsu-shūgō). However, these estimates are based on people affiliated with a temple, rather than the number of true believers. The number of Shinto shrines in Japan is estimated to be around 100,000. Other studies have suggested that only 30 percent of the population identify themselves as belonging to a religion. According to Edwin Reischauer and Marius Jansen, some 70–80% of the Japanese do not consider themselves believers in any religion. Nevertheless, the level of participation remains high, especially during festivals and occasions such as the first shrine visit of the New Year. Taoism and Confucianism from China have also influenced Japanese beliefs and customs. Japanese streets are decorated on Tanabata, Obon and Christmas.
Shinto is the largest religion in Japan, practiced by nearly 80% of the population, yet only a small percentage of these identify themselves as "Shintoists" in surveys. This is due to the fact that "Shinto" has different meanings in Japan: most of the Japanese attend Shinto shrines and beseech kami without belonging to Shinto organisations, and since there are no formal rituals to become a member of folk "Shinto", "Shinto membership" is often estimated counting those who join organised Shinto sects. Shinto has 100,000 shrines and 78,890 priests in the country. Buddhism first arrived in Japan in the 6th century; it was introduced in the year 538 or 552 from the kingdom of Baekje in Korea.
Christianity was first introduced into Japan by Jesuit missions starting in 1549. Today, fewer than 1% to 2.3% are Christians. Most of them living in the western part of the country, where the missionaries' activities were greatest during the 16th century. Nagasaki Prefecture has the highest percentage of Christians: about 5.1% in 1996. As of 2007 there are 32,036 Christian priests and pastors in Japan. Throughout the latest century, some Western customs originally related to Christianity (including Western style weddings, Valentine's Day and Christmas) have become popular as secular customs among many Japanese.
Islam in Japan is mostly represented by small immigrant communities from other parts of Asia. In 2008, Keiko Sakurai estimated that 80–90% of the Muslims in Japan were foreign born migrants primarily from Indonesia, Pakistan, Bangladesh, and Iran. It has been estimated that the Muslim immigrant population amounts to 70,000–100,000 people, while the "estimated number of Japanese Muslims ranges from thousands to tens of thousands".
Other minority religions include Hinduism, Sikhism, and Judaism, and since the mid-19th century numerous new religious movements have emerged in Japan.




More than 99 percent of the population speaks Japanese as their first language. Japanese is an agglutinative language distinguished by a system of honorifics reflecting the hierarchical nature of Japanese society, with verb forms and particular vocabulary indicating the relative status of speaker and listener. Japanese writing uses kanji (Chinese characters) and two sets of kana (syllabaries based on cursive script and radical of kanji), as well as the Latin alphabet and Arabic numerals.
Besides Japanese, the Ryukyuan languages (Amami, Kunigami, Okinawan, Miyako, Yaeyama, Yonaguni), also part of the Japonic language family, are spoken in the Ryukyu Islands chain. Few children learn these languages, but in recent years the local governments have sought to increase awareness of the traditional languages. The Okinawan Japanese dialect is also spoken in the region. The Ainu language, which has no proven relationship to Japanese or any other language, is moribund, with only a few elderly native speakers remaining in Hokkaido. Public and private schools generally require students to take Japanese language classes as well as English language courses.



The changes in demographic structure have created a number of social issues, particularly a potential decline in workforce population and increase in the cost of social security benefits like the public pension plan. A growing number of younger Japanese are not marrying or remain childless. In 2011, Japan's population dropped for a fifth year, falling by 204,000 people to 126.24 million people. This was the greatest decline since at least 1947, when comparable figures were first compiled. This decline was made worse by the March 11 earthquake and tsunami, which killed nearly 16,000 people with approximately another 2,600 still listed as missing as of 2014.
Japan's population is expected to drop to 95 million by 2050; demographers and government planners are currently in a heated debate over how to cope with this problem. Immigration and birth incentives are sometimes suggested as a solution to provide younger workers to support the nation's aging population. Japan accepts a steady flow of 15,000 new Japanese citizens by naturalization (帰化) per year. According to the UNHCR, in 2012 Japan accepted just 18 refugees for resettlement, while the US took in 76,000.
Japan suffers from a high suicide rate. In 2009, the number of suicides exceeded 30,000 for the twelfth straight year. Suicide is the leading cause of death for people under 30.




Primary schools, secondary schools and universities were introduced in 1872 as a result of the Meiji Restoration. Since 1947, compulsory education in Japan comprises elementary and middle school, which together last for nine years (from age 6 to age 15). Almost all children continue their education at a three-year senior high school.
Japan's education system played a central part in the country's recovery and rapid economic growth in the decades following the end of World War II. After World War II, the Fundamental Law of Education and the School Education Law were enacted. The latter law defined the school system that would be in effect for many decades: six years of elementary school, three years of junior high school, three years of high school, and two or four years of university. Starting in April 2016, various schools began the academic year with elementary school and junior high school integrated into one nine-year compulsory schooling program, in hopes to mitigate bullying and truancy; MEXT plans for this approach to be adopted nationwide in the coming years. In Japan, having a strong educational background greatly improves the likelihood of finding a job and earning enough money to support oneself. Highly educated individuals are less affected by unemployment trends as higher levels of educational attainment make an individual more attractive in the workforce. The lifetime earnings also increase with each level of education attained. Furthermore, skills needed in the modern 21st century labor market are becoming more knowledge-based and strong aptitude in science and mathematics are more strong predictors of employment prospects in Japan's highly technological economy.
Japan is one of the top-performing OECD countries in reading literacy, maths and sciences with the average student scoring 540 and has one of the worlds highest-educated labor forces among OECD countries. The Japanese populace is well educated and its society highly values education as a platform for social mobility and for gaining employment in the country's competitive high-tech economy. The country's large pool of highly educated and skilled individuals is largely responsible for ushering Japan's post-war economic growth. Tertiary-educated adults in Japan, particularly graduates in sciences and engineering benefit economically and socially from their education and skills in the country's high tech economy. Spending on education as a proportion of GDP is below the OECD average. Although expenditure per student is comparatively high in Japan, total expenditure relative to GDP remains small. In 2015, Japan's public spending on education amounted to just 3.5 percent of its GDP, below the OECD average of 4.7%. In 2014, the country ranked fourth for the percentage of 25- to 64-year-olds that have attained tertiary education with 48 percent. In addition, bachelor's degrees are held by 59 percent of Japanese aged 25–34, the second most in the OECD after South Korea. As the Japanese economy is largely scientific and technological based, the labor market demands people who have achieved some form of higher education, particularly related to science and engineering in order to gain a competitive edge when searching for employment opportunities. About 75.9 percent of high school graduates attended a university, junior college, trade school, or other higher education institution.
The two top-ranking universities in Japan are the University of Tokyo and Kyoto University. The Programme for International Student Assessment coordinated by the OECD currently ranks the overall knowledge and skills of Japanese 15-year-olds as sixth best in the world.




In Japan, health care is provided by national and local governments. Payment for personal medical services is offered through a universal health insurance system that provides relative equality of access, with fees set by a government committee. People without insurance through employers can participate in a national health insurance program administered by local governments. Since 1973, all elderly persons have been covered by government-sponsored insurance. Patients are free to select the physicians or facilities of their choice.




Japanese culture has evolved greatly from its origins. Contemporary culture combines influences from Asia, Europe and North America. Traditional Japanese arts include crafts such as ceramics, textiles, lacquerware, swords and dolls; performances of bunraku, kabuki, noh, dance, and rakugo; and other practices, the tea ceremony, ikebana, martial arts, calligraphy, origami, onsen, Geisha and games. Japan has a developed system for the protection and promotion of both tangible and intangible Cultural Properties and National Treasures. Nineteen sites have been inscribed on the UNESCO World Heritage List, fifteen of which are of cultural significance.




Japanese architecture is a combination between local and other influences. It has traditionally been typified by wooden structures, elevated slightly off the ground, with tiled or thatched roofs. Sliding doors (fusuma) were used in place of walls, allowing the internal configuration of a space to be customized for different occasions. People usually sat on cushions or otherwise on the floor, traditionally; chairs and high tables were not widely used until the 20th century. Since the 19th century, however, Japan has incorporated much of Western, modern, and post-modern architecture into construction and design, and is today a leader in cutting-edge architectural design and technology.
The introduction of Buddhism during the sixth century was a catalyst for large-scale temple building using complicated techniques in wood. Influence from the Chinese Tang and Sui Dynasties led to the foundation of the first permanent capital in Nara. Its checkerboard street layout used the Chinese capital of Chang'an as a template for its design. A gradual increase in the size of buildings led to standard units of measurement as well as refinements in layout and garden design. The introduction of the tea ceremony emphasised simplicity and modest design as a counterpoint to the excesses of the aristocracy.
During the Meiji Restoration of 1868 the history of Japanese architecture was radically changed by two important events. The first was the Kami and Buddhas Separation Act of 1868, which formally separated Buddhism from Shinto and Buddhist temples from Shinto shrines, breaking an association between the two which had lasted well over a thousand years.
Second, it was then that Japan underwent a period of intense Westernization in order to compete with other developed countries. Initially architects and styles from abroad were imported to Japan but gradually the country taught its own architects and began to express its own style. Architects returning from study with western architects introduced the International Style of modernism into Japan. However, it was not until after the Second World War that Japanese architects made an impression on the international scene, firstly with the work of architects like Kenzo Tange and then with theoretical movements like Metabolism.




The Shrines of Ise have been celebrated as the prototype of Japanese architecture. Largely of wood, traditional housing and many temple buildings see the use of tatami mats and sliding doors that break down the distinction between rooms and indoor and outdoor space. Japanese sculpture, largely of wood, and Japanese painting are among the oldest of the Japanese arts, with early figurative paintings dating back to at least 300 BC. The history of Japanese painting exhibits synthesis and competition between native Japanese aesthetics and adaptation of imported ideas.
The interaction between Japanese and European art has been significant: for example ukiyo-e prints, which began to be exported in the 19th century in the movement known as Japonism, had a significant influence on the development of modern art in the West, most notably on post-Impressionism. Famous ukiyo-e artists include Hokusai and Hiroshige. Hokusai coined the term manga. Japanese comics now known as manga developed in the 20th century and have become popular worldwide. Japanese animation is called anime. Japanese-made video game consoles have been popular since the 1980s.




Japanese music is eclectic and diverse. Many instruments, such as the koto, were introduced in the 9th and 10th centuries. The accompanied recitative of the Noh drama dates from the 14th century and the popular folk music, with the guitar-like shamisen, from the sixteenth. Western classical music, introduced in the late 19th century, now forms an integral part of Japanese culture. The imperial court ensemble Gagaku has influenced the work of some modern Western composers.
Notable classical composers from Japan include Toru Takemitsu and Rentarō Taki. Popular music in post-war Japan has been heavily influenced by American and European trends, which has led to the evolution of J-pop, or Japanese popular music. Karaoke is the most widely practiced cultural activity in Japan. A 1993 survey by the Cultural Affairs Agency found that more Japanese had sung karaoke that year than had participated in traditional pursuits such as flower arranging (ikebana) or tea ceremonies.




The earliest works of Japanese literature include the Kojiki and Nihon Shoki chronicles and the Man'yōshū poetry anthology, all from the 8th century and written in Chinese characters. In the early Heian period, the system of phonograms known as kana (Hiragana and Katakana) was developed. The Tale of the Bamboo Cutter is considered the oldest Japanese narrative. An account of Heian court life is given in The Pillow Book by Sei Shōnagon, while The Tale of Genji by Murasaki Shikibu is often described as the world's first novel.
During the Edo period, the chōnin ("townspeople") overtook the samurai aristocracy as producers and consumers of literature. The popularity of the works of Saikaku, for example, reveals this change in readership and authorship, while Bashō revivified the poetic tradition of the Kokinshū with his haikai (haiku) and wrote the poetic travelogue Oku no Hosomichi. The Meiji era saw the decline of traditional literary forms as Japanese literature integrated Western influences. Natsume Sōseki and Mori Ōgai were the first "modern" novelists of Japan, followed by Ryūnosuke Akutagawa, Jun'ichirō Tanizaki, Yukio Mishima and, more recently, Haruki Murakami. Japan has two Nobel Prize-winning authors—Yasunari Kawabata (1968) and Kenzaburō Ōe (1994).




Japanese Philosophy has historically been a fusion of both foreign; particularly Chinese and Western, and uniquely Japanese elements. In its literary forms, Japanese philosophy began about fourteen centuries ago.
Archaeological evidence and early historical accounts suggest that Japan was originally an animistic culture, which viewed the world as infused with kami (神) or sacred presence as taught by Shinto, though it is not a philosophy as such, but has greatly influenced all other philosophies in their Japanese interpretations.
Confucianism entered Japan from China around the 5th century A.D., as did Buddhism. Confucian ideals are still evident today in the Japanese concept of society and the self, and in the organization of the government and the structure of society. Buddhism has profoundly impacted Japanese psychology, metaphysics, and aesthetics.
Neo-Confucianism, which became prominent in the sixteenth century during the Tokugawa era, shaped Japanese ideas of virtue and social responsibility, and, through its emphasis on investigating the principle or configuration of things, stimulated the Japanese study of the natural world. Also since the 16th century, certain indigenous ideas of loyalty and honour have been held. Western philosophy has had its major impact in Japan only since the middle of the 19th century.




Japanese cuisine is based on combining staple foods, typically Japanese rice or noodles, with a soup and okazu—dishes made from fish, vegetable, tofu and the like—to add flavor to the staple food. In the early modern era ingredients such as red meats that had previously not been widely used in Japan were introduced. Japanese cuisine is known for its emphasis on seasonality of food, quality of ingredients and presentation. Japanese cuisine offers a vast array of regional specialties that use traditional recipes and local ingredients. The phrase ichijū-sansai (一汁三菜, "one soup, three sides") refers to the makeup of a typical meal served, but has roots in classic kaiseki, honzen, and yūsoku cuisine. The term is also used to describe the first course served in standard kaiseki cuisine nowadays.
Traditional Japanese sweets are known as wagashi. Ingredients such as red bean paste and mochi are used. More modern-day tastes includes green tea ice cream, a very popular flavor. Almost all manufacturers produce a version of it. Kakigori is a shaved ice dessert flavored with syrup or condensed milk. It is usually sold and eaten at summer festivals. Popular Japanese beverages such as sake, which is a brewed rice beverage that, typically, contains 15%~17% alcohol and is made by multiple fermentation of rice. Other beverage like beer is produced in some region such as Sapporo Brewery, the oldest Japan beer's brand. The Michelin Guide has awarded restaurants in Japan more Michelin stars than the rest of the world combined.




Officially, Japan has 15 national, government-recognized holidays. Public holidays in Japan are regulated under the Public Holiday Law (国民の祝日に関する法律 Kokumin no Shukujitsu ni Kansuru Hōritsu) of 1948. Beginning in 2000, Japan implemented the Happy Monday System, which moved a number of national holidays to Monday in order to obtain a long weekend. In 2006, the country decided to add Shōwa Day, a new national holiday, in place of Greenery Day on April 29, and to move Greenery Day to May 4. These changes took effect in 2007. In 2014, the House of Councillors decided to add Mountain Day (山の日, Yama no Hi) to the Japanese calendar on August 11, after lobbying by the Japanese Alpine Club. It is intended to coincide with the Bon Festival vacation time, giving Japanese people an opportunity to appreciate Japan's mountains.
The national holidays in Japan are New Year's Day on January 1, Coming of Age Day on Second Monday of January, National Foundation Day on February 11, Vernal Equinox Day on March 20 or 21, Shōwa Day on April 29, Constitution Memorial Day on May 3, Greenery Day on May 4, Children's Day on May 5, Marine Day on Third Monday of July, Mountain Day on August 11, Respect for the Aged Day on Third Monday of September, Autumnal Equinox on September 23 or 24, Health and Sports Day on Second Monday of October, Culture Day on November 3, Labour Thanksgiving Day on November 23, and The Emperor's Birthday on December 23.




There are many festivals in Japan, which are called in Japanese as matsuri (祭) which celebrate annually. There are no specific festival days for all of Japan; dates vary from area to area, and even within a specific area, but festival days do tend to cluster around traditional holidays such as Setsubun or Obon. Festivals are often based around one event, with food stalls, entertainment, and carnival games to keep people entertained. Its usually sponsored by a local shrine or temple, though they can be secular.
Notable festival often feature processions which may include elaborate floats. Preparation for these processions is usually organised at the level of neighborhoods, or machi (町). Prior to these, the local kami may be ritually installed in mikoshi and paraded through the streets, such as Gion in Kyoto, and Hadaka in Okayama.




Traditionally, sumo is considered Japan's national sport. Japanese martial arts such as judo, karate and kendo are also widely practiced and enjoyed by spectators in the country. After the Meiji Restoration, many Western sports were introduced in Japan and began to spread through the education system. Japan hosted the Summer Olympics in Tokyo in 1964. Japan has hosted the Winter Olympics twice: Sapporo in 1972 and Nagano in 1998. Tokyo will host the 2020 Summer Olympics, making Tokyo the first Asian city to host the Olympics twice. Japan is the most successful Asian Rugby Union country, winning the Asian Five Nations a record 6 times and winning the newly formed IRB Pacific Nations Cup in 2011. Japan will host the 2019 IRB Rugby World Cup.
Baseball is currently the most popular spectator sport in the country. Japan's top professional league, now known as Nippon Professional Baseball, was established in 1936. Since the establishment of the Japan Professional Football League in 1992, association football has also gained a wide following. Japan was a venue of the Intercontinental Cup from 1981 to 2004 and co-hosted the 2002 FIFA World Cup with South Korea. Japan has one of the most successful football teams in Asia, winning the Asian Cup four times. Also, Japan recently won the FIFA Women's World Cup in 2011. Golf is also popular in Japan, as are forms of auto racing like the Super GT series and Formula Nippon. The country has produced one NBA player, Yuta Tabuse.




Television and newspapers take an important role in Japanese mass media, though radio and magazines also take apart. For a long time, newspapers were regarded as the most influential information medium in Japan, although audience attitudes towards television changed with the emergence of commercial news broadcasting in the mid-1980s. Over the last decade, television has clearly come to surpass newspapers as Japan's main information and entertainment medium.
There are 6 nationwide television networks, such as NHK, Nippon Television (NTV), Tokyo Broadcasting System (TBS), Fuji Network System (FNS), TV Asahi, and TV Tokyo Network (TXN). For the most part, television networks were established based on capital investments by existing radio networks. Variety shows, serial dramas, and news constitute a large percentage of Japanese television show. According to the fourth NHK survey on television viewing in Japan, 95 percent of Japanese watch television every day. The average daily duration of television viewing ranged from approximately four hours.
Japanese readers have a choice of approximately 120 daily newspapers with a total of 50 million copies of 'set paper' with an average subscription rate of 1.13 newspapers per household. The main newspaper's publishers are Yomiuri Shimbun, Asahi Shimbun, Mainichi Shimbun, Nikkei Shimbun, and Sankei Shimbun. According to a survey conducted by the Japanese Newspaper Association in June 1999, 85.4 per cent of men and 75 per cent of women read a newspaper every day. Average daily reading times vary with 27.7 minutes on weekdays and 31.7 minutes on holidays and Sunday.




Index of Japan-related articles
Outline of Japan













Government
Kantei.go.jp, official site of the Prime Minister of Japan and His Cabinet
Kunaicho.go.jp, official site of the Imperial House
National Diet Library
Public Relations Office
Japan National Tourist Organization
General information
"Japan". The World Factbook. Central Intelligence Agency. 
Japan from UCB Libraries GovPubs
Japan at DMOZ
Japan Encyclopædia Britannica entry
Japan profile from BBC News
Japan from the OECD
 Wikimedia Atlas of Japan
 Geographic data related to Japan at OpenStreetMap
Key Development Forecasts for Japan from International FuturesEgypt (/ˈiːdʒɪpt/ EE-jipt; Arabic: مِصر‎‎ Miṣr, Egyptian Arabic: مَصر‎‎ Maṣr, Coptic: Ⲭⲏⲙⲓ Kimi), officially the Arab Republic of Egypt, is a transcontinental country spanning the northeast corner of Africa and southwest corner of Asia by a land bridge formed by the Sinai Peninsula. Egypt is a Mediterranean country bordered by the Gaza Strip and Israel to the northeast, the Gulf of Aqaba to the east, the Red Sea to the east and south, Sudan to the south, and Libya to the west. Across the Gulf of Aqaba lies Jordan, and across from the Sinai Peninsula lies Saudi Arabia, although Jordan and Saudi Arabia do not share a land border with Egypt. It is the world's only contiguous Afrasian nation.
Egypt has among the longest histories of any modern country, emerging as one of the world's first nation states in the tenth millennium BC. Considered a cradle of civilisation, Ancient Egypt experienced some of the earliest developments of writing, agriculture, urbanisation, organised religion and central government. Iconic monuments such as the Giza Necropolis and its Great Sphinx, as well the ruins of Memphis, Thebes, Karnak, and the Valley of the Kings, reflect this legacy and remain a significant focus of archaeological study and popular interest worldwide. Egypt's rich cultural heritage is an integral part of its national identity, which has endured, and at times assimilated, various foreign influences, including Greek, Persian, Roman, Arab, Ottoman, and European. One of the earliest centres of Christianity, Egypt was Islamised in the seventh century and remains a predominantly Muslim country, albeit with a significant Christian minority.
With over 92 million inhabitants, Egypt is the most populous country in North Africa and the Arab world, the third-most populous in Africa (after Nigeria and Ethiopia), and the fifteenth-most populous in the world. The great majority of its people live near the banks of the Nile River, an area of about 40,000 square kilometres (15,000 sq mi), where the only arable land is found. The large regions of the Sahara desert, which constitute most of Egypt's territory, are sparsely inhabited. About half of Egypt's residents live in urban areas, with most spread across the densely populated centres of greater Cairo, Alexandria and other major cities in the Nile Delta.
Modern Egypt is considered to be a regional and middle power, with significant cultural, political, and military influence in North Africa, the Middle East and the Muslim world. Egypt's economy is one of the largest and most diversified in the Middle East, and is projected to become one of the largest in the 21st century. Egypt is a member of the United Nations, Non-Aligned Movement, Arab League, African Union, and Organisation of Islamic Cooperation.




Miṣr (IPA: [mi̠sˤr] or Egyptian Arabic pronunciation: [mesˤɾ]; Arabic: مِصر‎‎) is the Classical Quranic Arabic and modern official name of Egypt, while Maṣr (IPA: [mɑsˤɾ]; Egyptian Arabic: مَصر‎‎) is the local pronunciation in Egyptian Arabic. The name is of Semitic origin, directly cognate with other Semitic words for Egypt such as the Hebrew מִצְרַיִם‎ (Mitzráyim). The oldest attestation of this name for Egypt is the Akkadian 𒆳 𒈪 𒄑 𒊒 KURmi-iṣ-ru miṣru, related to miṣru/miṣirru/miṣaru, meaning "border" or "frontier".







There is evidence of rock carvings along the Nile terraces and in desert oases. In the 10th millennium BC, a culture of hunter-gatherers and fishers was replaced by a grain-grinding culture. Climate changes or overgrazing around 8000 BC began to desiccate the pastoral lands of Egypt, forming the Sahara. Early tribal peoples migrated to the Nile River where they developed a settled agricultural economy and more centralised society.
By about 6000 BC, a Neolithic culture rooted in the Nile Valley. During the Neolithic era, several predynastic cultures developed independently in Upper and Lower Egypt. The Badarian culture and the successor Naqada series are generally regarded as precursors to dynastic Egypt. The earliest known Lower Egyptian site, Merimda, predates the Badarian by about seven hundred years. Contemporaneous Lower Egyptian communities coexisted with their southern counterparts for more than two thousand years, remaining culturally distinct, but maintaining frequent contact through trade. The earliest known evidence of Egyptian hieroglyphic inscriptions appeared during the predynastic period on Naqada III pottery vessels, dated to about 3200 BC.

A unified kingdom was founded c. 3150 BC by King Menes, leading to a series of dynasties that ruled Egypt for the next three millennia. Egyptian culture flourished during this long period and remained distinctively Egyptian in its religion, arts, language and customs. The first two ruling dynasties of a unified Egypt set the stage for the Old Kingdom period, c. 2700–2200 BC., which constructed many pyramids, most notably the Third Dynasty pyramid of Djoser and the Fourth Dynasty Giza pyramids.
The First Intermediate Period ushered in a time of political upheaval for about 150 years. Stronger Nile floods and stabilisation of government, however, brought back renewed prosperity for the country in the Middle Kingdom c. 2040 BC, reaching a peak during the reign of Pharaoh Amenemhat III. A second period of disunity heralded the arrival of the first foreign ruling dynasty in Egypt, that of the Semitic Hyksos. The Hyksos invaders took over much of Lower Egypt around 1650 BC and founded a new capital at Avaris. They were driven out by an Upper Egyptian force led by Ahmose I, who founded the Eighteenth Dynasty and relocated the capital from Memphis to Thebes.
The New Kingdom c. 1550–1070 BC began with the Eighteenth Dynasty, marking the rise of Egypt as an international power that expanded during its greatest extension to an empire as far south as Tombos in Nubia, and included parts of the Levant in the east. This period is noted for some of the most well known Pharaohs, including Hatshepsut, Thutmose III, Akhenaten and his wife Nefertiti, Tutankhamun and Ramesses II. The first historically attested expression of monotheism came during this period as Atenism. Frequent contacts with other nations brought new ideas to the New Kingdom. The country was later invaded and conquered by Libyans, Nubians and Assyrians, but native Egyptians eventually drove them out and regained control of their country.
In 525 BC, the powerful Achaemenid Persians, led by Cambyses II, began their conquest of Egypt, eventually capturing the pharaoh Psamtik III at the battle of Pelusium. Cambyses II then assumed the formal title of pharaoh, but ruled Egypt from his home of Susa in Persia (modern Iran), leaving Egypt under the control of a satrapy. The entire Twenty-seventh Dynasty of Egypt, from 525 BC to 402 BC, save for Petubastis III, was an entirely Persian ruled period, with the Achaemenid kings all being granted the title of pharaoh. A few temporarily successful revolts against the Persians marked the fifth century BC, but Egypt was never able to permanently overthrow the Persians.
The Thirtieth Dynasty was the last native ruling dynasty during the Pharaonic epoch. It fell to the Persians again in 343 BC after the last native Pharaoh, King Nectanebo II, was defeated in battle. This Thirty-first Dynasty of Egypt, however, did not last long, for the Persians were toppled several decades later by Alexander the Great.




The Ptolemaic Kingdom was a powerful Hellenistic state, extending from southern Syria in the east, to Cyrene to the west, and south to the frontier with Nubia. Alexandria became the capital city and a centre of Greek culture and trade. To gain recognition by the native Egyptian populace, they named themselves as the successors to the Pharaohs. The later Ptolemies took on Egyptian traditions, had themselves portrayed on public monuments in Egyptian style and dress, and participated in Egyptian religious life.
The last ruler from the Ptolemaic line was Cleopatra VII, who committed suicide following the burial of her lover Mark Antony who had died in her arms (from a self-inflicted stab wound), after Octavian had captured Alexandria and her mercenary forces had fled. The Ptolemies faced rebellions of native Egyptians often caused by an unwanted regime and were involved in foreign and civil wars that led to the decline of the kingdom and its annexation by Rome. Nevertheless, Hellenistic culture continued to thrive in Egypt well after the Muslim conquest.
Christianity was brought to Egypt by Saint Mark the Evangelist in the 1st century. Diocletian's reign (from 284 to 305 AD) marked the transition from the Roman to the Byzantine era in Egypt, when a great number of Egyptian Christians were persecuted. The New Testament had by then been translated into Egyptian. After the Council of Chalcedon in AD 451, a distinct Egyptian Coptic Church was firmly established.




The Byzantines were able to regain control of the country after a brief Sasanian Persian invasion early in the 7th century amidst the Byzantine–Sasanian War of 602–628 during which they established a new short-lived province for ten years known as Sasanian Egypt, until 639–42, when Egypt was invaded and conquered by the Islamic Empire by the Muslim Arabs. When they defeated the Byzantine Armies in Egypt, the Arabs brought Sunni Islam to the country. Early in this period, Egyptians began to blend their new faith with indigenous beliefs and practices, leading to various Sufi orders that have flourished to this day. These earlier rites had survived the period of Coptic Christianity.
Muslim rulers nominated by the Caliphate remained in control of Egypt for the next six centuries, with Cairo as the seat of the Fatimid Caliphate. With the end of the Kurdish Ayyubid dynasty, the Mamluks, a Turco-Circassian military caste, took control about 1250. By the late 13th century, Egypt linked the Red Sea, India, Malaya, and East Indies. The mid-14th-century Black Death killed about 40% of the country's population.




Egypt was conquered by the Ottoman Turks in 1517, after which it became a province of the Ottoman Empire. The defensive militarisation damaged its civil society and economic institutions. The weakening of the economic system combined with the effects of plague left Egypt vulnerable to foreign invasion. Portuguese traders took over their trade. Between 1687 and 1731, Egypt experienced six famines. The 1784 famine cost it roughly one-sixth of its population.
Egypt was always a difficult province for the Ottoman Sultans to control, due in part to the continuing power and influence of the Mamluks, the Egyptian military caste who had ruled the country for centuries.

Egypt remained semi-autonomous under the Mamluks until it was invaded by the French forces of Napoleon Bonaparte 1798 (see French campaign in Egypt and Syria). After the French were defeated by the British, a power vacuum was created in Egypt, and a three-way power struggle ensued between the Ottoman Turks, Egyptian Mamluks who had ruled Egypt for centuries, and Albanian mercenaries in the service of the Ottomans.




After the French were expelled, power was seized in 1805 by Muhammad Ali Pasha, an Albanian military commander of the Ottoman army in Egypt. While he carried the title of viceroy of Egypt, his subordination to the Ottoman porte was merely nominal. Muhammad Ali established a dynasty that was to rule Egypt until the revolution of 1952.
The introduction in 1820 of long-staple cotton transformed its agriculture into a cash-crop monoculture before the end of the century, concentrating land ownership and shifting production towards international markets.
Muhammad Ali annexed Northern Sudan (1820–1824), Syria (1833), and parts of Arabia and Anatolia; but in 1841 the European powers, fearful lest he topple the Ottoman Empire itself, forced him to return most of his conquests to the Ottomans. His military ambition required him to modernise the country: he built industries, a system of canals for irrigation and transport, and reformed the civil service.
He constructed a military state with around four percent of the populace serving the army to raise Egypt to a powerful positioning in the Ottoman Empire in a way showing various similarities to the Soviet strategies (without communism) conducted in the 20th century.
Muhammad Ali Pasha evolved the military from one that convened under the tradition of the corvée to a great modernised army. He introduced conscription of the male peasantry in 19th century Egypt, and took a novel approach to create his great army, strengthening it with numbers and in skill. Education and training of the new soldiers was not an option; the new concepts were furthermore enforced by isolation. The men were held in barracks to avoid distraction of their growth as a military unit to be reckoned with. The resentment for the military way of life eventually faded from the men and a new ideology took hold, one of nationalism and pride. It was with the help of this newly reborn martial unit that Muhammad Ali imposed his rule over Egypt.
The policy that Mohammad Ali Pasha followed during his reign explains partly why the numeracy in Egypt compared to other North-African and Middle-Eastern countries increased only at a remarkably small rate, as investment in further education only took place in the military and industrial sector.
Muhammad Ali was succeeded briefly by his son Ibrahim (in September 1848), then by a grandson Abbas I (in November 1848), then by Said (in 1854), and Isma'il (in 1863) who encouraged science and agriculture and banned slavery in Egypt.




Egypt under the Muhammad Ali dynasty remained nominally an Ottoman province. It was granted the status of an autonomous vassal state or Khedivate in 1867, a status which was to remain in place until 1914.
The Suez Canal, built in partnership with the French, was completed in 1869. Its construction led to enormous debt to European banks, and caused popular discontent because of the onerous taxation it required. In 1875 Ismail was forced to sell Egypt's share in the canal to the British government. Within three years this led to the imposition of British and French controllers who sat in the Egyptian cabinet, and, "with the financial power of the bondholders behind them, were the real power in the Government."
Other circumstances like epidemic diseases (cattle disease in the 1880s), floods and wars drove the economic downturn and increased Egypt's dependency on foreign debt even further.
In later years, the dynasty became a British puppet. Isma'il and Tewfik Pasha governed Egypt as a quasi-independent state under Ottoman suzerainty until the British occupation of 1882.

Local dissatisfaction with Ismail and with European intrusion led to the formation of the first nationalist groupings in 1879, with Ahmad Urabi a prominent figure. Fearing a reduction of their control, the UK and France intervened militarily, bombarding Alexandria and crushing the Egyptian army at the battle of Tel El Kebir. They reinstalled Ismail's son Tewfik as figurehead of a de facto British protectorate.
In 1906, the Dinshaway Incident prompted many neutral Egyptians to join the nationalist movement.




The Khedivate of Egypt remained a de jure Ottoman province until 5 November 1914, when it was declared a British protectorate in reaction to the decision of the Young Turks of the Ottoman Empire to join World War I on the side of the Central Powers.
In 1914, the Protectorate was made official, and the title of the head of state was changed to sultan, to repudiate the vestigial suzerainty of the Ottoman sultan, who was backing the Central powers in World War I. Abbas II was deposed as khedive and replaced by his uncle, Hussein Kamel, as sultan.
After World War I, Saad Zaghlul and the Wafd Party led the Egyptian nationalist movement to a majority at the local Legislative Assembly. When the British exiled Zaghlul and his associates to Malta on 8 March 1919, the country arose in its first modern revolution. The revolt led the UK government to issue a unilateral declaration of Egypt's independence on 22 February 1922.

The new government drafted and implemented a constitution in 1923 based on a parliamentary system. Saad Zaghlul was popularly elected as Prime Minister of Egypt in 1924. In 1936, the Anglo-Egyptian Treaty was concluded. Continued instability due to remaining British influence and increasing political involvement by the king led to the dissolution of the parliament in a military coup d'état known as the 1952 Revolution. The Free Officers Movement forced King Farouk to abdicate in support of his son Fuad. British military presence in Egypt lasted until 1954.




Following the 1952 Revolution by the Free Officers Movement, the rule of Egypt passed to military hands. On 18 June 1953, the Egyptian Republic was declared, with General Muhammad Naguib as the first President of the Republic.




Naguib was forced to resign in 1954 by Gamal Abdel Nasser – the real architect of the 1952 movement – and was later put under house arrest. Nasser assumed power as President in June 1956. British forces completed their withdrawal from the occupied Suez Canal Zone on 13 June 1956. He nationalised the Suez Canal on 26 July 1956, prompting the 1956 Suez Crisis.
In 1958, Egypt and Syria formed a sovereign union known as the United Arab Republic. The union was short-lived, ending in 1961 when Syria seceded, thus ending the union. During most of its existence, the United Arab Republic was also in a loose confederation with North Yemen (or the Mutawakkilite Kingdom of Yemen), known as the United Arab States. In 1959, the All-Palestine Government of the Gaza Strip, an Egyptian client state, was absorbed into the United Arab Republic under the pretext of Arab union, and was never restored.
In the early 1960s, Egypt became fully involved in the North Yemen Civil War. The Egyptian President, Gamal Abdel Nasser, supported the Yemeni republicans with as many as 70,000 Egyptian troops and chemical weapons. Despite several military moves and peace conferences, the war sank into a stalemate. Egyptian commitment in Yemen was greatly undermined later.
In mid May 1967, the Soviet Union issued warnings to Nasser of an impending Israeli attack on Syria. Although the chief of staff Mohamed Fawzi verified them as "baseless", Nasser took three successive steps that made the war virtually inevitable: On 14 May he deployed his troops in Sinai near the border with Israel, on 19 May he expelled the UN peacekeepers stationed in the Sinai Peninsula border with Israel, and on 23 May he closed the Straits of Tiran to Israeli shipping. On 26 May Nasser declared, "The battle will be a general one and our basic objective will be to destroy Israel".
Israel re-iterated that the Straits of Tiran closure was a Casus belli. In the 1967 Six Day War, Israel attacked Egypt, and occupied Sinai Peninsula and the Gaza Strip, which Egypt had occupied since the 1948 Arab–Israeli War. During the 1967 war, an Emergency Law was enacted, and remained in effect until 2012, with the exception of an 18-month break in 1980/81. Under this law, police powers were extended, constitutional rights suspended and censorship legalised.
At the time of the fall of the Egyptian monarchy in the early 1950s, less than half a million Egyptians were considered upper class and rich, four million middle class and 17 million lower class and poor. Fewer than half of all primary-school-age children attended school, most of them being boys. Nasser's policies changed this. Land reform and distribution, the dramatic growth in university education, and government support to national industries greatly improved social mobility and flattened the social curve. From academic year 1953–54 through 1965–66, overall public school enrolments more than doubled. Millions of previously poor Egyptians, through education and jobs in the public sector, joined the middle class. Doctors, engineers, teachers, lawyers, journalists, constituted the bulk of the swelling middle class in Egypt under Nasser. During the 1960s, the Egyptian economy went from sluggish to the verge of collapse, the society became less free, and Nasser's appeal waned considerably.




In 1970, President Nasser died and was succeeded by Anwar Sadat. Sadat switched Egypt's Cold War allegiance from the Soviet Union to the United States, expelling Soviet advisors in 1972. He launched the Infitah economic reform policy, while clamping down on religious and secular opposition. In 1973, Egypt, along with Syria, launched the October War, a surprise attack to regain part of the Sinai territory Israel had captured 6 years earlier. It presented Sadat with a victory that allowed him to regain the Sinai later in return for peace with Israel.

In 1975, Sadat shifted Nasser's economic policies and sought to use his popularity to reduce government regulations and encourage foreign investment through his program of Infitah. Through this policy, incentives such as reduced taxes and import tariffs attracted some investors, but investments were mainly directed at low risk and profitable ventures like tourism and construction, abandoning Egypt's infant industries. Even though Sadat's policy was intended to modernise Egypt and assist the middle class, it mainly benefited the higher class, and, because of the elimination of subsidies on basic foodstuffs, led to the 1977 Egyptian Bread Riots.
Sadat made a historic visit to Israel in 1977, which led to the 1979 peace treaty in exchange for Israeli withdrawal from Sinai. Sadat's initiative sparked enormous controversy in the Arab world and led to Egypt's expulsion from the Arab League, but it was supported by most Egyptians. Sadat was assassinated by an Islamic extremist in October 1981.



Hosni Mubarak came to power after the assassination of Sadat in a referendum in which he was the only candidate.

Hosni Mubarak reaffirmed Egypt's relationship with Israel yet eased the tensions with Egypt's Arab neighbours. Domestically, Mubarak faced serious problems. Even though farm and industry output expanded, the economy could not keep pace with the population boom. Mass poverty and unemployment led rural families to stream into cities like Cairo where they ended up in crowded slums, barely managing to survive.
In the 1980s, 1990s, and 2000s, terrorist attacks in Egypt became numerous and severe, and began to target Christian Copts, foreign tourists and government officials. In the 1990s an Islamist group, Al-Gama'a al-Islamiyya, engaged in an extended campaign of violence, from the murders and attempted murders of prominent writers and intellectuals, to the repeated targeting of tourists and foreigners. Serious damage was done to the largest sector of Egypt's economy—tourism—and in turn to the government, but it also devastated the livelihoods of many of the people on whom the group depended for support.
During Mubarak's reign, the political scene was dominated by the National Democratic Party, which was created by Sadat in 1978. It passed the 1993 Syndicates Law, 1995 Press Law, and 1999 Nongovernmental Associations Law which hampered freedoms of association and expression by imposing new regulations and draconian penalties on violations. As a result, by the late 1990s parliamentary politics had become virtually irrelevant and alternative avenues for political expression were curtailed as well.

On 17 November 1997, 62 people, mostly tourists, were massacred near Luxor.
In late February 2005, Mubarak announced a reform of the presidential election law, paving the way for multi-candidate polls for the first time since the 1952 movement. However, the new law placed restrictions on the candidates, and led to Mubarak's easy re-election victory. Voter turnout was less than 25%. Election observers also alleged government interference in the election process. After the election, Mubarak imprisoned Ayman Nour, the runner-up.
Human Rights Watch's 2006 report on Egypt detailed serious human rights violations, including routine torture, arbitrary detentions and trials before military and state security courts. In 2007, Amnesty International released a report alleging that Egypt had become an international centre for torture, where other nations send suspects for interrogation, often as part of the War on Terror. Egypt's foreign ministry quickly issued a rebuttal to this report.
Constitutional changes voted on 19 March 2007 prohibited parties from using religion as a basis for political activity, allowed the drafting of a new anti-terrorism law, authorised broad police powers of arrest and surveillance, and gave the president power to dissolve parliament and end judicial election monitoring. In 2009, Dr. Ali El Deen Hilal Dessouki, Media Secretary of the National Democratic Party (NDP), described Egypt as a "pharaonic" political system, and democracy as a "long-term goal". Dessouki also stated that "the real center of power in Egypt is the military".




On 25 January 2011, widespread protests began against Mubarak's government. On 11 February 2011, Mubarak resigned and fled Cairo. Jubilant celebrations broke out in Cairo's Tahrir Square at the news. The Egyptian military then assumed the power to govern. Mohamed Hussein Tantawi, chairman of the Supreme Council of the Armed Forces, became the de facto interim head of state. On 13 February 2011, the military dissolved the parliament and suspended the constitution.
A constitutional referendum was held on 19 March 2011. On 28 November 2011, Egypt held its first parliamentary election since the previous regime had been in power. Turnout was high and there were no reports of major irregularities or violence. Mohamed Morsi was elected president on 24 June 2012. On 2 August 2012, Egypt's Prime Minister Hisham Qandil announced his 35-member cabinet comprising 28 newcomers including four from the Muslim Brotherhood.
Liberal and secular groups walked out of the constituent assembly because they believed that it would impose strict Islamic practices, while Muslim Brotherhood backers threw their support behind Morsi. On 22 November 2012, President Morsi issued a temporary declaration immunising his decrees from challenge and seeking to protect the work of the constituent assembly.
The move led to massive protests and violent action throughout Egypt. On 5 December 2012, tens of thousands of supporters and opponents of president Morsi clashed, in what was described as the largest violent battle between Islamists and their foes since the country's revolution. Mohamed Morsi offered a "national dialogue" with opposition leaders but refused to cancel the December 2012 constitutional referendum.
On 3 July 2013, after a wave of public discontent with autocratic excesses of Morsi's Muslim Brotherhood government, the military removed President Morsi from power in a coup d'état and installed an interim government.
On 4 July 2013, 68-year-old Chief Justice of the Supreme Constitutional Court of Egypt Adly Mansour was sworn in as acting president over the new government following the removal of Morsi. The military-backed Egyptian authorities cracked down on the Muslim Brotherhood and its supporters, jailing thousands and killing hundreds of street protesters. Many of the Muslim Brotherhood leaders and activists have either been sentenced to death or life imprisonment in a series of mass trials.
On 18 January 2014, the interim government instituted a new constitution following a referendum in which 98.1% of voters were supportive. Participation was low with only 38.6% of registered voters participating although this was higher than the 33% who voted in a referendum during Morsi's tenure. On 26 March 2014 Abdel Fattah el-Sisi the head of the Egyptian Armed Forces, who at this time was in control of the country, resigned from the military, announcing he would stand as a candidate in the 2014 presidential election. The poll, held between 26 and 28 May 2014, resulted in a landslide victory for el-Sisi. Sisi was sworn into office as President of Egypt on 8 June 2014. The Muslim Brotherhood and some liberal and secular activist groups boycotted the vote. Even though the military-backed authorities extended voting to a third day, the 46% turnout was lower than the 52% turnout in the 2012 election.




Egypt lies primarily between latitudes 22° and 32°N, and longitudes 25° and 35°E. At 1,001,450 square kilometres (386,660 sq mi), it is the world's 30th-largest country. Due to the extreme aridity of Egypt's climate, population centres are concentrated along the narrow Nile Valley and Delta, meaning that about 99% of the population uses about 5.5% of the total land area. 98% of Egyptians live on 3% of the territory.
Egypt is bordered by Libya to the west, the Sudan to the south, and the Gaza Strip and Israel to the east. Egypt's important role in geopolitics stems from its strategic position: a transcontinental nation, it possesses a land bridge (the Isthmus of Suez) between Africa and Asia, traversed by a navigable waterway (the Suez Canal) that connects the Mediterranean Sea with the Indian Ocean by way of the Red Sea.
Apart from the Nile Valley, the majority of Egypt's landscape is desert, with a few oases scattered about. Winds create prolific sand dunes that peak at more than 100 feet (30 m) high. Egypt includes parts of the Sahara desert and of the Libyan Desert. These deserts protected the Kingdom of the Pharaohs from western threats and were referred to as the "red land" in ancient Egypt.
Towns and cities include Alexandria, the second largest city; Aswan; Asyut; Cairo, the modern Egyptian capital and largest city; El Mahalla El Kubra; Giza, the site of the Pyramid of Khufu; Hurghada; Luxor; Kom Ombo; Port Safaga; Port Said; Sharm El Sheikh; Suez, where the south end of the Suez Canal is located; Zagazig; and Minya. Oases include Bahariya, Dakhla, Farafra, Kharga and Siwa. Protectorates include Ras Mohamed National Park, Zaranik Protectorate and Siwa.
On 13 March 2015, plans for a proposed new capital of Egypt were announced.




Most of Egypt's rain falls in the winter months. South of Cairo, rainfall averages only around 2 to 5 mm (0.1 to 0.2 in) per year and at intervals of many years. On a very thin strip of the northern coast the rainfall can be as high as 410 mm (16.1 in), mostly between October and March. Snow falls on Sinai's mountains and some of the north coastal cities such as Damietta, Baltim and Sidi Barrani, and rarely in Alexandria. A very small amount of snow fell on Cairo on 13 December 2013, the first time in many decades. Frost is also known in mid-Sinai and mid-Egypt. Egypt is the driest and the sunniest country in the world, and most of its land surface is desert.

Egypt has an unusually hot, sunny and dry climate. Average high temperatures are high in the north but very to extremely high in the rest of the country during summer. The cooler Mediterranean winds consistently blow over the northern sea coast, which helps to get more moderated temperatures, especially at the height of the summertime. The Khamaseen is a hot, dry wind that originates from the vast deserts in the south and blows in the spring or in the early summer.
It bringing scorching sand and dust particles, and usually brings daytime temperatures over 40 °C (104 °F) and sometimes over 50 °C (122 °F) more in the interior, while the relative humidity can drop to 5% or even less. The absolute highest temperatures in Egypt occur when the Khamaseen blows. The weather is always sunny and clear in Egypt, especially in cities such as Aswan, Luxor and Asyut. It is one of the least cloudy and least rainy regions on Earth.
Prior to the construction of the Aswan Dam, the Nile flooded annually (colloquially The Gift of the Nile) replenishing Egypt's soil. This gave Egypt a consistent harvest throughout the years.
The potential rise in sea levels due to global warming could threaten Egypt's densely populated coastal strip and have grave consequences for the country's economy, agriculture and industry. Combined with growing demographic pressures, a significant rise in sea levels could turn millions of Egyptians into environmental refugees by the end of the 21st century, according to some climate experts.




Egypt signed the Rio Convention on Biological Diversity on 9 June 1992, and became a party to the convention on 2 June 1994. It has subsequently produced a National Biodiversity Strategy and Action Plan, which was received by the convention on 31 July 1998. Where many CBD National Biodiversity Strategy and Action Plans neglect biological kingdoms apart from animals and plants, Egypt's plan was unusual in providing balanced information about all forms of life.
The plan stated that the following numbers of species of different groups had been recorded from Egypt: algae (1483 species), animals (about 15,000 species of which more than 10,000 were insects), fungi (more than 627 species), monera (319 species), plants (2426 species), protozoans (371 species). For some major groups, for example lichen-forming fungi and nematode worms, the number was not known. Apart from small and well-studied groups like amphibians, birds, fish, mammals and reptiles, the many of those numbers are likely to increase as further species are recorded from Egypt. For the fungi, including lichen-forming species, for example, subsequent work has shown that over 2200 species have been recorded from Egypt, and the final figure of all fungi actually occurring in the country is expected to be much higher. For the grasses, 284 native and naturalised species have been identified and recorded in Egypt.




The House of Representatives, whose members are elected to serve five-year terms, specialises in legislation. Elections were last held between November 2011 and January 2012 which was later dissolved. The next parliamentary election was announced to be held within 6 months of the constitution's ratification on 18 January 2014, and were held in two phases, from 17 October to 2 December 2015. Originally, the parliament was to be formed before the president was elected, but interim president Adly Mansour pushed the date. The Egyptian presidential election, 2014, took place on 26–28 May 2014. Official figures showed a turnout of 25,578,233 or 47.5%, with Abdel Fattah el-Sisi winning with 23.78 million votes, or 96.91% compared to 757,511 (3.09%) for Hamdeen Sabahi.
After a wave of public discontent with autocratic excesses of the Muslim Brotherhood government of President Mohamed Morsi, on 3 July 2013 General Abdel Fattah el-Sisi announced the removal of Morsi from office and the suspension of the constitution. A 50-member constitution committee was formed for modifying the constitution which was later published for public voting and was adopted on 18 January 2014.
In 2013, Freedom House rated political rights in Egypt at "5" (with 1 representing the most free and 7 the least), and civil liberties at "5", which gave it the freedom rating of "Partly Free".
Egyptian nationalism predates its Arab counterpart by many decades, having roots in the 19th century and becoming the dominant mode of expression of Egyptian anti-colonial activists and intellectuals until the early 20th century. The ideology espoused by Islamists such as the Muslim Brotherhood is mostly supported by the lower-middle strata of Egyptian society.
Egypt has the oldest continuous parliamentary tradition in the Arab world. The first popular assembly was established in 1866. It was disbanded as a result of the British occupation of 1882, and the British allowed only a consultative body to sit. In 1923, however, after the country's independence was declared, a new constitution provided for a parliamentary monarchy.




The legal system is based on Islamic and civil law (particularly Napoleonic codes); and judicial review by a Supreme Court, which accepts compulsory International Court of Justice jurisdiction only with reservations.
Islamic jurisprudence is the principal source of legislation. Sharia courts and qadis are run and licensed by the Ministry of Justice. The personal status law that regulates matters such as marriage, divorce and child custody is governed by Sharia. In a family court, a woman's testimony is worth half of a man's testimony.
On 26 December 2012, the Muslim Brotherhood attempted to institutionalise a controversial new constitution. It was approved by the public in a referendum held 15–22 December 2012 with 64% support, but with only 33% electorate participation. It replaced the 2011 Provisional Constitution of Egypt, adopted following the revolution.
The Penal code was unique as it contains a "Blasphemy Law." The present court system allows a death penalty including against an absent individual tried in absentia. Several Americans and Canadians were sentenced to death in 2012.
On 18 January 2014, the interim government successfully institutionalised a more secular constitution. The president is elected to a four-year term and may serve 2 terms. The parliament may impeach the president. Under the constitution, there is a guarantee of gender equality and absolute freedom of thought. The military retains the ability to appoint the national Minister of Defence for the next 8 years. Under the constitution, political parties may not be based on "religion, race, gender or geography".




The Egyptian Organization for Human Rights is one of the longest-standing bodies for the defence of human rights in Egypt. In 2003, the government established the National Council for Human Rights. The council came under heavy criticism by local activists, who contend it was a propaganda tool for the government to excuse its own violations and to give legitimacy to repressive laws such as the Emergency Law.

The Pew Forum on Religion & Public Life ranks Egypt as the fifth worst country in the world for religious freedom. The United States Commission on International Religious Freedom, a bipartisan independent agency of the US government, has placed Egypt on its watch list of countries that require close monitoring due to the nature and extent of violations of religious freedom engaged in or tolerated by the government. According to a 2010 Pew Global Attitudes survey, 84% of Egyptians polled supported the death penalty for those who leave Islam; 77% supported whippings and cutting off of hands for theft and robbery; and 82% support stoning a person who commits adultery.
In April 2016, such violations have also affected international students and tourists, when an Italian PhD student from the University of Cambridge was found brutally murdered in Cairo after he went missing in January of the same year. Subsequently, Italy withdrew its ambassador to Egypt for consultations in Rome regarding the criminal death of Giulio Regeni, who, at the time, conducted critical academic research on Egyptian labour rights and trade unions. Egyptian law enforcement produced conflicting information on the fate of the Italian citizen, which was unacceptable to Italian investigators. As a result, the Italian press and foreign ministry pointed at the systematic human right violations in Egypt, and threatened with political sanctions unless police leadership and practices undergo significant revisions.
Coptic Christians face discrimination at multiple levels of the government, ranging from disproportionate representation in government ministries to laws that limit their ability to build or repair churches. Intolerance of Bahá'ís and non-orthodox Muslim sects, such as Sufis, Shi'a and Ahmadis, also remains a problem. When the government moved to computerise identification cards, members of religious minorities, such as Bahá'ís, could not obtain identification documents. An Egyptian court ruled in early 2008 that members of other faiths may obtain identity cards without listing their faiths, and without becoming officially recognised.

Clashes continue between police and supporters of former President Mohamed Morsi, at least 595 civilians were killed in Cairo on 14 August 2013, the worst mass killing in Egypt's modern history.
Egypt actively practices capital punishment. Egypt's authorities do not release figures on death sentences and executions, despite repeated requests over the years by human rights organisations. The United Nations human rights office and various NGOs expressed "deep alarm" after an Egyptian Minya Criminal Court sentenced 529 people to death in a single hearing on 25 March 2014. Sentenced supporters of former President Mohamed Morsi will be executed for their alleged role in violence following his ousting in July 2013. The judgment was condemned as a violation of international law. By May 2014, approximately 16,000 people (and as high as more than 40,000 by one independent count), mostly Brotherhood members or supporters, have been imprisoned after the coup  after the Muslim Brotherhood was labelled as terrorist organisation by the post-coup interim Egyptian government.
After Morsi was ousted by the military, the judiciary system aligned itself with the new government, actively supporting the repression of Muslim Brotherhood members. This resulted in a sharp increase in mass death sentences that arose criticism from the US president Barack Obama and the General Secretary of the UN, Ban Ki Moon. In April 2013, one judge of the Minya governatorate of Upper Egypt, sentenced 1,212 people to death. In December 2014 the judge Mohammed Nagi Shahata, notorious for his fierceness in passing on death sentences, condemned to the capital penalty 188 members of the Muslim Brotherhood, for assaulting a police station. Various Egyptian and international human rights organisations have already pointed out the lack of fair trials, that often last only a few minutes and do not take into consideration the procedural standards of fair trials.



Reporters Without Borders ranked Egypt in their World Press Freedom Index as #158 out of 180. At least 18 journalists were imprisoned in Egypt in August 2015. A new anti-terror law was enacted in August 2015 that threatens members of the media with fines ranging from about US$25,000 to 60,000 for the distribution of wrong information on acts of terror inside the country "that differ from official declarations of the Egyptian Department of Defense".




The military is influential in the political and economic life of Egypt and exempts itself from laws that apply to other sectors. It enjoys considerable power, prestige and independence within the state and has been widely considered part of the Egyptian "deep state".
According to the former chair of Israel's Knesset Foreign Affairs and Defense Committee, Yuval Steinitz, the Egyptian Air Force has roughly the same number of modern warplanes as the Israeli Air Force and far more Western tanks, artillery, anti-aircraft batteries and warships than the IDF. Egypt is speculated by Israel to be the second country in the region with a spy satellite, EgyptSat 1 in addition to EgyptSat 2 launched on 16 April 2014.

The United States provides Egypt with annual military assistance, which in 2015 amounted to US$1.3 billion. In 1989, Egypt was designated as a major non-NATO ally of the United States. Nevertheless, ties between the two countries have partially soured since the July 2013 military coup that deposed Islamist president Mohamed Morsi, with the Obama administration condemning Egypt's violent crackdown on the Muslim Brotherhood and its supporters, and cancelling future military exercises involving the two countries. There have been recent attempts, however, to normalise relations between the two, with both governments frequently calling for mutual support in the fight against regional and international terrorism.
The Egyptian military has dozens of factories manufacturing weapons as well as consumer goods. The Armed Forces' inventory includes equipment from different countries around the world. Equipment from the former Soviet Union is being progressively replaced by more modern US, French, and British equipment, a significant portion of which is built under license in Egypt, such as the M1 Abrams tank. Relations with Russia have improved significantly following Mohamed Morsi's removal and both countries have worked since then to strengthen military and trade ties among other aspects of bilateral co-operation. Relations with China have also improved considerably. In 2014, Egypt and China have established a bilateral "comprehensive strategic partnership".
The permanent headquarters of the Arab League are located in Cairo and the body's secretary general has traditionally been Egyptian. This position is currently held by former foreign minister Nabil el-Araby. The Arab League briefly moved from Egypt to Tunis in 1978 to protest the Egypt-Israel Peace Treaty, but it later returned to Cairo in 1989. Gulf monarchies, including the United Arab Emirates and Saudi Arabia, have pledged billions of dollars to help Egypt overcome its economic difficulties since the July 2013 coup.
Following the 1973 war and the subsequent peace treaty, Egypt became the first Arab nation to establish diplomatic relations with Israel. Despite that, Israel is still widely considered as a hostile state by the majority of Egyptians. Egypt has played a historical role as a mediator in resolving various disputes in the Middle East, most notably its handling of the Israeli-Palestinian conflict and the peace process. Egypt's ceasefire and truce brokering efforts in Gaza have hardly been challenged following Israel's evacuation of its settlements from the strip in 2005, despite increasing animosity towards the Hamas government in Gaza following the ouster of Mohamed Morsi, and despite recent attempts by countries like Turkey and Qatar to take over this role.
Ties between Egypt and other non-Arab Middle Eastern nations, including Iran and Turkey, have often been strained. Tensions with Iran are mostly due to Egypt's peace treaty with Israel and its rivalry with traditional Egyptian allies in the Gulf. Turkey's recent support for the now-banned Muslim Brotherhood in Egypt and its alleged involvement in Libya also made of both countries bitter regional rivals.
Egypt is a founding member of the Non-Aligned Movement and the United Nations. It is also a member of the Organisation internationale de la francophonie, since 1983. Former Egyptian Deputy Prime Minister Boutros Boutros-Ghali served as Secretary-General of the United Nations from 1991 to 1996.
In the 21st century, Egypt has had a major problem with immigration, as millions of persons from other African nations flee poverty and war. Border control methods can be "harsh, sometimes lethal."




Egypt is divided into 27 governorates. The governorates are further divided into regions. The regions contain towns and villages. Each governorate has a capital, sometimes carrying the same name as the governorate.




Egypt's economy depends mainly on agriculture, media, petroleum imports, natural gas, and tourism; there are also more than three million Egyptians working abroad, mainly in Saudi Arabia, the Persian Gulf and Europe. The completion of the Aswan High Dam in 1970 and the resultant Lake Nasser have altered the time-honoured place of the Nile River in the agriculture and ecology of Egypt. A rapidly growing population, limited arable land, and dependence on the Nile all continue to overtax resources and stress the economy.
The government has invested in communications and physical infrastructure. Egypt has received United States foreign aid since 1979 (an average of $2.2 billion per year) and is the third-largest recipient of such funds from the United States following the Iraq war. Egypt's economy mainly relies on these sources of income: tourism, remittances from Egyptians working abroad and revenues from the Suez Canal.
Egypt has a developed energy market based on coal, oil, natural gas, and hydro power. Substantial coal deposits in the northeast Sinai are mined at the rate of about 600,000 tonnes (590,000 long tons; 660,000 short tons) per year. Oil and gas are produced in the western desert regions, the Gulf of Suez, and the Nile Delta. Egypt has huge reserves of gas, estimated at 2,180 cubic kilometres (520 cu mi), and LNG up to 2012 exported to many countries. In 2013, the Egyptian General Petroleum Co (EGPC) said the country will cut exports of natural gas and tell major industries to slow output this summer to avoid an energy crisis and stave off political unrest, Reuters has reported. Egypt is counting on top liquid natural gas (LNG) exporter Qatar to obtain additional gas volumes in summer, while encouraging factories to plan their annual maintenance for those months of peak demand, said EGPC chairman, Tarek El Barkatawy. Egypt produces its own energy, but has been a net oil importer since 2008 and is rapidly becoming a net importer of natural gas.

Economic conditions have started to improve considerably, after a period of stagnation, due to the adoption of more liberal economic policies by the government as well as increased revenues from tourism and a booming stock market. In its annual report, the International Monetary Fund (IMF) has rated Egypt as one of the top countries in the world undertaking economic reforms. Some major economic reforms undertaken by the government since 2003 include a dramatic slashing of customs and tariffs. A new taxation law implemented in 2005 decreased corporate taxes from 40% to the current 20%, resulting in a stated 100% increase in tax revenue by the year 2006.

Foreign direct investment (FDI) in Egypt increased considerably before the removal of Hosni Mubarak, exceeding $6 billion in 2006, due to economic liberalisation and privatisation measures taken by minister of investment Mahmoud Mohieddin. Since the fall of Hosni Mubarak in 2011, Egypt has experienced a drastic fall in both foreign investment and tourism revenues, followed by a 60% drop in foreign exchange reserves, a 3% drop in growth, and a rapid devaluation of the Egyptian pound.
Although one of the main obstacles still facing the Egyptian economy is the limited trickle down of wealth to the average population, many Egyptians criticise their government for higher prices of basic goods while their standards of living or purchasing power remains relatively stagnant. Corruption is often cited by Egyptians as the main impediment to further economic growth. The government promised major reconstruction of the country's infrastructure, using money paid for the newly acquired third mobile license ($3 billion) by Etisalat in 2006. In the Corruption Perceptions Index 2013, Egypt was ranked 114 out of 177.

Egypt's most prominent multinational companies are the Orascom Group and Raya Contact Center. The information technology (IT) sector has expanded rapidly in the past few years, with many start-ups selling outsourcing services to North America and Europe, operating with companies such as Microsoft, Oracle and other major corporations, as well as many small and medium size enterprises. Some of these companies are the Xceed Contact Center, Raya, E Group Connections and C3. The IT sector has been stimulated by new Egyptian entrepreneurs with government encouragement.
An estimated 2.7 million Egyptians abroad contribute actively to the development of their country through remittances (US$7.8 billion in 2009), as well as circulation of human and social capital and investment. Remittances, money earned by Egyptians living abroad and sent home, reached a record US$21 billion in 2012, according to the World Bank.
Egyptian society is moderately unequal in terms of income distribution, with an estimated 35 – 40% of Egypt's population earning less than the equivalent of $2 a day, while only around 2–3% may be considered wealthy.




Tourism is one of the most important sectors in Egypt's economy. More than 12.8 million tourists visited Egypt in 2008, providing revenues of nearly $11 billion. The tourism sector employs about 12% of Egypt's workforce. Tourism Minister Hisham Zaazou told industry professionals and reporters that tourism generated some $9.4 billion in 2012, a slight increase over the $9 billion seen in 2011.

The Giza Necropolis is Egypt's most iconic site. It is also Egypt's most popular tourist destination since antiquity, and was popularised in Hellenistic times when the Great Pyramid was listed by Antipater of Sidon as one of the Seven Wonders of the World. Today it is the only one of those wonders still in existence.
Egypt has a wide range of beaches situated on the Mediterranean and the Red Sea that extend to over 3,000 kilometres (1,900 miles). The Red Sea has serene waters, coloured coral reefs, rare fish and beautiful mountains. The Akba Gulf beaches also provide facilities for practising sea sports. Safaga tops the Red Sea zone with its beautiful location on the Suez Gulf. Last but not least, Sharm El Sheikh, Hurghada, Luxor (known as world's greatest open-air museum/ or City of the ⅓ of world monuments), Dahab, Ras Sidr, Marsa Alam, Safaga and the northern coast of the Mediterranean are major tourist's destinations of the recreational tourism.
With a lot of touristic activities in Egypt it's considered a fun place for historical, religious, medical and entertainment tourism. To enter Egypt, it is necessary to have a valid passport and in most cases a visa.




Egypt was producing 691,000 bbl/d of oil and 2,141.05 Tcf of natural gas (in 2013), which makes Egypt as the largest oil producer not member of the Organization of the Petroleum Exporting Countries (OPEC) and the second-largest dry natural gas producer in Africa. In 2013, Egypt was the largest consumer of oil and natural gas in Africa, as more than 20% of total oil consumption and more than 40% of total dry natural gas consumption in Africa. Also, Egypt possesses the largest oil refinery capacity in Africa 726,000 bbl/d (in 2012). Egypt is currently planning to build its first nuclear power plant in El Dabaa, on the northern coast of the country.




Transport in Egypt is centred around Cairo and largely follows the pattern of settlement along the Nile. The main line of the nation's 40,800-kilometre (25,400 mi) railway network runs from Alexandria to Aswan and is operated by Egyptian National Railways. The vehicle road network has expanded rapidly to over 21,000 miles, consisting of 28 line, 796 stations, 1800 train covering the Nile Valley and Nile Delta, the Mediterranean and Red Sea coasts, the Sinai, and the Western oases.

The Cairo Metro in Egypt is the first of only two full-fledged metro systems in Africa and the Arab World. It is considered one of the most important recent projects in Egypt which cost around 12 billion Egyptian pounds. The system consists of three operational lines with a fourth line expected in the future.
Egypt is considered one of the pioneer countries in using air transport having established its most important and main flag carrier airline of Egypt, EgyptAir in 1932, founded by Egyptian industrialist Talaat Harb, today owned by the Egyptian government. The airline is based at Cairo International Airport, its main hub, operating scheduled passenger and freight services to more than 75 destinations in the Middle East, Europe, Africa, Asia, and the Americas. The Current EgyptAir fleet includes 80 aeroplane.




The Suez Canal is an artificial sea-level waterway in Egypt considered the most important centre of the maritime transport in the Middle East, connecting the Mediterranean Sea and the Red Sea. Opened in November 1869 after 10 years of construction work, it allows ship transport between Europe and Asia without navigation around Africa. The northern terminus is Port Said and the southern terminus is Port Tawfiq at the city of Suez. Ismailia lies on its west bank, 3 kilometres (1.9 miles) from the half-way point.
The canal is 193.30 kilometres (120.11 miles) long, 24 metres (79 feet) deep and 205 metres (673 feet) wide as of 2010. It consists of the northern access channel of 22 kilometres (14 miles) (14 mi), the canal itself of 162.25 kilometres (100.82 miles) and the southern access channel of 9 kilometres (5.6 miles). The canal is a single lane with passing places in the "Ballah By-Pass" and the Great Bitter Lake. It contains no locks; seawater flows freely through the canal. In general, the canal north of the Bitter Lakes flows north in winter and south in summer. The current south of the lakes changes with the tide at Suez.
On 26 August 2014 a proposal was made for opening a New Suez Canal. Work on the New Suez Canal was completed in July 2015. The channel was officially inaugurated with a ceremony attended by foreign leaders and featuring military flyovers on 6 August 2015, in accordance with the budgets laid out for the project.




Drinking water supply and sanitation in Egypt is characterised by both achievements and challenges. Among the achievements are an increase of piped water supply between 1990 and 2010 from 89% to 100% in urban areas and from 39% to 93% in rural areas despite rapid population growth, the elimination of open defecation in rural areas during the same period, and in general a relatively high level of investment in infrastructure. Access to an improved water source in Egypt is now practically universal with a rate of 99%. About one half of the population is connected to sanitary sewers.
Partly because of low sanitation coverage about 17,000 children die each year because of diarrhoea. Another challenge is low cost recovery due to water tariffs that are among the lowest in the world. This in turn requires government subsidies even for operating costs, a situation that has been aggravated by salary increases without tariff increases after the Arab Spring. Poor operation of facilities, such as water and wastewater treatment plants, as well as limited government accountability and transparency, are also issues.




Egypt is the most populated country in the Middle East, and the third most populous on the African continent, with about 88 million inhabitants as of 2015. Its population grew rapidly from 1970 to 2010 due to medical advances and increases in agricultural productivity  enabled by the Green Revolution. Egypt's population was estimated at 3 million when Napoleon invaded the country in 1798.
Egypt's people are highly urbanised, being concentrated along the Nile (notably Cairo and Alexandria), in the Delta and near the Suez Canal. Egyptians are divided demographically into those who live in the major urban centres and the fellahin, or farmers, that reside in rural villages.
While emigration was restricted under Nasser, thousands of Egyptian professionals were dispatched abroad in the context of the Arab Cold War. Egyptian emigration was liberalised in 1971, under President Sadat, reaching record numbers after the 1973 oil crisis. An estimated 2.7 million Egyptians live abroad. Approximately 70% of Egyptian migrants live in Arab countries (923,600 in Saudi Arabia, 332,600 in Libya, 226,850 in Jordan, 190,550 in Kuwait with the rest elsewhere in the region) and the remaining 30% reside mostly in Europe and North America (318,000 in the United States, 110,000 in Canada and 90,000 in Italy). The process of emigrating to non-Arab states has been ongoing since the 1950s.
Among the people of the ancient Near East, only the Egyptians have stayed where they were and remained what they were, although they have changed their language once and their religion twice. In a sense, they constitute the world's oldest nation. For most of their history, Egypt has been a state, but only in recent years has it been truly a nation-state, with a government claiming the allegiance of its subjects on the basis of a common identity.



Ethnic Egyptians are by far the largest ethnic group in the country, constituting 91% of the total population. Ethnic minorities include the Abazas, Turks, Greeks, Bedouin Arab tribes living in the eastern deserts and the Sinai Peninsula, the Berber-speaking Siwis (Amazigh) of the Siwa Oasis, and the Nubian communities clustered along the Nile. There are also tribal Beja communities concentrated in the south-eastern-most corner of the country, and a number of Dom clans mostly in the Nile Delta and Faiyum who are progressively becoming assimilated as urbanisation increases.
Egypt also hosts an unknown number of refugees and asylum seekers, estimated to be between 500,000 and 3 million. There are some 70,000 Palestinian refugees, and about 150,000 recently arrived Iraqi refugees, but the number of the largest group, the Sudanese, is contested. The once-vibrant and ancient Greek and Jewish communities in Egypt have almost disappeared, with only a small number remaining in the country, but many Egyptian Jews visit on religious or other occasions and tourism. Several important Jewish archaeological and historical sites are found in Cairo, Alexandria and other cities.




The official language of the Republic is Modern Standard Arabic. Arabic was adopted by the Egyptians after the Arab invasion of Egypt. The spoken languages are: Egyptian Arabic (68%), Sa'idi Arabic (29%), Eastern Egyptian Bedawi Arabic (1.6%), Sudanese Arabic (0.6%), Domari (0.3%), Nobiin (0.3%), Beja (0.1%), Siwi and others. Additionally, Greek, Armenian and Italian are the main languages of immigrants. In Alexandria in the 19th century there was a large community of Italian Egyptians and Italian was the "lingua franca" of the city.
The main foreign languages taught in schools, by order of popularity, are English, French, German and Italian.
Historical Egyptian languages, also known as Copto-Egyptian, consist of ancient Egyptian and Coptic, and form a separate branch among the family of Afroasiatic languages. The "Koiné" dialect of the Greek language, though not native to Egypt, was important in Hellenistic Alexandria. It was used extensively in the philosophy and science of that culture. Later translations from Greek to Arabic became the subject of study by Arab scholars.




Egypt is a predominantly Sunni Muslim country with Islam as its state religion. The percentage of adherents of various religions is a controversial topic in Egypt. An estimated 90% are identified as Muslim, 9% as Coptic Christians, and 1% as other Christian denominations. Non-denominational Muslims form roughly 12% of the population.
Although Egypt was a Christian country before the 7th Century, after Islam arrived, the country was gradually Islamised into a majority-Muslim country. It is not known when Muslims reached a majority variously estimated from ca. 1000 A.D. to as late as the 14th century. Egypt emerged as a centre of politics and culture in the Muslim world. Under Anwar Sadat, Islam became the official state religion and Sharia the main source of law. It is estimated that 15 million Egyptians follow Native Sufi orders, with the Sufi leadership asserting that the numbers are much greater as many Egyptian Sufis are not officially registered with a Sufi order.
There is also a Shi'a minority. The Jerusalem Center for Public Affairs estimates the Shia population at 1 to 2.2 million and could measure as much as 3 million. The Ahmadiyya population is estimated at less than 50,000, whereas the Salafi (ultra-conservative) population is estimated at five to six million. Cairo is famous for its numerous mosque minarets and has been dubbed "The City of 1,000 Minarets".

Of the Christian minority in Egypt over 90% belong to the native Coptic Orthodox Church of Alexandria, an Oriental Orthodox Christian Church. Other native Egyptian Christians are adherents of the Coptic Catholic Church, the Evangelical Church of Egypt and various other Protestant denominations. Non-native Christian communities are largely found in the urban regions of Cairo and Alexandria, such as the Syro-Lebanese, who belong to Greek Catholic, Greek Orthodox, and Maronite Catholic denominations.
Ethnic Greeks also made up a large Greek Orthodox population in the past. Likewise, Armenians made up the then larger Armenian Orthodox and Catholic communities. Egypt also used to have a large Roman Catholic community, largely made up of Italians and Maltese. These non-native communities were much larger in Egypt before the Nasser regime and the nationalisation that took place.
Egypt hosts two major religious institutions, the Coptic Orthodox Church of Alexandria, established in the middle of the 1st century CE by Saint Mark the Evangelist, and Al-Azhar University, founded in 970 CE by the Fatimids as the first Islamic School and University in the world.
Egypt recognises only three religions: Islam, Christianity, and Judaism. Other faiths and minority Muslim sects practised by Egyptians, such as the small Bahá'í and Ahmadi community, are not recognised by the state and face persecution since they are labelled as far right groups that threaten Egypt's national security. Individuals, particularly Baha'is and atheists, wishing to include their religion (or lack thereof) on their mandatory state issued identification cards are denied this ability (see Egyptian identification card controversy), and are put in the position of either not obtaining required identification or lying about their faith. A 2008 court ruling allowed members of unrecognised faiths to obtain identification and leave the religion field blank.







Egypt is a recognised cultural trend-setter of the Arabic-speaking world. Contemporary Arabic and Middle-Eastern culture is heavily influenced by Egyptian literature, music, film and television. Egypt gained a regional leadership role during the 1950s and 1960s, giving a further enduring boost to the standing of Egyptian culture in the Arabic-speaking world.
Egyptian identity evolved in the span of a long period of occupation to accommodate Islam, Christianity and Judaism; and a new language, Arabic, and its spoken descendant, Egyptian Arabic which is also based on many Ancient Egyptian words.
The work of early 19th-century scholar Rifa'a al-Tahtawi renewed interest in Egyptian antiquity and exposed Egyptian society to Enlightenment principles. Tahtawi co-founded with education reformer Ali Mubarak a native Egyptology school that looked for inspiration to medieval Egyptian scholars, such as Suyuti and Maqrizi, who themselves studied the history, language and antiquities of Egypt.
Egypt's renaissance peaked in the late 19th and early 20th centuries through the work of people like Muhammad Abduh, Ahmed Lutfi el-Sayed, Muhammad Loutfi Goumah, Tawfiq el-Hakim, Louis Awad, Qasim Amin, Salama Moussa, Taha Hussein and Mahmoud Mokhtar. They forged a liberal path for Egypt expressed as a commitment to personal freedom, secularism and faith in science to bring progress.




The Egyptians were one of the first major civilisations to codify design elements in art and architecture. Egyptian blue, also known as calcium copper silicate is a pigment used by Egyptians for thousands of years. It is considered to be the first synthetic pigment. The wall paintings done in the service of the Pharaohs followed a rigid code of visual rules and meanings. Egyptian civilisation is renowned for its colossal pyramids, temples and monumental tombs.
Well-known examples are the Pyramid of Djoser designed by ancient architect and engineer Imhotep, the Sphinx, and the temple of Abu Simbel. Modern and contemporary Egyptian art can be as diverse as any works in the world art scene, from the vernacular architecture of Hassan Fathy and Ramses Wissa Wassef, to Mahmoud Mokhtar's sculptures, to the distinctive Coptic iconography of Isaac Fanous. The Cairo Opera House serves as the main performing arts venue in the Egyptian capital.




Egyptian literature traces its beginnings to ancient Egypt and is some of the earliest known literature. Indeed, the Egyptians were the first culture to develop literature as we know it today, that is, the book. It is an important cultural element in the life of Egypt. Egyptian novelists and poets were among the first to experiment with modern styles of Arabic literature, and the forms they developed have been widely imitated throughout the Middle East. The first modern Egyptian novel Zaynab by Muhammad Husayn Haykal was published in 1913 in the Egyptian vernacular. Egyptian novelist Naguib Mahfouz was the first Arabic-language writer to win the Nobel Prize in Literature. Egyptian women writers include Nawal El Saadawi, well known for her feminist activism, and Alifa Rifaat who also writes about women and tradition.
Vernacular poetry is perhaps the most popular literary genre among Egyptians, represented by the works of Ahmed Fouad Negm (Fagumi), Salah Jaheen and Abdel Rahman el-Abnudi.



Egypt's media industry has flourished, with more than thirty satellite channels and over one hundred motion pictures produced each year.
Egyptian media are highly influential throughout the Arab World, attributed to large audiences and increasing freedom from government control. Freedom of the media is guaranteed in the constitution; however, many laws still restrict this right.




Egyptian cinema became a regional force with the coming of sound. In 1936, Studio Misr, financed by industrialist Talaat Harb, emerged as the leading Egyptian studio, a role the company retained for three decades. For over 100 years, more than 4000 films have been produced in Egypt, three quarters of the total Arab production. Egypt is considered the leading country in the field of cinema in the Middle East. Actors from all over the Arab World seek to appear in the Egyptian cinema for the sake of fame. The Cairo International Film Festival has been rated as one of 11 festivals with a top class rating worldwide by the International Federation of Film Producers' Associations.




Egyptian music is a rich mixture of indigenous, Mediterranean, African and Western elements. It has been an integral part of Egyptian culture since antiquity. The ancient Egyptians credited one of their gods Hathor with the invention of music, which Osiris in turn used as part of his effort to civilise the world. Egyptians used music instruments since then.
Contemporary Egyptian music traces its beginnings to the creative work of people such as Abdu al-Hamuli, Almaz and Mahmoud Osman, who influenced the later work of Sayed Darwish, Umm Kulthum, Mohammed Abdel Wahab and Abdel Halim Hafez whose age is considered the golden age of music in Egypt and the whole Middle East and North-Africa. Prominent contemporary Egyptian pop singers include Amr Diab and Mohamed Mounir.




Today, Egypt is often considered the home of belly dance. Egyptian belly dance has two main styles – raqs baladi and raqs sharqi. There are also numerous folkloric and character dances that may be part of an Egyptian-style belly dancer's repertoire, as well as the modern shaabi street dance which shares some elements with raqs baladi.




Egypt has one of the oldest civilisations in the world. It has been in contact with many other civilisations and nations and has been through so many eras, starting from prehistoric age to the modern age, passing through so many ages such as; Pharonic, Roman, Greek, Islamic and many other ages. Because of this wide variation of ages, the continuous contact with other nations and the big number of conflicts Egypt had been through, at least 60 museums may be found in Egypt, mainly covering a wide area of these ages and conflicts.

The three main museums in Egypt are The Egyptian Museum which has more than 120,000 items, the Egyptian National Military Museum and the 6th of October Panorama.
The Grand Egyptian Museum (GEM), also known as the Giza Museum, is an under construction museum that will house the largest collection of ancient Egyptian artifacts in the world, it has been described as the world's largest archaeological museum. The museum was scheduled to open in 2015 and will be sited on 50 hectares (120 acres) of land approximately two kilometres (1.2 miles) from the Giza Necropolis and is part of a new master plan for the plateau. The Minister of Antiquities Mamdouh al-Damaty announced in May 2015 that the museum will be partially opened in May 2018.



Egypt celebrates many festivals and religious carnivals, also known as mulid. They are usually associated with a particular Coptic or Sufi saint, but are often celebrated by Egyptians irrespective of creed or religion. Ramadan has a special flavour in Egypt, celebrated with sounds, lights (local lanterns known as fawanees) and much flare that many Muslim tourists from the region flock to Egypt to witness during Ramadan.
The ancient spring festival of Sham en Nisim (Coptic: Ϭⲱⲙ‘ⲛⲛⲓⲥⲓⲙ shom en nisim) has been celebrated by Egyptians for thousands of years, typically between the Egyptian months of Paremoude (April) and Pashons (May), following Easter Sunday.




Egyptian cuisine is notably conducive to vegetarian diets, as it relies heavily on vegetable dishes. Though food in Alexandria and the coast of Egypt tends to use a great deal of fish and other seafood, for the most part Egyptian cuisine is based on foods that grow out of the ground. Meat has been very expensive for most Egyptians throughout history, so a great number of vegetarian dishes have been developed.
Some consider kushari (a mixture of rice, lentils, and macaroni) to be the national dish. Fried onions can be also added to kushari. In addition, ful medames (mashed fava beans) is one of the most popular dishes. Fava bean is also used in making falafel (also known as "ta‘miya"), which may have originated in Egypt and spread to other parts of the Middle East. Garlic fried with coriander is added to molokhiya, a popular green soup made from finely chopped jute leaves, sometimes with chicken or rabbit.




Football is the most popular national sport of Egypt. The Cairo Derby is one of the fiercest derbies in Africa, and the BBC picked it as one of the 7 toughest derbies in the world. Al Ahly is the most successful club of the 20th century in the African continent according to CAF, closely followed by their rivals Zamalek SC. Al Ahly was named in 2000 by the Confederation of African Football as the "African Club of the Century". With twenty titles, Al Ahly is currently the world's most successful club in terms of international trophies, surpassing Italy's A.C. Milan and Argentina's Boca Juniors, both having eighteen.
The Egyptian national football team known as the "Pharaohs" won the African Cup of Nations seven times, including three times in a row in 2006, 2008, and 2010. Considered the most successful African national team and one of the very few African teams that reached the 9th ranking on the FIFA world ranks, Egypt has only qualified to the FIFA World Cup two times only though. The Egyptian Youth National team "Young Pharaohs" won the Bronze Medal of the 2001 FIFA youth world cup in Argentina.
Squash and tennis are other popular sports in Egypt. The Egyptian squash team has been known for its fierce competition in international championships since the 1930s. Amr Shabana and Ramy Ashour are Egypt's best players and both were ranked as "World's Number One Squash Player".
Among all African nations, the Egypt national basketball team holds the record for best performance at the Basketball World Cup and at the Summer Olympics. Further, the team has won a record number of 16 medals at the African Championship.
In 1999, Egypt hosted the IHF World Men's Handball Championship, and in 2001, the national handball team achieved its best result in the tournament by reaching the fourth place. Egypt has won first place five times in the African Men's Handball Championship, five times second place, and four times third place. In addition to that, it also championed the Mediterranean Games in 2013, the Beach Handball World Championships in 2004 and the Summer Youth Olympics in 2010.
Egypt has taken part in the Summer Olympic Games since 1912 and hosted the first Mediterranean Games in 1951, Alexandria.
Egypt has hosted several international competitions. the last one was 2009 FIFA U-20 World Cup which took place between 24 September – 16 October 2009.
On Friday 19 September of the year 2014, Guinness World Records has announced that Egyptian scuba diver Ahmed Gabr is the new title holder for deepest salt water scuba dive, at 332.35 metres (1,090.4 feet). Ahmed set a new world record Friday when he reached a depth of more than 1,000 feet (300 metres). The 14-hour feat took Gabr 1,066 feet (325 metres) down into the abyss near the Egyptian town of Dahab in the Red Sea, where he works as a diving instructor.
On 1 September 2015 Raneem El Weleily was ranked as the world number one woman squash player. Other Egyptian squash player women are Nour El Tayeb, Omneya Abdel Kawy, Kanzy Emad El-Defrawy and Nour El Sherbini.




The wired and wireless telecommunication industry in Egypt started in 1854 with the launch of the country's first telegram line connecting Cairo and Alexandria. The first telephone line between the two cities was installed in 1881. In September 1999 a national project for a technological renaissance was announced reflecting the commitment of the Egyptian government to developing the country's IT-sector.



Cellular GSM services were first launched in Egypt in 1996. As of June 2011, it is currently offering 2G/3G service, while LTE is under trials. Egypt has 3 companies offering cellular services:
Orange Egypt, owned by Orange S.A.;
Vodafone Egypt, owned by Vodafone and Telecom Egypt; and
Etisalat Egypt, owned by Emirates Telecommunication Corporation.




Egypt Post is the company responsible for postal service in Egypt. Established in 1865, it is one of the oldest governmental institutions in the country. Egypt is one of 21 countries that contributed to the establishment of the Universal Postal Union, initially named the General Postal Union, as signatory of the Treaty of Bern.




The illiteracy rate has decreased since 1996 from 39.4 to 25.9 percent in 2013.The adult literacy rate as of July 2014 was estimated at 73.9%. The illiteracy rate is highest among those over 60 years of age being estimated at around 64.9%, while illiteracy among youth between 15 and 24 years of age was listed at 8.6 percent.
A European-style education system was first introduced in Egypt by the Ottomans in the early 19th century to nurture a class of loyal bureaucrats and army officers. Under British occupation investment in education was curbed drastically, and secular public schools, which had previously been free, began to charge fees.
In the 1950s, president Nasser phased in free education for all Egyptians. The Egyptian curriculum influenced other Arab education systems, which often employed Egyptian-trained teachers. Demand soon outstripped the level of available state resources, causing the quality of public education to deteriorate. Today this trend has culminated in poor teacher–student ratios (often around one to fifty) and persistent gender inequality.
Basic education, which includes six years of primary and three years of preparatory school, is a right for Egyptian children from the age of six. After grade 9, students are tracked into one of two strands of secondary education: general or technical schools. General secondary education prepares students for further education, and graduates of this track normally join higher education institutes based on the results of the Thanaweya Amma, the leaving exam.
Technical secondary education has two strands, one lasting three years and a more advanced education lasting five. Graduates of these schools may have access to higher education based on their results on the final exam, but this is generally uncommon.
Cairo University is ranked as 401–500 according to the Academic Ranking of World Universities (Shanghai Ranking) and 551–600 according to QS World University Rankings. American University in Cairo is ranked as 360 according to QS World University Rankings and Al-Azhar University, Alexandria University and Ain Shams University fall in the 701+ range. Egypt is currently opening new research institutes for the aim of modernising research in the nation, the most recent example of which is Zewail City of Science and Technology.




Egyptian life expectancy at birth was 73.20 years in 2011, or 71.30 years for males and 75.20 years for females. Egypt spends 3.7 percent of its gross domestic product on health including treatment costs 22 percent incurred by citizens and the rest by the state. In 2010, spending on healthcare accounted for 4.66% of the country's GDP. In 2009, there were 16.04 physicians and 33.80 nurses per 10,000 inhabitants.
As a result of modernisation efforts over the years, Egypt's healthcare system has made great strides forward. Access to healthcare in both urban and rural areas greatly improved and immunisation programs are now able to cover 98% of the population. Life expectancy increased from 44.8 years during the 1960s to 72.12 years in 2009. There was a noticeable decline of the infant mortality rate (during the 1970s to the 1980s the infant mortality rate was 101-132/1000 live births, in 2000 the rate was 50-60/1000, and in 2008 it was 28-30/1000).
According to the World Health Organization in 2008, an estimated 91.1% of Egypt's girls and women aged 15 to 49 have been subjected to genital mutilation, despite being illegal in the country. In 2016 the law was amended to impose tougher penalties on those convicted of performing the procedure, pegging the highest jail term at 15 years. Those who escort victims to the procedure can also face jail terms up to 3 years.
The Egyptian government has been keen on extending the coverage of health insurance. The total number of insured Egyptians reached 37 million in 2009, of which 11 million are minors, providing an insurance coverage of approximately 52 percent of Egypt's population.




Index of Egypt-related articles
Outline of ancient Egypt
Outline of Egypt









Shaw, Ian (2003). The Oxford History of Ancient Egypt. Oxford, England: Oxford University Press. ISBN 0-19-280458-8. 




Government
Egypt's Government Services Portal (Arabic, English)
Egypt Information Portal (Arabic, English)
Egypt Information and Decision Support Center (Arabic, English)
Egypt State Information Services (Arabic, English, French)
Chief of State and Cabinet Members
Egyptian Tourist Authority
General
Country Profile from the BBC News
"Egypt". The World Factbook. Central Intelligence Agency. 
Egypt profile from Africa.com
Egypt web resources provided by GovPubs at the University of Colorado–Boulder Libraries
Egypt news
Egypt profiles of people and institutions provided by the Arab Decision project
Egypt at DMOZ
 Wikimedia Atlas of Egypt
 Geographic data related to Egypt at OpenStreetMap
Egypt Maps – Perry-Castañeda Library Map Collection, University of Texas at Austin
Trade
World Bank Summary Trade Statistics Egypt
Other
History of Egypt, Chaldea, Syria, Babylonia, and Assyria in the Light of Recent Discovery by Leonard William King, at Project Gutenberg.
Egyptian History (urdu)
By Nile and Tigris – a narrative of journeys in Egypt and Mesopotamia on behalf of the British museum between 1886 and 1913, by Sir E. A. Wallis Budge, 1920 (DjVu and layered PDF formats)
Napoleon on the Nile: Soldiers, Artists, and the Rediscovery of Egypt.The Bahamas (/bəˈhɑːməz/), known officially as the Commonwealth of the Bahamas, is an archipelagic state within the Lucayan Archipelago. It consists of more than 700 islands, cays, and islets in the Atlantic Ocean and is located north of Cuba and Hispaniola (Haiti and the Dominican Republic); northwest of the Turks and Caicos Islands; southeast of the US state of Florida and east of the Florida Keys. The state capital is Nassau on the island of New Providence. The designation of "The Bahamas" can refer to either the country or the larger island chain that it shares with the Turks and Caicos Islands. As stated in the mandate/manifesto of the Royal Bahamas Defence Force, the Bahamas territory encompasses 470,000 km2 (180,000 sq mi) of ocean space.
The Bahamas were the site of Columbus' first landfall in the New World in 1492. At that time, the islands were inhabited by the Lucayan, a branch of the Arawakan-speaking Taino people. Although the Spanish never colonised the Bahamas, they shipped the native Lucayans to slavery in Hispaniola. The islands were mostly deserted from 1513 until 1648, when English colonists from Bermuda settled on the island of Eleuthera.
The Bahamas became a British Crown colony in 1718, when the British clamped down on piracy. After the American War of Independence, the Crown resettled thousands of American Loyalists in the Bahamas; they brought their slaves with them and established plantations on land grants. Africans constituted the majority of the population from this period. The Bahamas became a haven for freed African slaves: the Royal Navy resettled Africans here liberated from illegal slave ships; American slaves and Seminoles escaped here from Florida; and the government freed American slaves carried on United States domestic ships that had reached the Bahamas due to weather. Slavery in the Bahamas was abolished in 1834. Today the descendants of slaves and free Africans make up nearly 90% of the population; issues related to the slavery years are part of society.
The Bahamas became an independent Commonwealth realm in 1973, retaining Queen Elizabeth II as its monarch. In terms of gross domestic product per capita, the Bahamas is one of the richest countries in the Americas (following the United States and Canada), with an economy based on tourism and finance.



The name Bahamas is derived from either the Taino ba ha ma ("big upper middle land"), which was a term for the region used by the indigenous Amerindians, or from the Spanish baja mar ("shallow water or sea" or "low tide") reflecting the shallow waters of the area. Alternatively, it may originate from Guanahani, a local name of unclear meaning. In English, The Bahamas is one of only two countries whose self-standing short name begins with the word "the", along with The Gambia.




Taino people moved into the uninhabited southern Bahamas from Hispaniola and Cuba around the 11th century, having migrated there from South America. They came to be known as the Lucayan people. An estimated 30,000 Lucayan inhabited the Bahamas at the time of Christopher Columbus' arrival in 1492.
Columbus's first landfall in the New World was on an island he named San Salvador (known to the Lucayan as Guanahani). Some researchers believe this site to be present-day San Salvador Island (formerly known as Watling's Island), situated in the southeastern Bahamas. An alternative theory holds that Columbus landed to the southeast on Samana Cay, according to calculations made in 1986 by National Geographic writer and editor Joseph Judge, based on Columbus's log. Evidence in support of this remains inconclusive. On the landfall island, Columbus made first contact with the Lucayan and exchanged goods with them.
The Spanish forced much of the Lucayan population to Hispaniola for use as forced labour. The slaves suffered from harsh conditions and most died from contracting diseases to which they had no immunity; half of the Taino died from smallpox alone. The population of the Bahamas was severely diminished.
In 1648, the Eleutherian Adventurers, led by William Sayle, migrated from Bermuda. These English Puritans established the first permanent European settlement on an island which they named Eleuthera—the name derives from the Greek word for freedom. They later settled New Providence, naming it Sayle's Island after one of their leaders. To survive, the settlers salvaged goods from wrecks.
In 1670 King Charles II granted the islands to the Lords Proprietors of the Carolinas in North America. They rented the islands from the king with rights of trading, tax, appointing governors, and administering the country. In 1684 Spanish corsair Juan de Alcon raided the capital, Charles Town (later renamed Nassau). In 1703 a joint Franco-Spanish expedition briefly occupied the Bahamian capital during the War of the Spanish Succession.




During proprietary rule, the Bahamas became a haven for pirates, including the infamous Blackbeard (c.1680–1718). To put an end to the 'Pirates' republic' and restore orderly government, Britain made the Bahamas a crown colony in 1718 under the royal governorship of Woodes Rogers. After a difficult struggle, he succeeded in suppressing piracy. In 1720, Rogers led local militia to drive off a Spanish attack.
During the American War of Independence in the late 18th century, the islands became a target for American naval forces under the command of Commodore Esek Hopkins. US Marines occupied the capital of Nassau for a fortnight.
In 1782, following the British defeat at Yorktown, a Spanish fleet appeared off the coast of Nassau. The city surrendered without a fight. Spain returned possession of the Bahamas to Britain the following year, under the terms of the Treaty of Paris. Before the news was received, however, the islands were recaptured by a small British force led by Andrew Deveaux.
After American independence, the British resettled some 7,300 Loyalists with their slaves in the Bahamas, and granted land to the planters to help compensate for losses on the continent. These Loyalists, who included Deveaux, established plantations on several islands and became a political force in the capital. European Americans were outnumbered by the African-American slaves they brought with them, and ethnic Europeans remained a minority in the territory.
In 1807, the British abolished the slave trade, followed by the United States the next year. During the following decades, the Royal Navy intercepted the trade; they resettled in the Bahamas thousands of Africans liberated from slave ships.
In the 1820s during the period of the Seminole Wars in Florida, hundreds of American slaves and African Seminoles escaped from Cape Florida to the Bahamas. They settled mostly on northwest Andros Island, where they developed the village of Red Bays. From eyewitness accounts, 300 escaped in a mass flight in 1823, aided by Bahamians in 27 sloops, with others using canoes for the journey. This was commemorated in 2004 by a large sign at Bill Baggs Cape Florida State Park. Some of their descendants in Red Bays continue African Seminole traditions in basket making and grave marking.
The United States' National Park Service, which administers the National Underground Railroad Network to Freedom, is working with the African Bahamian Museum and Research Center (ABAC) in Nassau on development to identify Red Bays as a site related to American slaves' search for freedom. The museum has researched and documented the African Seminoles' escape from southern Florida. It plans to develop interpretive programs at historical sites in Red Bay associated with the period of their settlement in the Bahamas.
In 1818, the Home Office in London had ruled that "any slave brought to the Bahamas from outside the British West Indies would be manumitted." This led to a total of nearly 300 slaves owned by US nationals being freed from 1830 to 1835. The American slave ships Comet and Encomium used in the United States domestic coastwise slave trade, were wrecked off Abaco Island in December 1830 and February 1834, respectively. When wreckers took the masters, passengers and slaves into Nassau, customs officers seized the slaves and British colonial officials freed them, over the protests of the Americans. There were 165 slaves on the Comet and 48 on the Encomium. Britain finally paid an indemnity to the United States in those two cases in 1855, under the Treaty of Claims of 1853, which settled several compensation cases between the two nations.
Slavery was abolished in the British Empire on 1 August 1834. After that British colonial officials freed 78 American slaves from the Enterprise, which went into Bermuda in 1835; and 38 from the Hermosa, which wrecked off Abaco Island in 1840. The most notable case was that of the Creole in 1841: as a result of a slave revolt on board, the leaders ordered the American brig to Nassau. It was carrying 135 slaves from Virginia destined for sale in New Orleans. The Bahamian officials freed the 128 slaves who chose to stay in the islands. The Creole case has been described as the "most successful slave revolt in U.S. history".
These incidents, in which a total of 447 slaves belonging to US nationals were freed from 1830 to 1842, increased tension between the United States and Great Britain. They had been co-operating in patrols to suppress the international slave trade. But, worried about the stability of its large domestic slave trade and its value, the United States argued that Britain should not treat its domestic ships that came to its colonial ports under duress, as part of the international trade. The United States worried that the success of the Creole slaves in gaining freedom would encourage more slave revolts on merchant ships.




In August 1940, after his abdication of the British throne, the Duke of Windsor was installed as Governor of the Bahamas, arriving with his wife, the Duchess. Although disheartened at the condition of Government House, they "tried to make the best of a bad situation". He did not enjoy the position, and referred to the islands as "a third-class British colony".
He opened the small local parliament on 29 October 1940. The couple visited the "Out Islands" that November, on Axel Wenner-Gren's yacht, which caused controversy; the British Foreign Office strenuously objected because they had been advised (mistakenly) by United States intelligence that Wenner-Gren was a close friend of the Luftwaffe commander Hermann Göring of Nazi Germany.
The Duke was praised at the time for his efforts to combat poverty on the islands. A 1991 biography by Philip Ziegler, however, described him as contemptuous of the Bahamians and other non-white peoples of the Empire. He was praised for his resolution of civil unrest over low wages in Nassau in June 1942, when there was a "full-scale riot". Ziegler said that the Duke blamed the trouble on "mischief makers – communists" and "men of Central European Jewish descent, who had secured jobs as a pretext for obtaining a deferment of draft".
The Duke resigned the post on 16 March 1945.




Modern political development began after the Second World War. The first political parties were formed in the 1950s. The British Parliament authorised the islands as internally self-governing in 1964, with Sir Roland Symonette, of the United Bahamian Party, as the first Premier.
A new constitution granting the Bahamas internal autonomy went into effect on 7 January 1964. In 1967, Lynden Pindling of the Progressive Liberal Party, became the first black Premier of the majority-black colony; in 1968 the title of the position was changed to Prime Minister. In 1968, Pindling announced that the Bahamas would seek full independence. A new constitution giving the Bahamas increased control over its own affairs was adopted in 1968.
The British House of Lords voted to give the Bahamas its independence on 22 June 1973. Prince Charles delivered the official documents to Prime Minister Lynden Pindling, officially declaring the Bahamas a fully independent nation on 10 July 1973. It joined the Commonwealth of Nations on the same day. Sir Milo Butler was appointed the first Governor-General of the Bahamas (the official representative of Queen Elizabeth II) shortly after independence. The Bahamas joined the International Monetary Fund and the World Bank on 22 August 1973, and it joined the United Nations on 18 September 1973.
Based on the twin pillars of tourism and offshore finance, the Bahamian economy has prospered since the 1950s. Significant challenges in areas such as education, health care, housing, international narcotics trafficking and illegal immigration from Haiti continue to be issues.
The University of The Bahamas is the national higher education/tertiary system. Offering baccalaureate, masters and associate degrees, COB has three campuses, and teaching and research centres throughout the Bahamas. COB is on track to become the national "University of The Bahamas" (UOB) in 2015.




The country lies between latitudes 20° and 28°N, and longitudes 72° and 80°W.
In 1864, the Governor of the Bahamas reported that there were 29 islands, 661 cays, and 2,387 rocks in the colony.
The closest island to the United States is Bimini, which is also known as the gateway to the Bahamas. The island of Abaco is to the east of Grand Bahama. The southeasternmost island is Inagua. The largest island is Andros Island. Other inhabited islands include Eleuthera, Cat Island, Long Island, San Salvador Island, Acklins, Crooked Island, Exuma, Berry Islands and Mayaguana. Nassau, capital city of the Bahamas, lies on the island of New Providence.
All the islands are low and flat, with ridges that usually rise no more than 15 to 20 m (49 to 66 ft). The highest point in the country is Mount Alvernia (formerly Como Hill) on Cat Island. It has an elevation of 63 metres (207 ft).

To the southeast, the Turks and Caicos Islands, and three more extensive submarine features called Mouchoir Bank, Silver Bank and Navidad Bank, are geographically a continuation of the Bahamas.




The climate of the Bahamas is tropical savannah climate or Aw according to Köppen climate classification. As such, there has never been a frost or freeze reported in the Bahamas, although every few decades low temperatures can fall into the 3–5 °C (37–41 °F) range for a few hours when a severe cold outbreak comes off the North American landmass. Otherwise, the low latitude, warm tropical Gulf Stream, and low elevation give the Bahamas a warm and winterless climate. There is only an 8 °C difference between the warmest month and coolest month in most of the Bahama islands. As with most tropical climates, seasonal rainfall follows the sun, and summer is the wettest season. The Bahamas are often sunny and dry for long periods of time, and average more than 3,000 hours or 340 days of sunlight annually.
Tropical storms and hurricanes affect the Bahamas. In 1992, Hurricane Andrew passed over the northern portions of the islands, and Hurricane Floyd passed near the eastern portions of the islands in 1999.




The Bahamas is a parliamentary constitutional monarchy headed by Queen Elizabeth II in her role as Queen of the Bahamas. Political and legal traditions closely follow those of the United Kingdom and the Westminster system. The Bahamas is a member of the Commonwealth of Nations as a Commonwealth realm, retaining the Queen as head of state (represented by a Governor-General).
Legislative power is vested in a bicameral parliament, which consists of a 38-member House of Assembly (the lower house), with members elected from single-member districts, and a 16-member Senate, with members appointed by the Governor-General, including nine on the advice of the Prime Minister, four on the advice of the Leader of Her Majesty's Loyal Opposition, and three on the advice of the Prime Minister after consultation with the Leader of the Opposition. The House of Assembly carries out all major legislative functions. As under the Westminster system, the Prime Minister may dissolve Parliament and call a general election at any time within a five-year term.
The Prime Minister is the head of government and is the leader of the party with the most seats in the House of Assembly. Executive power is exercised by the Cabinet, selected by the Prime Minister and drawn from his supporters in the House of Assembly. The current Governor-General is Dame Marguerite Pindling, and the current Prime Minister is The Rt. Hon. Perry Christie, P.C., M.P..
Constitutional safeguards include freedom of speech, press, worship, movement and association. The judiciary is independent of the executive and the legislature. Jurisprudence is based on English law.



The Bahamas has a two-party system dominated by the centre-left Progressive Liberal Party and the centre-right Free National Movement. A handful of splinter parties have been unable to win election to parliament. These parties have included the Bahamas Democratic Movement, the Coalition for Democratic Reform, Bahamian Nationalist Party and the Democratic National Alliance.




The Bahamas has strong bilateral relationships with the United States and the United Kingdom, represented by an ambassador in Washington and High Commissioner in London. The Bahamas also associates closely with other nations of the Caribbean Community (CARICOM).




Its military is the Royal Bahamas Defence Force (the RBDF), the navy of the Bahamas which includes a land unit called Commando Squadron (Regiment) and an Air Wing (Air Force). Under the Defence Act, the RBDF has been mandated, in the name of the Queen, to defend the Bahamas, protect its territorial integrity, patrol its waters, provide assistance and relief in times of disaster, maintain order in conjunction with the law enforcement agencies of the Bahamas, and carry out any such duties as determined by the National Security Council. The Defence Force is also a member of the Caribbean Community (CARICOM)'s Regional Security Task Force.
The RBDF came into existence on 31 March 1980. Their duties include defending the Bahamas, stopping drug smuggling, illegal immigration and poaching, and providing assistance to mariners. The Defence Force has a fleet of 26 coastal and inshore patrol craft along with 3 aircraft and over 1,100 personnel including 65 officers and 74 women.




The districts of the Bahamas provide a system of local government everywhere except New Providence (which holds 70% of the national population), whose affairs are handled directly by the central government. In 1996, the Bahamian Parliament passed the "Local Government Act" to facilitate the establishment of Family Island Administrators, Local Government Districts, Local District Councillors and Local Town Committees for the various island communities. The overall goal of this act is to allow the various elected leaders to govern and oversee the affairs of their respective districts without the interference of Central Government. In total, there are 32 districts, with elections being held every five years. There are 110 Councillors and 281 Town Committee members are elected to represent the various districts.
Each Councillor or Town Committee member is responsible for the proper use of public funds for the maintenance and development of their constituency.
The Bahamas uses drive-on-the-Left traffic rules throughout the Commonwealth.
The districts other than New Providence are:




The colours embodied in the design of the Bahamian flag symbolise the image and aspirations of the people of the Bahamas; the design reflects aspects of the natural environment (sun, sand and sea) and the economic and social development. The flag is a black equilateral triangle against the mast, superimposed on a horizontal background made up of two colours on three equal stripes of aquamarine, gold and aquamarine.




The coat of arms of the Bahamas contains a shield with the national symbols as its focal point. The shield is supported by a marlin and a flamingo, which are the national animals of the Bahamas. The flamingo is located on the land, and the marlin on the sea, indicating the geography of the islands.
On top of the shield is a conch shell, which represents the varied marine life of the island chain. The conch shell rests on a helmet. Below this is the actual shield, the main symbol of which is a ship representing the Santa María of Christopher Columbus, shown sailing beneath the sun. Along the bottom, below the shield appears a banner upon which is the national motto:

"Forward, Upward, Onward Together."



The yellow elder was chosen as the national flower of the Bahamas because it is native to the Bahama islands, and it blooms throughout the year.
Selection of the yellow elder over many other flowers was made through the combined popular vote of members of all four of New Providence's garden clubs of the 1970s—the Nassau Garden Club, the Carver Garden Club, the International Garden Club and the Y.W.C.A. Garden Club.
They reasoned that other flowers grown there—such as the bougainvillea, hibiscus and poinciana—had already been chosen as the national flowers of other countries. The yellow elder, on the other hand, was unclaimed by other countries (although it is now also the national flower of the United States Virgin Islands) and also the yellow elder is native to the family islands.




By the terms of GDP per capita, the Bahamas is one of the richest countries in the Americas.




The Bahamas relies on tourism to generate most of its economic activity. Tourism as an industry not only accounts for over 60% of the Bahamian GDP, but provides jobs for more than half the country's workforce. The Bahamas attracted 5.8 million visitors in 2012, more than 70% of which were cruise visitors.



After tourism, the next most important economic sector is banking and international financial services, accounting for some 15% of GDP.
The government has adopted incentives to encourage foreign financial business, and further banking and finance reforms are in progress. The government plans to merge the regulatory functions of key financial institutions, including the Central Bank of the Bahamas (CBB) and the Securities and Exchange Commission. The Central Bank administers restrictions and controls on capital and money market instruments. The Bahamas International Securities Exchange consists of 19 listed public companies. Reflecting the relative soundness of the banking system (mostly populated by Canadian banks), the impact of the global financial crisis on the financial sector has been limited.

The economy has a very competitive tax regime. The government derives its revenue from import tariffs, VAT, licence fees, property and stamp taxes, but there is no income tax, corporate tax, capital gains tax, or wealth tax. Payroll taxes fund social insurance benefits and amount to 3.9% paid by the employee and 5.9% paid by the employer. In 2010, overall tax revenue as a percentage of GDP was 17.2%.




Agriculture is the third largest sector of the Bahamian economy, representing 5–7% of total GDP. An estimated 80% of the Bahamian food supply is imported. Major crops include onions, okra, and tomatoes, oranges, grapefruit, cucumbers, sugar cane, lemons, limes and sweet potatoes.




The Bahamas has an estimated population of 392,718, of which 25.9% are under 14, 67.2% 15 to 64 and 6.9% over 65. It has a population growth rate of 0.925% (2010), with a birth rate of 17.81/1,000 population, death rate of 9.35/1,000, and net migration rate of −2.13 migrant(s)/1,000 population. The infant mortality rate is 23.21 deaths/1,000 live births. Residents have a life expectancy at birth of 69.87 years: 73.49 years for females, 66.32 years for males. The total fertility rate is 2.0 children born/woman (2010).
The most populous islands are New Providence, where Nassau, the capital and largest city, is located; and Grand Bahama, home to the second largest city of Freeport.



According to the 99% response rate obtained from the race question on the 2010 Census questionnaire, 90.6% of the population identified themselves as being Africans or Afro-Bahamian, 4.7% Europeans or Euro-Bahamian and 2.1% of a mixed race (African and European). Three centuries prior, in 1722 when the first official census of The Bahamas was taken, 74% of the population was White and 26% Black.

Afro-Bahamians are Bahamian nationals whose primary ancestry was based in West Africa. The first Africans to arrive to the Bahamas were freed slaves from Bermuda; they arrived with the Eleutheran Adventurers looking for new lives.
Since the colonial era of plantations, Africans or Afro-Bahamians have been the largest ethnic group in the Bahamas; in the 21st century, they account for some 91% of the country's population. The Haitian community is also largely of African descent and numbers about 80,000. Because of an extremely high immigration of Haitians to the Bahamas, the Bahamian government started deporting illegal Haitian immigrants to their homeland in late 2014.
16,598 (5%) of the total population are descendants of Europeans or European Bahamians at the 2010 census. European Bahamians, or Bahamians of European and mixed European descent form the largest minority, and are mainly the descendants of the English Puritans looking to flee religious persecution in England and American Loyalists escaping the American Revolution who arrived in 1649 and 1783, respectively. Many Southern Loyalists went to the Abaco Islands, half of whose population was of European descent as of 1985. A small portion of the Euro-Bahamian population is descended from Greek labourers who came to help develop the sponging industry in the 1900s. They make up less than 1% of the nation's population, but have still preserved their distinct Greek Bahamian culture.



The official language of the Bahamas is English. Many residents speak the Bahamian dialect. According to 1995 estimates 98.2% of the adult population is literate.




According to International Religious Freedom Report 2008 prepared by United States Bureau of Democracy, Human Rights and Labor, the islands' population is predominantly Christian. Protestant denominations are widespread with Baptists representing 35% of the population, Anglicans 15%, Pentecostals 8%, Church of God 5%, Seventh-day Adventists 5% and Methodists 4%, but there is also a significant Roman Catholic community accounting for about 14%. There are also smaller communities of Jews, Muslims, Baha'is, Hindus, Rastafarians and practitioners of Obeah.




In the less developed outer islands (or Family Islands), handicrafts include basketry made from palm fronds. This material, commonly called "straw", is plaited into hats and bags that are popular tourist items. Another use is for so-called "Voodoo dolls", even though such dolls are the result of the American imagination and not based on historic fact.
A form of folk magic (obeah) is practiced by some Bahamians, mainly in the Family Islands (out-islands) of the Bahamas. The practice of obeah is illegal in the Bahamas and punishable by law.
Junkanoo is a traditional Afro-Bahamian street parade of 'rushing', music, dance and art held in Nassau (and a few other settlements) every Boxing Day and New Year's Day. Junkanoo is also used to celebrate other holidays and events such as Emancipation Day.
Regattas are important social events in many family island settlements. They usually feature one or more days of sailing by old-fashioned work boats, as well as an onshore festival.
Many dishes are associated with Bahamian cuisine, which reflects Caribbean, African and European influences. Some settlements have festivals associated with the traditional crop or food of that area, such as the "Pineapple Fest" in Gregory Town, Eleuthera or the "Crab Fest" on Andros. Other significant traditions include story telling.
Bahamians have created a rich literature of poetry, short stories, plays and short fictional works. Common themes in these works are (1) an awareness of change, (2) a striving for sophistication, (3) a search for identity, (4) nostalgia for the old ways and (5) an appreciation of beauty. Some contributing writers are Susan Wallace, Percival Miller, Robert Johnson, Raymond Brown, O.M. Smith, William Johnson, Eddie Minnis and Winston Saunders.
Bahamas culture is rich with beliefs, traditions, folklore and legend. The most well-known folklore and legends in the Bahamas includes Lusca in Andros Bahamas, Pretty Molly on Exuma Bahamas, the Chickcharnies of Andro Bahamas, and the Lost City of Atlantis on Bimini Bahamas.



Sport is a significant part of Bahamian culture. The national sport is Cricket. Cricket has been played in the Bahamas from 1846. It is the oldest sport being played in the country today. The Bahamas Cricket Association was formed in 1936 as an organised body. From the 1940s to the 1970s, cricket was played amongst many Bahamians. Bahamas is not a part of the West Indies Board, so players are not eligible to play for the West Indies cricket team. The late 1970s saw the game begin to decline in the country as teachers, who had previously come from the United Kingdom with a passion for cricket were replaced by teachers who had been trained in the United States. The Bahamian Physical education teachers had no knowledge of the game and instead taught track & field, basketball, baseball, softball, volleyball and football where primary and high schools compete against each other. Today cricket is still enjoyed by a few locals and immigrants in the country usually from Jamaica, Guyana, Haiti and Barbados. Cricket is played on Saturdays and Sundays at Windsor Park and Haynes Oval.
The only other sporting event that began before cricket was horse racing, which started in 1796. The most popular spectator sports are those imported from United States such as basketball, American football and baseball rather than Great Britain due to the country's close proximity to the United States. Unlike their other Caribbean counterparts, cricket has proven to be more popular.
Dexter Cambridge, Rick Fox, Ian Lockhart and Buddy Hield are a few Bahamians who joined Bahamian Mychal Thompson of the Los Angeles Lakers in the NBA ranks, Over the years American football has become much more popular than association football, though not implemented in the high school system yet. Leagues for teens and adults have been developed by the Bahamas American Football Federation. However association football, commonly known as 'soccer' in the country, is still a very popular sport amongst high school pupils. Leagues are governed by the Bahamas Football Association. Recently the Bahamian government has been working closely with Tottenham Hotspur of London to promote the sport in the country as well as promoting the Bahamas in the European market. In 2013 'Spurs' became the first Premier League club to play an exhibition match in the Bahamas to face the Jamaica national football team. Joe Lewis the owner of the Tottenham Hotspur club is based in the Bahamas.
Other popular sports are swimming, tennis and boxing where Bahamians have enjoyed some level of success at the international level. Other sports such as golf, rugby league, rugby union and beach soccer are considered growing sports. Athletics commonly known as track and field in the country is the most successful sport by far amongst Bahamians. Bahamians have a strong tradition in the sprints and jumps. Track and field is probably the most popular spectator sport in the country next to basketball due to their success over the years. Triathlons are gaining popularity in Nassau and the Family Islands.
Bahamians have gone on to win numerous track and field medals at the Olympic Games, IAAF World Championships in Athletics, Commonwealth Games and Pan American Games. Frank Rutherford is the first athletics olympic medalist for the country. He won a bronze medal for triple jump during the 1992 Summer Olympics. Pauline Davis-Thompson, Debbie Ferguson, Chandra Sturrup, Savatheda Fynes and Eldece Clarke-Lewis teamed up for the first athletics Olympic Gold medal for the country when they won the 4 × 100 m relay at the 2000 Summer Olympics. They are affectionately known as the "Golden Girls". Tonique Williams-Darling became the first athletics individual Olympic gold medalist when she won the 400m sprint in 2004 Summer Olympics. In 2007, with the disqualification of Marion Jones, Pauline Davis-Thompson was advanced to the gold medal position in the 200 metres at the 2000 Olympics, predating William-Darling.



Scenes from the original Jaws movie were filmed on a New Providence Island beach now known as "Jaws Beach".
The fourth official James Bond film, Thunderball (1965), was partly filmed in Nassau, where much of the story is set. Eon Productions were to return for filming underwater sequences in the famously clear waters, even when a Bond film's story was set elsewhere; for example, for The Spy Who Loved Me (1977).
The unofficial remake of Thunderball, Never Say Never Again (1983), was similarly partly filmed in the islands, though this version of the story was not as extensively set there.
The twenty-first official James Bond film, Casino Royale (2006), was in part set and filmed in the islands.
The Beatles' film Help! was filmed in part on New Providence Island and Paradise Island also in 1965.
Nassau is featured in the 2013 video game Assassin's Creed IV: Black Flag as a pirate haven, housing the main protagonists. Historical pirates are encountered there such as Benjamin Hornigold, Edward Teach/Blackbeard, Charles Vane, "Calico" Jack Rackham, Anne Bonney and Mary Read.




Outline of the Bahamas
Index of Bahamas-related articles
Bibliography of the Bahamas
 Bahamas – Wikipedia book






Horne, Gerald (2012). Negro Comrades of the Crown: African Americans and the British Empire Fight the U.S. Before Emancipation. NYU Press. ISBN 978-0-8147-4463-5. 
Higham, Charles (1988). The Dutchess of Windsor: The Secret Life. McGraw Hill. ISBN 0471485233. 






Cash Philip et al. (Don Maples, Alison Packer). The Making of The Bahamas: A History for Schools. London: Collins, 1978.
Miller, Hubert W. The Colonization of The Bahamas, 1647–1670, The William and Mary Quarterly 2 no.1 (January 1945): 33–46.
Craton, Michael. A History of The Bahamas. London: Collins, 1962.
Craton, Michael and Saunders, Gail. Islanders in the Stream: A History of the Bahamian People. Athens: University of Georgia Press, 1992
Collinwood, Dean. "Columbus and the Discovery of Self," Weber Studies, Vol. 9 No. 3 (Fall) 1992: 29–44.
Dodge, Steve. Abaco: The History of an Out Island and its Cays, Tropic Isle Publications, 1983.
Dodge, Steve. The Compleat Guide to Nassau, White Sound Press, 1987.
Boultbee, Paul G. The Bahamas. Oxford: ABC-Clio Press, 1990.
Wood, David E., comp., A Guide to Selected Sources to the History of the Seminole Settlements of Red Bays, Andros, 1817–1980, Nassau: Department of Archives



Johnson, Howard. The Bahamas in Slavery and Freedom. Kingston: Ian Randle Publishing, 1991.
Johnson, Howard. The Bahamas from Slavery to Servitude, 1783–1933. Gainesville: University of Florida Press, 1996.
Alan A. Block. Masters of Paradise, New Brunswick and London, Transaction Publishers, 1998.
Storr, Virgil H. Enterprising Slaves and Master Pirates: Understanding Economic Life in the Bahamas. New York: Peter Lang, 2004.



Johnson, Wittington B. Race Relations in the Bahamas, 1784–1834: The Nonviolent Transformation from a Slave to a Free Society, Fayetteville: University of Arkansas, 2000.
Shirley, Paul. "Tek Force Wid Force", History Today 54, no. 41 (April 2004): 30–35.
Saunders, Gail. The Social Life in the Bahamas 1880s–1920s. Nassau: Media Publishing, 1996.
Saunders, Gail. Bahamas Society After Emancipation. Kingston: Ian Randle Publishing, 1990.
Curry, Jimmy. Filthy Rich Gangster/First Bahamian Movie. Movie Mogul Pictures: 1996.
Curry, Jimmy. To the Rescue/First Bahamian Rap/Hip Hop Song. Royal Crown Records, 1985.
Collinwood, Dean. The Bahamas Between Worlds, White Sound Press, 1989.
Collinwood, Dean and Steve Dodge. Modern Bahamian Society, Caribbean Books, 1989.
Dodge, Steve, Robert McIntire and Dean Collinwood. The Bahamas Index, White Sound Press, 1989.
Collinwood, Dean. "The Bahamas," in The Whole World Handbook 1992–1995, 12th ed., New York: St. Martin's Press, 1994.
Collinwood, Dean. "The Bahamas," chapters in Jack W. Hopkins, ed., Latin American and Caribbean Contemporary Record, Vols. 1,2,3,4, Holmes and Meier Publishers, 1983, 1984, 1985, 1986.
Collinwood, Dean. "Problems of Research and Training in Small Islands with a Social Science Faculty," in Social Science in Latin America and the Caribbean, UNESCO, No. 48, 1982.
Collinwood, Dean and Rick Phillips, "The National Literature of the New Bahamas," Weber Studies, Vol.7, No. 1 (Spring) 1990: 43–62.
Collinwood, Dean. "Writers, Social Scientists and Sexual Norms in the Caribbean," Tsuda Review, No. 31 (November) 1986: 45–57.
Collinwood, Dean. "Terra Incognita: Research on the Modern Bahamian Society," Journal of Caribbean Studies,Vol. 1, Nos. 2–3 (Winter) 1981: 284–297.
Collinwood, Dean and Steve Dodge. "Political Leadership in the Bahamas," The Bahamas Research Institute, No.1, May 1987.




Official website
 Wikimedia Atlas of Bahamas
"Bahamas". The World Factbook. Central Intelligence Agency. 
The Bahamas from UCB Libraries GovPubs
The Bahamas at DMOZ
The Bahamas from the BBC News
Key Development Forecasts for The Bahamas from International Futures
Maps of the Bahamas from the American Geographical Society LibraryAfrica is the world's second-largest and second-most-populous continent. At about 30.3 million km² (11.7 million square miles) including adjacent islands, it covers six per cent of Earth's total surface area and 20.4 per cent of its total land area. With 1.1 billion people as of 2013, it accounts for about 15% of the world's human population. The continent is surrounded by the Mediterranean Sea to the north, both the Suez Canal and the Red Sea along the Sinai Peninsula to the northeast, the Indian Ocean to the southeast, and the Atlantic Ocean to the west. The continent includes Madagascar and various archipelagos. It contains 54 fully recognized sovereign states (countries), nine territories and two de facto independent states with limited or no recognition.
Africa's population is the youngest amongst all the continents; the median age in 2012 was 19.7, when the worldwide median age was 30.4. Algeria is Africa's largest country by area, and Nigeria by population. Africa, particularly central Eastern Africa, is widely accepted as the place of origin of humans and the Hominidae clade (great apes), as evidenced by the discovery of the earliest hominids and their ancestors, as well as later ones that have been dated to around seven million years ago, including Sahelanthropus tchadensis, Australopithecus africanus, A. afarensis, Homo erectus, H. habilis and H. ergaster – with the earliest Homo sapiens (modern human) found in Ethiopia being dated to circa 200,000 years ago. Africa straddles the equator and encompasses numerous climate areas; it is the only continent to stretch from the northern temperate to southern temperate zones.
Africa hosts a large diversity of ethnicities, cultures and languages. In the late 19th century European countries colonized most of Africa. Africa also varies greatly with regard to environments, economics, historical ties and government systems. However, most present states in Africa originate from a process of decolonization in the 20th century.




Afri was a Latin name used to refer to the inhabitants of Africa, which in its widest sense referred to all lands south of the Mediterranean (Ancient Libya). This name seems to have originally referred to a native Libyan tribe; see Terence#Biography for discussion. The name is usually connected with Hebrew or Phoenician ʿafar 'dust', but a 1981 hypothesis has asserted that it stems from the Berber ifri (plural ifran) "cave", in reference to cave dwellers. The same word may be found in the name of the Banu Ifran from Algeria and Tripolitania, a Berber tribe originally from Yafran (also known as Ifrane) in northwestern Libya.
Under Roman rule, Carthage became the capital of the province of Africa Proconsularis, which also included the coastal part of modern Libya. The Latin suffix -ica can sometimes be used to denote a land (e.g., in Celtica from Celtae, as used by Julius Caesar). The later Muslim kingdom of Ifriqiya, modern-day Tunisia, also preserved a form of the name.
According to the Romans, Africa lay to the west of Egypt, while "Asia" was used to refer to Anatolia and lands to the east. A definite line was drawn between the two continents by the geographer Ptolemy (85–165 AD), indicating Alexandria along the Prime Meridian and making the isthmus of Suez and the Red Sea the boundary between Asia and Africa. As Europeans came to understand the real extent of the continent, the idea of "Africa" expanded with their knowledge.
Other etymological hypotheses have been postulated for the ancient name "Africa":
The 1st-century Jewish historian Flavius Josephus (Ant. 1.15) asserted that it was named for Epher, grandson of Abraham according to Gen. 25:4, whose descendants, he claimed, had invaded Libya.
Isidore of Seville in Etymologiae XIV.5.2. suggests "Africa comes from the Latin aprica, meaning "sunny".
Massey, in 1881, stated that Africa is derived from the Egyptian af-rui-ka, meaning "to turn toward the opening of the Ka." The Ka is the energetic double of every person and the "opening of the Ka" refers to a womb or birthplace. Africa would be, for the Egyptians, "the birthplace."
Michèle Fruyt proposed linking the Latin word with africus "south wind", which would be of Umbrian origin and mean originally "rainy wind".
Robert R. Stieglitz of Rutgers University proposed: "The name Africa, derived from the Latin *Aphir-ic-a, is cognate to Hebrew Ophir."







Africa is considered by most paleoanthropologists to be the oldest inhabited territory on Earth, with the human species originating from the continent. During the mid-20th century, anthropologists discovered many fossils and evidence of human occupation perhaps as early as 7 million years ago (BP=before present). Fossil remains of several species of early apelike humans thought to have evolved into modern man, such as Australopithecus afarensis (radiometrically dated to approximately 3.9–3.0 million years BP, Paranthropus boisei (c. 2.3–1.4 million years BP) and Homo ergaster (c. 1.9 million–600,000 years BP) have been discovered.
After the evolution of Homo sapiens sapiens approximately 150,000 to 100,000 years BP in Africa, the continent was mainly populated by groups of hunter-gatherers. These first modern humans left Africa and populated the rest of the globe during the Out of Africa II migration dated to approximately 50,000 years BP, exiting the continent either across Bab-el-Mandeb over the Red Sea, the Strait of Gibraltar in Morocco, or the Isthmus of Suez in Egypt.
Other migrations of modern humans within the African continent have been dated to that time, with evidence of early human settlement found in Southern Africa, Southeast Africa, North Africa, and the Sahara.
The size of the Sahara has historically been extremely variable, with its area rapidly fluctuating and at times disappearing depending on global climactic conditions. At the end of the Ice ages, estimated to have been around 10,500 BC, the Sahara had again become a green fertile valley, and its African populations returned from the interior and coastal highlands in Sub-Saharan Africa, with rock art paintings depicting a fertile Sahara and large populations discovered in Tassili n'Ajjer dating back perhaps 10 millennia. However, the warming and drying climate meant that by 5000 BC, the Sahara region was becoming increasingly dry and hostile. Around 3500 BC, due to a tilt in the earth's orbit, the Sahara experienced a period of rapid desertification. The population trekked out of the Sahara region towards the Nile Valley below the Second Cataract where they made permanent or semi-permanent settlements. A major climatic recession occurred, lessening the heavy and persistent rains in Central and Eastern Africa. Since this time, dry conditions have prevailed in Eastern Africa and, increasingly during the last 200 years, in Ethiopia.
The domestication of cattle in Africa preceded agriculture and seems to have existed alongside hunter-gatherer cultures. It is speculated that by 6000 BC, cattle were domesticated in North Africa. In the Sahara-Nile complex, people domesticated many animals, including the donkey and a small screw-horned goat which was common from Algeria to Nubia.
Around 4000 BC, the Saharan climate started to become drier at an exceedingly fast pace. This climate change caused lakes and rivers to shrink significantly and caused increasing desertification. This, in turn, decreased the amount of land conducive to settlements and helped to cause migrations of farming communities to the more tropical climate of West Africa.
By the first millennium BC, ironworking had been introduced in Northern Africa and quickly spread across the Sahara into the northern parts of sub-Saharan Africa, and by 500 BC, metalworking began to become commonplace in West Africa. Ironworking was fully established by roughly 500 BC in many areas of East and West Africa, although other regions didn't begin ironworking until the early centuries AD. Copper objects from Egypt, North Africa, Nubia, and Ethiopia dating from around 500 BC have been excavated in West Africa, suggesting that Trans-Saharan trade networks had been established by this date.




At about 3300 BC, the historical record opens in Northern Africa with the rise of literacy in the Pharaonic civilization of Ancient Egypt. One of the world's earliest and longest-lasting civilizations, the Egyptian state continued, with varying levels of influence over other areas, until 343 BC. Egyptian influence reached deep into modern-day Libya and Nubia, and, according to Martin Bernal, as far north as Crete.
An independent centre of civilization with trading links to Phoenicia was established by Phoenicians from Tyre on the north-west African coast at Carthage.
European exploration of Africa began with Ancient Greeks and Romans. In 332 BC, Alexander the Great was welcomed as a liberator in Persian-occupied Egypt. He founded Alexandria in Egypt, which would become the prosperous capital of the Ptolemaic dynasty after his death.
Following the conquest of North Africa's Mediterranean coastline by the Roman Empire, the area was integrated economically and culturally into the Roman system. Roman settlement occurred in modern Tunisia and elsewhere along the coast. The first Roman emperor native to North Africa was Septimius Severus, born in Leptis Magna in present-day Libya—his mother was Italian Roman and his father was Punic.
Christianity spread across these areas at an early date, from Judaea via Egypt and beyond the borders of the Roman world into Nubia; by AD 340 at the latest, it had become the state religion of the Aksumite Empire. Syro-Greek missionaries, who arrived by way of the Red Sea, were responsible for this theological development.
In the early 7th century, the newly formed Arabian Islamic Caliphate expanded into Egypt, and then into North Africa. In a short while, the local Berber elite had been integrated into Muslim Arab tribes. When the Umayyad capital Damascus fell in the 8th century, the Islamic centre of the Mediterranean shifted from Syria to Qayrawan in North Africa. Islamic North Africa had become diverse, and a hub for mystics, scholars, jurists, and philosophers. During the above-mentioned period, Islam spread to sub-Saharan Africa, mainly through trade routes and migration.




Pre-colonial Africa possessed perhaps as many as 10,000 different states and polities characterized by many different sorts of political organization and rule. These included small family groups of hunter-gatherers such as the San people of southern Africa; larger, more structured groups such as the family clan groupings of the Bantu-speaking peoples of central, southern, and eastern Africa; heavily structured clan groups in the Horn of Africa; the large Sahelian kingdoms; and autonomous city-states and kingdoms such as those of the Akan; Edo, Yoruba, and Igbo people in West Africa; and the Swahili coastal trading towns of Southeast Africa.
By the ninth century AD, a string of dynastic states, including the earliest Hausa states, stretched across the sub-Saharan savannah from the western regions to central Sudan. The most powerful of these states were Ghana, Gao, and the Kanem-Bornu Empire. Ghana declined in the eleventh century, but was succeeded by the Mali Empire which consolidated much of western Sudan in the thirteenth century. Kanem accepted Islam in the eleventh century.
In the forested regions of the West African coast, independent kingdoms grew with little influence from the Muslim north. The Kingdom of Nri was established around the ninth century and was one of the first. It is also one of the oldest kingdoms in present-day Nigeria and was ruled by the Eze Nri. The Nri kingdom is famous for its elaborate bronzes, found at the town of Igbo-Ukwu. The bronzes have been dated from as far back as the ninth century.

The Kingdom of Ife, historically the first of these Yoruba city-states or kingdoms, established government under a priestly oba ('king' or 'ruler' in the Yoruba language), called the Ooni of Ife. Ife was noted as a major religious and cultural centre in West Africa, and for its unique naturalistic tradition of bronze sculpture. The Ife model of government was adapted at the Oyo Empire, where its obas or kings, called the Alaafins of Oyo, once controlled a large number of other Yoruba and non-Yoruba city-states and kingdoms; the Fon Kingdom of Dahomey was one of the non-Yoruba domains under Oyo control.
The Almoravids were a Berber dynasty from the Sahara that spread over a wide area of northwestern Africa and the Iberian peninsula during the eleventh century. The Banu Hilal and Banu Ma'qil were a collection of Arab Bedouin tribes from the Arabian Peninsula who migrated westwards via Egypt between the eleventh and thirteenth centuries. Their migration resulted in the fusion of the Arabs and Berbers, where the locals were Arabized, and Arab culture absorbed elements of the local culture, under the unifying framework of Islam.

Following the breakup of Mali, a local leader named Sonni Ali (1464–1492) founded the Songhai Empire in the region of middle Niger and the western Sudan and took control of the trans-Saharan trade. Sonni Ali seized Timbuktu in 1468 and Jenne in 1473, building his regime on trade revenues and the cooperation of Muslim merchants. His successor Askia Mohammad I (1493–1528) made Islam the official religion, built mosques, and brought to Gao Muslim scholars, including al-Maghili (d.1504), the founder of an important tradition of Sudanic African Muslim scholarship. By the eleventh century, some Hausa states – such as Kano, jigawa, Katsina, and Gobir – had developed into walled towns engaging in trade, servicing caravans, and the manufacture of goods. Until the fifteenth century, these small states were on the periphery of the major Sudanic empires of the era, paying tribute to Songhai to the west and Kanem-Borno to the east.




Slavery had long been practised in Africa. Between the 7th and 20th centuries, Arab slave trade (also known as slavery in the East) took 18 million slaves from Africa via trans-Saharan and Indian Ocean routes. Between the 15th and the 19th centuries (500 years), the Atlantic slave trade took an estimated 7–12 million slaves to the New World. More than 1 million Europeans were captured by Barbary pirates and sold as slaves in North Africa between the 16th and 19th centuries.
In West Africa, the decline of the Atlantic slave trade in the 1820s caused dramatic economic shifts in local polities. The gradual decline of slave-trading, prompted by a lack of demand for slaves in the New World, increasing anti-slavery legislation in Europe and America, and the British Royal Navy's increasing presence off the West African coast, obliged African states to adopt new economies. Between 1808 and 1860, the British West Africa Squadron seized approximately 1,600 slave ships and freed 150,000 Africans who were aboard.

Action was also taken against African leaders who refused to agree to British treaties to outlaw the trade, for example against "the usurping King of Lagos", deposed in 1851. Anti-slavery treaties were signed with over 50 African rulers. The largest powers of West Africa (the Asante Confederacy, the Kingdom of Dahomey, and the Oyo Empire) adopted different ways of adapting to the shift. Asante and Dahomey concentrated on the development of "legitimate commerce" in the form of palm oil, cocoa, timber and gold, forming the bedrock of West Africa's modern export trade. The Oyo Empire, unable to adapt, collapsed into civil wars.




In the late 19th century, the European imperial powers engaged in a major territorial scramble and occupied most of the continent, creating many colonial territories, and leaving only two fully independent states: Ethiopia (known to Europeans as "Abyssinia"), and Liberia. Egypt and Sudan were never formally incorporated into any European colonial empire; however, after the British occupation of 1882, Egypt was effectively under British administration until 1922.



The Berlin Conference held in 1884–85 was an important event in the political future of African ethnic groups. It was convened by King Leopold II of Belgium, and attended by the European powers that laid claim to African territories. It sought to end the European powers' Scramble for Africa, by agreeing on political division and spheres of influence. They set up the political divisions of the continent, by spheres of interest, that exist in Africa today.



Imperial rule by Europeans would continue until after the conclusion of World War II, when almost all remaining colonial territories gradually obtained formal independence. Independence movements in Africa gained momentum following World War II, which left the major European powers weakened. In 1951, Libya, a former Italian colony, gained independence. In 1956, Tunisia and Morocco won their independence from France. Ghana followed suit the next year (March 1957), becoming the first of the sub-Saharan colonies to be granted independence. Most of the rest of the continent became independent over the next decade.
Portugal's overseas presence in Sub-Saharan Africa (most notably in Angola, Cape Verde, Mozambique, Guinea-Bissau and São Tomé and Príncipe) lasted from the 16th century to 1975, after the Estado Novo regime was overthrown in a military coup in Lisbon. Rhodesia unilaterally declared independence from the United Kingdom in 1965, under the white minority government of Ian Smith, but was not internationally recognized as an independent state (as Zimbabwe) until 1980, when black nationalists gained power after a bitter guerrilla war. Although South Africa was one of the first African countries to gain independence, the state remained under the control of the country's white minority through a system of racial segregation known as apartheid until 1994.




Today, Africa contains 54 sovereign countries, most of which have borders that were drawn during the era of European colonialism. Since colonialism, African states have frequently been hampered by instability, corruption, violence, and authoritarianism. The vast majority of African states are republics that operate under some form of the presidential system of rule. However, few of them have been able to sustain democratic governments on a permanent basis, and many have instead cycled through a series of coups, producing military dictatorships.
Great instability was mainly the result of marginalization of ethnic groups, and graft under these leaders. For political gain, many leaders fanned ethnic conflicts, some of which had been exacerbated, or even created, by colonial rule. In many countries, the military was perceived as being the only group that could effectively maintain order, and it ruled many nations in Africa during the 1970s and early 1980s. During the period from the early 1960s to the late 1980s, Africa had more than 70 coups and 13 presidential assassinations. Border and territorial disputes were also common, with the European-imposed borders of many nations being widely contested through armed conflicts.

Cold War conflicts between the United States and the Soviet Union, as well as the policies of the International Monetary Fund, also played a role in instability. When a country became independent for the first time, it was often expected to align with one of the two superpowers. Many countries in Northern Africa received Soviet military aid, while others in Central and Southern Africa were supported by the United States, France or both. The 1970s saw an escalation of Cold War intrigues, as newly independent Angola and Mozambique aligned themselves with the Soviet Union, and the West and South Africa sought to contain Soviet influence by supporting friendly regimes or insurgency movements. In Rhodesia, Soviet and Chinese-backed leftist guerrillas of the Zimbabwe Patriotic Front waged a brutal guerrilla war against the country's white government. There was a major famine in Ethiopia, when hundreds of thousands of people starved. Some claimed that Marxist economic policies made the situation worse. The most devastating military conflict in modern independent Africa has been the Second Congo War; this conflict and its aftermath has killed an estimated 5.5 million people. Since 2003 there has been an ongoing conflict in Darfur which has become a humanitarian disaster. Another notable tragic event is the 1994 Rwandan Genocide in which an estimated 800,000 people were murdered. AIDS in post-colonial Africa has also been a prevalent issue.
In the 21st century, however, the number of armed conflicts in Africa has steadily declined. For instance, the civil war in Angola came to an end in 2002 after nearly 30 years. This has coincided with many countries abandoning communist-style command economies and opening up for market reforms. The improved stability and economic reforms have led to a great increase in foreign investment into many African nations, mainly from China, which has spurred quick economic growth in many countries, seemingly ending decades of stagnation and decline. Several African economies are among the world's fastest growing as of 2016. A significant part of this growth, which is sometimes referred to as Africa Rising, can also be attributed to the facilitated diffusion of information technologies and specifically the mobile telephone.




Africa is the largest of the three great southward projections from the largest landmass of the Earth. Separated from Europe by the Mediterranean Sea, it is joined to Asia at its northeast extremity by the Isthmus of Suez (transected by the Suez Canal), 163 km (101 mi) wide. (Geopolitically, Egypt's Sinai Peninsula east of the Suez Canal is often considered part of Africa, as well.)
From the most northerly point, Ras ben Sakka in Tunisia (37°21' N), to the most southerly point, Cape Agulhas in South Africa (34°51'15" S), is a distance of approximately 8,000 km (5,000 mi); from Cape Verde, 17°33'22" W, the westernmost point, to Ras Hafun in Somalia, 51°27'52" E, the most easterly projection, is a distance of approximately 7,400 km (4,600 mi). The coastline is 26,000 km (16,000 mi) long, and the absence of deep indentations of the shore is illustrated by the fact that Europe, which covers only 10,400,000 km2 (4,000,000 sq mi) – about a third of the surface of Africa – has a coastline of 32,000 km (20,000 mi).
Africa's largest country is Algeria, and its smallest country is Seychelles, an archipelago off the east coast. The smallest nation on the continental mainland is The Gambia.
Geologically, Africa includes the Arabian Peninsula; the Zagros Mountains of Iran and the Anatolian Plateau of Turkey mark where the African Plate collided with Eurasia. The Afrotropic ecozone and the Saharo-Arabian desert to its north unite the region biogeographically, and the Afro-Asiatic language family unites the north linguistically.




The climate of Africa ranges from tropical to subarctic on its highest peaks. Its northern half is primarily desert, or arid, while its central and southern areas contain both savanna plains and dense jungle (rainforest) regions. In between, there is a convergence, where vegetation patterns such as sahel and steppe dominate. Africa is the hottest continent on earth and 60% of the entire land surface consists of drylands and deserts. The record for the highest-ever recorded temperature, in Libya in 1922 (58 °C (136 °F)), was discredited in 2013.




Africa boasts perhaps the world's largest combination of density and "range of freedom" of wild animal populations and diversity, with wild populations of large carnivores (such as lions, hyenas, and cheetahs) and herbivores (such as buffalo, elephants, camels, and giraffes) ranging freely on primarily open non-private plains. It is also home to a variety of "jungle" animals including snakes and primates and aquatic life such as crocodiles and amphibians. In addition, Africa has the largest number of megafauna species, as it was least affected by the extinction of the Pleistocene megafauna.




Africa has over 3,000 protected areas, with 198 marine protected areas, 50 biosphere reserves, and 80 wetlands reserves. Significant habitat destruction, increases in human population and poaching are reducing Africa's biological diversity and arable land. Human encroachment, civil unrest and the introduction of non-native species threaten biodiversity in Africa. This has been exacerbated by administrative problems, inadequate personnel and funding problems.
Deforestation is affecting Africa at twice the world rate, according to the United Nations Environment Programme (UNEP). According to the University of Pennsylvania African Studies Center, 31% of Africa's pasture lands and 19% of its forests and woodlands are classified as degraded, and Africa is losing over four million hectares of forest per year, which is twice the average deforestation rate for the rest of the world. Some sources claim that approximately 90% of the original, virgin forests in West Africa have been destroyed. Over 90% of Madagascar's original forests have been destroyed since the arrival of humans 2000 years ago. About 65% of Africa's agricultural land suffers from soil degradation.




There are clear signs of increased networking among African organizations and states. For example, in the civil war in the Democratic Republic of the Congo (former Zaire), rather than rich, non-African countries intervening, neighbouring African countries became involved (see also Second Congo War). Since the conflict began in 1998, the estimated death toll has reached 5 million.




The African Union (AU) is a 54-member federation consisting of all of Africa's states except Morocco. The union was formed, with Addis Ababa, Ethiopia, as its headquarters, on 26 June 2001. The union was officially established on 9 July 2002 as a successor to the Organisation of African Unity (OAU). In July 2004, the African Union's Pan-African Parliament (PAP) was relocated to Midrand, in South Africa, but the African Commission on Human and Peoples' Rights remained in Addis Ababa. There is a policy in effect to decentralize the African Federation's institutions so that they are shared by all the states.
The African Union, not to be confused with the AU Commission, is formed by the Constitutive Act of the African Union, which aims to transform the African Economic Community, a federated commonwealth, into a state under established international conventions. The African Union has a parliamentary government, known as the African Union Government, consisting of legislative, judicial and executive organs. It is led by the African Union President and Head of State, who is also the President of the Pan-African Parliament. A person becomes AU President by being elected to the PAP, and subsequently gaining majority support in the PAP. The powers and authority of the President of the African Parliament derive from the Constitutive Act and the Protocol of the Pan-African Parliament, as well as the inheritance of presidential authority stipulated by African treaties and by international treaties, including those subordinating the Secretary General of the OAU Secretariat (AU Commission) to the PAP. The government of the AU consists of all-union (federal), regional, state, and municipal authorities, as well as hundreds of institutions, that together manage the day-to-day affairs of the institution.
Political associations such as the African Union offer hope for greater co-operation and peace between the continent's many countries. Extensive human rights abuses still occur in several parts of Africa, often under the oversight of the state. Most of such violations occur for political reasons, often as a side effect of civil war. Countries where major human rights violations have been reported in recent times include the Democratic Republic of the Congo, Sierra Leone, Liberia, Sudan, Zimbabwe, and Côte d'Ivoire.




Although it has abundant natural resources, Africa remains the world's poorest and most underdeveloped continent, the result of a variety of causes that may include corrupt governments that have often committed serious human rights violations, failed central planning, high levels of illiteracy, lack of access to foreign capital, and frequent tribal and military conflict (ranging from guerrilla warfare to genocide). According to the United Nations' Human Development Report in 2003, the bottom 24 ranked nations (151st to 175th) were all African.
Poverty, illiteracy, malnutrition and inadequate water supply and sanitation, as well as poor health, affect a large proportion of the people who reside in the African continent. In August 2008, the World Bank announced revised global poverty estimates based on a new international poverty line of $1.25 per day (versus the previous measure of $1.00). 80.5% of the Sub-Saharan Africa population was living on less than $2.50 (PPP) per day in 2005, compared with 85.7% for India.
Sub-Saharan Africa is the least successful region of the world in reducing poverty ($1.25 per day); some 50% of the population living in poverty in 1981 (200 million people), a figure that rose to 58% in 1996 before dropping to 50% in 2005 (380 million people). The average poor person in sub-Saharan Africa is estimated to live on only 70 cents per day, and was poorer in 2003 than in 1973, indicating increasing poverty in some areas. Some of it is attributed to unsuccessful economic liberalization programmes spearheaded by foreign companies and governments, but other studies have cited bad domestic government policies more than external factors.
From 1995 to 2005, Africa's rate of economic growth increased, averaging 5% in 2005. Some countries experienced still higher growth rates, notably Angola, Sudan and Equatorial Guinea, all of which had recently begun extracting their petroleum reserves or had expanded their oil extraction capacity. The continent is believed to hold 90% of the world's cobalt, 90% of its platinum, 50% of its gold, 98% of its chromium, 70% of its tantalite, 64% of its manganese and one-third of its uranium. The Democratic Republic of the Congo (DRC) has 70% of the world's coltan, a mineral used in the production of tantalum capacitors for electronic devices such as cell phones. The DRC also has more than 30% of the world's diamond reserves. Guinea is the world's largest exporter of bauxite. As the growth in Africa has been driven mainly by services and not manufacturing or agriculture, it has been growth without jobs and without reduction in poverty levels. In fact, the food security crisis of 2008 which took place on the heels of the global financial crisis has pushed back 100 million people into food insecurity.
In recent years, the People's Republic of China has built increasingly stronger ties with African nations and is Africa's largest trading partner. In 2007, Chinese companies invested a total of US$1 billion in Africa.
A Harvard University study led by professor Calestous Juma showed that Africa could feed itself by making the transition from importer to self-sufficiency. "African agriculture is at the crossroads; we have come to the end of a century of policies that favoured Africa's export of raw materials and importation of food. Africa is starting to focus on agricultural innovation as its new engine for regional trade and prosperity."
During US President Barack Obama's visit to Africa in July 2013, he announced a US$7 billion plan to further develop infrastructure and work more intensively with African heads of state. He also announced a new programme named Trade Africa, designed to boost trade within the continent as well as between Africa and the US.




Africa's population has rapidly increased over the last 40 years, and consequently, it is relatively young. In some African states, more than half the population is under 25 years of age. The total number of people in Africa increased from 229 million in 1950 to 630 million in 1990. As of 2014, the population of Africa is estimated at 1.2 billion. Africa's total population surpassing other continents is fairly recent; African population surpassed Europe in the 1990s, while the Americas was overtaken sometime around the year 2000; Africa's rapid population growth is expected to overtake the only two nations currently larger than its population, at roughly the same time - India and China's 1.4 billion people each will swap ranking around the year 2022.

Speakers of Bantu languages (part of the Niger–Congo family) are the majority in southern, central and southeast Africa. The Bantu-speaking peoples from The Sahel progressively expanded over most of Sub-Saharan Africa. But there are also several Nilotic groups in South Sudan and East Africa, the mixed Swahili people on the Swahili Coast, and a few remaining indigenous Khoisan ("San" or "Bushmen") and Pygmy peoples in southern and central Africa, respectively. Bantu-speaking Africans also predominate in Gabon and Equatorial Guinea, and are found in parts of southern Cameroon. In the Kalahari Desert of Southern Africa, the distinct people known as the Bushmen (also "San", closely related to, but distinct from "Hottentots") have long been present. The San are physically distinct from other Africans and are the indigenous people of southern Africa. Pygmies are the pre-Bantu indigenous peoples of central Africa.
The peoples of West Africa primarily speak Niger–Congo languages, belonging mostly to its non-Bantu branches, though some Nilo-Saharan and Afro-Asiatic speaking groups are also found. The Niger–Congo-speaking Yoruba, Igbo, Fulani, Akan and Wolof ethnic groups are the largest and most influential. In the central Sahara, Mandinka or Mande groups are most significant. Chadic-speaking groups, including the Hausa, are found in more northerly parts of the region nearest to the Sahara, and Nilo-Saharan communities, such as the Songhai, Kanuri and Zarma, are found in the eastern parts of West Africa bordering Central Africa.
The peoples of North Africa consist of three main indigenous groups: Berbers in the northwest, Egyptians in the northeast, and Nilo-Saharan-speaking peoples in the east. The Arabs who arrived in the 7th century AD introduced the Arabic language and Islam to North Africa. The Semitic Phoenicians (who founded Carthage) and Hyksos, the Indo-Iranian Alans, the Indo- European Greeks, Romans, and Vandals settled in North Africa as well. Significant Berber communities remain within Morocco and Algeria in the 21st century, while, to a lesser extent, Berber speakers are also present in some regions of Tunisia and Libya. The Berber-speaking Tuareg and other often-nomadic peoples are the principal inhabitants of the Saharan interior of North Africa. In Mauritania, there is a small but near-extinct Berber community in the north and Niger–Congo-speaking peoples in the south, though in both regions Arabic and Arab culture predominates. In Sudan, although Arabic and Arab culture predominate, it is mostly inhabited by groups that originally spoke Nilo-Saharan, such as the Nubians, Fur, Masalit and Zaghawa, who, over the centuries, have variously intermixed with migrants from the Arabian peninsula. Small communities of Afro-Asiatic-speaking Beja nomads can also be found in Egypt and Sudan.

In the Horn of Africa, some Ethiopian and Eritrean groups (like the Amhara and Tigrayans, collectively known as Habesha) speak languages from the Semitic branch of the Afro-Asiatic language family, while the Oromo and Somali speak languages from the Cushitic branch of Afro-Asiatic.
Prior to the decolonization movements of the post-World War II era, Europeans were represented in every part of Africa. Decolonization during the 1960s and 1970s often resulted in the mass emigration of white settlers – especially from Algeria and Morocco (1.6 million pieds-noirs in North Africa), Kenya, Congo, Rhodesia, Mozambique and Angola. Between 1975 and 1977, over a million colonials returned to Portugal alone. Nevertheless, white Africans remain an important minority in many African states, particularly Zimbabwe, Namibia, Réunion, and the Republic of South Africa. The country with the largest white African population is South Africa. Dutch and British diasporas represent the largest communities of European ancestry on the continent today.
European colonization also brought sizable groups of Asians, particularly from the Indian subcontinent, to British colonies. Large Indian communities are found in South Africa, and smaller ones are present in Kenya, Tanzania, and some other southern and southeast African countries. The large Indian community in Uganda was expelled by the dictator Idi Amin in 1972, though many have since returned. The islands in the Indian Ocean are also populated primarily by people of Asian origin, often mixed with Africans and Europeans. The Malagasy people of Madagascar are an Austronesian people, but those along the coast are generally mixed with Bantu, Arab, Indian and European origins. Malay and Indian ancestries are also important components in the group of people known in South Africa as Cape Coloureds (people with origins in two or more races and continents). During the 20th century, small but economically important communities of Lebanese and Chinese have also developed in the larger coastal cities of West and East Africa, respectively.




By most estimates, well over a thousand languages (UNESCO has estimated around two thousand) are spoken in Africa. Most are of African origin, though some are of European or Asian origin. Africa is the most multilingual continent in the world, and it is not rare for individuals to fluently speak not only multiple African languages, but one or more European ones as well. There are four major language families indigenous to Africa:
The Afroasiatic languages are a language family of about 240 languages and 285 million people widespread throughout the Horn of Africa, North Africa, the Sahel, and Southwest Asia.
The Nilo-Saharan language family consists of more than a hundred languages spoken by 30 million people. Nilo-Saharan languages are spoken by ethnic groups in Chad, Ethiopia, Kenya, Nigeria, Sudan, South Sudan, Uganda, and northern Tanzania.
The Niger–Congo language family covers much of Sub-Saharan Africa. In terms of number of languages, it is the largest language family in Africa and perhaps the largest in the world.
The Khoisan languages number about fifty and are spoken in Southern Africa by approximately 400,000 people. Many of the Khoisan languages are endangered. The Khoi and San peoples are considered the original inhabitants of this part of Africa.
Following the end of colonialism, nearly all African countries adopted official languages that originated outside the continent, although several countries also granted legal recognition to indigenous languages (such as Swahili, Yoruba, Igbo and Hausa). In numerous countries, English and French (see African French) are used for communication in the public sphere such as government, commerce, education and the media. Arabic, Portuguese, Afrikaans and Spanish are examples of languages that trace their origin to outside of Africa, and that are used by millions of Africans today, both in the public and private spheres. Italian is spoken by some in former Italian colonies in Africa. German is spoken in Namibia, as it was a former German protectorate.




Some aspects of traditional African cultures have become less practised in recent years as a result of neglect and suppression by colonial and post-colonial regimes. For example, African customs were discouraged, and African languages were prohibited in mission schools. Leopold II of Belgium attempted to "civilize" Africans by discouraging polygamy and witchcraft.
Obidoh Freeborn posits that colonialism is one element that has created the character of modern African art. According to authors Douglas Fraser and Herbert M. Cole, "The precipitous alterations in the power structure wrought by colonialism were quickly followed by drastic iconographic changes in the art."  Fraser and Cole assert that, in Igboland, some art objects "lack the vigor and careful craftsmanship of the earlier art objects that served traditional functions. Author Chika Okeke-Agulu states that "the racist infrastructure of British imperial enterprise forced upon the political and cultural guardians of empire a denial and suppression of an emergent sovereign Africa and modernist art."  In Soweto, the West Rand Administrative Board established a Cultural Section to collect, read, and review scripts before performances could occur. Editors F. Abiola Irele and Simon Gikandi comment that the current identity of African literature had its genesis in the "traumatic encounter between Africa and Europe."  On the other hand, Mhoze Chikowero believes that Africans deployed music, dance, spirituality, and other performative cultures to (re)asset themselves as active agents and indigenous intellectuals, to unmake their colonial marginalization and reshape their own destinies." 
There is now a resurgence in the attempts to rediscover and revalue African traditional cultures, under such movements as the African Renaissance, led by Thabo Mbeki, Afrocentrism, led by a group of scholars, including Molefi Asante, as well as the increasing recognition of traditional spiritualism through decriminalization of Vodou and other forms of spirituality.



African art and architecture reflect the diversity of African cultures. The region's oldest known beads were made from Nassarius shells and worn as personal ornaments 72,000 years ago. The Great Pyramid of Giza in Egypt was the world's tallest structure for 4,000 years, until the completion of Lincoln Cathedral around the year 1300. The stone ruins of Great Zimbabwe are also noteworthy for their architecture, as are the monolithic churches at Lalibela, Ethiopia, such as the Church of Saint George.




Egypt has long been a cultural focus of the Arab world, while remembrance of the rhythms of sub-Saharan Africa, in particular West Africa, was transmitted through the Atlantic slave trade to modern samba, blues, jazz, reggae, hip hop, and rock. The 1950s through the 1970s saw a conglomeration of these various styles with the popularization of Afrobeat and Highlife music. Modern music of the continent includes the highly complex choral singing of southern Africa and the dance rhythms of the musical genre of soukous, dominated by the music of the Democratic Republic of Congo. Indigenous musical and dance traditions of Africa are maintained by oral traditions, and they are distinct from the music and dance styles of North Africa and Southern Africa. Arab influences are visible in North African music and dance and, in Southern Africa, Western influences are apparent due to colonization.



Fifty-four African countries have football (soccer) teams in the Confederation of African Football. Egypt has won the African Cup seven times, and a record-making three times in a row. Cameroon, Nigeria, Senegal, Ghana, and Algeria have advanced to the knockout stage of recent FIFA World Cups. South Africa hosted the 2010 World Cup tournament, becoming the first African country to do so.
Cricket is popular in some African nations. South Africa and Zimbabwe have Test status, while Kenya is the leading non-test team and previously had One-Day International cricket (ODI) status (from 10 October 1997, until 30 January 2014). The three countries jointly hosted the 2003 Cricket World Cup. Namibia is the other African country to have played in a World Cup. Morocco in northern Africa has also hosted the 2002 Morocco Cup, but the national team has never qualified for a major tournament. Rugby is a popular sport in South Africa, Namibia, and Zimbabwe.




Africans profess a wide variety of religious beliefs, and statistics on religious affiliation are difficult to come by since they are often a sensitive a topic for governments with mixed religious populations. According to the World Book Encyclopedia, Islam is the largest religion in Africa, followed by Christianity. According to Encyclopedia Britannica, 45% of the population are Christians, 40% are Muslims, and 10% follow traditional religions. A small number of Africans are Hindu, Buddhist, Confucianist, Baha'i, or Jewish. There is also a minority of Africans who are irreligious.




The countries in this table are categorized according to the scheme for geographic subregions used by the United Nations, and data included are per sources in cross-referenced articles. Where they differ, provisos are clearly indicated.




African Union
Afro-Eurasia
Index of Africa-related articles
List of African millionaires
List of highest mountain peaks of Africa
Lists of cities in Africa
Outline of Africa
Urbanization in Africa






Asante, Molefi (2007). The History of Africa. USA: Routledge. ISBN 0-415-77139-0. 
Clark, J. Desmond (1970). The Prehistory of Africa. London: Thames and Hudson. ISBN 978-0-500-02069-2. 
Crowder, Michael (1978). The Story of Nigeria. London: Faber. ISBN 978-0-571-04947-9. 
Davidson, Basil (1966). The African Past: Chronicles from Antiquity to Modern Times. Harmondsworth: Penguin. OCLC 2016817. 
Gordon, April A.; Donald L. Gordon (1996). Understanding Contemporary Africa. Boulder: Lynne Rienner Publishers. ISBN 978-1-55587-547-3. 
Khapoya, Vincent B. (1998). The African experience: an introduction. Upper Saddle River, NJ: Prentice Hall. ISBN 978-0-13-745852-3. 
Moore, Clark D., and Ann Dunbar (1968). Africa Yesterday and Today, in series, The George School Readings on Developing Lands. New York: Praeger Publishers.
Naipaul, V. S.. The Masque of Africa: Glimpses of African Belief. Picador, 2010. ISBN 978-0-330-47205-0
Besenyő, János. Western Sahara (2009), free online PDF book, Publikon Publishers, Pécs, ISBN 978-963-88332-0-4, 2009
Wade, Lizzie. Drones and satellites spot lost civilizations in unlikely places, Science (American Association for the Advancement of Science), DOI: 10.1126/science.aaa7864, 2015




General information
Africa at DMOZ
African & Middle Eastern Reading Room from the United States Library of Congress
Africa South of the Sahara from Stanford University
The Index on Africa from The Norwegian Council for Africa
Aluka Digital library of scholarly resources from and about Africa
Africa Interactive Map from the United States Army Africa
One of the new competitors in Africa
History
African Kingdoms
The Story of Africa from BBC World Service
Africa Policy Information Center (APIC)
Hungarian military forces in Africa
News media
allAfrica.com current news, events and statistics
Focus on Africa magazine from BBC World ServiceAsia (/ˈeɪʒə, ˈeɪʃə/) is Earth's largest and most populous continent, located primarily in the eastern and northern hemispheres and sharing the continental landmass of Eurasia with the continent of Europe. Asia covers an area of 44,579,000 square kilometres (17,212,000 sq mi), about 30% of Earth's total land area and 8.7% of the Earth's total surface area. The continent, which has long been home to the majority of the human population, was the site of many of the first civilizations. Asia is notable for not only its overall large size and population, but also dense and large settlements as well as vast barely populated regions within the continent of 4.4 billion people.
In general terms, Asia is bounded on the east by the Pacific Ocean, on the south by the Indian Ocean and on the north by the Arctic Ocean. The western boundary with Europe is a historical and cultural construct, as there is no clear physical and geographical separation between them. The most commonly accepted boundaries place Asia to the east of the Suez Canal, the Ural River, and the Ural Mountains, and south of the Caucasus Mountains and the Caspian and Black Seas.
China and India alternated in being the largest economies in the world from 1 to 1800 A.D. China was a major economic power and attracted many to the east, and for many the legendary wealth and prosperity of the ancient culture of India personified Asia, attracting European commerce, exploration and colonialism. The accidental discovery of America by Columbus in search for India demonstrates this deep fascination. The Silk Road became the main East-West trading route in the Asian hitherland while the Straits of Malacca stood as a major sea route. Asia has exhibited economic dynamism (particularly East Asia) as well as robust population growth during the 20th century, but overall population growth has since fallen. Asia was the birthplace of most of the world's mainstream religions including Christianity, Islam, Judaism, Hinduism, Buddhism, Confucianism, Taoism (or Daoism), Jainism, Sikhism, Zoroastranism, as well as many other religions.
Given its size and diversity, the concept of Asia—a name dating back to classical antiquity—may actually have more to do with human geography than physical geography. Asia varies greatly across and within its regions with regard to ethnic groups, cultures, environments, economics, historical ties and government systems. It also has a mix of many different climates ranging from the equatorial south via the hot desert in the Middle East, temperate areas in the east and the extremely continental centre to vast subarctic and polar areas in Siberia.






The boundary between Asia and Africa is the Red Sea, the Gulf of Suez, and the Suez Canal. This makes Egypt a transcontinental country, with the Sinai peninsula in Asia and the remainder of the country in Africa.




The border between Asia and Europe was historically defined by European academics. The Don River became unsatisfactory to northern Europeans when Peter the Great, king of the Tsardom of Russia, defeating rival claims of Sweden and the Ottoman Empire to the eastern lands, and armed resistance by the tribes of Siberia, synthesized a new Russian Empire extending to the Ural Mountains and beyond, founded in 1721. The major geographical theorist of the empire was actually a former Swedish prisoner-of-war, taken at the Battle of Poltava in 1709 and assigned to Tobolsk, where he associated with Peter's Siberian official, Vasily Tatishchev, and was allowed freedom to conduct geographical and anthropological studies in preparation for a future book.
In Sweden, five years after Peter's death, in 1730 Philip Johan von Strahlenberg published a new atlas proposing the Urals as the border of Asia. The Russians were enthusiastic about the concept, which allowed them to keep their European identity in geography. Tatishchev announced that he had proposed the idea to von Strahlenberg. The latter had suggested the Emba River as the lower boundary. Over the next century various proposals were made until the Ural River prevailed in the mid-19th century. The border had been moved perforce from the Black Sea to the Caspian Sea into which the Ural River projects. The border between the Black Sea and the Caspian is usually placed along the crest of the Caucasus Mountains, although it is sometimes placed further north.



The border between Asia and the loosely defined region of Oceania is usually placed somewhere in the Malay Archipelago. The terms Southeast Asia and Oceania, devised in the 19th century, have had several vastly different geographic meanings since their inception. The chief factor in determining which islands of the Malay Archipelago are Asian has been the location of the colonial possessions of the various empires there (not all European). Lewis and Wigen assert, "The narrowing of 'Southeast Asia' to its present boundaries was thus a gradual process."




Geographical Asia is a cultural artifact of European conceptions of the world, beginning with the Ancient Greeks, being imposed onto other cultures, an imprecise concept causing endemic contention about what it means. Asia is larger and more culturally diverse than Europe. It does not exactly correspond to the cultural borders of its various types of constituents.
From the time of Herodotus a minority of geographers have rejected the three-continent system (Europe, Africa, Asia) on the grounds that there is no or is no substantial physical separation between them. For example, Sir Barry Cunliffe, the emeritus professor of European archeology at Oxford, argues that Europe has been geographically and culturally merely "the western excrescence of the continent of Asia". Geographically, Asia is the major eastern constituent of the continent of Eurasia with Europe being a northwestern peninsula of the landmass. Asia, Europe and Africa make up a single continuous landmass - Afro-Eurasia (except for the Suez Canal) and share a common continental shelf. Almost all of Europe and the better part of Asia sit atop the Eurasian Plate, adjoined on the south by the Arabian and Indian Plate and with the easternmost part of Siberia (east of the Chersky Range) on the North American Plate.




The English word, "Asia," was originally a concept of Greek civilization. The place name, "Asia", in various forms in a large number of modern languages is of unknown ultimate provenience. Its etymology and language of origin are uncertain. It appears to be one of the most ancient of recorded names. A number of theories have been published. English Asia can be traced through the formation of English literature to Latin literature, where it has the same form, Asia. Whether all uses and all forms of the name derive also from the Latin of the Roman Empire is much less certain.



Before Greek poetry, the Aegean Sea area was in a Greek Dark Age, at the beginning of which syllabic writing was lost and alphabetic writing had not begun. Prior to then in the Bronze Age the records of the Assyrian Empire, the Hittite Empire and the various Mycenaean states of Greece mention a region undoubtedly Asia, certainly in Anatolia, including if not identical to Lydia. These records are administrative and do not include poetry.
The Mycenaean states were destroyed about 1200 BC by unknown agents although one school of thought assigns the Dorian invasion to this time. The burning of the palaces baked clay diurnal administrative records written in a Greek syllabic script called Linear B, deciphered by a number of interested parties, most notably by a young World War II cryptographer, Michael Ventris, subsequently assisted by the scholar, John Chadwick. A major cache discovered by Carl Blegen at the site of ancient Pylos included hundreds of male and female names formed by different methods.
Some of these are of women held in servitude (as study of the society implied by the content reveals). They were used in trades, such as cloth-making, and usually came with children. The epithet, lawiaiai, "captives," associated with some of them identifies their origin. Some are ethnic names. One in particular, aswiai, identifies "women of Asia." Perhaps they were captured in Asia, but some others, Milatiai, appear to have been of Miletus, a Greek colony, which would not have been raided for slaves by Greeks. Chadwick suggests that the names record the locations where these foreign women were purchased. The name is also in the singular, Aswia, which refers both to the name of a country and to a female of it. There is a masculine form, aswios. This Aswia appears to have been a remnant of a region known to the Hittites as Assuwa, centered on Lydia, or "Roman Asia." This name, Assuwa, has been suggested as the origin for the name of the continent "Asia". The Assuwa league was a confederation of states in western Anatolia, defeated by the Hittites under Tudhaliya I around 1400 BC.
Alternatively, the etymology of the term may be from the Akkadian word (w)aṣû(m), which means 'to go outside' or 'to ascend', referring to the direction of the sun at sunrise in the Middle East and also likely connected with the Phoenician word asa meaning east. This may be contrasted to a similar etymology proposed for Europe, as being from Akkadian erēbu(m) 'to enter' or 'set' (of the sun).
T.R. Reid supports this alternative etymology, noting that the ancient Greek name must have derived from asu, meaning 'east' in Assyrian (ereb for Europe meaning 'west'). The ideas of Occidental (form Latin Occidens 'setting') and Oriental (from Latin Oriens for 'rising') are also European invention, synonymous with Western and Eastern. Reid further emphasizes that it explains the Western point of view of placing all the peoples and cultures of Asia into a single classification, almost as if there were a need for setting the distinction between Western and Eastern civilizations on the Eurasian continent. Ogura Kazuo and Tenshin Okakura are two outspoken Japanese figures on the subject.




Latin Asia and Greek Ἀσία appear to be the same word. Roman authors translated Ἀσία as Asia. The Romans named a province Asia, which roughly corresponds with modern-day central-western Turkey. There was an Asia Minor and an Asia Major located in modern-day Iraq. As the earliest evidence of the name is Greek, it is likely circumstantially that Asia came from Ἀσία, but ancient transitions, due to the lack of literary contexts, are difficult to catch in the act. The most likely vehicles were the ancient geographers and historians, such as Herodotus, who were all Greek. Ancient Greek certainly evidences early and rich uses of the name.
The first continental use of Asia is attributed to Herodotus (about 440 BC), not because he innovated it, but because his Histories are the earliest surviving prose to describe it in any detail. He defines it carefully, mentioning the previous geographers whom he had read, but whose works are now missing. By it he means Anatolia and the Persian Empire, in contrast to Greece and Egypt. Herodotus comments that he is puzzled as to why three women's names were "given to a tract which is in reality one" (Europa, Asia, and Libya, referring to Africa), stating that most Greeks assumed that Asia was named after the wife of Prometheus (i.e. Hesione), but that the Lydians say it was named after Asies, son of Cotys, who passed the name on to a tribe at Sardis. In Greek mythology, "Asia" (Ἀσία) or "Asie" (Ἀσίη) was the name of a "Nymph or Titan goddess of Lydia."
In ancient Greek religion, places were under the care of female divinities, parallel to guardian angels. The poets detailed their doings and generations in allegoric language salted with entertaining stories, which subsequently playwrights transformed into classical Greek drama and became "Greek mythology." For example, Hesiod mentions the daughters of Tethys and Ocean, among whom are a "holy company", "who with the Lord Apollo and the Rivers have youths in their keeping." Many of these are geographic: Doris, Rhodea, Europa, Asia. Hesiod explains:

"For there are three-thousand neat-ankled daughters of Ocean who are dispersed far and wide, and in every place alike serve the earth and the deep waters."

The Iliad (attributed by the ancient Greeks to Homer) mentions two Phrygians (the tribe that replaced the Luvians in Lydia) in the Trojan War named Asios (an adjective meaning "Asian"); and also a marsh or lowland containing a marsh in Lydia as ασιος.




The history of Asia can be seen as the distinct histories of several peripheral coastal regions: East Asia, South Asia, Southeast Asia and the Middle East, linked by the interior mass of the Central Asian steppes.
The coastal periphery was home to some of the world's earliest known civilizations, each of them developing around fertile river valleys. The civilizations in Mesopotamia, the Indus Valley and the Yellow River shared many similarities. These civilizations may well have exchanged technologies and ideas such as mathematics and the wheel. Other innovations, such as writing, seem to have been developed individually in each area. Cities, states and empires developed in these lowlands.
The central steppe region had long been inhabited by horse-mounted nomads who could reach all areas of Asia from the steppes. The earliest postulated expansion out of the steppe is that of the Indo-Europeans, who spread their languages into the Middle East, South Asia, and the borders of China, where the Tocharians resided. The northernmost part of Asia, including much of Siberia, was largely inaccessible to the steppe nomads, owing to the dense forests, climate and tundra. These areas remained very sparsely populated.

The center and the peripheries were mostly kept separated by mountains and deserts. The Caucasus and Himalaya mountains and the Karakum and Gobi deserts formed barriers that the steppe horsemen could cross only with difficulty. While the urban city dwellers were more advanced technologically and socially, in many cases they could do little in a military aspect to defend against the mounted hordes of the steppe. However, the lowlands did not have enough open grasslands to support a large horsebound force; for this and other reasons, the nomads who conquered states in China, India, and the Middle East often found themselves adapting to the local, more affluent societies.
The Islamic Caliphate took over the Middle East and Central Asia during the Muslim conquests of the 7th century. The Mongol Empire conquered a large part of Asia in the 13th century, an area extending from China to Europe. Before the Mongol invasion, Song dynasty reportedly had approximately 120 million citizens; the 1300 census which followed the invasion reported roughly 60 million people.
The Black Death, one of the most devastating pandemics in human history, is thought to have originated in the arid plains of central Asia, where it then travelled along the Silk Road.
The Russian Empire began to expand into Asia from the 17th century, and would eventually take control of all of Siberia and most of Central Asia by the end of the 19th century. The Ottoman Empire controlled Anatolia, the Middle East, North Africa and the Balkans from the 16th century onwards. In the 17th century, the Manchu conquered China and established the Qing Dynasty. The Islamic Mughal Empire and the Hindu Maratha Empire controlled much of India in the 16th and 18th centuries respectively.




Asia is the largest continent on Earth. It covers 8.8% of the Earth's total surface area (or 30% of its land area), and has the largest coastline, at 62,800 kilometres (39,022 mi). Asia is generally defined as comprising the eastern four-fifths of Eurasia. It is located to the east of the Suez Canal and the Ural Mountains, and south of the Caucasus Mountains (or the Kuma–Manych Depression) and the Caspian and Black Seas. It is bounded on the east by the Pacific Ocean, on the south by the Indian Ocean and on the north by the Arctic Ocean. Asia is subdivided into 48 countries, two of them (Russia and Turkey) having part of their land in Europe.
Asia has extremely diverse climates and geographic features. Climates range from arctic and subarctic in Siberia to tropical in southern India and Southeast Asia. It is moist across southeast sections, and dry across much of the interior. Some of the largest daily temperature ranges on Earth occur in western sections of Asia. The monsoon circulation dominates across southern and eastern sections, due to the presence of the Himalayas forcing the formation of a thermal low which draws in moisture during the summer. Southwestern sections of the continent are hot. Siberia is one of the coldest places in the Northern Hemisphere, and can act as a source of arctic air masses for North America. The most active place on Earth for tropical cyclone activity lies northeast of the Philippines and south of Japan. The Gobi Desert is in Mongolia and the Arabian Desert stretches across much of the Middle East. The Yangtze River in China is the longest river in the continent. The Himalayas between Nepal and China is the tallest mountain range in the world. Tropical rainforests stretch across much of southern Asia and coniferous and deciduous forests lie farther north.



A survey carried out in 2010 by global risk analysis farm Maplecroft identified 16 countries that are extremely vulnerable to climate change. Each nation's vulnerability was calculated using 42 socio, economic and environmental indicators, which identified the likely climate change impacts during the next 30 years. The Asian countries of Bangladesh, India, Vietnam, Thailand, Pakistan and Sri Lanka were among the 16 countries facing extreme risk from climate change. Some shifts are already occurring. For example, in tropical parts of India with a semi-arid climate, the temperature increased by 0.4 °C between 1901 and 2003. A 2013 study by the International Crops Research Institute for the Semi-Arid Tropics (ICRISAT) aimed to find science-based, pro-poor approaches and techniques that would enable Asia's agricultural systems to cope with climate change, while benefitting poor and vulnerable farmers. The study's recommendations ranged from improving the use of climate information in local planning and strengthening weather-based agro-advisory services, to stimulating diversification of rural household incomes and providing incentives to farmers to adopt natural resource conservation measures to enhance forest cover, replenish groundwater and use renewable energy.




Asia has the second largest nominal GDP of all continents, after Europe, but the largest when measured in purchasing power parity. As of 2011, the largest economies in Asia are China, Japan, India, South Korea and Indonesia. Based on Global Office Locations 2011, Asia dominated the office locations with 4 of top 5 being in Asia, Hong Kong, Singapore, Tokyo, Seoul and Shanghai. Around 68 percent of international firms have office in Hong Kong.
In the late 1990s and early 2000s, the economies of the China and India have been growing rapidly, both with an average annual growth rate of more than 8%. Other recent very-high-growth nations in Asia include Israel, Malaysia, Indonesia, Bangladesh, Pakistan, Thailand, Vietnam, Mongolia, Uzbekistan, Cyprus and the Philippines, and mineral-rich nations such as Kazakhstan, Turkmenistan, Iran, Brunei, United Arab Emirates, Qatar, Kuwait, Saudi Arabia, Bahrain and Oman.
According to economic historian Angus Maddison in his book The World Economy: A Millennial Perspective, India had the world's largest economy during 0 BCE and 1000 BCE. China was the largest and most advanced economy on earth for much of recorded history, until the British Empire (excluding India) overtook it in the mid-19th century. For several decades in the late twentieth century Japan was the largest economy in Asia and second-largest of any single nation in the world, after surpassing the Soviet Union (measured in net material product) in 1986 and Germany in 1968. (NB: A number of supernational economies are larger, such as the European Union (EU), the North American Free Trade Agreement (NAFTA) or APEC). This ended in 2010 when China overtook Japan to become the world's second largest economy.
In the late 1980s and early 1990s, Japan's GDP was almost as large (current exchange rate method) as that of the rest of Asia combined. In 1995, Japan's economy nearly equaled that of the USA as the largest economy in the world for a day, after the Japanese currency reached a record high of 79 yen/US$. Economic growth in Asia since World War II to the 1990s had been concentrated in Japan as well as the four regions of South Korea, Taiwan, Hong Kong and Singapore located in the Pacific Rim, known as the Asian tigers, which have now all received developed country status, having the highest GDP per capita in Asia.

It is forecasted that India will overtake Japan in terms of nominal GDP by 2020. By 2027, according to Goldman Sachs, China will have the largest economy in the world. Several trade blocs exist, with the most developed being the Association of Southeast Asian Nations.
Asia is the largest continent in the world by a considerable margin, and it is rich in natural resources, such as petroleum, forests, fish, water, rice, copper and silver. Manufacturing in Asia has traditionally been strongest in East and Southeast Asia, particularly in China, Taiwan, South Korea, Japan, India, the Philippines, and Singapore. Japan and South Korea continue to dominate in the area of multinational corporations, but increasingly the PRC and India are making significant inroads. Many companies from Europe, North America, South Korea and Japan have operations in Asia's developing countries to take advantage of its abundant supply of cheap labour and relatively developed infrastructure.
According to Citigroup 9 of 11 Global Growth Generators countries came from Asia driven by population and income growth. They are Bangladesh, China, India, Indonesia, Iraq, Mongolia, Philippines, Sri Lanka and Vietnam. Asia has four main financial centers: Tokyo, Hong Kong, Singapore and Shanghai. Call centers and business process outsourcing (BPOs) are becoming major employers in India and the Philippines due to the availability of a large pool of highly skilled, English-speaking workers. The increased use of outsourcing has assisted the rise of India and the China as financial centers. Due to its large and extremely competitive information technology industry, India has become a major hub for outsourcing.
In 2010, Asia had 3.3 million millionaires (people with net worth over US$1 million excluding their homes), slightly below North America with 3.4 million millionaires. Last year Asia had toppled Europe. Citigroup in The Wealth Report 2012 stated that Asian centa-millionaire overtook North America's wealth for the first time as the world's "economic center of gravity" continued moving east. At the end of 2011, there were 18,000 Asian people mainly in Southeast Asia, China and Japan who have at least $100 million in disposable assets, while North America with 17,000 people and Western Europe with 14,000 people.




With growing Regional Tourism with domination of Chinese visitors, MasterCard has released Global Destination Cities Index 2013 with 10 of 20 are dominated by Asia and Pacific Region Cities and also for the first time a city of a country from Asia (Bangkok) set in the top-ranked with 15.98 international visitors.




East Asia had by far the strongest overall Human Development Index (HDI) improvement of any region in the world, nearly doubling average HDI attainment over the past 40 years, according to the report's analysis of health, education and income data. China, the second highest achiever in the world in terms of HDI improvement since 1970, is the only country on the "Top 10 Movers" list due to income rather than health or education achievements. Its per capita income increased a stunning 21-fold over the last four decades, also lifting hundreds of millions out of income poverty. Yet it was not among the region's top performers in improving school enrollment and life expectancy.Nepal, a South Asian country, emerges as one of the world's fastest movers since 1970 mainly due to health and education achievements. Its present life expectancy is 25 years longer than in the 1970s. More than four of every five children of school age in Nepal now attend primary school, compared to just one in five 40 years ago.
Japan and South Korea ranked highest among the countries grouped on the HDI (number 11 and 12 in the world, which are in the "very high human development" category), followed by Hong Kong (21) and Singapore (27). Afghanistan (155) ranked lowest amongst Asian countries out of the 169 countries assessed.




Asia is home to several language families and many language isolates. Most Asian countries have more than one language that is natively spoken. For instance, according to Ethnologue, more than 600 languages are spoken in Indonesia, more than 800 languages spoken in India, and more than 100 are spoken in the Philippines. China has many languages and dialects in different provinces.




Many of the world's major religions have their origins in Asia, including the five most practiced in the world (excluding irreligion), which are Christianity, Islam, Hinduism, Chinese folk religion (classified as Confucianism and Taoism), and Buddhism respectively. Asian mythology is complex and diverse. The story of the Great Flood for example, as presented to Christians in the Old Testament in the narrative of Noah, is first found in Mesopotamian mythology, in the Epic of Gilgamesh. Likewise, the same story of Great Flood is presented to Muslims in the Holy Quran, again in the narrative of Noah, Who according to Islamic mythology was a Prophet and Built an Ark on Allah's Command to save the True Believers from the Great Flood (Great Calamity). Hindu mythology also tells about an Avatar of the God Vishnu in the form of a fish who warned Manu of a terrible flood. In ancient Chinese mythology, Shan Hai Jing, the Chinese ruler Da Yu, had to spend 10 years to control a deluge which swept out most of ancient China and was aided by the goddess Nüwa who literally fixed the broken sky through which huge rains were pouring.



The Abrahamic religions of Judaism, Christianity, Islam and Bahá'í Faith originated in West Asia.
Judaism, the oldest of the Abrahamic faiths, is practiced primarily in Israel, the birthplace and historical homeland of the Hebrew nation which today consists equally of those Israelites who remained in Asia/North Africa and those who returned from diaspora in Europe, North America, and other regions, though sizable communities continue to live abroad. Jews are the predominant ethnic group in Israel (75.6%) numbering at about 6.1 million, although the levels of adherence to Jewish religion are unspecified. Outside of Israel there are small ancient communities of Jewish still live in Turkey (17,400), Azerbaijan (9,100), Iran (8,756), India (5,000) and Uzbekistan (4,000).
Christianity is a widespread religion in Asia with more than 286 million adherents according to Pew Research Center in 2010, and nearly 364 million according to Britannica Book of the Year 2014. constituting around 12.6% of the total population of Asia. In the Philippines and East Timor, Roman Catholicism is the predominant religion; it was introduced by the Spaniards and the Portuguese, respectively. In Armenia, Cyprus, Georgia and Asian Russia, Eastern Orthodoxy is the predominant religion. Various Christian denominations have adherents in portions of the Middle East, as well as China and India. Saint Thomas Christians in India trace their origins to the evangelistic activity of Thomas the Apostle in the 1st century.
Islam, which originated in Saudi Arabia, is the largest and most widely spread religion in Asia with at least 1 billion Muslims. With 12.7% of the world Muslim population, the country currently with the largest Muslim population in the world is Indonesia, followed by Pakistan, India, Bangladesh, Iran and Turkey. Mecca, Medina and to a lesser extent Jerusalem are the holiest cities for Islam in all the world. These religious sites attract large numbers of devotees from all over the world, particularly during the Hajj and Umrah seasons. Iran is the largest Shi'a country.
The Bahá'í Faith originated in Asia, in Iran (Persia), and spread from there to the Ottoman Empire, Central Asia, India, and Burma during the lifetime of Bahá'u'lláh. Since the middle of the 20th century, growth has particularly occurred in other Asian countries, because Bahá'í activities in many Muslim countries has been severely suppressed by authorities. Lotus Temple is a big Baha'i Temple in India.




Almost all Asian religions have philosophical character and Asian philosophical traditions cover a large spectrum of philosophical thoughts and writings. Indian philosophy includes Hindu philosophy and Buddhist philosophy. They include elements of nonmaterial pursuits, whereas another school of thought from India, Cārvāka, preached the enjoyment of the material world. The religions of Hinduism, Buddhism, Jainism and Sikhism originated in India, South Asia. In East Asia, particularly in China and Japan, Confucianism, Taoism and Zen Buddhism took shape.
As of 2012, Hinduism has around 1.1 billion adherents. The faith represents around 25% of Asia's population and is the second largest religion in Asia. However, it is mostly concentrated in South Asia. Over 80% of the populations of both India and Nepal adhere to Hinduism, alongside significant communities in Bangladesh, Pakistan, Bhutan, Sri Lanka and Bali, Indonesia. Many overseas Indians in countries such as Burma, Singapore and Malaysia also adhere to Hinduism.
Buddhism has a great following in mainland Southeast Asia and East Asia. Buddhism is the religion of the majority of the populations of Cambodia (96%), Thailand (95%), Burma (80%–89%), Japan (36%–96%), Bhutan (75%–84%), Sri Lanka (70%), Laos (60%–67%) and Mongolia (53%–93%). Large Buddhist populations also exist in Singapore (33%–51%), Taiwan (35%–93%), South Korea (23%–50%), Malaysia (19%–21%), Nepal (9%–11%), Vietnam (10%–75%), China (20%–50%), North Korea (1.5%–14%), and small communities in India and Bangladesh. In many Chinese communities, Mahayana Buddhism is easily syncretized with Taoism, thus exact religious statistics is difficult to obtain and may be understated or overstated. The Communist-governed countries of China, Vietnam and North Korea are officially atheist, thus the number of Buddhists and other religious adherents may be under-reported.
Jainism is found mainly in India and in oversea Indian communities such as the United States and Malaysia. Sikhism is found in Northern India and amongst overseas Indian communities in other parts of Asia, especially Southeast Asia. Confucianism is found predominantly in Mainland China, South Korea, Taiwan and in overseas Chinese populations. Taoism is found mainly in Mainland China, Taiwan, Malaysia and Singapore. Taoism is easily syncretized with Mahayana Buddhism for many Chinese, thus exact religious statistics is difficult to obtain and may be understated or overstated.




Some of the events pivotal in the Asia territory related to the relationship with the outside world in the post-Second World War were:
The Korean War
The French-Indochina War
The Vietnam War
The Indonesia–Malaysia confrontation
The Sino-Vietnamese War
The Bangladesh Liberation War
The Yom Kippur War
The Iranian Revolution
The Soviet war in Afghanistan
The Iran–Iraq War
The Indonesian occupation of East Timor
The Cambodian Killing Fields
The Insurgency in Laos
The Lebanese Civil War
The Sri Lankan Civil War
The Dissolution of the Soviet Union
The Gulf War
The Nepalese Civil War
The India-Pakistan Wars
The Nagorno-Karabakh War
The War in Afghanistan
The Iraq War
The 2006 Thai coup d'état
The Burmese Civil War
The Saffron Revolution
The Arab Spring
The Arab–Israeli conflict
The Syrian Civil War
The Sino-Indian War
The 2014 Thai coup d'état
The Islamic State of Iraq and the Levant







The polymath Rabindranath Tagore, a Bengali poet, dramatist, and writer from Santiniketan, now in West Bengal, India, became in 1913 the first Asian Nobel laureate. He won his Nobel Prize in Literature for notable impact his prose works and poetic thought had on English, French, and other national literatures of Europe and the Americas. He is also the writer of the national anthems of Bangladesh and India.
Other Asian writers who won Nobel Prize for literature include Yasunari Kawabata (Japan, 1968), Kenzaburō Ōe (Japan, 1994), Gao Xingjian (China, 2000), Orhan Pamuk (Turkey, 2006), and Mo Yan (China, 2012). Some may consider the American writer, Pearl S. Buck, an honorary Asian Nobel laureate, having spent considerable time in China as the daughter of missionaries, and based many of her novels, namely The Good Earth (1931) and The Mother (1933), as well as the biographies of her parents of their time in China, The Exile and Fighting Angel, all of which earned her the Literature prize in 1938.
Also, Mother Teresa of India and Shirin Ebadi of Iran were awarded the Nobel Peace Prize for their significant and pioneering efforts for democracy and human rights, especially for the rights of women and children. Ebadi is the first Iranian and the first Muslim woman to receive the prize. Another Nobel Peace Prize winner is Aung San Suu Kyi from Burma for her peaceful and non-violent struggle under a military dictatorship in Burma. She is a nonviolent pro-democracy activist and leader of the National League for Democracy in Burma (Myanmar) and a noted prisoner of conscience. She is a Buddhist and was awarded the Nobel Peace Prize in 1991. Chinese dissident Liu Xiaobo was awarded the Nobel Peace Prize for "his long and non-violent struggle for fundamental human rights in China" on 8 October 2010. He is the first Chinese citizen to be awarded a Nobel Prize of any kind while residing in China. In 2014, Kailash Satyarthi from India and Malala Yousafzai from Pakistan were awarded the Nobel Peace Prize "for their struggle against the suppression of children and young people and for the right of all children to education".
Sir C. V. Raman is the first Asian to get a Nobel prize in Sciences. He won the Nobel Prize in Physics "for his work on the scattering of light and for the discovery of the effect named after him".
Japan has won the most Nobel Prizes of any Asian nation with 24 followed by India which has won 13.
Amartya Sen, (born 3 November 1933) is an Indian economist who was awarded the 1998 Nobel Memorial Prize in Economic Sciences for his contributions to welfare economics and social choice theory, and for his interest in the problems of society's poorest members.
Other Asian Nobel Prize winners include Subrahmanyan Chandrasekhar, Abdus Salam, Robert Aumann, Menachem Begin, Aaron Ciechanover, Avram Hershko, Daniel Kahneman, Shimon Peres, Yitzhak Rabin, Ada Yonath, Yasser Arafat, José Ramos-Horta and Bishop Carlos Filipe Ximenes Belo of Timor Leste, Kim Dae-jung, and 13 Japanese scientists. Most of the said awardees are from Japan and Israel except for Chandrasekhar and Raman (India), Salam (Pakistan), Arafat (Palestinian Territories), Kim (South Korea), and Horta and Belo (Timor Leste).
In 2006, Dr. Muhammad Yunus of Bangladesh was awarded the Nobel Peace Prize for the establishment of Grameen Bank, a community development bank that lends money to poor people, especially women in Bangladesh. Dr. Yunus received his PhD in economics from Vanderbilt University, United States. He is internationally known for the concept of micro credit which allows poor and destitute people with little or no collateral to borrow money. The borrowers typically pay back money within the specified period and the incidence of default is very low.
The Dalai Lama has received approximately eighty-four awards over his spiritual and political career. On 22 June 2006, he became one of only four people ever to be recognized with Honorary Citizenship by the Governor General of Canada. On 28 May 2005, he received the Christmas Humphreys Award from the Buddhist Society in the United Kingdom. Most notable was the Nobel Peace Prize, presented in Oslo, Norway on 10 December 1989.




Within the above-mentioned states are several partially recognized countries with limited to no international recognition. None of them are members of the UN:




References to articles:
Subregions of Asia
Special topics:
Asian Century
Asian cuisine
Asian furniture
Asian Games
Asian Monetary Unit
Asian people
Eastern world
Eurasia
Far East
East Asia
Southeast Asia
South Asia
Central Asia
Fauna of Asia
Flags of Asia
Middle East
Eastern Mediterranean
Levant
Near East

Pan-Asianism
Lists:
List of cities in Asia
List of metropolitan areas in Asia by population
List of sovereign states and dependent territories in Asia






Lewis, Martin W.; Wigen, Kären (1997). The myth of continents: a critique of metageography. Berkeley and Los Angeles: University of California Press. ISBN 0-520-20743-2. 
Ventris, Michael; Chadwick, John (1973). Documents in Mycenaean Greek (2nd ed.). Cambridge: University Press. 



Higham, Charles. Encyclopedia of Ancient Asian Civilizations. Facts on File library of world history. New York: Facts On File, 2004.
Kamal, Niraj. "Arise Asia: Respond to White Peril". New Delhi:Wordsmith,2002, ISBN 978-81-87412-08-3
Kapadia, Feroz, and Mandira Mukherjee. Encyclopaedia of Asian Culture and Society. New Delhi: Anmol Publications, 1999.
Levinson, David, and Karen Christensen. Encyclopedia of Modern Asia. New York: Charles Scribner's Sons, 2002.




"Display Maps". The Soil Maps of Asia. European Digital Archive of Soil Maps – EuDASM. Retrieved 26 July 2011. 
"Asia Maps". Perry-Castañeda Library Map Collection. University of Texas Libraries. Archived from the original on 18 July 2011. Retrieved 20 July 2011. 
"Asia". Norman B. Leventhal Map Center at the Boston Public Library. Retrieved 26 July 2011. 
Bowring, Philip (12 February 1987). "What is Asia?". Eastern Economic Review. Columbia University Asia For Educators. 135 (7).Europe is a continent that comprises the westernmost part of Eurasia. Europe is bordered by the Arctic Ocean to the north, the Atlantic Ocean to the west, and the Mediterranean Sea to the south. To the east and southeast, Europe is generally considered as separated from Asia by the watershed divides of the Ural and Caucasus Mountains, the Ural River, the Caspian and Black Seas, and the waterways of the Turkish Straits. Yet the non-oceanic borders of Europe—a concept dating back to classical antiquity—are arbitrary. The primarily physiographic term "continent" as applied to Europe also incorporates cultural and political elements whose discontinuities are not always reflected by the continent's current overland boundaries.
Europe covers about 10,180,000 square kilometres (3,930,000 sq mi), or 2% of the Earth's surface (6.8% of land area). Politically, Europe is divided into about fifty sovereign states of which the Russian Federation is the largest and most populous, spanning 39% of the continent and comprising 15% of its population. Europe had a total population of about 740 million (about 11% of world population) as of 2012.
The European climate is largely affected by warm Atlantic currents that temper winters and summers on much of the continent, even at latitudes along which the climate in Asia and North America is severe. Further from the sea, seasonal differences are more noticeable than close to the coast.
Europe, in particular ancient Greece, was the birthplace of Western civilization. The fall of the Western Roman Empire, during the migration period, marked the end of ancient history and the beginning of an era known as the Middle Ages. Renaissance humanism, exploration, art, and science led to the modern era. From the Age of Discovery onwards, Europe played a predominant role in global affairs. Between the 16th and 20th centuries, European powers controlled at various times the Americas, most of Africa, Oceania, and the majority of Asia.
The Industrial Revolution, which began in Great Britain at the end of the 18th century, gave rise to radical economic, cultural, and social change in Western Europe, and eventually the wider world. Both world wars took place for the most part in Europe, contributing to a decline in Western European dominance in world affairs by the mid-20th century as the United States and Soviet Union took prominence. During the Cold War, Europe was divided along the Iron Curtain between NATO in the west and the Warsaw Pact in the east, until the revolutions of 1989 and fall of the Berlin Wall.
In 1955, the Council of Europe was formed following a speech by Sir Winston Churchill, with the idea of unifying Europe to achieve common goals. It includes all states except for Belarus, Kazakhstan and Vatican City. Further European integration by some states led to the formation of the European Union, a separate political entity that lies between a confederation and a federation. The EU originated in Western Europe but has been expanding eastward since the fall of the Soviet Union in 1991. The currency of most countries of the European Union, the euro, is the most commonly used among Europeans; and the EU's Schengen Area abolishes border and immigration controls among most of its member states. The European Anthem is "Ode to Joy" and states celebrate peace and unity on Europe Day.




In classical Greek mythology, Europa (Ancient Greek: Εὐρώπη, Eurṓpē) is the name of either a Phoenician princess or of a queen of Crete. The name contains the elements εὐρύς (eurus), "wide, broad" and ὤψ/ὠπ-/ὀπτ- (ōps/ōp-/opt-) "eye, face, countenance", hence Eurṓpē, "wide-gazing", "broad of aspect". Broad has been an epithet of Earth herself in the reconstructed Proto-Indo-European religion. A historical suggestion attempted to connect a Semitic term for "west".
Most major world languages use words derived from "Europa" to refer to the continent. Chinese, for example, uses the word Ōuzhōu (歐洲/欧洲); a similar Chinese-derived term Ōshū (欧州) is also sometimes used in Japanese such as in the Japanese name of the European Union, Ōshū Rengō (欧州連合), despite the katakana Yōroppa (ヨーロッパ) being more commonly used. In some Turkic languages the originally Persian name Frangistan ("land of the Franks") is used casually in referring to much of Europe, besides official names such as Avrupa or Evropa.







The prevalent definition of Europe as a geographical term has been in use since the mid-19th century. Europe is taken to be bounded by large bodies of water to the north, west and south; Europe's limits to the far east are usually taken to be the Urals, the Ural River, and the Caspian Sea; to the southeast, including the Caucasus Mountains, the Black Sea and the waterways connecting the Black Sea to the Mediterranean Sea.
Islands are generally grouped with the nearest continental landmass, hence Iceland is generally considered to be part of Europe, while the nearby island of Greenland is usually assigned to North America. Nevertheless, there are some exceptions based on sociopolitical and cultural differences. Cyprus is closest to Anatolia (or Asia Minor), but is usually considered part of Europe both culturally and politically and is a member state of the EU. Malta was considered an island of North Africa for centuries.
"Europe" as used specifically in British English may also refer to Continental Europe exclusively.







The first recorded usage of Eurṓpē as a geographic term is in the Homeric Hymn to Delian Apollo, in reference to the western shore of the Aegean Sea. As a name for a part of the known world, it is first used in the 6th century BC by Anaximander and Hecataeus. Anaximander placed the boundary between Asia and Europe along the Phasis River (the modern Rioni) in the Caucasus, a convention still followed by Herodotus in the 5th century BC. Herodotus mentioned that the world had been divided by unknown persons into three parts, Europe, Asia, and Libya (Africa), with the Nile and the River Phasis forming their boundaries—though he also states that some considered the River Don, rather than the Phasis, as the boundary between Europe and Asia. Europe's eastern frontier was defined in the 1st century by geographer Strabo at the River Don. The Book of Jubilees described the continents as the lands given by Noah to his three sons; Europe was defined as stretching from the Pillars of Hercules at the Strait of Gibraltar, separating it from North Africa, to the Don, separating it from Asia.
The convention received by the Middle Ages and surviving into modern usage is that of the Roman era used by Roman era authors such as Posidonius, Strabo and Ptolemy, who took the Tanais (the modern Don River) as the boundary.
The term "Europe" is first used for a cultural sphere in the Carolingian Renaissance of the 9th century. From that time, the term designated the sphere of influence of the Western Church, as opposed to both the Eastern Orthodox churches and to the Islamic world.
A cultural definition of Europe as the lands of Latin Christendom coalesced in the 8th century, signifying the new cultural condominium created through the confluence of Germanic traditions and Christian-Latin culture, defined partly in contrast with Byzantium and Islam, and limited to northern Iberia, the British Isles, France, Christianised western Germany, the Alpine regions and northern and central Italy. The concept is one of the lasting legacies of the Carolingian Renaissance: "Europa" often figures in the letters of Charlemagne's court scholar, Alcuin.




The question of defining a precise eastern boundary of Europe arises in the Early Modern period, as the eastern extension of Muscovy began to include Northern Asia.
Throughout the Middle Ages and into the 18th century, the traditional division of the landmass of Eurasia into two continents, Europe and Asia, followed Ptolemy, with the boundary following the Turkish Straits, the Black Sea, the Kerch Strait, the Sea of Azov and the Don (ancient Tanais). But maps produced during the 16th to 18th centuries tended to differ in how to continue the boundary beyond the Don bend at Kalach-na-Donu (where it is closest to the Volga, now joined with it by the Volga–Don Canal), into territory not described in any detail by the ancient geographers.
Philip Johan von Strahlenberg in 1725 was the first to depart from the classical Don boundary by drawing the line along the Volga, following the Volga north until the Samara Bend, along Obshchy Syrt (the drainage divide between Volga and Ural) and then north along Ural Mountains. introducing the convention that would eventually become adopted as standard.
The mapmakers continued to differ on the boundary between the lower Don and Samara well into the 19th century. The 1745 atlas published by the Russian Academy of Sciences has the boundary follow the Don beyond Kalach as far as Serafimovich before cutting north towards Arkhangelsk, while other 18th- to 19th-century mapmakers such as John Cary followed Strahlenberg's prescription. To the south, the Kuma–Manych Depression was identified circa 1773 by a German naturalist, Peter Simon Pallas, as a valley that, once upon a time, connected the Black Sea and the Caspian Sea, and subsequently was proposed as a natural boundary between continents.
By the mid-19th century, there were three main conventions, one following the Don, the Volga–Don Canal and the Volga, the other following the Kuma–Manych Depression to the Caspian and then the Ural River, and the third abandoning the Don altogether, following the Greater Caucasus watershed to the Caspian. The question was still treated as a "controversy" in geographical literature of the 1860s, with Douglas Freshfield advocating the Caucasus crest boundary as the "best possible", citing support from various "modern geographers".
In Russia and the Soviet Union, the boundary along the Kuma–Manych Depression was the most commonly used as early as 1906. In 1958, the Soviet Geographical Society formally recommended that the boundary between the Europe and Asia be drawn in textbooks from Baydaratskaya Bay, on the Kara Sea, along the eastern foot of Ural Mountains, then following the Ural River until the Mugodzhar Hills, and then the Emba River; and Kuma–Manych Depression, thus placing the Caucasus entirely in Asia and the Urals entirely in Europe. However, most geographers in the Soviet Union favoured the boundary along the Caucasus crest and this became the standard convention in the later 20th century, although the Kuma–Manych boundary remained in use in some 20th-century maps.







Homo erectus georgicus, which lived roughly 1.8 million years ago in Georgia, is the earliest hominid to have been discovered in Europe. Other hominid remains, dating back roughly 1 million years, have been discovered in Atapuerca, Spain. Neanderthal man (named after the Neandertal valley in Germany) appeared in Europe 150,000 years ago and disappeared from the fossil record about 28,000 BC, with this extinction probably due to climate change, and their final refuge being present-day Portugal. The Neanderthals were supplanted by modern humans (Cro-Magnons), who appeared in Europe around 43 to 40 thousand years ago.

The European Neolithic period—marked by the cultivation of crops and the raising of livestock, increased numbers of settlements and the widespread use of pottery—began around 7000 BC in Greece and the Balkans, probably influenced by earlier farming practices in Anatolia and the Near East. It spread from the Balkans along the valleys of the Danube and the Rhine (Linear Pottery culture) and along the Mediterranean coast (Cardial culture). Between 4500 and 3000 BC, these central European neolithic cultures developed further to the west and the north, transmitting newly acquired skills in producing copper artefacts. In Western Europe the Neolithic period was characterised not by large agricultural settlements but by field monuments, such as causewayed enclosures, burial mounds and megalithic tombs. The Corded Ware cultural horizon flourished at the transition from the Neolithic to the Chalcolithic. During this period giant megalithic monuments, such as the Megalithic Temples of Malta and Stonehenge, were constructed throughout Western and Southern Europe.
The European Bronze Age began c. 3200 BC in Greece with the Minoan civilization on Crete, the first advanced civilization in Europe. The Minoans were followed by the Myceneans, who collapsed suddenly around 1200 BC, ushering the European Iron Age. Iron Age colonisation by the Greeks and Phoenicians gave rise to early Mediterranean cities. Early Iron Age Italy and Greece from around the 8th century BC gradually gave rise to historical Classical antiquity, whose beginning is sometimes dated to 776 BC, the year the first Olympic Games.




Ancient Greece was the founding culture of Western civilisation. Western democratic and rationalist culture are often attributed to Ancient Greece. The Greeks city-state, the polis, was the fundamental political unit of classical Greece. In 508 BC, Cleisthenes instituted the world's first democratic system of government in Athens. The Greek political ideals were rediscovered in the late 18th century by European philosophers and idealists. Greece also generated many cultural contributions: in philosophy, humanism and rationalism under Aristotle, Socrates and Plato; in history with Herodotus and Thucydides; in dramatic and narrative verse, starting with the epic poems of Homer; in drama with Sophocles and Euripides, in medicine with Hippocrates and Galen; and in science with Pythagoras, Euclid and Archimedes. In the course of the 5th century BC, several of the Greek city states would ultimately check the Achaemenid Persian advance in Europe through the Greco-Persian Wars, considered a pivotal moment in world history, as the 50 years of peace that followed are known as Golden Age of Athens, the seminal period of ancient Greece that laid many of the foundations of Western civilization.

Greece was followed by Rome, which left its mark on law, politics, language, engineering, architecture, government and many more key aspects in western civilisation. Expanding from their base in Italy beginning in the 3rd century BC, the Romans gradually expanded to eventually rule the entire Mediterranean basin and western Europe by the turn of the millennium. The Roman Republic ended in 27 BC, when Augustus proclaimed the Roman Empire. The two centuries that followed are known as the pax romana, a period of unprecedented peace, prosperity, and political stability in most of Europe.
The empire continued to expand under emperors such as Antoninus Pius and Marcus Aurelius, who spent time on the Empire's northern border fighting Germanic, Pictish and Scottish tribes. The Empire began to decline in the 3rd century, particularly in the west. Christianity was legalised by Constantine I in 313 AD after three centuries of imperial persecution. Constantine also permanently moved the capital of the empire from Rome to the city of Byzantium, which was renamed Constantinople in his honour (modern-day Istanbul) in 330 AD. Christianity became the sole official religion of the empire in 380 AD, and in 391-392 AD, the emperor Theodosius outlawed pagan religions. This is sometimes considered to mark the end of antiquity; alternatively antiquity is considered to end with the fall of the Western Roman Empire in 476 AD; the closure of the pagan Platonic Academy of Athens in 529 AD; or the rise of Islam in the early 7th century AD.




During the decline of the Roman Empire, Europe entered a long period of change arising from what historians call the "Age of Migrations". There were numerous invasions and migrations amongst the Ostrogoths, Visigoths, Goths, Vandals, Huns, Franks, Angles, Saxons, Slavs, Avars, Bulgars and, later on, the Vikings, Pechenegs, Cumans and Magyars. Renaissance thinkers such as Petrarch would later refer to this as the "Dark Ages". Isolated monastic communities were the only places to safeguard and compile written knowledge accumulated previously; apart from this very few written records survive and much literature, philosophy, mathematics, and other thinking from the classical period disappeared from Western Europe though they were preserved in the east, in the Byzantine Empire.
While the Roman empire in the west continued to decline, Roman traditions and the Roman state remained strong in the predominantly Greek-speaking Eastern Roman Empire, also known as the Byzantine Empire. During most of its existence, the Byzantine Empire was the most powerful economic, cultural, and military force in Europe. Emperor Justinian I presided over Constantinople's first golden age: he established a legal code that forms the basis of many modern legal systems, funded the construction of the Hagia Sophia, and brought the Christian church under state control.

From the 7th century onwards, as the Byzantines and neighbouring Sasanid Persians were severely weakened due the protracted, centuries-lasting and frequent Byzantine–Sasanian wars, the Muslim Arabs began to make inroads into historically Roman territory, taking the Levant and North Africa and making inroads into Asia Minor. In the mid 7th century AD, following the Muslim conquest of Persia, Islam penetrated into the Caucasus region. Over the next centuries Muslim forces took Cyprus, Malta, Crete, Sicily and parts of southern Italy. Between 711 and 720, most of the Iberian Peninsula was brought under Muslim rule — save for small areas in the northwest (Asturias) and largely Basque regions in the Pyrenees. This territory, under the Arabic name Al-Andalus, became part of the expanding Umayyad Caliphate. The unsuccessful second siege of Constantinople (717) weakened the Umayyad dynasty and reduced their prestige. The Umayyads were then defeated by the Frankish leader Charles Martel at the Battle of Poitiers in 732, which ended their northward advance.
During the Dark Ages, the Western Roman Empire fell under the control of various tribes. The Germanic and Slav tribes established their domains over Western and Eastern Europe respectively. Eventually the Frankish tribes were united under Clovis I. Charlemagne, a Frankish king of the Carolingian dynasty who had conquered most of Western Europe, was anointed "Holy Roman Emperor" by the Pope in 800. This led in 962 to the founding of the Holy Roman Empire, which eventually became centred in the German principalities of central Europe.
East Central Europe saw the creation of the first Slavic states and the adoption of Christianity (circa 1000 AD). The powerful West Slavic state of Great Moravia spread its territory all the way south to the Balkans, reaching its largest territorial extent under Svatopluk I and causing a series of armed conflicts with East Francia. Further south, the first South Slavic states emerged in the late 7th and 8th century and adopted Christianity: the First Bulgarian Empire, the Serbian Principality (later Kingdom and Empire), and the Duchy of Croatia (later Kingdom of Croatia). To the East, the Kievan Rus expanded from its capital in Kiev to become the largest state in Europe by the 10th century. In 988, Vladimir the Great adopted Orthodox Christianity as the religion of state. Further East, Volga Bulgaria became an Islamic state in the 10th century, but was eventually absorbed into Russia several centuries later.




The period between the year 1000 and 1300 is known as the High Middle Ages, during which the population of Europe experienced significant growth, culminating in the Renaissance of the 12th century. Economic growth, together with the lack of safety on the mainland trading routes, made possible the development of major commercial routes along the coast of the Mediterranean and Baltic Seas. The growing wealth and independence acquired by some coastal cities gave the Maritime Republics a leading role in the European scene.

The Middle Ages on the mainland were dominated by the two upper echelons of the social structure: the nobility and the clergy. Feudalism developed in France in the Early Middle Ages and soon spread throughout Europe. A struggle for influence between the nobility and the monarchy in England led to the writing of the Magna Carta and the establishment of a parliament. The primary source of culture in this period came from the Roman Catholic Church. Through monasteries and cathedral schools, the Church was responsible for education in much of Europe.
The Papacy reached the height of its power during the High Middle Ages. An East-West Schism in 1054 split the former Roman Empire religiously, with the Eastern Orthodox Church in the Byzantine Empire and the Roman Catholic Church in the former Western Roman Empire. In 1095 Pope Urban II called for a crusade against Muslims occupying Jerusalem and the Holy Land. In Europe itself, the Church organised the Inquisition against heretics. In Spain, the Reconquista concluded with the fall of Granada in 1492, ending over seven centuries of Islamic rule in the Iberian Peninsula.
In the east a resurgent Byzantine Empire recaptured Crete and Cyprus from the Muslims and reconquered the Balkans. Constantinople was the largest and wealthiest city in Europe from the 9th to the 12th centuries, with a population of approximately 400,000. The Empire was weakened following the defeat at Manzikert and was weakened considerably by the sack of Constantinople in 1204, during the Fourth Crusade. Although it would recover Constantinople in 1261, Byzantium fell in 1453 when Constantinople was taken by the Ottoman Empire.

In the 11th and 12th centuries, constant incursions by nomadic Turkic tribes, such as the Pechenegs and the Cuman-Kipchaks, caused a massive migration of Slavic populations to the safer, heavily forested regions of the north and temporarily halted the expansion of the Rus' state to the south and east. Like many other parts of Eurasia, these territories were overrun by the Mongols. The invaders, who became known as Tatars, were mostly Turkic-speaking peoples under Mongol suzerainty. They established the state of the Golden Horde with headquarters in Crimea, which later adopted Islam as a religion and ruled over modern-day southern and central Russia for more than three centuries. After the collapse of Mongol dominions, the first Romanian states (principalities) emerged in the 14th century: Moldova and Walachia. Previously, these territories were under the successive control of Pechenegs and Cumans. From the 12th to the 15th centuries, the Grand Duchy of Moscow grew from a small principality under Mongol rule to the largest state in Europe, overthrowing the Mongols in 1480 and eventually becoming the Tsardom of Russia. The state was consolidated under Ivan III the Great and Ivan the Terrible, steadily expanding to the east and south over the next centuries.
The Great Famine of 1315–1317 was the first crisis that would strike Europe in the late Middle Ages. The period between 1348 and 1420 witnessed the heaviest loss. The population of France was reduced by half. Medieval Britain was afflicted by 95 famines, and France suffered the effects of 75 or more in the same period. Europe was devastated in the mid-14th century by the Black Death, one of the most deadly pandemics in human history which killed an estimated 25 million people in Europe alone—a third of the European population at the time.
The plague had a devastating effect on Europe's social structure; it induced people to live for the moment as illustrated by Giovanni Boccaccio in The Decameron (1353). It was a serious blow to the Roman Catholic Church and led to increased persecution of Jews, foreigners, beggars and lepers. The plague is thought to have returned every generation with varying virulence and mortalities until the 18th century. During this period, more than 100 plague epidemics swept across Europe.




The Renaissance was a period of cultural change originating in Florence and later spreading to the rest of Europe. The rise of a new humanism was accompanied by the recovery of forgotten classical Greek and Arabic knowledge from monastic libraries, often translated from Arabic into Latin. The Renaissance spread across Europe between the 14th and 16th centuries: it saw the flowering of art, philosophy, music, and the sciences, under the joint patronage of royalty, the nobility, the Roman Catholic Church, and an emerging merchant class. Patrons in Italy, including the Medici family of Florentine bankers and the Popes in Rome, funded prolific quattrocento and cinquecento artists such as Raphael, Michelangelo, and Leonardo da Vinci.
Political intrigue within the Church in the mid-14th century caused the Western Schism. During this forty-year period, two popes—one in Avignon and one in Rome—claimed rulership over the Church. Although the schism was eventually healed in 1417, the papacy's spiritual authority had suffered greatly.

The Church's power was further weakened by the Protestant Reformation (1517–1648), initially sparked by the works of German theologian Martin Luther, an attempt to start a reform within the Church. The Reformation also damaged the Holy Roman Emperor's influence, as German princes became divided between Protestant and Roman Catholic faiths. This eventually led to the Thirty Years War (1618–1648), which crippled the Holy Roman Empire and devastated much of Germany, killing between 25 and 40 percent of its population. In the aftermath of the Peace of Westphalia, France rose to predominance within Europe.
The 17th century in southern, central and eastern Europe was a period of general decline. Central and Eastern Europe experienced more than 150 famines in a 200-year period between 1501 and 1700. From the Union of Krewo (1385) central and eastern Europe was dominated by Kingdom of Poland and Grand Duchy of Lithuania. Between 1648 and 1655 in the central and eastern Europe ended hegemony of the Polish–Lithuanian Commonwealth. From the 15th to 18th centuries, when the disintegrating khanates of the Golden Horde were conquered by Russia, Tatars from the Crimean Khanate frequently raided Eastern Slavic lands to capture slaves. Further east, the Nogai Horde and Kazakh Khanate frequently raided the Slavic-speaking areas of Russia, Ukraine and Poland for hundreds of years, until the Russian expansion and conquest of most of northern Eurasia (i.e. Eastern Europe, Central Asia and Siberia). Meanwhile, in the south, the Ottomans had conquered the Balkans by the 15th century, laying siege to Vienna in 1529. In the Battle of Lepanto in 1571, the Holy League checked Ottoman power in the Mediterranean. The Ottomans again laid siege to Vienna in 1683, but the Battle of Vienna permanently ended their advance into Europe, and marked the political hegemony of the Habsburg dynasty in central Europe.
The Renaissance and the New Monarchs marked the start of an Age of Discovery, a period of exploration, invention, and scientific development. Among the great figures of the Western scientific revolution of the 16th and 17th centuries were Copernicus, Kepler, Galileo, and Isaac Newton. According to Peter Barrett, "It is widely accepted that 'modern science' arose in the Europe of the 17th century (towards the end of the Renaissance), introducing a new understanding of the natural world." In the 15th century, Portugal and Spain, two of the greatest naval powers of the time, took the lead in exploring the world. Christopher Columbus reached the New World in 1492 and Vasco da Gama opened the ocean route to the East in 1498, and soon after the Spanish and Portuguese began establishing colonial empires in the Americas and Asia. France, the Netherlands and England soon followed in building large colonial empires with vast holdings in Africa, the Americas, and Asia.




The Age of Enlightenment was a powerful intellectual movement during the 18th century promoting scientific and reason-based thoughts. Discontent with the aristocracy and clergy's monopoly on political power in France resulted in the French Revolution and the establishment of the First Republic as a result of which the monarchy and many of the nobility perished during the initial reign of terror. Napoleon Bonaparte rose to power in the aftermath of the French Revolution and established the First French Empire that, during the Napoleonic Wars, grew to encompass large parts of Europe before collapsing in 1815 with the Battle of Waterloo. Napoleonic rule resulted in the further dissemination of the ideals of the French Revolution, including that of the nation-state, as well as the widespread adoption of the French models of administration, law, and education. The Congress of Vienna, convened after Napoleon's downfall, established a new balance of power in Europe centred on the five "Great Powers": the UK, France, Prussia, Austria, and Russia. This balance would remain in place until the Revolutions of 1848, during which liberal uprisings affected all of Europe except for Russia and the UK. These revolutions were eventually put down by conservative elements and few reforms resulted. The year 1859 saw the unification of Romania, as a nation-state, from smaller principalities. In 1867, the Austro-Hungarian empire was formed; and 1871 saw the unifications of both Italy and Germany as nation-states from smaller principalities.

In parallel, the Eastern Question grew more complex ever since the Ottoman defeat in the Russo-Turkish War (1768–1774). As the dissolution of the Ottoman Empire seemed imminent, the Great Powers struggled to safeguard their strategic and commercial interests in the Ottoman domains. The Russian Empire stood to benefit from the decline, whereas the Habsburg Empire and Britain perceived the preservation of the Ottoman Empire to be in their best interests. Meanwhile, the Serbian revolution (1804) and Greek War of Independence (1821) marked the beginning of the end of Ottoman rule in the Balkans, which ended with the Balkan Wars in 1912-1913. Formal recognition of the de facto independent principalities of Montenegro, Serbia and Romania ensued at the Congress of Berlin in 1878.

The Industrial Revolution started in Great Britain in the last part of the 18th century and spread throughout Europe. The invention and implementation of new technologies resulted in rapid urban growth, mass employment, and the rise of a new working class. Reforms in social and economic spheres followed, including the first laws on child labour, the legalisation of trade unions, and the abolition of slavery. In Britain, the Public Health Act of 1875 was passed, which significantly improved living conditions in many British cities. Europe's population increased from about 100 million in 1700 to 400 million by 1900. The last major famine recorded in Western Europe, the Irish Potato Famine, caused death and mass emigration of millions of Irish people. In the 19th century, 70 million people left Europe in migrations to various European colonies abroad and to the United States. Demographic growth meant that, by 1900, Europe's share of the world's population was 25%.




Two world wars and an economic depression dominated the first half of the 20th century. World War I was fought between 1914 and 1918. It started when Archduke Franz Ferdinand of Austria was assassinated by the Yugoslav nationalist Gavrilo Princip. Most European nations were drawn into the war, which was fought between the Entente Powers (France, Belgium, Serbia, Portugal, Russia, the United Kingdom, and later Italy, Greece, Romania, and the United States) and the Central Powers (Austria-Hungary, Germany, Bulgaria, and the Ottoman Empire). The war left more than 16 million civilians and military dead. Over 60 million European soldiers were mobilised from 1914 to 1918.

Russia was plunged into the Russian Revolution, which threw down the Tsarist monarchy and replaced it with the communist Soviet Union. Austria-Hungary and the Ottoman Empire collapsed and broke up into separate nations, and many other nations had their borders redrawn. The Treaty of Versailles, which officially ended World War I in 1919, was harsh towards Germany, upon whom it placed full responsibility for the war and imposed heavy sanctions.
Excess deaths in Russia over the course of World War I and the Russian Civil War (including the postwar famine) amounted to a combined total of 18 million. In 1932–1933, under Stalin's leadership, confiscations of grain by the Soviet authorities contributed to the second Soviet famine which caused millions of deaths; surviving kulaks were persecuted and many sent to Gulags to do forced labour. Stalin was also responsible for the Great Purge of 1937–38 in which the NKVD executed 681,692 people; millions of people were deported and exiled to remote areas of the Soviet Union.
Economic instability, caused in part by debts incurred in the First World War and 'loans' to Germany played havoc in Europe in the late 1920s and 1930s. This and the Wall Street Crash of 1929 brought about the worldwide Great Depression. Helped by the economic crisis, social instability and the threat of communism, fascist movements developed throughout Europe placing Adolf Hitler of Nazi Germany, Francisco Franco of Spain and Benito Mussolini of Italy in power.
In 1933, Hitler became the leader of Germany and began to work towards his goal of building Greater Germany. Germany re-expanded and took back the Saarland and Rhineland in 1935 and 1936. In 1938, Austria became a part of Germany following the Anschluss. Later that year, following the Munich Agreement signed by Germany, France, the United Kingdom and Italy, Germany annexed the Sudetenland, which was a part of Czechoslovakia inhabited by ethnic Germans, and in early 1939, the remainder of Czechoslovakia was split into the Protectorate of Bohemia and Moravia, controlled by Germany, and the Slovak Republic. At the time, Britain and France preferred a policy of appeasement.

With tensions mounting between Germany and Poland over the future of Danzig, the Germans turned to the Soviets, and signed the Molotov–Ribbentrop Pact, which allowed the Soviets to invade the Baltic states and parts of Poland and Romania. Germany invaded Poland on 1 September 1939, prompting France and the United Kingdom to declare war on Germany on 3 September, opening the European Theatre of World War II. The Soviet invasion of Poland started on 17 September and Poland fell soon thereafter. On 24 September, the Soviet Union attacked the Baltic countries and later, Finland. The British hoped to land at Narvik and send troops to aid Finland, but their primary objective in the landing was to encircle Germany and cut the Germans off from Scandinavian resources. Around the same time, Germany moved troops into Denmark. The Phoney War continued.
In May 1940, Germany attacked France through the Low Countries. France capitulated in June 1940. By August Germany began a bombing offensive on Britain, but failed to convince the Britons to give up. In 1941, Germany invaded the Soviet Union in the Operation Barbarossa. On 7 December 1941 Japan's attack on Pearl Harbor drew the United States into the conflict as allies of the British Empire and other allied forces.

After the staggering Battle of Stalingrad in 1943, the German offensive in the Soviet Union turned into a continual fallback. The Battle of Kursk, which involved the largest tank battle in history, was the last major German offensive on the Eastern Front. In 1944, British and American forces invaded France in the D-Day landings, opening a new front against Germany. Berlin finally fell in 1945, ending World War II in Europe. The war was the largest and most destructive in human history, with 60 million dead across the world. More than 40 million people in Europe had died as a result of World War II, including between 11 and 17 million people who perished during the Holocaust. The Soviet Union lost around 27 million people (mostly civilians) during the war, about half of all World War II casualties. By the end of World War II, Europe had more than 40 million refugees. Several post-war expulsions in Central and Eastern Europe displaced a total of about 20 million people.

World War I and especially World War II diminished the eminence of Western Europe in world affairs. After World War II the map of Europe was redrawn at the Yalta Conference and divided into two blocs, the Western countries and the communist Eastern bloc, separated by what was later called by Winston Churchill an "Iron Curtain". The United States and Western Europe established the NATO alliance and later the Soviet Union and Central Europe established the Warsaw Pact.
The two new superpowers, the United States and the Soviet Union, became locked in a fifty-year-long Cold War, centred on nuclear proliferation. At the same time decolonisation, which had already started after World War I, gradually resulted in the independence of most of the European colonies in Asia and Africa. In the 1980s the reforms of Mikhail Gorbachev and the Solidarity movement in Poland accelerated the collapse of the Eastern bloc and the end of the Cold War. Germany was reunited, after the symbolic fall of the Berlin Wall in 1989, and the maps of Central and Eastern Europe were redrawn once more.

European integration also grew after World War II. The Treaty of Rome in 1957 established the European Economic Community between six Western European states with the goal of a unified economic policy and common market. In 1967 the EEC, European Coal and Steel Community and Euratom formed the European Community, which in 1993 became the European Union. The EU established a parliament, court and central bank and introduced the euro as a unified currency. In 2004 and 2007, more Central and Eastern European countries began joining, expanding the EU to its current size of 28 European countries, and once more making Europe a major economical and political centre of power.




Europe makes up the western fifth of the Eurasian landmass. It has a higher ratio of coast to landmass than any other continent or subcontinent. Its maritime borders consist of the Arctic Ocean to the north, the Atlantic Ocean to the west, and the Mediterranean, Black, and Caspian Seas to the south. Land relief in Europe shows great variation within relatively small areas. The southern regions are more mountainous, while moving north the terrain descends from the high Alps, Pyrenees, and Carpathians, through hilly uplands, into broad, low northern plains, which are vast in the east. This extended lowland is known as the Great European Plain, and at its heart lies the North German Plain. An arc of uplands also exists along the north-western seaboard, which begins in the western parts of the islands of Britain and Ireland, and then continues along the mountainous, fjord-cut spine of Norway.
This description is simplified. Sub-regions such as the Iberian Peninsula and the Italian Peninsula contain their own complex features, as does mainland Central Europe itself, where the relief contains many plateaus, river valleys and basins that complicate the general trend. Sub-regions like Iceland, Britain, and Ireland are special cases. The former is a land unto itself in the northern ocean which is counted as part of Europe, while the latter are upland areas that were once joined to the mainland until rising sea levels cut them off.




Europe lies mainly in the temperate climate zones, being subjected to prevailing westerlies. The climate is milder in comparison to other areas of the same latitude around the globe due to the influence of the Gulf Stream. The Gulf Stream is nicknamed "Europe's central heating", because it makes Europe's climate warmer and wetter than it would otherwise be. The Gulf Stream not only carries warm water to Europe's coast but also warms up the prevailing westerly winds that blow across the continent from the Atlantic Ocean.
Therefore, the average temperature throughout the year of Naples is 16 °C (61 °F), while it is only 12 °C (54 °F) in New York City which is almost on the same latitude. Berlin, Germany; Calgary, Canada; and Irkutsk, in the Asian part of Russia, lie on around the same latitude; January temperatures in Berlin average around 8 °C (14 °F) higher than those in Calgary, and they are almost 22 °C (40 °F) higher than average temperatures in Irkutsk. Similarly, northern parts of Scotland have a tempertate marine climate. The yearly average temperature in city of Inverness is 9.05 °C (48.29 °F). However, Churchill, Manitoba, Canada, is on roughly the same latitude and has an average temperature of −6.5 °C (20.3 °F), giving it a nearly subarctic climate.




The geological history of Europe traces back to the formation of the Baltic Shield (Fennoscandia) and the Sarmatian craton, both around 2.25 billion years ago, followed by the Volgo–Uralia shield, the three together leading to the East European craton (≈ Baltica) which became a part of the supercontinent Columbia. Around 1.1 billion years ago, Baltica and Arctica (as part of the Laurentia block) became joined to Rodinia, later resplitting around 550 million years ago to reform as Baltica. Around 440 million years ago Euramerica was formed from Baltica and Laurentia; a further joining with Gondwana then leading to the formation of Pangea. Around 190 million years ago, Gondwana and Laurasia split apart due to the widening of the Atlantic Ocean. Finally, and very soon afterwards, Laurasia itself split up again, into Laurentia (North America) and the Eurasian continent. The land connection between the two persisted for a considerable time, via Greenland, leading to interchange of animal species. From around 50 million years ago, rising and falling sea levels have determined the actual shape of Europe, and its connections with continents such as Asia. Europe's present shape dates to the late Tertiary period about five million years ago.

The geology of Europe is hugely varied and complex, and gives rise to the wide variety of landscapes found across the continent, from the Scottish Highlands to the rolling plains of Hungary. Europe's most significant feature is the dichotomy between highland and mountainous Southern Europe and a vast, partially underwater, northern plain ranging from Ireland in the west to the Ural Mountains in the east. These two halves are separated by the mountain chains of the Pyrenees and Alps/Carpathians. The northern plains are delimited in the west by the Scandinavian Mountains and the mountainous parts of the British Isles. Major shallow water bodies submerging parts of the northern plains are the Celtic Sea, the North Sea, the Baltic Sea complex and Barents Sea.
The northern plain contains the old geological continent of Baltica, and so may be regarded geologically as the "main continent", while peripheral highlands and mountainous regions in the south and west constitute fragments from various other geological continents. Most of the older geology of western Europe existed as part of the ancient microcontinent Avalonia.



Having lived side-by-side with agricultural peoples for millennia, Europe's animals and plants have been profoundly affected by the presence and activities of man. With the exception of Fennoscandia and northern Russia, few areas of untouched wilderness are currently found in Europe, except for various national parks.

The main natural vegetation cover in Europe is mixed forest. The conditions for growth are very favourable. In the north, the Gulf Stream and North Atlantic Drift warm the continent. Southern Europe could be described as having a warm, but mild climate. There are frequent summer droughts in this region. Mountain ridges also affect the conditions. Some of these (Alps, Pyrenees) are oriented east-west and allow the wind to carry large masses of water from the ocean in the interior. Others are oriented south-north (Scandinavian Mountains, Dinarides, Carpathians, Apennines) and because the rain falls primarily on the side of mountains that is oriented towards the sea, forests grow well on this side, while on the other side, the conditions are much less favourable. Few corners of mainland Europe have not been grazed by livestock at some point in time, and the cutting down of the pre-agricultural forest habitat caused disruption to the original plant and animal ecosystems.

Probably 80 to 90 percent of Europe was once covered by forest. It stretched from the Mediterranean Sea to the Arctic Ocean. Though over half of Europe's original forests disappeared through the centuries of deforestation, Europe still has over one quarter of its land area as forest, such as the broadleaf and mixed forests, taiga of Scandinavia and Russia, mixed rainforests of the Caucasus and the Cork oak forests in the western Mediterranean. During recent times, deforestation has been slowed and many trees have been planted. However, in many cases monoculture plantations of conifers have replaced the original mixed natural forest, because these grow quicker. The plantations now cover vast areas of land, but offer poorer habitats for many European forest dwelling species which require a mixture of tree species and diverse forest structure. The amount of natural forest in Western Europe is just 2–3% or less, in European Russia 5–10%. The country with the smallest percentage of forested area is Iceland (1%), while the most forested country is Finland (77%).
In temperate Europe, mixed forest with both broadleaf and coniferous trees dominate. The most important species in central and western Europe are beech and oak. In the north, the taiga is a mixed spruce–pine–birch forest; further north within Russia and extreme northern Scandinavia, the taiga gives way to tundra as the Arctic is approached. In the Mediterranean, many olive trees have been planted, which are very well adapted to its arid climate; Mediterranean Cypress is also widely planted in southern Europe. The semi-arid Mediterranean region hosts much scrub forest. A narrow east-west tongue of Eurasian grassland (the steppe) extends eastwards from Ukraine and southern Russia and ends in Hungary and traverses into taiga to the north.




Glaciation during the most recent ice age and the presence of man affected the distribution of European fauna. As for the animals, in many parts of Europe most large animals and top predator species have been hunted to extinction. The woolly mammoth was extinct before the end of the Neolithic period. Today wolves (carnivores) and bears (omnivores) are endangered. Once they were found in most parts of Europe. However, deforestation and hunting caused these animals to withdraw further and further. By the Middle Ages the bears' habitats were limited to more or less inaccessible mountains with sufficient forest cover. Today, the brown bear lives primarily in the Balkan peninsula, Scandinavia, and Russia; a small number also persist in other countries across Europe (Austria, Pyrenees etc.), but in these areas brown bear populations are fragmented and marginalised because of the destruction of their habitat. In addition, polar bears may be found on Svalbard, a Norwegian archipelago far north of Scandinavia. The wolf, the second largest predator in Europe after the brown bear, can be found primarily in Central and Eastern Europe and in the Balkans, with a handful of packs in pockets of Western Europe (Scandinavia, Spain, etc.).

European wild cat, foxes (especially the red fox), jackal and different species of martens, hedgehogs, different species of reptiles (like snakes such as vipers and grass snakes) and amphibians, different birds (owls, hawks and other birds of prey).
Important European herbivores are snails, larvae, fish, different birds, and mammals, like rodents, deer and roe deer, boars, and living in the mountains, marmots, steinbocks, chamois among others. A number of insects, such as the small tortoiseshell butterfly, add to the biodiversity.
The extinction of the dwarf hippos and dwarf elephants has been linked to the earliest arrival of humans on the islands of the Mediterranean.
Sea creatures are also an important part of European flora and fauna. The sea flora is mainly phytoplankton. Important animals that live in European seas are zooplankton, molluscs, echinoderms, different crustaceans, squids and octopuses, fish, dolphins, and whales.
Biodiversity is protected in Europe through the Council of Europe's Bern Convention, which has also been signed by the European Community as well as non-European states.




The political map of Europe is substantially derived from the re-organisation of Europe following the Napoleonic Wars in 1815. The prevalent form of government in Europe is parliamentary democracy, in most cases in the form of Republic; in 1815, the prevalent form of government was still the Monarchy. Europe's remaining eleven monarchies are constitutional.
European integration is the process of political, legal, economic (and in some cases social and cultural) integration of European states as it has been pursued by the powers sponsoring the Council of Europe since the end of World War II The European Union has been the focus of economic integration on the continent since its foundation in 1993. More recently, the Eurasian Economic Union has been established as a counterpart comprising former Soviet states.
28 European states are members of the politico-economic European Union, 26 of the border-free Schengen Area and 19 of the monetary union Eurozone. Among the smaller European organizations are the Nordic Council, the Benelux, the Baltic Assembly and the Visegrád Group.




The list below includes all entities falling even partially under any of the various common definitions of Europe, geographic or political.
Within the above-mentioned states are several de facto independent countries with limited to no international recognition. None of them are members of the UN:
Several dependencies and similar territories with broad autonomy are also found within or in close proximity to Europe. This includes Åland (a region of Finland), two constituent countries of the Kingdom Denmark (other than Denmark itself), three Crown dependencies, and two British Overseas Territories. Svalbard is also included due to its unique status within Norway, although it is not autonomous. Not included are the three countries of the United Kingdom with devolved powers and the two Autonomous Regions of Portugal, which despite having a unique degree of autonomy, are not largely self-governing in matters other than international affairs. Areas with little more than a unique tax status, such as Heligoland and the Canary Islands, are also not included for this reason.




As a continent, the economy of Europe is currently the largest on Earth and it is the richest region as measured by assets under management with over $32.7 trillion compared to North America's $27.1 trillion in 2008. In 2009 Europe remained the wealthiest region. Its $37.1 trillion in assets under management represented one-third of the world's wealth. It was one of several regions where wealth surpassed its precrisis year-end peak. As with other continents, Europe has a large variation of wealth among its countries. The richer states tend to be in the West; some of the Central and Eastern European economies are still emerging from the collapse of the Soviet Union and the breakup of Yugoslavia.
The European Union, a political entity composed of 28 European states, comprises the largest single economic area in the world. 18 EU countries share the euro as a common currency. Five European countries rank in the top ten of the world's largest national economies in GDP (PPP). This includes (ranks according to the CIA): Germany (5), the UK (6), Russia (7), France (8), and Italy (10).
There is huge disparity between many European countries in terms of their income. The richest in terms of GDP per capita is Monaco with its US$172,676 per capita (2009) and the poorest is Moldova with its GDP per capita of US$1,631 (2010). Monaco is the richest country in terms of GDP per capita in the world according to the World Bank report.



Industrial growth (1760–1945)
Capitalism has been dominant in the Western world since the end of feudalism. From Britain, it gradually spread throughout Europe. The Industrial Revolution started in Europe, specifically the United Kingdom in the late 18th century, and the 19th century saw Western Europe industrialise. Economies were disrupted by World War I but by the beginning of World War II they had recovered and were having to compete with the growing economic strength of the United States. World War II, again, damaged much of Europe's industries.
Cold War (1945–1991)

After World War II the economy of the UK was in a state of ruin, and continued to suffer relative economic decline in the following decades. Italy was also in a poor economic condition but regained a high level of growth by the 1950s. West Germany recovered quickly and had doubled production from pre-war levels by the 1950s. France also staged a remarkable comeback enjoying rapid growth and modernisation; later on Spain, under the leadership of Franco, also recovered, and the nation recorded huge unprecedented economic growth beginning in the 1960s in what is called the Spanish miracle. The majority of Central and Eastern European states came under the control of the Soviet Union and thus were members of the Council for Mutual Economic Assistance (COMECON).
The states which retained a free-market system were given a large amount of aid by the United States under the Marshall Plan.  The western states moved to link their economies together, providing the basis for the EU and increasing cross border trade. This helped them to enjoy rapidly improving economies, while those states in COMECON were struggling in a large part due to the cost of the Cold War. Until 1990, the European Community was expanded from 6 founding members to 12. The emphasis placed on resurrecting the West German economy led to it overtaking the UK as Europe's largest economy.
Reunification (1991–)

With the fall of communism in Central and Eastern Europe in 1991, the post-socialist states began free market reforms: Poland, Hungary, and Slovenia adopted them reasonably quickly, while Ukraine and Russia are still in the process of doing so.
After East and West Germany were reunited in 1990, the economy of West Germany struggled as it had to support and largely rebuild the infrastructure of East Germany.
By the millennium change, the EU dominated the economy of Europe comprising the five largest European economies of the time namely Germany, the United Kingdom, France, Italy, and Spain. In 1999, 12 of the 15 members of the EU joined the Eurozone replacing their former national currencies by the common euro. The three who chose to remain outside the Eurozone were: the United Kingdom, Denmark, and Sweden. The European Union is now the largest economy in the world.
Figures released by Eurostat in 2009 confirmed that the Eurozone had gone into recession in 2008. It impacted much of the region. In 2010, fears of a sovereign debt crisis developed concerning some countries in Europe, especially Greece, Ireland, Spain, and Portugal. As a result, measures were taken, especially for Greece, by the leading countries of the Eurozone. The EU-27 unemployment rate was 10.3% in 2012. For those aged 15–24 it was 22.4%.




In 2005, the population of Europe was estimated to be 731 million according to the United Nations, which is slightly more than one-ninth of the world's population. A century ago, Europe had nearly a quarter of the world's population. The population of Europe has grown in the past century, but in other areas of the world (in particular Africa and Asia) the population has grown far more quickly. Among the continents, Europe has a relatively high population density, second only to Asia. The most densely populated country in Europe (and in the world) is the microstate of Monaco.




Pan and Pfeil (2004) count 87 distinct "peoples of Europe", of which 33 form the majority population in at least one sovereign state, while the remaining 54 constitute ethnic minorities. According to UN population projection, Europe's population may fall to about 7% of world population by 2050, or 653 million people (medium variant, 556 to 777 million in low and high variants, respectively). Within this context, significant disparities exist between regions in relation to fertility rates. The average number of children per female of child bearing age is 1.52. According to some sources, this rate is higher among Muslims in Europe. The UN predicts a steady population decline in Central and Eastern Europe as a result of emigration and low birth rates.




Europe is home to the highest number of migrants of all global regions at 70.6 million people, the IOM's report said. In 2005, the EU had an overall net gain from immigration of 1.8 million people. This accounted for almost 85% of Europe's total population growth. The European Union plans to open the job centres for legal migrant workers from Africa. In 2008, 696,000 persons were given citizenship of an EU27 member state, a decrease from 707,000 the previous year.
Emigration from Europe began with Spanish and Portuguese settlers in the 16th century, and French and English settlers in the 17th century. But numbers remained relatively small until waves of mass emigration in the 19th century, when millions of poor families left Europe.
Today, large populations of European descent are found on every continent. European ancestry predominates in North America, and to a lesser degree in South America (particularly in Uruguay, Argentina, Chile and Brazil, while most of the other Latin American countries also have a considerable population of European origins). Australia and New Zealand have large European derived populations. Africa has no countries with European-derived majorities (or with the exception of Cape Verde and probably São Tomé and Príncipe, depending on context), but there are significant minorities, such as the White South Africans. In Asia, European-derived populations predominate in Northern Asia (specifically Russians), some parts of Northern Kazakhstan and Israel.




European languages mostly fall within three Indo-European language groups: the Romance languages, derived from the Latin of the Roman Empire; the Germanic languages, whose ancestor language came from southern Scandinavia; and the Slavic languages.
Slavic languages are most spoken by the number of native speakers in Europe, they are spoken in Central, Eastern, and Southeastern Europe. Romance languages are spoken primarily in south-western Europe as well as in Romania and Moldova, in Central or Eastern Europe. Germanic languages are spoken in Northern Europe, the British Isles and some parts of Central Europe.
Many other languages outside the three main groups exist in Europe. Other Indo-European languages include the Baltic group (that is, Latvian and Lithuanian), the Celtic group (that is, Irish, Scottish Gaelic, Manx, Welsh, Cornish, and Breton), Greek, Armenian, and Albanian. In addition, a distinct non-Indo-European family of Uralic languages (Estonian, Finnish, and Hungarian) is spoken mainly in Estonia, Finland, and Hungary, while Kartvelian languages (Georgian, Mingrelian, and Svan), are spoken primarily in Georgia, and two other language families reside in the North Caucasus (termed Northeast Caucasian, most notably including Chechen, Avar and Lezgin and Northwest Caucasian, notably including Adyghe). Maltese is the only Semitic language that is official within the EU, while Basque is the only European language isolate. Turkic languages include Azerbaijani and Turkish, in addition to the languages of minority nations in Russia.
Multilingualism and the protection of regional and minority languages are recognised political goals in Europe today. The Council of Europe Framework Convention for the Protection of National Minorities and the Council of Europe's European Charter for Regional or Minority Languages set up a legal framework for language rights in Europe.




"Europe" as a cultural concept is substantially derived from the shared heritage of the Roman Empire and its culture. The boundaries of Europe were historically understood as those of Christendom (or more specifically Latin Christendom), as established or defended throughout the medieval and early modern history of Europe, especially against Islam, as in the Reconquista and the Ottoman wars in Europe.
This shared cultural heritage is combined by overlapping indigenous national cultures and folklores, roughly divided into Slavic, Latin (Romance) and Germanic, but with several components not part of either of these group (notably Greek and Celtic). Cultural contact and mixtures characterise much of European regional cultures; Kaplan (2014) describes Europe as "embracing maximum cultural diversity at minimal geographical distances".
More recently, traditional culture has been overlaid by modern and postmodern innovation and counterculture.




Historically, religion in Europe has been a major influence on European art, culture, philosophy and law.
The largest religion in Europe is Christianity, with 76.2% of Europeans considering themselves Christians, including Catholic, Eastern Orthodox and various Protestant denominations. Among Protestants, the most popular are historically state-supported European denominations such as Lutheranism, Anglicanism and the Reformed faith. Other Protestant denominations such as historically significant ones like Anabaptists were never supported by any state and thus are not so widespread, as well as these newly arriving from the United States such as Pentecostalism, Adventism, Methodism, Baptists and various Evangelical Protestants; although Methodism and Baptists both have European origins. The notion of "Europe" and the "Western World" has been intimately connected with the concept of "Christianity and Christendom"; many even attribute Christianity for being the link that created a unified European identity.

Christianity, including the Roman Catholic Church, has played a prominent role in the shaping of Western civilization since at least the 4th century, and for at least a millennium and a half, Europe has been nearly equivalent to Christian culture, even though the religion was inherited from the Middle East. Christian culture was the predominant force in western civilization, guiding the course of philosophy, art, and science.
The second most popular religion is Islam (6%) concentrated mainly in the Balkans and eastern Europe (Bosnia and Herzegovina, Albania, Kosovo, Kazakhstan, North Cyprus, Turkey, Azerbaijan, North Caucasus, and the Volga-Ural region). Other religions, including Judaism, Hinduism, and Buddhism are minority religions (though Tibetan Buddhism is the majority religion of Russia's Republic of Kalmykia). The 20th century saw the revival of Neopaganism through movements such as Wicca and Druidry.
Europe has become a relatively secular continent, with an increasing number and proportion of irreligious, atheist and agnostic people, who make up about 18.2% of Europe's population, actually the largest secular population in the Western world. There are a particularly high number of self-described non-religious people in the Czech Republic, Estonia, Sweden, former East Germany, and France.












National Geographic Society (2005). National Geographic Visual History of the World. Washington, D.C.: National Geographic Society. ISBN 0-7922-3695-5.
Bulliet, Richard; Crossley, Pamela; Headrick, Daniel; Hirsch, Steven; Johnson, Lyman (2011). The Earth and Its Peoples, Brief Edition. 1. Cengage Learning. ISBN 978-0495913115. 
Brown, Stephen F.; Anatolios, Khaled; Palmer, Martin (2009). O'Brien, Joanne, ed. Catholicism & Orthodox Christianity. Infobase Publishing. ISBN 978-1604131062. 




Council of Europe
European Union
The Columbia Gazetteer of the World Online Columbia University Press
"Introducing Europe" from Lonely Planet Travel Guides and Information
Historical Maps
Borders in Europe 3000BC to the present Geacron Historical atlas
Online history of Europe in 21 mapsSouth America is a continent located in the western hemisphere, mostly in the southern hemisphere, with a relatively small portion in the northern hemisphere. It is also considered a subcontinent of the Americas, which is the model used in nations that speak Romance languages. The reference to South America instead of other regions (like Latin America or the Southern Cone) has increased in the last decades due to changing geopolitical dynamics (in particular, the rise of Brazil).
It is bordered on the west by the Pacific Ocean and on the north and east by the Atlantic Ocean; North America and the Caribbean Sea lie to the northwest. It includes twelve sovereign states (Argentina, Bolivia, Brazil, Chile, Colombia, Ecuador, Guyana, Paraguay, Peru, Suriname, Uruguay, and Venezuela), a part of France (French Guiana), and a non-sovereign area (the Falkland Islands, a British Overseas Territory though this is disputed by Argentina). In addition to this, the ABC islands of the Kingdom of the Netherlands, Trinidad and Tobago, and Panama may also be considered part of South America.
South America has an area of 17,840,000 square kilometers (6,890,000 sq mi). Its population as of 2005 has been estimated at more than 371,090,000. South America ranks fourth in area (after Asia, Africa, and North America) and fifth in population (after Asia, Africa, Europe, and North America). Brazil is by far the most populous South American country, with more than half of the continent's population, followed by Colombia, Argentina, Venezuela and Peru. In recent decades Brazil has also concentrated half of the region's GDP and has become a first regional power.
Most of the population lives near the continent's western or eastern coasts while the interior and the far south are sparsely populated. The geography of western South America is dominated by the Andes mountains; in contrast, the eastern part contains both highland regions and large lowlands where rivers such as the Amazon, Orinoco, and Paraná flow. Most of the continent lies in the tropics.
The continent's cultural and ethnic outlook has its origin with the interaction of indigenous peoples with European conquerors and immigrants and, more locally, with African slaves. Given a long history of colonialism, the overwhelming majority of South Americans speak Portuguese or Spanish, and societies and states commonly reflect Western traditions.




South America occupies the southern portion of the Americas. The continent is generally delimited on the northwest by the Darién watershed along the Colombia–Panama border, although some may consider the border instead to be the Panama Canal. Geopolitically and geographically all of Panama – including the segment east of the Panama Canal in the isthmus – is typically included in North America alone and among the countries of Central America. Almost all of mainland South America sits on the South American Plate.
South America is home to the world's highest uninterrupted waterfall, Angel Falls in Venezuela; the highest single drop waterfall Kaieteur Falls in Guyana; the largest river (by volume), the Amazon River; the longest mountain range, the Andes (whose highest mountain is Aconcagua at 6,962 m [22,841 ft]); the driest non-polar place on earth, the Atacama Desert; the largest rainforest, the Amazon Rainforest; the highest capital city, La Paz, Bolivia; the highest commercially navigable lake in the world, Lake Titicaca; and, excluding research stations in Antarctica, the world's southernmost permanently inhabited community, Puerto Toro, Chile.

South America's major mineral resources are gold, silver, copper, iron ore, tin, and petroleum. These resources found in South America have brought high income to its countries especially in times of war or of rapid economic growth by industrialized countries elsewhere. However, the concentration in producing one major export commodity often has hindered the development of diversified economies. The fluctuation in the price of commodities in the international markets has led historically to major highs and lows in the economies of South American states, often causing extreme political instability. This is leading to efforts to diversify production to drive away from staying as economies dedicated to one major export.
South America is one of the most biodiverse continents on earth. South America is home to many interesting and unique species of animals including the llama, anaconda, piranha, jaguar, vicuña, and tapir. The Amazon rainforests possess high biodiversity, containing a major proportion of the Earth's species.
Brazil is the largest country in South America, encompassing around half of the continent's land area and population. The remaining countries and territories are divided among three regions: The Andean States, the Guianas and the Southern Cone.



Traditionally, South America also includes some of the nearby islands. Aruba, Bonaire, Curaçao, Trinidad, Tobago, and the federal dependencies of Venezuela sit on the northerly South American continental shelf and are often considered part of the continent. Geo-politically, the island states and overseas territories of the Caribbean are generally grouped as a part or subregion of North America, since they are more distant on the Caribbean Plate, even though San Andres and Providencia are politically part of Colombia and Aves Island is controlled by Venezuela.
Other islands that are included with South America are the Galápagos Islands that belong to Ecuador and Easter Island (in Oceania but belonging to Chile), Robinson Crusoe Island, Chiloé (both Chilean) and Tierra del Fuego (split between Chile and Argentina). In the Atlantic, Brazil owns Fernando de Noronha, Trindade and Martim Vaz, and the Saint Peter and Saint Paul Archipelago, while the Falkland Islands are governed by the United Kingdom, whose sovereignty over the islands is disputed by Argentina. South Georgia and the South Sandwich Islands may be associated with either South America or Antarctica.







South America is believed to have been joined with Africa from the late Paleozoic Era to the early Mesozoic Era, until the supercontinent Pangaea began to rift and break apart about 225 million years ago. Therefore, South America and Africa share similar fossils and rock layers.
South America is thought to have been first inhabited by humans when people were crossing the Bering Land Bridge (now the Bering Strait) at least 15,000 years ago from the territory that is present-day Russia. They migrated south through North America, and eventually reached South America through the Isthmus of Panama.
The first evidence for the existence of the human race in South America dates back to about 9000 BC, when squashes, chili peppers and beans began to be cultivated for food in the highlands of the Amazon Basin. Pottery evidence further suggests that manioc, which remains a staple food today, was being cultivated as early as 2000 BC.
By 2000 BC, many agrarian communities had been settled throughout the Andes and the surrounding regions. Fishing became a widespread practice along the coast, helping establish fish as a primary source of food. Irrigation systems were also developed at this time, which aided in the rise of an agrarian society.
South American cultures began domesticating llamas, vicuñas, guanacos, and alpacas in the highlands of the Andes circa 3500 BC. Besides their use as sources of meat and wool, these animals were used for transportation of goods.




The rise of plant growing and the subsequent appearance of permanent human settlements allowed for the multiple and overlapping beginnings of civilizations in South America.
One of the earliest known South American civilizations was at Norte Chico, on the central Peruvian coast. Though a pre-ceramic culture, the monumental architecture of Norte Chico is contemporaneous with the pyramids of Ancient Egypt. Norte Chico governing class established a trade network and developed agriculture then followed by Chavín by 900 BC, according to some estimates and archaeological finds. Artifacts were found at a site called Chavín de Huantar in modern Peru at an elevation of 3,177 meters. Chavín civilization spanned 900 BC to 300 BC.
In the central coast of Peru, around the beginning of the 1st millennium AD, Moche (100 BC – 700 AD, at the northern coast of Peru), Paracas and Nazca (400 BC – 800 AD, Peru) cultures flourished with centralized states with permanent militia improving agriculture through irrigation and new styles of ceramic art. At the Altiplano, Tiahuanaco or Tiwanaku (100 BC – 1200 AD, Bolivia) managed a large commercial network based on religion.
Around 7th century, both Tiahuanaco and Wari or Huari Empire (600–1200, Central and northern Peru) expanded its influence to all the Andean region, imposing the Huari urbanism and tiahuanaco religious iconography.
The Muisca were the main indigenous civilization in what is now Colombia. They established the Muisca Confederation of many clans, or cacicazgos, that had a free trade network among themselves. They were goldsmiths and farmers.
Other important Pre-Columbian cultures include: the Cañaris (in south central Ecuador), Chimú Empire (1300–1470, Peruvian northern coast), Chachapoyas, and the Aymaran kingdoms (1000–1450, Western Bolivia and southern Peru).
Holding their capital at the great city of Cusco, the Inca civilization dominated the Andes region from 1438 to 1533. Known as Tawantin suyu, and "the land of the four regions," in Quechua, the Inca civilization was highly distinct and developed. Inca rule extended to nearly a hundred linguistic or ethnic communities, some 9 to 14 million people connected by a 25,000 kilometer road system. Cities were built with precise, unmatched stonework, constructed over many levels of mountain terrain. Terrace farming was a useful form of agriculture.
The Mapuche in Central and Southern Chile resisted the European and Chilean settlers, waging the Arauco War for more than 300 years.




In 1494, Portugal and Spain, the two great maritime European powers of that time, on the expectation of new lands being discovered in the west, signed the Treaty of Tordesillas, by which they agreed, with the support of the Pope, that all the land outside Europe should be an exclusive duopoly between the two countries.
The treaty established an imaginary line along a north-south meridian 370 leagues west of the Cape Verde Islands, roughly 46° 37' W. In terms of the treaty, all land to the west of the line (known to comprise most of the South American soil) would belong to Spain, and all land to the east, to Portugal. As accurate measurements of longitude were impossible at that time, the line was not strictly enforced, resulting in a Portuguese expansion of Brazil across the meridian.
Beginning in the 1530s, the people and natural resources of South America were repeatedly exploited by foreign conquistadors, first from Spain and later from Portugal. These competing colonial nations claimed the land and resources as their own and divided it in colonies.
European infectious diseases (smallpox, influenza, measles, and typhus) – to which the native populations had no immune resistance – caused large-scale depopulation of the native population under Spanish control. Systems of forced labor, such as the haciendas and mining industry's mit'a also contributed to the depopulation. After this, African slaves, who had developed immunities to these diseases, were quickly brought in to replace them.
The Spaniards were committed to convert their native subjects to Christianity and were quick to purge any native cultural practices that hindered this end; however, many initial attempts at this were only partially successful, as native groups simply blended Catholicism with their established beliefs and practices. Furthermore, the Spaniards brought their language to the degree they did with their religion, although the Roman Catholic Church's evangelization in Quechua, Aymara, and Guaraní actually contributed to the continuous use of these native languages albeit only in the oral form.
Eventually, the natives and the Spaniards interbred, forming a mestizo class. At the beginning, many mestizos of the Andean region were offspring of Amerindian mothers and Spanish fathers. After independence, most mestizos had native fathers and European or mestizo mothers.
Many native artworks were considered pagan idols and destroyed by Spanish explorers; this included many gold and silver sculptures and other artifacts found in South America, which were melted down before their transport to Spain or Portugal. Spaniards and Portuguese brought the western European architectural style to the continent, and helped to improve infrastructures like bridges, roads, and the sewer system of the cities they discovered or conquered. They also significantly increased economic and trade relations, not just between the old and new world but between the different South American regions and peoples. Finally, with the expansion of the Portuguese and Spanish languages, many cultures that were previously separated became united through that of Latin American.
Guyana was first a Dutch, and then a British colony, though there was a brief period during the Napoleonic Wars when it was colonized by the French. The country was once partitioned into three parts, each being controlled by one of the colonial powers until the country was finally taken over fully by the British.




Indigenous peoples of the Americas in various European colonies were forced to work in European plantations and mines; along with African slaves who were also introduced in the proceeding centuries.




The European Peninsular War (1807–1814), a theater of the Napoleonic Wars, changed the political situation of both the Spanish and Portuguese colonies. First, Napoleon invaded Portugal, but the House of Braganza avoided capture by escaping to Brazil. Napoleon also captured King Ferdinand VII of Spain, and appointed his own brother instead. This appointment provoked severe popular resistance, which created Juntas to rule in the name of the captured king.
Many cities in the Spanish colonies, however, considered themselves equally authorized to appoint local Juntas like those of Spain. This began the Spanish American wars of independence between the patriots, who promoted such autonomy, and the royalists, who supported Spanish authority over the Americas. The Juntas, in both Spain and the Americas, promoted the ideas of the Enlightenment. Five years after the beginning of the war, Ferdinand VII returned to the throne and began the Absolutist Restoration as the royalists got the upper hand in the conflict.
The independence of South America was secured by Simón Bolívar (Venezuela) and José de San Martín (Argentina), the two most important Libertadores. Bolívar led a great uprising in the north, then led his army southward towards Lima, the capital of the Viceroyalty of Peru. Meanwhile, San Martín led an army across the Andes Mountains, along with Chilean expatriates, and liberated Chile. He organized a fleet to reach Peru by sea, and sought the military support of various rebels from the Viceroyalty of Peru. The two armies finally met in Guayaquil, Ecuador, where they cornered the Royal Army of the Spanish Crown and forced its surrender.
In the Portuguese Kingdom of Brazil, Dom Pedro I (also Pedro IV of Portugal), son of the Portuguese King Dom João VI, proclaimed the independent Kingdom of Brazil in 1822, which later became the Empire of Brazil. Despite the Portuguese loyalties of garrisons in Bahia, Cisplatina and Pará, independence was diplomatically accepted by the crown in Portugal, on condition of a high compensation paid by Brazil.



The newly independent nations began a process of fragmentation, with several civil and international wars. However, it was not as strong as in Central America. Some countries created from provinces of larger countries stayed as such up to modern day (such as Paraguay or Uruguay), while others were reconquered and reincorporated into their former countries (such as the Republic of Entre Ríos and the Riograndense Republic).
The Peru–Bolivian Confederation, a short-lived union of Peru and Bolivia, was blocked by Chile in the War of the Confederation (1836–1839) and again during the War of the Pacific (1879–1883). Paraguay was virtually destroyed by Argentina and Brazil in the Paraguayan War.




Wars became less frequent in the 20th century, with Bolivia-Paraguay and Peru-Ecuador fighting the last inter-state wars. Early in the 20th century, the three wealthiest South American countries engaged in a vastly expensive naval arms race which was catalyzed by the introduction of a new warship type, the "dreadnought". At one point, the Argentine government was spending a fifth of its entire yearly budget for just two dreadnoughts, a price that did not include later in-service costs, which for the Brazilian dreadnoughts was sixty percent of the initial purchase.
The continent became a battlefield of the Cold War in the late 20th century. Some democratically elected governments of Argentina, Brazil, Chile, Uruguay and Paraguay were overthrown or displaced by military dictatorships in the 1960s and 1970s. To curtail opposition, their governments detained tens of thousands of political prisoners, many of whom were tortured and/or killed on inter-state collaboration. Economically, they began a transition to neoliberal economic policies. They placed their own actions within the US Cold War doctrine of "National Security" against internal subversion. Throughout the 1980s and 1990s, Peru suffered from an internal conflict.
Argentina and Britain fought the Falklands War in 1982.
Colombia has had an ongoing, though diminished internal conflict, which started in 1964 with the creation of Marxist guerrillas (FARC-EP) and then involved several illegal armed groups of leftist-leaning ideology as well as the private armies of powerful drug lords. Many of these are now defunct, and only a small portion of the ELN remains, along with the stronger, though also greatly reduced FARC. These leftist groups smuggle narcotics out of Colombia to fund their operations, while also using kidnapping, bombings, land mines and assassinations as weapons against both elected and non-elected citizens.

Revolutionary movements and right-wing military dictatorships became common after World War II, but since the 1980s, a wave of democratization came through the continent, and democratic rule is widespread now. Nonetheless, allegations of corruption are still very common, and several countries have developed crises which have forced the resignation of their governments, although, in most occasions, regular civilian succession has continued.
International indebtedness turned into a severe problem in late 1980s, and some countries, despite having strong democracies, have not yet developed political institutions capable of handling such crises without resorting to unorthodox economic policies, as most recently illustrated by Argentina's default in the early 21st century. The last twenty years have seen an increased push towards regional integration, with the creation of uniquely South American institutions such as the Andean Community, Mercosur and Unasur. Notably, starting with the election of Hugo Chávez in Venezuela in 1998, the region experienced what has been termed a pink tide – the election of several leftist and center-left administrations to most countries of the area, except for the Guianas and Colombia.






During the first decade of the 21st century, South American governments have drifted to the political left, with leftist leaders being elected in Chile, Uruguay, Brazil, Argentina, Ecuador, Bolivia, Paraguay, Peru and Venezuela. Most South American countries are making an increasing use of protectionist policies, helping local development.
Recently, an intergovernmental entity has been formed which aims to merge the two existing customs unions: Mercosur and the Andean Community, thus forming the third-largest trade bloc in the world. This new political organization known as Union of South American Nations seeks to establish free movement of people, economic development, a common defense policy and the elimination of tariffs.






Genetic admixture occurs at very high levels in South America. In Argentina the European influence accounts for 65%–79% of the genetic background, 17%–31% of the Amerindian and 2%–4% of sub-Saharan African. In Colombia the European genetic background varied from 46% to 79% depending on the region. In Peru European ancestries ranged from 1% to 31%, while the African contribution was only 1% to 3%. The Genographic Project determined the average Peruvian from Lima had about 28% of European ancestry, 68% of native American and 2% of sub-Saharan Africa.



Descendants of indigenous peoples, such as the Quechua and Aymara, or the Urarina of Amazonia make up the majority of the population in Bolivia (56%) and, per some sources, in Peru (44%). In Ecuador, Amerindians are a large minority that comprises two-fifths of the population. The native European population is also a significant element in most other former Portuguese colonies.
South America is also home to one of the largest populations of Africans. This group is also significantly present in Guyana, Brazil, Colombia, Suriname, French Guiana, Venezuela and Ecuador. Mestizos (mixed European and Amerindian) are the largest ethnic group in Paraguay, Venezuela, Colombia (49%) and Ecuador and the second group in Peru. East Indians form the largest ethnic group in Guyana and Suriname. Brazil followed by Peru also have the largest Japanese, Korean and Chinese communities in South America.
The demographics of Colombia include approximately 37% native European descendants, while in Peru, European descendants are the third group in importance (15%). Compared to other South American countries, the people who identify as of primarily or totally European descent, or identify their phenotype as corresponding to such group, are more of a majority in Argentina, Chile and Uruguay, and are about half of the population of Brazil. In Venezuela, according to the national census 42% of the population is primarily native Spanish, Italian and Portuguese descendants.




In many places indigenous people still practice a traditional lifestyle based on subsistence agriculture or as hunter-gatherers. There are still some uncontacted tribes residing in the Amazon Rainforest.




South America relies less on the export of both manufactured goods and natural resources than the world average; merchandise exports from the continent were 16% of GDP on an exchange rate basis, compared to 25% for the world as a whole. Brazil (the seventh largest economy in the world and the largest in South America) leads in terms of merchandise exports at $251 billion, followed by Venezuela at $93 billion, Chile at $86 billion, and Argentina at $84 billion.
The economic gap between the rich and poor in most South American nations is larger than in most other continents. The richest 10% receive over 40% of the nation's income in Bolivia, Brazil, Chile, Colombia, and Paraguay, while the poorest 20% receive 3% or less in Bolivia, Brazil, and Colombia. This wide gap can be seen in many large South American cities where makeshift shacks and slums lie in the vicinity of skyscrapers and upper-class luxury apartments; nearly one in nine in South America live on less than $2 per day (on a purchasing power parity basis).






Tourism has increasingly become a significant source of income for many South American countries. Historical relics, architectural and natural wonders, a diverse range of foods and culture, vibrant and colorful cities, and stunning landscapes attract millions of tourists every year to South America. Some of the most visited places in the region are Iguazu Falls, Recife, Olinda, Machu Picchu, the Amazon rainforest, Rio de Janeiro, São Luís, Salvador, Fortaleza, Maceió, Buenos Aires, Florianópolis, San Ignacio Miní, Isla Margarita, Natal, Lima, São Paulo, Angel Falls, Brasília, Nazca Lines, Cuzco, Belo Horizonte, Lake Titicaca, Salar de Uyuni, Jesuit Missions of Chiquitos, Los Roques archipelago, Gran Sabana, Patagonia, Tayrona National Natural Park, Santa Marta, Bogotá, Medellín, Cartagena, Perito Moreno Glacier and the Galápagos Islands.
In 2016 Brazil hosted the 2016 Summer Olympics.




South Americans are culturally influenced by their indigenous peoples, the historic connection with the Iberian Peninsula and Africa, and waves of immigrants from around the globe.
South American nations have a rich variety of music. Some of the most famous genres include vallenato and cumbia from Colombia, pasillo from Colombia and Ecuador, samba, bossa nova and música sertaneja from Brazil, and tango from Argentina and Uruguay. Also well known is the non-commercial folk genre Nueva Canción movement which was founded in Argentina and Chile and quickly spread to the rest of the Latin America. People on the Peruvian coast created the fine guitar and cajon duos or trios in the most mestizo (mixed) of South American rhythms such as the Marinera (from Lima), the Tondero (from Piura), the 19th century popular Creole Valse or Peruvian Valse, the soulful Arequipan Yaravi, and the early 20th century Paraguayan Guarania. In the late 20th century, Spanish rock emerged by young hipsters influenced by British pop and American rock. Brazil has a Portuguese-language pop rock industry as well a great variety of other music genres.
The literature of South America has attracted considerable critical and popular acclaim, especially with the Latin American Boom of the 1960s and 1970s, and the rise of authors such as Mario Vargas Llosa, Gabriel García Márquez in novels and Jorge Luis Borges and Pablo Neruda in other genres. The Brazilians Machado de Assis and João Guimarães Rosa are widely regarded as the greatest Brazilian writers.
Because of South America's broad ethnic mix, South American cuisine has African, South American Indian, Asian, and European influences. Bahia, Brazil, is especially well known for its West African–influenced cuisine. Argentines, Chileans, Uruguayans, Brazilians, Bolivians, and Venezuelans regularly consume wine. Argentina, Paraguay, Uruguay, and people in southern Chile, Bolivia and Brazil drink mate, a herb which is brewed. The Paraguayan version, terere, differs from other forms of mate in that it is served cold. Pisco is a liquor distilled from grapes in Peru and Chile. Peruvian cuisine mixes elements from Chinese, Japanese, Spanish, Italian, African, Arab, Andean, and Amazonic food.




Spanish and Portuguese are the most spoken languages in South America, with approximately 200 million speakers each. Spanish is the official language of most countries, along with other native languages in some countries. Portuguese is the official language of Brazil. Dutch is the official language of Suriname; English is the official language of Guyana, although there are at least twelve other languages spoken in the country, including Portuguese, Chinese, Hindustani and several native languages. English is also spoken in the Falkland Islands. French is the official language of French Guiana and the second language in Amapá, Brazil.
Indigenous languages of South America include Quechua in Peru, Bolivia, Ecuador, Argentina, Chile and Colombia; Wayuunaiki in northern Colombia (La Guajira) and northwestern Venezuela (Zulia); Guaraní in Paraguay and, to a much lesser extent, in Bolivia; Aymara in Bolivia, Peru, and less often in Chile; and Mapudungun is spoken in certain pockets of southern Chile and, more rarely, Argentina. At least three South American indigenous languages (Quechua, Aymara, and Guarani) are recognized along with Spanish as national languages.
Other languages found in South America include, Hindustani and Javanese in Suriname; Italian in Argentina, Brazil, Uruguay, Venezuela and Chile; and German in certain pockets of Argentina, Brazil, and Chile. German is also spoken in many regions of the southern states of Brazil, Riograndenser Hunsrückisch being the most widely spoken German dialect in the country; among other Germanic dialects, a Brazilian form of Pomeranian is also well represented and is experiencing a revival. Welsh remains spoken and written in the historic towns of Trelew and Rawson in the Argentine Patagonia. There are also small clusters of Japanese-speakers in Brazil, Colombia and Peru. Arabic speakers, often of Lebanese, Syrian, or Palestinian descent, can be found in Arab communities in Argentina, Colombia, Brazil, Venezuela and in Paraguay.




A wide range of sports are played in the continent of South America, with football being the most popular overall, while baseball is the most popular in Venezuela.
Other sports include basketball, cycling, polo, volleyball, futsal, motorsports, rugby (mostly in Argentina and Uruguay), handball, tennis, golf, field hockey and boxing.
South America hosted its first Olympic Games in Rio de Janeiro, Brazil in 2016 and will host the Youth Olympic Games in Buenos Aires, Argentina in 2018.

South America shares with Europe the supremacy over the sport of football as all winners in FIFA World Cup history and all winning teams in the FIFA Club World Cup have come from these two continents. Brazil holds the record at the FIFA World Cup with five titles in total. Argentina and Uruguay have two titles each. So far four South American nations have hosted the tournament including the first edition in Uruguay (1930). The other three were Brazil (1950, 2014), Chile (1962), and Argentina (1978).
South America is home to the longest running international football tournament; the Copa América, which has been regularly contested since 1916. Uruguay have won the Copa América a record 15 times, surpassing hosts Argentina in 2011 to reach 15 titles (they were previously equal on 14 titles each during the 2011 Copa América). The continent has produced many of the most famous and most talented players including Diego Maradona, Pelé, Alfredo Di Stéfano, Lionel Messi, Ronaldo, Ronaldinho, Rivaldo, Teófilo Cubillas, Mario Kempes, Gabriel Batistuta, César Cueto, Juan Sebastián Verón, Arsenio Erico, Alberto Spencer, Carlos Valderrama, Elias Figueroa, Marcelo Salas, Juan Arango, Neymar, and Luis Suárez.
Also, in South America, a multi-sport event, the South American Games, are held every four years. The first edition was held in La Paz in 1978 and the most recent took place in Santiago in 2014.




Americas (terminology)
Bibliography of South America
Flags of South America






^ Continent model: In some parts of the world South America is viewed as a subcontinent of the Americas (a single continent in these areas), for example Latin America, Latin Europe, and Iran. In most of the countries with English as an official language, however, it is considered a continent; see Americas (terminology).






"South America". The Columbia Gazetteer of the World Online. 2005. New York: Columbia University Press.
Latin American Network Information Database


A desert is a barren area of land where little precipitation occurs and consequently living conditions are hostile for plant and animal life. The lack of vegetation exposes the unprotected surface of the ground to the processes of denudation. About one third of the land surface of the world is arid or semi-arid. This includes much of the polar regions where little precipitation occurs and which are sometimes called polar deserts or "cold deserts". Deserts can be classified by the amount of precipitation that falls, by the temperature that prevails, by the causes of desertification or by their geographical location.
Deserts are formed by weathering processes as large variations in temperature between day and night put strains on the rocks which consequently break in pieces. Although rain seldom occurs in deserts, there are occasional downpours that can result in flash floods. Rain falling on hot rocks can cause them to shatter and the resulting fragments and rubble strewn over the desert floor is further eroded by the wind. This picks up particles of sand and dust and wafts them aloft in sand or dust storms. Wind-blown sand grains striking any solid object in their path can abrade the surface. Rocks are smoothed down, and the wind sorts sand into uniform deposits. The grains end up as level sheets of sand or are piled high in billowing sand dunes. Other deserts are flat, stony plains where all the fine material has been blown away and the surface consists of a mosaic of smooth stones. These areas are known as desert pavements and little further erosion takes place. Other desert features include rock outcrops, exposed bedrock and clays once deposited by flowing water. Temporary lakes may form and salt pans may be left when waters evaporate. There may be underground sources of water in the form of springs and seepages from aquifers. Where these are found, oases can occur.
Plants and animals living in the desert need special adaptations to survive in the harsh environment. Plants tend to be tough and wiry with small or no leaves, water-resistant cuticles and often spines to deter herbivory. Some annual plants germinate, bloom and die in the course of a few weeks after rainfall while other long-lived plants survive for years and have deep root systems able to tap underground moisture. Animals need to keep cool and find enough food and water to survive. Many are nocturnal and stay in the shade or underground during the heat of the day. They tend to be efficient at conserving water, extracting most of their needs from their food and concentrating their urine. Some animals remain in a state of dormancy for long periods, ready to become active again when the rare rains fall. They then reproduce rapidly while conditions are favorable before returning to dormancy.
People have struggled to live in deserts and the surrounding semi-arid lands for millennia. Nomads have moved their flocks and herds to wherever grazing is available and oases have provided opportunities for a more settled way of life. The cultivation of semi-arid regions encourages erosion of soil and is one of the causes of increased desertification. Desert farming is possible with the aid of irrigation and the Imperial Valley in California provides an example of how previously barren land can be made productive by the import of water from an outside source. Many trade routes have been forged across deserts, especially across the Sahara Desert, and traditionally were used by caravans of camels carrying salt, gold, ivory and other goods. Large numbers of slaves were also taken northwards across the Sahara. Some mineral extraction also takes place in deserts and the uninterrupted sunlight gives potential for the capture of large quantities of solar energy.



English desert and its Romance cognates (including Italian and Portuguese deserto, French désert and Spanish desierto) all come from the ecclesiastical Latin dēsertum (originally "an abandoned place"), a participle of dēserere, "to abandon". The correlation between aridity and sparse population is complex and dynamic, varying by culture, era, and technologies; thus the use of the word desert can cause confusion. In English before the 20th century, desert was often used in the sense of "unpopulated area", without specific reference to aridity; but today the word is most often used in its climate-science sense (an area of low precipitation). Phrases such as "desert island" and "Great American Desert" in previous centuries did not necessarily imply sand or aridity; their focus was the sparse population.



A desert is a region of land that is very dry because it receives low amounts of precipitation (usually in the form of rain but may be snow, mist or fog), often has little coverage by plants, and in which streams dry up unless they are supplied by water from outside the area. Deserts can also be described as areas where more water is lost by evapotranspiration than falls as precipitation. Deserts generally receive less than 250 mm (10 in) of precipitation each year. Semideserts are regions which receive between 250 and 500 mm (10 and 20 in) and when clad in grass, these are known as steppes.



Deserts have been defined and classified in a number of ways, generally combining total precipitation, number of days on which this falls, temperature, and humidity, and sometimes additional factors. For example, Phoenix, Arizona, receives less than 250 mm (9.8 in) of precipitation per year, and is immediately recognized as being located in a desert because of its aridity-adapted plants. The North Slope of Alaska's Brooks Range also receives less than 250 mm (9.8 in) of precipitation per year and is often classified as a cold desert. Other regions of the world have cold deserts, including areas of the Himalayas and other high-altitude areas in other parts of the world. Polar deserts cover much of the ice-free areas of the Arctic and Antarctic. A non-technical definition is that deserts are those parts of the Earth's surface that have insufficient vegetation cover to support a human population.
Potential evapotranspiration supplements the measurement of precipitation in providing a scientific measurement-based definition of a desert. The water budget of an area can be calculated using the formula P − PE ± S, wherein P is precipitation, PE is potential evapotranspiration rates and S is amount of surface storage of water. Evapotranspiration is the combination of water loss through atmospheric evaporation and through the life processes of plants. Potential evapotranspiration, then, is the amount of water that could evaporate in any given region. As an example, Tucson, Arizona receives about 300 mm (12 in) of rain per year, however about 2,500 mm (98 in) of water could evaporate over the course of a year. In other words, about eight times more water could evaporate from the region than actually falls as rain. Rates of evapotranspiration in cold regions such as Alaska are much lower because of the lack of heat to aid in the evaporation process.
Deserts are sometimes classified as "hot" or "cold", "semiarid" or "coastal". The characteristics of hot deserts include high temperatures in summer; greater evaporation than precipitation usually exacerbated by high temperatures, strong winds and lack of cloud cover; considerable variation in the occurrence of precipitation, its intensity and distribution; and low humidity. Winter temperatures vary considerably between different deserts and are often related to the location of the desert on the continental landmass and the latitude. Daily variations in temperature can be as great as 22 °C (40 °F) or more, with heat loss by radiation at night being increased by the clear skies.

Cold deserts, sometimes known as temperate deserts, occur at higher latitudes than hot deserts, and the aridity is caused by the dryness of the air. Some cold deserts are far from the ocean and others are separated by mountain ranges from the sea and in both cases there is insufficient moisture in the air to cause much precipitation. The largest of these deserts are found in Central Asia. Others occur on the eastern side of the Rocky Mountains, the eastern side of the southern Andes and in southern Australia. Polar deserts are a particular class of cold desert. The air is very cold and carries little moisture so little precipitation occurs and what does fall, usually as snow, is carried along in the often strong wind and may form blizzards, drifts and dunes similar to those caused by dust and sand in other desert regions. In Antarctica, for example, the annual precipitation is about 50 mm (2 in) on the central plateau and some ten times that amount on some major peninsulas.
Based on precipitation alone, hyperarid deserts receive less than 25 mm (1 in) of rainfall a year; they have no annual seasonal cycle of precipitation and experience twelve-month periods with no rainfall at all. Arid deserts receive between 25 and 200 mm (1 and 8 in) in a year and semiarid deserts between 200 and 500 mm (8 and 20 in). However, such factors as the temperature, humidity, rate of evaporation and evapotranspiration, and the moisture storage capacity of the ground have a marked effect on the degree of aridity and the plant and animal life that can be sustained. Rain falling in the cold season may be more effective at promoting plant growth, and defining the boundaries of deserts and the semiarid regions that surround them on the grounds of precipitation alone is problematic.
Coastal deserts are mostly found on the western edges of continental land masses in regions where cold currents approach the land or cold water upwellings rise from the ocean depths. The cool winds crossing this water pick up little moisture and the coastal regions have low temperatures and very low rainfall, the main precipitation being in the form of fog and dew. The range of temperatures on a daily and annual scale is relatively low, being 11 °C (20 °F) and 5 °C (9 °F) respectively in the Atacama Desert. Deserts of this type are often long and narrow and bounded to the east by mountain ranges. They occur in Namibia, Chile, southern California and Baja California. Other coastal deserts influenced by cold currents are found in Western Australia, the Arabian Peninsula and Horn of Africa, and the western fringes of the Sahara.
In 1961, Peveril Meigs divided desert regions on Earth into three categories according to the amount of precipitation they received. In this now widely accepted system, extremely arid lands have at least twelve consecutive months without precipitation, arid lands have less than 250 mm (10 in) of annual precipitation, and semiarid lands have a mean annual precipitation of between 250 and 500 mm (10–20 in). Both extremely arid and arid lands are considered to be deserts while semiarid lands are generally referred to as steppes when they are grasslands.

Deserts are also classified, according to their geographical location and dominant weather pattern, as trade wind, mid-latitude, rain shadow, coastal, monsoon, or polar deserts. Trade wind deserts occur either side of the horse latitudes at 30° to 35° North and South. These belts are associated with the subtropical anticyclone and the large-scale descent of dry air moving from high-altitudes toward the poles. The Sahara Desert is of this type. Mid-latitude deserts occur between 30° and 50° North and South. They are mostly in areas remote from the sea where most of the moisture has already precipitated from the prevailing winds. They include the Tengger and Sonoran Deserts. Monsoon deserts are similar. They occur in regions where large temperature differences occur between sea and land. Moist warm air rises over the land, deposits its water content and circulates back to sea. Further inland, areas receive very little precipitation. The Thar Desert near the India/Pakistan border is of this type.
In some parts of the world, deserts are created by a rain shadow effect. Orographic lift occurs as air masses rise to pass over high ground. In the process they cool and lose much of their moisture by precipitation on the windward slope of the mountain range. When they descend on the leeward side, they warm and their capacity to hold moisture increases so an area with relatively little precipitation occurs. The Taklamakan Desert is an example, lying in the rain shadow of the Himalayas and receiving less than 38 mm (1.5 in) precipitation annually. Other areas are arid by virtue of being a very long way from the nearest available sources of moisture.
Montane deserts are arid places with a very high altitude; the most prominent example is found north of the Himalayas, in the Kunlun Mountains and the Tibetan Plateau. Many locations within this category have elevations exceeding 3,000 m (9,800 ft) and the thermal regime can be hemiboreal. These places owe their profound aridity (the average annual precipitation is often less than 40 mm or 1.5 in) to being very far from the nearest available sources of moisture and are often in the lee of mountain ranges. Montane deserts are normally cold, or may be scorchingly hot by day and very cold by night as is true of the northeastern slopes of Mount Kilimanjaro.
Polar deserts such as McMurdo Dry Valleys remain ice-free because of the dry katabatic winds that flow downhill from the surrounding mountains. Former desert areas presently in non-arid environments, such as the Sandhills in Nebraska, are known as paleodeserts. In the Köppen climate classification system, deserts are classed as BWh (hot desert) or BWk (temperate desert). In the Thornthwaite climate classification system, deserts would be classified as arid megathermal climates.




Deserts usually have a large diurnal and seasonal temperature range, with high daytime temperatures falling sharply at night. The diurnal range may be as much as 20 to 30 °C (36 to 54 °F) and the rock surface experiences even greater temperature differentials. During the day the sky is usually clear and most of the sun's radiation reaches the ground, but as soon as the sun sets, the desert cools quickly by radiating heat into space. In hot deserts, the temperature during daytime can exceed 45 °C (113 °F) in summer and plunge below freezing point at night during winter.

Such large temperature variations have a destructive effect on the exposed rocky surfaces. The repeated fluctuations put a strain on exposed rock and the flanks of mountains crack and shatter. Fragmented strata slide down into the valleys where they continue to break into pieces due to the relentless sun by day and chill by night. Successive strata are exposed to further weathering. The relief of the internal pressure that has built up in rocks that have been underground for aeons can cause them to shatter. Exfoliation also occurs when the outer surfaces of rocks split off in flat flakes. This is believed to be caused by the stresses put on the rock by repeated expansions and contractions which induces fracturing parallel to the original surface. Chemical weathering processes probably play a more important role in deserts than was previously thought. The necessary moisture may be present in the form of dew or mist. Ground water may be drawn to the surface by evaporation and the formation of salt crystals may dislodge rock particles as sand or disintegrate rocks by exfoliation. Shallow caves are sometimes formed at the base of cliffs by this means.
As the desert mountains decay, large areas of shattered rock and rubble occur. The process continues and the end products are either dust or sand. Dust is formed from solidified clay or volcanic deposits whereas sand results from the fragmentation of harder granites, limestone and sandstone. There is a certain critical size (about 0.5 mm) below which further temperature-induced weathering of rocks does not occur and this provides a minimum size for sand grains.
As the mountains are eroded, more and more sand is created. At high wind speeds, sand grains are picked up off the surface and blown along, a process known as saltation. The whirling airborne grains act as a sand blasting mechanism which grinds away solid objects in its path as the kinetic energy of the wind is transferred to the ground. The sand eventually ends up deposited in level areas known as sand-fields or sand-seas, or piled up in dunes.




Sand and dust storms are natural events that occur in arid regions where the land is not protected by a covering of vegetation. Dust storms usually start in desert margins rather than the deserts themselves where the finer materials have already been blown away. As a steady wind begins to blow, fine particles lying on the exposed ground begin to vibrate. At greater wind speeds, some particles are lifted into the air stream. When they land, they strike other particles which may be jerked into the air in their turn, starting a chain reaction. Once ejected, these particles move in one of three possible ways, depending on their size, shape and density; suspension, saltation or creep. Suspension is only possible for particles less than 0.1 mm (0.004 in) in diameter. In a dust storm, these fine particles are lifted up and wafted aloft to heights of up to 6 km (3.7 mi). They reduce visibility and can remain in the atmosphere for days on end, conveyed by the trade winds for distances of up to 6,000 km (3,700 mi). Denser clouds of dust can be formed in stronger winds, moving across the land with a billowing leading edge. The sunlight can be obliterated and it may become as dark as night at ground level. In a study of a dust storm in China in 2001, it was estimated that 6.5 million tons of dust were involved, covering an area of 134,000,000 km2 (52,000,000 sq mi). The mean particle size was 1.44 μm. A much smaller scale, short-lived phenomenon can occur in calm conditions when hot air near the ground rises quickly through a small pocket of cooler, low-pressure air above forming a whirling column of particles, a dust devil.

Sandstorms occur with much less frequency than dust storms. They are often preceded by severe dust storms and occur when the wind velocity increases to a point where it can lift heavier particles. These grains of sand, up to about 0.5 mm (0.020 in) in diameter are jerked into the air but soon fall back to earth, ejecting other particles in the process. Their weight prevents them from being airborne for long and most only travel a distance of a few meters (yards). The sand streams along above the surface of the ground like a fluid, often rising to heights of about 30 cm (12 in). In a really severe steady blow, 2 m (6 ft 7 in) is about as high as the sand stream can rise as the largest sand grains do not become airborne at all. They are transported by creep, being rolled along the desert floor or performing short jumps.
During a sandstorm, the wind-blown sand particles become electrically charged. Such electric fields, which range in size up to 80 kV/m, can produce sparks and cause interference with telecommunications equipment. They are also unpleasant for humans and can cause headaches and nausea. The electric fields are caused by collision between airborne particles and by the impacts of saltating sand grains landing on the ground. The mechanism is little understood but the particles usually have a negative charge when their diameter is under 250 μm and a positive one when they are over 500 μm.




Deserts take up about one third of the Earth's land surface. Bottomlands may be salt-covered flats. Eolian processes are major factors in shaping desert landscapes. Polar deserts (also seen as "cold deserts") have similar features, except the main form of precipitation is snow rather than rain. Antarctica is the world's largest cold desert (composed of about 98% thick continental ice sheet and 2% barren rock). Some of the barren rock is to be found in the so-called Dry Valleys of Antarctica that almost never get snow, which can have ice-encrusted saline lakes that suggest evaporation far greater than the rare snowfall due to the strong katabatic winds that even evaporate ice.

Deserts, both hot and cold, play a part in moderating the Earth's temperature. This is because they reflect more of the incoming light and their albedo is higher than that of forests or the sea.




Many people think of deserts as consisting of extensive areas of billowing sand dunes because that is the way they are often depicted on TV and in films, but deserts do not always look like this. Across the world, around 20% of desert is sand, varying from only 2% in North America to 30% in Australia and over 45% in Central Asia. Where sand does occur, it is usually in large quantities in the form of sand sheets or extensive areas of dunes.
A sand sheet is a near-level, firm expanse of partially consolidated particles in a layer that varies from a few centimeters to a few meters thick. The structure of the sheet consists of thin horizontal layers of coarse silt and very fine to medium grain sand, separated by layers of coarse sand and pea-gravel which are a single grain thick. These larger particles anchor the other particles in place and may also be packed together on the surface so as to form a miniature desert pavement. Small ripples form on the sand sheet when the wind exceeds 24 km/h (15 mph). They form perpendicular to the wind direction and gradually move across the surface as the wind continues to blow. The distance between their crests corresponds to the average length of jumps made by particles during saltation. The ripples are ephemeral and a change in wind direction causes them to reorganise.

Sand dunes are accumulations of windblown sand piled up in mounds or ridges. They form downwind of copious sources of dry, loose sand and occur when topographic and climatic conditions cause airborne particles to settle. As the wind blows, saltation and creep take place on the windward side of the dune and individual grains of sand move uphill. When they reach the crest, they cascade down the far side. The upwind slope typically has a gradient of 10° to 20° while the lee slope is around 32°, the angle at which loose dry sand will slip. As this wind-induced movement of sand grains takes place, the dune moves slowly across the surface of the ground. Dunes are sometimes solitary, but they are more often grouped together in dune fields. When these are extensive, they are known as sand seas or ergs.
The shape of the dune depends on the characteristics of the prevailing wind. Barchan dunes are produced by strong winds blowing across a level surface, and are crescent-shaped with the concave side away from the wind. When there are two directions from which winds regularly blow, a series of long, linear dunes known as seif dunes may form. These also occur parallel to a strong wind that blows in one general direction. Transverse dunes run at a right angle to the prevailing wind direction. Star dunes are formed by variable winds, and have several ridges and slip faces radiating from a central point. They tend to grow vertically; they can reach a height of 500 m (1,600 ft), making them the tallest type of dune. Rounded mounds of sand without a slip face are the rare dome dunes, found on the upwind edges of sand seas.

A large part of the surface area of the world's deserts consists of flat, stone-covered plains dominated by wind erosion. In "eolian deflation", the wind continually removes fine-grained material, which becomes wind-blown sand. This exposes coarser-grained material, mainly pebbles with some larger stones or cobbles, leaving a desert pavement, an area of land overlaid by closely packed smooth stones forming a tessellated mosaic. Different theories exist as to how exactly the pavement is formed. It may be that after the sand and dust is blown away by the wind the stones jiggle themselves into place; alternatively, stones previously below ground may in some way work themselves to the surface. Very little further erosion takes place after the formation of a pavement, and the ground becomes stable. Evaporation brings moisture to the surface by capillary action and calcium salts may be precipitated, binding particles together to form a desert conglomerate. In time, bacteria that live on the surface of the stones accumulate a film of minerals and clay particles, forming a shiny brown coating known as desert varnish.
Other non-sandy deserts consist of exposed outcrops of bedrock, dry soils or aridisols, and a variety of landforms affected by flowing water, such as alluvial fans, sinks or playas, temporary or permanent lakes, and oases. A hamada is a type of desert landscape consisting of a high rocky plateau where the sand has been removed by aeolian processes. Other landforms include plains largely covered by gravels and angular boulders, from which the finer particles have been stripped by the wind. These are called "reg" in the western Sahara, "serir" in the eastern Sahara, "gibber plains" in Australia and "saï" in central Asia. The Tassili Plateau in Algeria is an impressive jumble of eroded sandstone outcrops, canyons, blocks, pinnacles, fissures, slabs and ravines. In some places the wind has carved holes or arches and in others it has created mushroom-like pillars narrower at the base than the top. In the Colorado Plateau it is water that has been the eroding force. Here the Colorado River has cut its way over the millennia through the high desert floor creating a canyon that is over a mile (6,000 feet or 1,800 meters) deep in places, exposing strata that are over two billion year old.




One of the driest places on Earth is the Atacama Desert. It is virtually devoid of life because it is blocked from receiving precipitation by the Andes mountains to the east and the Chilean Coast Range to the west. The cold Humboldt Current and the anticyclone of the Pacific are essential to keep the dry climate of the Atacama. The average precipitation in the Chilean region of Antofagasta is just 1 mm (0.039 in) per year. Some weather stations in the Atacama have never received rain. Evidence suggests that the Atacama may not have had any significant rainfall from 1570 to 1971. It is so arid that mountains that reach as high as 6,885 m (22,589 ft) are completely free of glaciers and, in the southern part from 25°S to 27°S, may have been glacier-free throughout the Quaternary, though permafrost extends down to an altitude of 4,400 m (14,400 ft) and is continuous above 5,600 m (18,400 ft). Nevertheless, there is some plant life in the Atacama, in the form of specialist plants that obtain moisture from dew and the fogs that blow in from the Pacific.

When rain falls in deserts, as it occasionally does, it is often with great violence. The desert surface is evidence of this with dry stream channels known as arroyos or wadis meandering across its surface. These can experience flash floods, becoming raging torrents with surprising rapidity after a storm that may be many kilometers away. Most deserts are in basins with no drainage to the sea but some are crossed by exotic rivers sourced in mountain ranges or other high rainfall areas beyond their borders. The River Nile, the Colorado River and the Yellow River do this, losing much of their water through evaporation as they pass through the desert and raising groundwater levels nearby. There may also be underground sources of water in deserts in the form of springs, aquifers, underground rivers or lakes. Where these lie close to the surface, wells can be dug and oases may form where plant and animal life can flourish. The Nubian Sandstone Aquifer System under the Sahara Desert is the largest known accumulation of fossil water. The Great Man-Made River is a scheme launched by Libya's Colonel Gadaffi to tap this aquifer and supply water to coastal cities. Kharga Oasis in Egypt is 150 km (93 mi) long and is the largest oasis in the Libyan Desert. A lake occupied this depression in ancient times and thick deposits of sandy-clay resulted. Wells are dug to extract water from the porous sandstone that lies underneath. Seepages may occur in the walls of canyons and pools may survive in deep shade near the dried up watercourse below.
Lakes may form in basins where there is sufficient precipitation or meltwater from glaciers above. They are usually shallow and saline, and wind blowing over their surface can cause stress, moving the water over nearby low-lying areas. When the lakes dry up, they leave a crust or hardpan behind. This area of deposited clay, silt or sand is known as a playa. The deserts of North America have more than one hundred playas, many of them relics of Lake Bonneville which covered parts of Utah, Nevada and Idaho during the last ice age when the climate was colder and wetter. These include the Great Salt Lake, Utah Lake, Sevier Lake and many dry lake beds. The smooth flat surfaces of playas have been used for attempted vehicle speed records at Black Rock Desert and Bonneville Speedway and the United States Air Force uses Rogers Dry Lake in the Mojave Desert as runways for aircraft and the space shuttle.







Plants face severe challenges in arid environments. Problems they need to solve include how to obtain enough water, how to avoid being eaten and how to reproduce. Photosynthesis is the key to plant growth. It can only take place during the day as energy from the sun is required, but during the day, many deserts become very hot. Opening stomata to allow in the carbon dioxide necessary for the process causes evapotranspiration, and conservation of water is a top priority for desert vegetation. Some plants have resolved this problem by adopting crassulacean acid metabolism, allowing them to open their stomata during the night to allow CO2 to enter, and close them during the day, or by using C4 carbon fixation.

Many desert plants have reduced the size of their leaves or abandoned them altogether. Cacti are desert specialists and in most species the leaves have been dispensed with and the chlorophyll displaced into the trunks, the cellular structure of which has been modified to allow them to store water. When rain falls, the water is rapidly absorbed by the shallow roots and retained to allow them to survive until the next downpour, which may be months or years away. The giant saguaro cacti of the Sonoran Desert form "forests", providing shade for other plants and nesting places for desert birds. Saguaro grow slowly but may live for up to two hundred years. The surface of the trunk is folded like a concertina, allowing it to expand, and a large specimen can hold eight tons of water after a good downpour.
Cacti are present in both North and South America with a post-Gondwana origin. Other xerophytic plants have developed similar strategies by a process known as convergent evolution. They limit water loss by reducing the size and number of stomata, by having waxy coatings and hairy or tiny leaves. Some are deciduous, shedding their leaves in the driest season, and others curl their leaves up to reduce transpiration. Others store water in succulent leaves or stems or in fleshy tubers. Desert plants maximize water uptake by having shallow roots that spread widely, or by developing long taproots that reach down to deep rock strata for ground water. The saltbush in Australia has succulent leaves and secretes salt crystals, enabling it to live in saline areas. In common with cacti, many have developed spines to ward off browsing animals.

Some desert plants produce seed which lies dormant in the soil until sparked into growth by rainfall. When annuals, such plants grow with great rapidity and may flower and set seed within weeks, aiming to complete their development before the last vestige of water dries up. For perennial plants, reproduction is more likely to be successful if the seed germinates in a shaded position, but not so close to the parent plant as to be in competition with it. Some seed will not germinate until it has been blown about on the desert floor to scarify the seed coat. The seed of the mesquite tree, which grows in deserts in the Americas, is hard and fails to sprout even when planted carefully. When it has passed through the gut of a pronghorn it germinates readily, and the little pile of moist dung provides an excellent start to life well away from the parent tree. The stems and leaves of some plants lower the surface velocity of sand-carrying winds and protect the ground from erosion. Even small fungi and microscopic plant organisms found on the soil surface (so-called cryptobiotic soil) can be a vital link in preventing erosion and providing support for other living organisms. Some plants, including the Plantago Lanceolata, have to reproduce via wind pollination due to living in the environment. Cold deserts often have high concentrations of salt in the soil. Grasses and low shrubs are the dominant vegetation here and the ground may be covered with lichens. Most shrubs have spiny leaves and shed them in the coldest part of the year.




Animals adapted to live in deserts are called xerocoles. There is no evidence that body temperature of mammals and birds is adaptive to the different climates, either of great heat or cold. In fact, with a very few exceptions, their basal metabolic rate is determined by body size, irrespective of the climate in which they live. Many desert animals (and plants) show especially clear evolutionary adaptations for water conservation or heat tolerance and so are often studied in comparative physiology, ecophysiology, and evolutionary physiology. One well-studied example is the specializations of mammalian kidneys shown by desert-inhabiting species. Many examples of convergent evolution have been identified in desert organisms, including between cacti and Euphorbia, kangaroo rats and jerboas, Phrynosoma and Moloch lizards.

Deserts present a very challenging environment for animals. Not only do they require food and water but they also need to keep their body temperature at a tolerable level. In many ways birds are the most able to do this of the higher animals. They can move to areas of greater food availability as the desert blooms after local rainfall and can fly to faraway waterholes. In hot deserts, gliding birds can remove themselves from the over-heated desert floor by using thermals to soar in the cooler air at great heights. In order to conserve energy, other desert birds run rather than fly. The cream-colored courser flits gracefully across the ground on its long legs, stopping periodically to snatch up insects. Like other desert birds it is well-camouflaged by its coloring and can merge into the landscape when stationary. The sandgrouse is an expert at this and nests on the open desert floor dozens of kilometers (miles) away from the waterhole it needs to visit daily. Some small diurnal birds are found in very restricted localities where their plumage matches the color of the underlying surface. The desert lark takes frequent dust baths which ensures that it matches its environment.
Water and carbon dioxide are metabolic end products of oxidation of fats, proteins, and carbohydrates. Oxidising a gram of carbohydrate produces 0.60 grams of water; a gram of protein produces 0.41 grams of water; and a gram of fat produces 1.07 grams of water, making it possible for xerocoles to live with little or no access to drinking water. The kangaroo rat for example makes use of this water of metabolism and conserves water both by having a low basal metabolic rate and by remaining underground during the heat of the day, reducing loss of water through its skin and respiratory system when at rest. Herbivorous mammals obtain moisture from the plants they eat. Species such as the addax antelope, dik-dik, Grant's gazelle and oryx are so efficient at doing this that they apparently never need to drink. The camel is a superb example of a mammal adapted to desert life. It minimizes its water loss by producing concentrated urine and dry dung, and is able to lose 40% of its body weight through water loss without dying of dehydration. Carnivores can obtain much of their water needs from the body fluids of their prey. Many other hot desert animals are nocturnal, seeking out shade during the day or dwelling underground in burrows. At depths of more than 50 cm (20 in), these remain at between 30 to 32 °C (86 to 90 °F) regardless of the external temperature. Jerboas, desert rats, kangaroo rats and other small rodents emerge from their burrows at night and so do the foxes, coyotes, jackals and snakes that prey on them. Kangaroos keep cool by increasing their respiration rate, panting, sweating and moistening the skin of their forelegs with saliva. Mammals living in cold deserts have developed greater insulation through warmer body fur and insulating layers of fat beneath the skin. The arctic weasel has a metabolic rate that is two or three times as high as would be expected for an animal of its size. Birds have avoided the problem of losing heat through their feet by not attempting to maintain them at the same temperature as the rest of their bodies, a form of adaptive insulation. The emperor penguin has dense plumage, a downy under layer, an air insulation layer next the skin and various thermoregulatory strategies to maintain its body temperature in one of the harshest environments on Earth.

Being ectotherms, reptiles are unable to live in cold deserts but are well-suited to hot ones. In the heat of the day in the Sahara, the temperature can rise to 50 °C (122 °F). Reptiles cannot survive at this temperature and lizards will be prostrated by heat at 45 °C (113 °F). They have few adaptations to desert life and are unable to cool themselves by sweating so they shelter during the heat of the day. In the first part of the night, as the ground radiates the heat absorbed during the day, they emerge and search for prey. Lizards and snakes are the most numerous in arid regions and certain snakes have developed a novel method of locomotion that enables them to move sidewards and navigate high sand-dunes. These include the horned viper of Africa and the sidewinder of North America, evolutionarily distinct but with similar behavioural patterns because of convergent evolution. Many desert reptiles are ambush predators and often bury themselves in the sand, waiting for prey to come within range.
Amphibians might seem unlikely desert-dwellers, because of their need to keep their skins moist and their dependence on water for reproductive purposes. In fact, the few species that are found in this habitat have made some remarkable adaptations. Most of them are fossorial, spending the hot dry months aestivating in deep burrows. While there they shed their skins a number of times and retain the remnants around them as a waterproof cocoon to retain moisture. In the Sonoran Desert, Couch's spadefoot toad spends most of the year dormant in its burrow. Heavy rain is the trigger for emergence and the first male to find a suitable pool calls to attract others. Eggs are laid and the tadpoles grow rapidly as they must reach metamorphosis before the water evaporates. As the desert dries out, the adult toads rebury themselves. The juveniles stay on the surface for a while, feeding and growing, but soon dig themselves burrows. Few make it to adulthood. The water holding frog in Australia has a similar life cycle and may aestivate for as long as five years if no rain falls. The Desert rain frog of Namibia is nocturnal and survives because of the damp sea fogs that roll in from the Atlantic.

Invertebrates, particularly arthropods, have successfully made their homes in the desert. Flies, beetles, ants, termites, locusts, millipedes, scorpions and spiders have hard cuticles which are impervious to water and many of them lay their eggs underground and their young develop away from the temperature extremes at the surface. The Saharan silver ant (Cataglyphis bombycina) uses a heat shock protein in a novel way and forages in the open during brief forays in the heat of the day. The long-legged darkling beetle in Namibia stands on its front legs and raises its carapace to catch the morning mist as condensate, funnelling the water into its mouth. Some arthropods make use of the ephemeral pools that form after rain and complete their life cycle in a matter of days. The desert shrimp does this, appearing "miraculously" in new-formed puddles as the dormant eggs hatch. Others, such as brine shrimps, fairy shrimps and tadpole shrimps, are cryptobiotic and can lose up to 92% of their bodyweight, rehydrating as soon as it rains and their temporary pools reappear.



Humans have long made use of deserts as places to live, and more recently have started to exploit them for minerals and energy capture. Deserts play a significant role in human culture with an extensive literature.




People have been living in deserts for millennia. Many, such as the Bushmen in the Kalahari, the Aborigines in Australia and various tribes of North American Indians, were originally hunter-gatherers. They developed skills in the manufacture and use of weapons, animal tracking, finding water, foraging for edible plants and using the things they found in their natural environment to supply their everyday needs. Their self-sufficient skills and knowledge were passed down through the generations by word of mouth. Other cultures developed a nomadic way of life as herders of sheep, goats, cattle, camels, yaks, llamas or reindeer. They travelled over large areas with their herds, moving to new pastures as seasonal and erratic rainfall encouraged new plant growth. They took with them their tents made of cloth or skins draped over poles and their diet included milk, blood and sometimes meat.

The desert nomads were also traders. The Sahara is a very large expanse of land stretching from the Atlantic rim to Egypt. Trade routes were developed linking the Sahel in the south with the fertile Mediterranean region to the north and large numbers of camels were used to carry valuable goods across the desert interior. The Tuareg were traders and the goods transported traditionally included slaves, ivory and gold going northwards and salt going southwards. Berbers with knowledge of the region were employed to guide the caravans between the various oases and wells. Several million slaves may have been taken northwards across the Sahara between the 8th and 18th centuries. Traditional means of overland transport declined with the advent of motor vehicles, shipping and air freight, but caravans still travel along routes between Agadez and Bilma and between Timbuktu and Taoudenni carrying salt from the interior to desert-edge communities.
Round the rims of deserts, where more precipitation occurred and conditions were more suitable, some groups took to cultivating crops. This may have happened when drought caused the death of herd animals, forcing herdsmen to turn to cultivation. With few inputs, they were at the mercy of the weather and may have lived at bare subsistence level. The land they cultivated reduced the area available to nomadic herders, causing disputes over land. The semi-arid fringes of the desert have fragile soils which are at risk of erosion when exposed, as happened in the American Dust Bowl in the 1930s. The grasses that held the soil in place were ploughed under, and a series of dry years caused crop failures, while enormous dust storms blew the topsoil away. Half a million Americans were forced to leave their land in this catastrophe.
Similar damage is being done today to the semi-arid areas that rim deserts and about twelve million hectares of land are being turned to desert each year. Desertification is caused by such factors as drought, climatic shifts, tillage for agriculture, overgrazing and deforestation. Vegetation plays a major role in determining the composition of the soil. In many environments, the rate of erosion and run off increases dramatically with reduced vegetation cover. Unprotected dry surfaces tend to be blown away by the wind or be washed away by flash floods, leaving infertile soil layers that bake in the sun and become unproductive hardpan. Although overgrazing has historically been considered to be a cause of desertification, there is some evidence that wild and domesticated animals actually improve fertility and vegetation cover, and that their removal encourages erosive processes.




Deserts contain substantial mineral resources, sometimes over their entire surface, giving them their characteristic colors. For example, the red of many sand deserts comes from laterite minerals. Geological processes in a desert climate can concentrate minerals into valuable deposits. Leaching by ground water can extract ore minerals and redeposit them, according to the water table, in concentrated form. Similarly, evaporation tends to concentrate minerals in desert lakes, creating dry lake beds or playas rich in minerals. Evaporation can concentrate minerals as a variety of evaporite deposits, including gypsum, sodium nitrate, sodium chloride and borates. Evaporites are found in the USA's Great Basin Desert, historically exploited by the "20-mule teams" pulling carts of borax from Death Valley to the nearest railway. A desert especially rich in mineral salts is the Atacama Desert, Chile, where sodium nitrate has been mined for explosives and fertilizer since around 1850. Other desert minerals are copper from Chile, Peru, and Iran, and iron and uranium in Australia. Many other metals, salts and commercially valuable types of rock such as pumice are extracted from deserts around the world.
Oil and gas form on the bottom of shallow seas when micro-organisms decompose under anoxic conditions and later become covered with sediment. Many deserts were at one time the sites of shallow seas and others have had underlying hydrocarbon deposits transported to them by the movement of tectonic plates. Some major oilfields such as Ghawar are found under the sands of Saudi Arabia. Geologists believe that other oil deposits were formed by aeolian processes in ancient deserts as may be the case with some of the major American oil fields.




Traditional desert farming systems have long been established in North Africa, irrigation being the key to success in an area where water stress is a limiting factor to growth. Techniques that can be used include drip irrigation, the use of organic residues or animal manures as fertilisers and other traditional agricultural management practises. Once fertility has been built up, further crop production preserves the soil from destruction by wind and other forms of erosion. It has been found that plant growth-promoting bacteria play a role in increasing the resistance of plants to stress conditions and these rhizobacterial suspensions could be inoculated into the soil in the vicinity of the plants. A study of these microbes found that desert farming hampers desertification by establishing islands of fertility allowing farmers to achieve increased yields despite the adverse environmental conditions. A field trial in the Sonoran Desert which exposed the roots of different species of tree to rhizobacteria and the nitrogen fixing bacterium Azospirillum brasilense with the aim of restoring degraded lands was only partially successful.
The Judean Desert was farmed in the 7th century BC during the Iron Age to supply food for desert forts. Native Americans in the south western United States became agriculturalists around 600 AD when seeds and technologies became available from Mexico. They used terracing techniques and grew gardens beside seeps, in moist areas at the foot of dunes, near streams providing flood irrigation and in areas irrigated by extensive specially built canals. The Hohokam tribe constructed over 500 miles (800 km) of large canals and maintained them for centuries, an impressive feat of engineering. They grew maize, beans, squash and peppers.

A modern example of desert farming is the Imperial Valley in California, which has high temperatures and average rainfall of just 3 in (76 mm) per year. The economy is heavily based on agriculture and the land is irrigated through a network of canals and pipelines sourced entirely from the Colorado River via the All-American Canal. The soil is deep and fertile, being part of the river's flood plains, and what would otherwise have been desert has been transformed into one of the most productive farming regions in California. Other water from the river is piped to urban communities but all this has been at the expense of the river, which below the extraction sites no longer has any above-ground flow during most of the year. Another problem of growing crops in this way is the build-up of salinity in the soil caused by evaporation of river water. The greening of the desert remains an aspiration and was at one time viewed as a future means for increasing food production for the world's growing population. This prospect has proved false as it disregarded the environmental damage caused elsewhere by the diversion of water for desert project irrigation.




Deserts are increasingly seen as sources for solar energy, partly due to low amounts of cloud cover. Many successful solar power plants have been built in the Mojave Desert. These plants have a combined capacity of 354 megawatts (MW) making them the largest solar power installation in the world. Large swaths of this desert are covered in mirrors, including nine fields of solar collectors. The Mojave Solar Park is currently under construction and will produce 280MW when completed.
The potential for generating solar energy from the Sahara Desert is huge, the highest found on the globe. Professor David Faiman of Ben-Gurion University has stated that the technology now exists to supply all of the world's electricity needs from 10% of the Sahara Desert. Desertec Industrial Initiative is a consortium seeking $560 billion to invest in North African solar and wind installations over the next forty years to supply electricity to Europe via cable lines running under the Mediterranean Sea. European interest in the Sahara Desert stems from its two aspects: the almost continual daytime sunshine and plenty of unused land. The Sahara receives more sunshine per acre than any part of Europe. The Sahara Desert also has the empty space totalling hundreds of square miles required to house fields of mirrors for solar plants.
The Negev Desert, Israel, and the surrounding area, including the Arava Valley, receive plenty of sunshine and are generally not arable. This has resulted in the construction of many solar plants. David Faiman has proposed that "giant" solar plants in the Negev could supply all of Israel's needs for electricity.




The Arabs were probably the first organized force to conduct successful battles in the desert. By knowing back routes and the locations of oases and by utilizing camels, Muslim Arab forces were able to successfully overcome both Roman and Persian forces in the period 600 to 700 AD during the expansion of the Islamic caliphate.
Many centuries later, both world wars saw fighting in the desert. In the First World War, the Ottoman Turks were engaged with the British regular army in a campaign that spanned the Arabian peninsula. The Turks were defeated by the British, who had the backing of irregular Arab forces that were seeking to revolt against the Turks in the Hejaz, made famous in T. E. Lawrence's book Seven Pillars of Wisdom.

In the Second World War, the Western Desert Campaign began in Italian Libya. Warfare in the desert offered great scope for tacticians to use the large open spaces without the distractions of casualties among civilian populations. Tanks and armoured vehicles were able to travel large distances unimpeded and land mines were laid in large numbers. However the size and harshness of the terrain meant that all supplies needed to be brought in from great distances. The victors in a battle would advance and their supply chain would necessarily become longer, while the defeated army could retreat, regroup and resupply. For these reasons, the front line moved back and forth through hundreds of kilometers as each side lost and regained momentum. Its most easterly point was at El Alamein in Egypt, where the Allies decisively defeated the Axis forces in 1942.




The desert is generally thought of as a barren and empty landscape. It has been portrayed by writers, film-makers, philosophers, artists and critics as a place of extremes, a metaphor for anything from death, war or religion to the primitive past or the desolate future.
There is an extensive literature on the subject of deserts. An early historical account is that of Marco Polo (c. 1254–1324), who travelled through Central Asia to China, crossing a number of deserts in his twenty four year trek. Some accounts give vivid descriptions of desert conditions, though often accounts of journeys across deserts are interwoven with reflection, as is the case in Charles Montagu Doughty's major work, Travels in Arabia Deserta (1888). Antoine de Saint-Exupéry described both his flying and the desert in Wind, Sand and Stars and Gertrude Bell travelled extensively in the Arabian desert in the early part of the 20th century, becoming an expert on the subject, writing books and advising the British government on dealing with the Arabs. Another woman explorer was Freya Stark who travelled alone in the Middle East, visiting Turkey, Arabia, Yemen, Syria, Persia and Afghanistan, writing over twenty books on her experiences. The German naturalist Uwe George spent several years living in deserts, recording his experiences and research in his book, In the Deserts of this Earth.
The American poet Robert Frost expressed his bleak thoughts in his poem, Desert Places, which ends with the stanza "They cannot scare me with their empty spaces / Between stars - on stars where no human race is. / I have it in me so much nearer home / To scare myself with my own desert places."




Mars is the only planet in the Solar System on which deserts have been identified. Despite its low surface atmospheric pressure (only 1/100 of that of the Earth), the patterns of atmospheric circulation on Mars have formed a sea of circumpolar sand more than 5 million km² (1.9 million sq mi) in area, much larger than deserts on Earth. The Martian deserts principally consist of dunes in the form of half-moons in flat areas near the permanent polar ice caps in the north of the planet. The smaller dune fields occupy the bottom of many of the craters situated in the Martian polar regions. Examination of the surface of rocks by laser beamed from the Mars Exploration Rover have shown a surface film that resembles the desert varnish found on Earth although it might just be surface dust. The surface of Titan, a moon of Saturn, also has a desert-like surface with dune seas.









George, Uwe (1978). In the Deserts of this Earth. Hamish Hamilton. ISBN 0-241-89777-7. 
Pye, Kenneth; Tsoar, Haim (2009). Aeolian Sand and Sand Dunes. Springer. ISBN 3-540-85910-1. 



Bagnold, Ralph A. (1941). The physics of blown sand and desert dunes. Methuen. 
Macmahon, James (1988). Deserts. National Audubon Society nature guides. Random House / Chanticleer Press. ISBN 978-0-394-73139-1. 



"Global Deserts Outlook". United Nations Environment Programme (UNEP). 2006. , a report in the Global Environment Outlook (GEO) series.
Global Deserts Outlook in PDF at the Wayback Machine (archived June 10, 2006)
Map with biodiversity scenarios for desert areas, from the Global Deserts Outlook.A mountain is a large landform that stretches above the surrounding land in a limited area, usually in the form of a peak. A mountain is generally steeper than a hill. Mountains are formed through tectonic forces or volcanism. These forces can locally raise the surface of the earth. Mountains erode slowly through the action of rivers, weather conditions, and glaciers. A few mountains are isolated summits, but most occur in huge mountain ranges.
High elevations on mountains produce colder climates than at sea level. These colder climates strongly affect the ecosystems of mountains: different elevations have different plants and animals. Because of the less hospitable terrain and climate, mountains tend to be used less for agriculture and more for resource extraction and recreation, such as mountain climbing.
The highest mountain on Earth is Mount Everest in the Himalayas of Asia, whose summit is 8,850 m (29,035 ft) above mean sea level. The highest known mountain on any planet in the Solar System is Olympus Mons on Mars at 21,171 m (69,459 ft).




There is no universally accepted definition of a mountain. Elevation, volume, relief, steepness, spacing and continuity have been used as criteria for defining a mountain. In the Oxford English Dictionary a mountain is defined as "a natural elevation of the earth surface rising more or less abruptly from the surrounding level and attaining an altitude which, relatively to the adjacent elevation, is impressive or notable."
Whether a landform is called a mountain may depend on local usage. The highest point in San Francisco, California, is called Mount Davidson, notwithstanding its height of 300 m (980 ft), which makes it twenty feet short of the minimum for a mountain by American designations. Similarly, Mount Scott outside Lawton, Oklahoma is only 251 m (823 ft) from its base to its highest point. Whittow's Dictionary of Physical Geography states "Some authorities regard eminences above 600 metres (2,000 ft) as mountains, those below being referred to as hills."
In the United Kingdom and the Republic of Ireland, a mountain is usually defined as any summit at least 2,000 feet (or 610 metres) high, whilst the official UK government's definition of a mountain, for the purposes of access, is a summit of 600 metres or higher. In addition, some definitions also include a topographical prominence requirement, typically 100 or 500 feet (30 or 152 m). For a while, the US defined a mountain as being 1,000 feet (300 m) or taller. Any similar landform lower than this height was considered a hill. However, today, the United States Geological Survey (USGS) concludes that these terms do not have technical definitions in the US.
The UN Environmental Programme's definition of "mountainous environment" includes any of the following:
Elevation of at least 2,500 m (8,200 ft);
Elevation of at least 1,500 m (4,900 ft), with a slope greater than 2 degrees;
Elevation of at least 1,000 m (3,300 ft), with a slope greater than 5 degrees;
Elevation of at least 300 m (980 ft), with a 300 m (980 ft) elevation range within 7 km (4.3 mi).
Using these definitions, mountains cover 33% of Eurasia, 19% of South America, 24% of North America, and 14% of Africa. As a whole, 24% of the Earth's land mass is mountainous.




There are three main types of mountains: volcanic, fold, and block. All three types are formed from plate tectonics: when portions of the Earth's crust move, crumple, and dive. Compressional forces, isostatic uplift and intrusion of igneous matter forces surface rock upward, creating a landform higher than the surrounding features. The height of the feature makes it either a hill or, if higher and steeper, a mountain. Major mountains tend to occur in long linear arcs, indicating tectonic plate boundaries and activity.




Volcanoes are formed when a plate is pushed below another plate, or at a mid-ocean ridge or hotspot. At a depth of around 100 km, melting occurs in rock above the slab (due to the addition of water), and forms magma that reaches the surface. When the magma reaches the surface, it often builds a volcanic mountain, such as a shield volcano or a stratovolcano. Examples of volcanoes include Mount Fuji in Japan and Mount Pinatubo in the Philippines. The magma does not have to reach the surface in order to create a mountain: magma that solidifies below ground can still form dome mountains, such as Navajo Mountain in the US.




Fold mountains occur when two plates collide: shortening occurs along thrust faults and the crust is overthickened. Since the less dense continental crust "floats" on the denser mantle rocks beneath, the weight of any crustal material forced upward to form hills, plateaus or mountains must be balanced by the buoyancy force of a much greater volume forced downward into the mantle. Thus the continental crust is normally much thicker under mountains, compared to lower lying areas. Rock can fold either symmetrically or asymmetrically. The upfolds are anticlines and the downfolds are synclines: in asymmetric folding there may also be recumbent and overturned folds. The Jura Mountains are an example of fold mountains.




Block mountains are caused by faults in the crust: a seam where rocks can move past each other. When rocks on one side of a fault rise relative to the other, it can form a mountain. The uplifted blocks are block mountains or horsts. The intervening dropped blocks are termed graben: these can be small or form extensive rift valley systems. This form of landscape can be seen in East Africa, the Vosges, the Basin and Range Province of Western North America and the Rhine valley. These areas often occur when the regional stress is extensional and the crust is thinned.




During and following uplift, mountains are subjected to the agents of erosion (water, wind, ice, and gravity) which gradually wear the uplifted area down. Erosion causes the surface of mountains to be younger than the rocks that form the mountains themselves. Glacial processes produce characteristic landforms, such as pyramidal peaks, knife-edge arêtes, and bowl-shaped cirques that can contain lakes. Plateau mountains, such as the Catskills, are formed from the erosion of an uplifted plateau.




Climate on mountains become colder at high elevations, due an interaction between radiation and convection. Sunlight in the visible spectrum hits the ground and heats it. The ground then heats the air at the surface. If radiation were the only way to transfer heat from the ground to space, the greenhouse effect of gases in the atmosphere would keep the ground at roughly 333 K (60 °C; 140 °F), and the temperature would decay exponentially with height.
However, when air is hot, it tends to expand, which lowers its density. Thus, hot air tends to rise and transfer heat upward. This is the process of convection. Convection comes to equilibrium when a parcel at air at a given altitude has the same density as its surroundings. Air is a poor conductor of heat, so a parcel of air will rise and fall without exchanging heat. This is known as an adiabatic process, which has a characteristic pressure-temperature curve. As the pressure gets lower, the temperature decreases. The rate of decrease of temperature with elevation is known as the adiabatic lapse rate, which is approximately 9.8 °C per kilometer (or 5.4 °F per 1000 feet) of altitude.
Note that the presence of water in the atmosphere complicates the process of convection. Water vapor contains latent heat of vaporization. As air rises and cools, it eventually becomes saturated and cannot hold its quantity of water vapor. The water vapor condenses (forming clouds), and releases heat, which changes the lapse rate from the dry adiabatic lapse rate to the moist adiabatic lapse rate (5.5 °C per kilometer or 3 °F per 1000 feet) The actual lapse rate can vary by altitude and by location.
Therefore, moving up 100 meters on a mountain is roughly equivalent to moving 80 kilometers (45 miles or 0.75° of latitude) towards the nearest pole. This relationship is only approximate, however, since local factors such as proximity to oceans (such as the Arctic Ocean) can drastically modify the climate. As the altitude increases, the main form of precipitation becomes snow and the winds increase.
The effect of the climate on the ecology at an elevation can be largely captured through a combination of amount of precipitation, and the biotemperature, as described by Leslie Holdridge in 1947. Biotemperature is the mean temperature; all temperatures below 0 °C (32 °F) are considered to be 0 °C. When the temperature is below 0 °C, plants are dormant, so the exact temperature is unimportant. The peaks of mountains with permanent snow can have a biotemperature below 1.5 °C (34.7 °F).




The colder climate on mountains affects the plants and animals residing on mountains. A particular set of plants and animals tend to be adapted to a relatively narrow range of climate. Thus, ecosystems tend to lie along elevation bands of roughly constant climate. This is called altitudinal zonation. In regions with dry climates, the tendency of mountains to have higher precipitation as well as lower temperatures also provides for varying conditions, which enhances zonation.
Some plants and animals found in altitudinal zones tend to become isolated since the conditions above and below a particular zone will be inhospitable and thus constrain their movements or dispersal. These isolated ecological systems are known as sky islands.
Altitudinal zones tend to follow a typical pattern. At the highest elevations, trees cannot grow, and whatever life may be present will be of the alpine type, resembling tundra. Just below the tree line, one may find subalpine forests of needleleaf trees, which can withstand cold, dry conditions. Below that, montane forests grow. In the temperate portions of the earth, those forests tend to be needleleaf trees, while in the tropics, they can be broadleaf trees growing in a rain forest.




Mountains are generally less preferable for human habitation than lowlands, because of harsh weather and little level ground suitable for agriculture. While 7% of the land area of Earth is above 2,500 metres (8,200 ft), only 140 million people live above that altitude and only 20-30 million people above 3,000 metres (9,800 ft) elevation. The decreasing atmospheric pressure with increasing elevation means that less oxygen is available for breathing, and there is less protection against solar radiation (UV). Due to decreasing oxygen, the highest known permanent habitation in the world is at 5,100 metres (16,700 ft), while the highest known permanently tolerable altitude is at 5,950 metres (19,520 ft). Above 8,000 metres (26,000 ft) elevation, there is not enough oxygen to support human life. This is known as the "death zone". The summits of Mount Everest and K2 are in the death zone.
About half of mountain dwellers live in the Andes, Central Asia, and Africa. Traditional mountain societies rely on agriculture, with higher risk of crop failure than at lower elevations. Minerals often occur in mountains, with mining being an important component of the economics of some montane societies. More recently, tourism supports mountain communities, with some intensive development around attractions such as national parks or ski resorts. About 80% of mountain people live below the poverty line.
Most of the world's rivers are fed from mountain sources, with snow acting as a storage mechanism for downstream users. More than half of humanity depends on mountains for water.
Mountaineering, mountain climbing, or alpinism is the sport, hobby or profession of hiking, skiing, and climbing mountains. While mountaineering began as attempts to reach the highest point of unclimbed big mountains it has branched into specializations that address different aspects of the mountain and consists of three areas: rock-craft, snow-craft and skiing, depending on whether the route chosen is over rock, snow or ice. All require experience, athletic ability, and technical knowledge to maintain safety.




Heights of mountains are typically measured above sea level. Using this metric, Mount Everest is the highest mountain on Earth, at 8,848 metres (29,029 ft). There are at least 100 mountains with heights of over 7,200 metres (23,622 ft) above sea level, all of which are located in central and southern Asia. The highest mountains above sea level are generally not the highest above the surrounding terrain. There is no precise definition of surrounding base, but Denali, Mount Kilimanjaro and Nanga Parbat are possible candidates for the tallest mountain on land by this measure. The bases of mountain islands are below sea level, and given this consideration Mauna Kea (4,207 m (13,802 ft) above sea level) is the world's tallest mountain and volcano, rising about 10,203 m (33,474 ft) from the Pacific Ocean floor.
The highest mountains are not generally the most voluminous. Mauna Loa (4,169 m or 13,678 ft) is the largest mountain on Earth in terms of base area (about 2,000 sq mi or 5,200 km2) and volume (about 18,000 cu mi or 75,000 km3). Mount Kilimanjaro is the largest non-shield volcano in terms of both base area (245 sq mi or 635 km2) and volume (1,150 cu mi or 4,793 km3). Mount Logan is the largest non-volcanic mountain in base area (120 sq mi or 311 km2).
The highest mountains above sea level are also not those with peaks farthest from the centre of the Earth, because the figure of the Earth is not spherical. Sea level closer to the equator is several miles farther from the centre of the Earth. The summit of Chimborazo, Ecuador's tallest mountain, is usually considered to be the farthest point from the Earth's centre, although the southern summit of Peru's tallest mountain, Huascarán, is another contender. Both have elevations above sea level more than 2 kilometres (6,600 ft) less than that of Everest.



Latin names of mountains
List of mountain ranges
List of peaks by prominence
List of ski areas and resorts
Lists of mountains
Mountain hut









 "Mountain". Encyclopædia Britannica. 18 (11th ed.). 1911. 
 Media related to Mountains at Wikimedia Commons
 Quotations related to Mountains at WikiquoteFor other uses see River (disambiguation)
For other uses of the plural Rivers see Rivers (disambiguation)
For the village in Canada, see Rivercourse, Alberta

A river is a natural flowing watercourse, usually freshwater, flowing towards an ocean, sea, lake or another river. In some cases a river flows into the ground and becomes dry at the end of its course without reaching another body of water. Small rivers can be referred to using names such as stream, creek, brook, rivulet, and rill. There are no official definitions for the generic term river as applied to geographic features, although in some countries or communities a stream is defined by its size. Many names for small rivers are specific to geographic location; examples are "run" in some parts of the United States, "burn" in Scotland and northeast England, and "beck" in northern England. Sometimes a river is defined as being larger than a creek, but not always: the language is vague.
Rivers are part of the hydrological cycle. Water generally collects in a river from precipitation through a drainage basin from surface runoff and other sources such as groundwater recharge, springs, and the release of stored water in natural ice and snowpacks (e.g. from glaciers). Potamology is the scientific study of rivers while limnology is the study of inland waters in general.
Extraterrestrial rivers of liquid hydrocarbons have recently been found on Titan. Channels may indicate past rivers on other planets, specifically outflow channels on Mars and rivers are theorised to exist on planets and moons in habitable zones of stars.




A river begins at a source (or more often several sources), follows a path called a course, and ends at a mouth or mouths. The water in a river is usually confined to a channel, made up of a stream bed between banks. In larger rivers there is often also a wider floodplain shaped by flood-waters over-topping the channel. Floodplains may be very wide in relation to the size of the river channel. This distinction between river channel and floodplain can be blurred, especially in urban areas where the floodplain of a river channel can become greatly developed by housing and industry.
Rivers can flow down mountains, through valleys (depressions) or along plains, and can create canyons or gorges.
The term upriver (or upstream) refers to the direction towards the source of the river, i.e. against the direction of flow. Likewise, the term downriver (or downstream) describes the direction towards the mouth of the river, in which the current flows.
The term left bank refers to the left bank in the direction of flow, right bank to the right.
The river channel typically contains a single stream of water, but some rivers flow as several interconnecting streams of water, producing a braided river. Extensive braided rivers are now found in only a few regions worldwide, such as the South Island of New Zealand. They also occur on peneplains and some of the larger river deltas. Anastamosing rivers are similar to braided rivers and are also quite rare. They have multiple sinuous channels carrying large volumes of sediment. There are rare cases of river bifurcation in which a river divides and the resultant flows ending in different seas. An example is the bifurcation of Nerodime River in Kosovo.

A river flowing in its channel is a source of energy which acts on the river channel to change its shape and form. In 1757, the German hydrologist Albert Brahms empirically observed that the submerged weight of objects that may be carried away by a river is proportional to the sixth power of the river flow speed. This formulation is also sometimes called Airy's law. Thus, if the speed of flow is doubled, the flow would dislodge objects with 64 times as much submerged weight. In mountainous torrential zones this can be seen as erosion channels through hard rocks and the creation of sands and gravels from the destruction of larger rocks. In U-shaped glaciated valleys, the subsequent river valley can often easily be identified by the V-shaped channel that it has carved. In the middle reaches where a river flows over flatter land, meanders may form through erosion of the river banks and deposition on the inside of bends. Sometimes the river will cut off a loop, shortening the channel and forming an oxbow lake or billabong. Rivers that carry large amounts of sediment may develop conspicuous deltas at their mouths. Rivers whose mouths are in saline tidal waters may form estuaries.
Throughout the course of the river, the total volume of water transported downstream will often be a combination of the free water flow together with a substantial volume flowing through sub-surface rocks and gravels that underlie the river and its floodplain (called the hyporheic zone). For many rivers in large valleys, this unseen component of flow may greatly exceed the visible flow.



Most but not all rivers flow on the surface. Subterranean rivers flow underground in caves or caverns. Such rivers are frequently found in regions with limestone geologic formations. Subglacial streams are the braided rivers that flow at the beds of glaciers and ice sheets, permitting meltwater to be discharged at the front of the glacier. Because of the gradient in pressure due to the overlying weight of the glacier, such streams can even flow uphill.



An intermittent river (or ephemeral river) only flows occasionally and can be dry for several years at a time. These rivers are found in regions with limited or highly variable rainfall, or can occur because of geologic conditions such as a highly permeable river bed. Some ephemeral rivers flow during the summer months but not in the winter. Such rivers are typically fed from chalk aquifers which recharge from winter rainfall. In England these rivers are called bournes and give their name to places such as Bournemouth and Eastbourne. Even in humid regions, the location where flow begins in the smallest tributary streams generally moves upstream in response to precipitation and downstream in its absence or when active summer vegetation diverts water for evapotranspiration. Normally-dry rivers in arid zones are often identified as arroyos or other regional names.
The meltwater from large hailstorms can create a slurry of water, hail and sand or soil, forming temporary rivers.




Rivers have been classified by many criteria including their topography, their biotic status, and their relevance to white water rafting or canoeing activities.



Rivers can generally be classified as either alluvial, bedrock, or some mix of the two. Alluvial rivers have channels and floodplains that are self-formed in unconsolidated or weakly consolidated sediments. They erode their banks and deposit material on bars and their floodplains. Bedrock rivers form when the river downcuts through the modern sediments and into the underlying bedrock. This occurs in regions that have experienced some kind of uplift (thereby steepening river gradients) or in which a particular hard lithology causes a river to have a steepened reach that has not been covered in modern alluvium. Bedrock rivers very often contain alluvium on their beds; this material is important in eroding and sculpting the channel. Rivers that go through patches of bedrock and patches of deep alluvial cover are classified as mixed bedrock-alluvial.
Alluvial rivers can be further classified by their channel pattern as meandering, braided, wandering, anastomose, or straight. The morphology of an alluvial river reach is controlled by a combination of sediment supply, substrate composition, discharge, vegetation, and bed aggradation.
At the start of the 20th century William Morris Davis devised the "cycle of erosion" method of classifying rivers based on their "age". Although Davis's system is still found in many books today, after the 1950s and 1960s it became increasingly criticized and rejected by geomorphologists. His scheme did not produce testable hypotheses and was therefore deemed non-scientific. Examples of Davis's river "ages" include:
Youthful river: A river with a steep gradient that has very few tributaries and flows quickly. Its channels erode deeper rather than wider. Examples are the Brazos, Trinity and Ebro rivers.
Mature river: A river with a gradient that is less steep than those of youthful rivers and flows more slowly. A mature river is fed by many tributaries and has more discharge than a youthful river. Its channels erode wider rather than deeper. Examples are the Mississippi, Saint Lawrence, Danube, Ohio, Thames and Paraná rivers.
Old river: A river with a low gradient and low erosive energy. Old rivers are characterized by flood plains. Examples are the Yellow, lower Ganges, Tigris, Euphrates, Indus and lower Nile rivers.
Rejuvenated river: A river with a gradient that is raised by tectonic uplift. Examples are the Rio Grande and Colorado River.
The ways in which a river's characteristics vary between its upper and lower course are summarized by the Bradshaw model. Power-law relationships between channel slope, depth, and width are given as a function of discharge by "river regime".



There are several systems of classification based on biotic conditions typically assigning classes from the most oligotrophic or unpolluted through to the most eutrophic or polluted. Other systems are based on a whole eco-system approach such as developed by the New Zealand Ministry for the Environment. In Europe, the requirements of the Water Framework Directive has led to the development of a wide range of classification methods including classifications based on fishery status A system of river zonation used in francophone communities divides rivers into three primary zones:
The crenon is the uppermost zone at the source of the river. It is further divided into the eucrenon (spring or boil zone) and the hypocrenon (brook or headstream zone). These areas have low temperatures, reduced oxygen content and slow moving water.
The rhithron is the upstream portion of the river that follows the crenon. It has relatively cool temperatures, high oxygen levels, and fast, turbulent, swift flow.
The potamon is the remaining downstream stretch of river. It has warmer temperatures, lower oxygen levels, slow flow and sandier bottoms.



The International Scale of River Difficulty is used to rate the challenges of navigation—particularly those with rapids. Class I is the easiest and Class VI is the hardest.



The Strahler Stream Order ranks rivers based on the connectivity and hierarchy of contributing tributaries. Headwaters are first order while the Amazon River is twelfth order. Approximately 80% of the rivers and streams in the world are of the first and second order.




Rivers have been used as a source of water, for obtaining food, for transport, as a defensive measure, as a source of hydropower to drive machinery, for bathing, and as a means of disposing of waste.
Rivers have been used for navigation for thousands of years. The earliest evidence of navigation is found in the Indus Valley Civilization, which existed in northwestern India around 3300 BC. Riverine navigation provides a cheap means of transport, and is still used extensively on most major rivers of the world like the Amazon, the Ganges, the Nile, the Mississippi, and the Indus. Since river boats are often not regulated, they contribute a large amount to global greenhouse gas emissions, and to local cancer due to inhaling of particulates emitted by the transports.
In some heavily forested regions such as Scandinavia and Canada, lumberjacks use the river to float felled trees downstream to lumber camps for further processing, saving much effort and cost by transporting the huge heavy logs by natural means.
Rivers have been a source of food since pre-history. They are often a rich source of fish and other edible aquatic life, and are a major source of fresh water, which can be used for drinking and irrigation. Most of the major cities of the world are situated on the banks of rivers. Rivers help to determine the urban form of cities and neighbourhoods and their corridors often present opportunities for urban renewal through the development of foreshoreways such as river walks. Rivers also provide an easy means of disposing of waste water and, in much of the less developed world, other wastes.

Fast flowing rivers and waterfalls are widely used as sources of energy, via watermills and hydroelectric plants. Evidence of watermills shows them in use for many hundreds of years, for instance in Orkney at Dounby Click Mill. Prior to the invention of steam power, watermills for grinding cereals and for processing wool and other textiles were common across Europe. In the 1890s the first machines to generate power from river water were established at places such as Cragside in Northumberland and in recent decades there has been a significant increase in the development of large scale power generation from water, especially in wet mountainous regions such as Norway.
The coarse sediments, gravel, and sand, generated and moved by rivers are extensively used in construction. In parts of the world this can generate extensive new lake habitats as gravel pits re-fill with water. In other circumstances it can destabilise the river bed and the course of the river and cause severe damage to spawning fish populations which rely on stable gravel formations for egg laying.
In upland rivers, rapids with whitewater or even waterfalls occur. Rapids are often used for recreation, such as whitewater kayaking.
Rivers have been important in determining political boundaries and defending countries. For example, the Danube was a long-standing border of the Roman Empire, and today it forms most of the border between Bulgaria and Romania. The Mississippi in North America and the Rhine in Europe are major east-west boundaries in those continents. The Orange and Limpopo Rivers in southern Africa form the boundaries between provinces and countries along their routes.




The organisms in the riparian zone respond to changes in river channel location and patterns of flow. The ecosystem of rivers is generally described by the river continuum concept, which has some additions and refinements to allow for dams and waterfalls and temporary extensive flooding. The concept describes the river as a system in which the physical parameters, the availability of food particles and the composition of the ecosystem are continuously changing along its length. The food (energy) that remains from the upstream part is used downstream.
The general pattern is that the first order streams contain particulate matter (decaying leaves from the surrounding forests) which is processed there by shredders like Plecoptera larvae. The products of these shredders are used by collectors, such as Hydropsychidae, and further downstream algae that create the primary production become the main food source of the organisms. All changes are gradual and the distribution of each species can be described as a normal curve, with the highest density where the conditions are optimal. In rivers succession is virtually absent and the composition of the ecosystem stays fixed in time.




The chemistry of rivers is complex and depends on inputs from the atmosphere, the geology through which it travels and the inputs from man's activities. The chemical composition of the water has a large impact on the ecology of that water for both plants and animals and it also affects the uses that may be made of the river water. Understanding and characterising river water chemistry requires a well designed and managed sampling and analysis.




Some rivers generate brackish water by having their river mouth in the ocean. This, in effect creates a unique environment in which certain species are found.




Flooding is a natural part of a river's cycle. The majority of the erosion of river channels and the erosion and deposition on the associated floodplains occur during the flood stage. In many developed areas, human activity has changed the form of river channels, altering magnitudes and frequencies of flooding. Some examples of this are the building of levees, the straightening of channels, and the draining of natural wetlands. In many cases human activities in rivers and floodplains have dramatically increased the risk of flooding. Straightening rivers allows water to flow more rapidly downstream, increasing the risk of flooding places further downstream. Building on flood plains removes flood storage, which again exacerbates downstream flooding. The building of levees only protects the area behind the levees and not those further downstream. Levees and flood-banks can also increase flooding upstream because of the back-water pressure as the river flow is impeded by the narrow channel banks.



Studying the flows of rivers is one aspect of hydrology.




Rivers flow downhill with their power derived from gravity. The direction can involve all directions of the compass and can be a complex meandering path.
Rivers flowing downhill, from river source to river mouth, do not necessarily take the shortest path. For alluvial streams, straight and braided rivers have very low sinuosity and flow directly down hill, while meandering rivers flow from side to side across a valley. Bedrock rivers typically flow in either a fractal pattern, or a pattern that is determined by weaknesses in the bedrock, such as faults, fractures, or more erodible layers.




Volumetric flow rate, also known as discharge, volume flow rate, and rate of water flow, is the volume of water which passes through a given cross-section of the river channel per unit time. It is typically measured in cubic metres per second (cumec) or cubic feet per second (cfs), where 1 m3/s = 35.51 ft3/s; it is sometimes also measured in litres or gallons per second.
Volumetric flow rate can be thought of as the mean velocity of the flow through a given cross-section, times that cross-sectional area. Mean velocity can be approximated through the use of the Law of the Wall. In general, velocity increases with the depth (or hydraulic radius) and slope of the river channel, while the cross-sectional area scales with the depth and the width: the double-counting of depth shows the importance of this variable in determining the discharge through the channel.



In the youthful stage;
V-shaped valleys: example. River Liffey, Dublin, Ireland.
When the river is subject to vertical erosion, deepening the valley. Hydraulic action loosens and dislodges the rock. The rivers load further erodes its banks and the river bed. Over time, this will deepen the river bed and create steeper sides which are then weathered.
The steepened nature of the banks causes the sides of the valley to move downslope causing the valley to become V-Shaped.
Waterfalls also form in the youthful river valley. example. Powerscourt Waterfall, County Wicklow, Ireland.
Waterfalls usually form where a band of hard rock lies next to a layer of soft rock (easier to erode). Differential erosion occurs as the river can erode the soft rock easier than the hard rock, this leaves the hard rock more elevated and stands out from the river below. Hydraulic action and abrasion are what erodes the soft rock and the water to fall down to the river bed. A plunge pool forms at the bottom and deepens as a result of hydraulic action and abrasion.



Sediment yield is the total quantity of particulate matter (suspended or bedload) reaching the outlet of a drainage basin over a fixed time frame. Yield is usually expressed as kilograms per square kilometre per year. Sediment delivery processes are affected by a myriad of factors such as drainage area size, basin slope, climate, sediment type (lithology), vegetation cover, and human land use / management practices. The theoretical concept of the 'sediment delivery ratio' (ratio between yield and total amount of sediment eroded) captures the fact that not all of the sediment is eroded within a certain catchment that reaches out to the outlet (due to, for example, deposition on floodplains). Such storage opportunities are typically increased in catchments of larger size, thus leading to a lower yield and sediment delivery ratio.




Rivers are often managed or controlled to make them more useful, or less disruptive, to human activity.
Dams or weirs may be built to control the flow, store water, or extract energy.
Levees, known as dikes in Europe, may be built to prevent river water from flowing on floodplains or floodways.
Canals connect rivers to one another for water transfer or navigation.
River courses may be modified to improve navigation, or straightened to increase the flow rate.
River management is a continuous activity as rivers tend to 'undo' the modifications made by people. Dredged channels silt up, sluice mechanisms deteriorate with age, levees and dams may suffer seepage or catastrophic failure. The benefits sought through managing rivers may often be offset by the social and economic costs of mitigating the bad effects of such management. As an example, in parts of the developed world, rivers have been confined within channels to free up flat flood-plain land for development. Floods can inundate such development at high financial cost and often with loss of life.
Rivers are increasingly managed for habitat conservation, as they are critical for many aquatic and riparian plants, resident and migratory fishes, waterfowl, birds of prey, migrating birds, and many mammals.




Lists of rivers
List of rivers by discharge
List of rivers by length
List of rivers by continent
Drought
Fluvial
List of international border rivers
List of waterways
The Riverkeepers (book)
Salt tide
Water conflict
Crossings
Bridges
Ferries
Fords
Tunnels
Transport
River transport
Barge
Riverboat
Sailing
Towpath







Jeffrey W. Jacobs. "Rivers, Major World". Water Encyclopaedia. 
Luna B. Leopold (1994). A View of the River. Harvard University Press. ISBN 0-674-93732-5. OCLC 28889034.  — a non-technical primer on the geomorphology and hydraulics of water.
Middleton, Nick (2012). Rivers: a very short introduction. New York: Oxford University Press. ISBN 9780199588671.A lake is an area of variable size filled with water, localized in a basin, that is surrounded by land, apart from any river or other outlet that serves to feed or drain the lake. Lakes lie on land and are not part of the ocean, and therefore are distinct from lagoons, and are also larger and deeper than ponds, though there are no official or scientific definitions. Lakes can be contrasted with rivers or streams, which are usually flowing. Most lakes are fed and drained by rivers and streams.
Natural lakes are generally found in mountainous areas, rift zones, and areas with ongoing glaciation. Other lakes are found in endorheic basins or along the courses of mature rivers. In some parts of the world there are many lakes because of chaotic drainage patterns left over from the last Ice Age. All lakes are temporary over geologic time scales, as they will slowly fill in with sediments or spill out of the basin containing them.
Many lakes are artificial and are constructed for industrial or agricultural use, for hydro-electric power generation or domestic water supply, or for aesthetic or recreational purposes or even for other activities .




The word lake comes from Middle English lake ("lake, pond, waterway"), from Old English lacu ("pond, pool, stream"), from Proto-Germanic *lakō ("pond, ditch, slow moving stream"), from the Proto-Indo-European root *leǵ- ("to leak, drain"). Cognates include Dutch laak ("lake, pond, ditch"), Middle Low German lāke ("water pooled in a riverbed, puddle") as in: de:Moorlake, de:Wolfslake, de:Butterlake, German Lache ("pool, puddle"), and Icelandic lækur ("slow flowing stream"). Also related are the English words leak and leach.
There is considerable uncertainty about defining the difference between lakes and ponds, and no current internationally accepted definition of either term across scientific disciplines or political boundaries exists. For example, limnologists have defined lakes as water bodies which are simply a larger version of a pond, which can have wave action on the shoreline or where wind-induced turbulence plays a major role in mixing the water column. None of these definitions completely excludes ponds and all are difficult to measure. For this reason, simple size-based definitions are increasingly used to separate ponds and lakes. One definition of lake is a body of water of 2 hectares (5 acres) or more in area; however, others have defined lakes as waterbodies of 5 hectares (12 acres) and above, or 8 hectares (20 acres) and above  (see also the definition of "pond"). Charles Elton, one of the founders of ecology, regarded lakes as waterbodies of 40 hectares (99 acres) or more. The term lake is also used to describe a feature such as Lake Eyre, which is a dry basin most of the time but may become filled under seasonal conditions of heavy rainfall. In common usage, many lakes bear names ending with the word pond, and a lesser number of names ending with lake are in quasi-technical fact, ponds. One textbook illustrates this point with the following: "In Newfoundland, for example, almost every lake is called a pond, whereas in Wisconsin, almost every pond is called a lake."
One hydrology book proposes to define the term "lake" as a body of water with the following five characteristics:
it partially or totally fills one or several basins connected by straits
has essentially the same water level in all parts (except for relatively short-lived variations caused by wind, varying ice cover, large inflows, etc.)
it does not have regular intrusion of seawater
a considerable portion of the sediment suspended in the water is captured by the basins (for this to happen they need to have a sufficiently small inflow-to-volume ratio)
the area measured at the mean water level exceeds an arbitrarily chosen threshold (for instance, one hectare)
With the exception of the seawater intrusion criterion, the others have been accepted or elaborated upon by other hydrology publications.




The majority of lakes on Earth are fresh water, and most lie in the Northern Hemisphere at higher latitudes. Canada, with a deranged drainage system has an estimated 31,752 lakes larger than 3 square kilometres (1.2 sq mi) and an unknown total number of lakes, but is estimated to be at least 2 million. Finland has 187,888 lakes 500 square metres (5,400 sq ft) or larger, of which 56,000 are large (10,000 square metres (110,000 sq ft) or larger).
Most lakes have at least one natural outflow in the form of a river or stream, which maintain a lake's average level by allowing the drainage of excess water. Some lakes do not have a natural outflow and lose water solely by evaporation or underground seepage or both. They are termed endorheic lakes.
Many lakes are artificial and are constructed for hydro-electric power generation, aesthetic purposes, recreational purposes, industrial use, agricultural use or domestic water supply.
Evidence of extraterrestrial lakes exists; "definitive evidence of lakes filled with methane" was announced by NASA as returned by the Cassini Probe observing the moon Titan, which orbits the planet Saturn.
Globally, lakes are greatly outnumbered by ponds: of an estimated 304 million standing water bodies worldwide, 91% are 1 hectare (2.5 acres) or less in area (see definition of ponds). Small lakes are also much more numerous than large lakes: in terms of area, one-third of the world's standing water is represented by lakes and ponds of 10 hectares (25 acres) or less. However, large lakes account for much of the area of standing water with 122 large lakes of 1,000 square kilometres (390 sq mi, 100,000 ha, 247,000 acres) or more representing about 29% of the total global area of standing inland water.




Since they progressively become filled by sediment, lakes are considered ephemeral over geological time scales, and long-living lakes imply that active processes keep forming the basins in which they form. There are a number of natural processes that can form lakes.



The longest-living lakes on Earth are related to tectonic processes which is created due to a tectonic uplift of a mountain range which create depressions that accumulate water and form lakes.



Lakes can also form by means of landslides or by glacial blockages. An example of the latter occurred during the last ice age in the U.S. state of Washington, when a huge lake formed behind a glacial flow; when the ice retreated, the result was an immense flood that created the Dry Falls at Sun Lakes, Washington.



Salt lakes (also called saline lakes) can form where there is no natural outlet or where the water evaporates rapidly and the drainage surface of the water table has a higher-than-normal salt content. Examples of salt lakes include Great Salt Lake, the Aral Sea, and the Dead Sea.



Small, crescent-shaped lakes called oxbow lakes can form in river valleys as a result of meandering. The slow-moving river forms a sinuous shape as the outer side of bends are eroded away more rapidly than the inner side. Eventually a horseshoe bend is formed and the river cuts through the narrow neck. This new passage then forms the main passage for the river and the ends of the bend become silted up, thus forming a bow-shaped lake.



Crater lakes are formed in volcanic craters and calderas which fill up with precipitation more rapidly than they empty via evaporation. Sometimes the latter are called caldera lakes, although often no distinction is made. An example is Crater Lake in Oregon, in the caldera of Mount Mazama. The caldera was created in a massive volcanic eruption that led to the subsidence of Mount Mazama around 4860 BC.



The advance and retreat of glaciers can scrape depressions in the surface where water accumulates; such lakes are common in Scandinavia, Patagonia, Siberia and Canada. The most notable examples are probably the Great Lakes of North America. As a particular case, gloe lakes are basins that have emerged from the sea as a consequence of post-glacial rebound, and are now filled with freshwater.



Some lakes, such as Lake Jackson in Florida, USA, come into existence as a result of sinkhole activity.
Lake Vostok is a subglacial lake in Antarctica, possibly the largest in the world. The pressure from the ice atop it and its internal chemical composition mean that, if the lake were drilled into, a fissure could result that would spray somewhat like a geyser.
Most lakes are geologically young and shrinking since the natural results of erosion will tend to wear away the sides and fill the basin. Exceptions are those such as Lake Baikal and Lake Tanganyika that lie along continental rift zones and are created by the crust's subsidence as two plates are pulled apart. These lakes are the oldest and deepest in the world. Lake Baikal, which is 25-30 million years old, is deepening at a faster rate than it is being filled by erosion and may be destined over millions of years to become attached to the global ocean. The Red Sea, for example, is thought to have originated as a rift-valley lake.




Periglacial lake: Part of the lake's margin is formed by an ice sheet, ice cap or glacier, the ice having obstructed the natural drainage of the land.
Subglacial lake: A lake which is permanently covered by ice. They can occur under glaciers, ice caps or ice sheets. There are many such lakes, but Lake Vostok in Antarctica is by far the largest. They are kept liquid because the overlying ice acts as a thermal insulator retaining energy introduced to its underside by friction, by water percolating through crevasses, by the pressure from the mass of the ice sheet above or by geothermal heating below.
Glacial lake: a lake with origins in a melted glacier, such as a kettle lake.
Artificial lake: A lake created by flooding land behind a dam, called an impoundment or reservoir, by deliberate human excavation, or by the flooding of an excavation incident to a mineral-extraction operation such as an open pit mine or quarry. Some of the world's largest lakes are reservoirs like Hirakud Dam in India.
Endorheic lake, terminal or closed: A lake which has no significant outflow, either through rivers or underground diffusion. Any water within an endorheic basin leaves the system only through evaporation or seepage. These lakes, such as Lake Eyre in central Australia, the Aral Sea in central Asia, or the Great Salt Lake in the Western United States, are most common in deserts.
Meromictic lake: A lake which has layers of water which do not intermix. The deepest layer of water in such a lake does not contain any dissolved oxygen. The layers of sediment at the bottom of a meromictic lake remain relatively undisturbed because there are no living aerobic organisms.
Fjord lake: A lake in a glacially eroded valley that has been eroded below sea level.
Oxbow lake: A lake which is formed when a wide meander from a stream or a river is cut off to form a lake. They are called "oxbow" lakes due to the distinctive curved shape that results from this process.
Rift lake or sag pond: A lake which forms as a result of subsidence along a geological fault in the Earth's tectonic plates. Examples include the Rift Valley lakes of eastern Africa and Lake Baikal in Siberia.
Underground lake: A lake which is formed under the surface of the Earth's crust. Such a lake may be associated with caves, aquifers or springs.
Crater lake: A lake which forms in a volcanic caldera or crater after the volcano has been inactive for some time. Water in this type of lake may be fresh or highly acidic, and may contain various dissolved minerals. Some also have geothermal activity, especially if the volcano is merely dormant rather than extinct.
Lava lake: A pool of molten lava contained in a volcanic crater or other depression. Lava lakes that have partly or completely solidified are also referred to as lava lakes.
Former: A lake which is no longer in existence. Such lakes include prehistoric lakes and lakes which have permanently dried up through evaporation or human intervention. Owens Lake in California, USA, is an example of a former lake. Former lakes are a common feature of the Basin and Range area of southwestern North America.
Ephemeral lake, intermittent lake, or seasonal lake: A seasonal lake that exists as a body of water during part of the year.
Shrunken: Closely related to former lakes, a shrunken lake is one which has drastically decreased in size over geological time. Lake Agassiz, which once covered much of central North America, is a good example of a shrunken lake. Two notable remnants of this lake are Lake Winnipeg and Lake Winnipegosis.
Eolic lake: A lake which forms in a depression created by the activity of the winds.
Vlei, in South Africa, shallow lakes which vary considerably with seasons.
Epishelf lakes, unique lakes which exist on top of a dense saltwater body and are surrounded by ice. These are mostly found in the Antarctica.




Lakes have numerous features in addition to lake type, such as drainage basin (also known as catchment area), inflow and outflow, nutrient content, dissolved oxygen, pollutants, pH, and sedimentation.
Changes in the level of a lake are controlled by the difference between the input and output compared to the total volume of the lake. Significant input sources are precipitation onto the lake, runoff carried by streams and channels from the lake's catchment area, groundwater channels and aquifers, and artificial sources from outside the catchment area. Output sources are evaporation from the lake, surface and groundwater flows, and any extraction of lake water by humans. As climate conditions and human water requirements vary, these will create fluctuations in the lake level.
Lakes can be also categorized on the basis of their richness in nutrients, which typically affect plant growth. Nutrient-poor lakes are said to be oligotrophic and are generally clear, having a low concentration of plant life. Mesotrophic lakes have good clarity and an average level of nutrients. Eutrophic lakes are enriched with nutrients, resulting in good plant growth and possible algal blooms. Hypertrophic lakes are bodies of water that have been excessively enriched with nutrients. These lakes typically have poor clarity and are subject to devastating algal blooms. Lakes typically reach this condition due to human activities, such as heavy use of fertilizers in the lake catchment area. Such lakes are of little use to humans and have a poor ecosystem due to decreased dissolved oxygen.
Due to the unusual relationship between water's temperature and its density, lakes form layers called thermoclines, layers of drastically varying temperature relative to depth. Fresh water is most dense at about 4 degrees Celsius (39.2 °F) at sea level. When the temperature of the water at the surface of a lake reaches the same temperature as deeper water, as it does during the cooler months in temperate climates, the water in the lake can mix, bringing oxygen-starved water up from the depths and bringing oxygen down to decomposing sediments. Deep temperate lakes can maintain a reservoir of cold water year-round, which allows some cities to tap that reservoir for deep lake water cooling.

Since the surface water of deep tropical lakes never reaches the temperature of maximum density, there is no process that makes the water mix. The deeper layer becomes oxygen starved and can become saturated with carbon dioxide, or other gases such as sulfur dioxide if there is even a trace of volcanic activity. Exceptional events, such as earthquakes or landslides, can cause mixing which rapidly brings the deep layers up to the surface and release a vast cloud of gas which lay trapped in solution in the colder water at the bottom of the lake. This is called a limnic eruption. An example is the disaster at Lake Nyos in Cameroon. The amount of gas that can be dissolved in water is directly related to pressure. As deep water surfaces, the pressure drops and a vast amount of gas comes out of solution. Under these circumstances carbon dioxide is hazardous because it is heavier than air and displaces it, so it may flow down a river valley to human settlements and cause mass asphyxiation.
The material at the bottom of a lake, or lake bed, may be composed of a wide variety of inorganics, such as silt or sand, and organic material, such as decaying plant or animal matter. The composition of the lake bed has a significant impact on the flora and fauna found within the lake's environs by contributing to the amounts and the types of nutrients available.
A paired (black and white) layer of the varved lake sediments correspond to a year. During winter, when organisms die, carbon is deposited down, resulting to a black layer. At the same year, during summer, only few organic materials are deposited, resulting to a white layer at the lake bed. These are commonly used to track past paleontological events.
Natural lakes provide a microcosm of living and nonliving elements that are relatively independent of their surrounding environments. Therefore, lake organisms can often be studied in isolation from the lake’s surroundings.




Limnology is the study of inland bodies of water and related ecosystems. Limnology divides lakes into three zones: the littoral zone, a sloped area close to land; the photic or open-water zone, where sunlight is abundant; and the deep-water profundal or benthic zone, where little sunlight can reach. The depth to which light can reach in lakes depends on turbidity, determined by the density and size of suspended particles. A particle is in suspension if its weight is less than the random turbidity forces acting upon it. These particles can be sedimentary or biological in origin and are responsible for the color of the water. Decaying plant matter, for instance, may be responsible for a yellow or brown color, while algae may cause greenish water. In very shallow water bodies, iron oxides make water reddish brown. Biological particles include algae and detritus. Bottom-dwelling detritivorous fish can be responsible for turbid waters, because they stir the mud in search of food. Piscivorous fish contribute to turbidity by eating plant-eating (planktonivorous) fish, thus increasing the amount of algae (see aquatic trophic cascade). The light depth or transparency is measured by using a Secchi disk, a 20-cm (8 in) disk with alternating white and black quadrants. The depth at which the disk is no longer visible is the Secchi depth, a measure of transparency. The Secchi disk is commonly used to test for eutrophication. For a detailed look at these processes, see lentic ecosystems.
A lake moderates the surrounding region's temperature and climate because water has a very high specific heat capacity (4,186 J·kg−1·K−1). In the daytime a lake can cool the land beside it with local winds, resulting in a sea breeze; in the night it can warm it with a land breeze.




The lake may be infilled with deposited sediment and gradually become a wetland such as a swamp or marsh. Large water plants, typically reeds, accelerate this closing process significantly because they partially decompose to form peat soils that fill the shallows. Conversely, peat soils in a marsh can naturally burn and reverse this process to recreate a shallow lake resulting in a dynamic equilibrium between marsh and lake. This is significant since wildfire has been largely suppressed in the developed world over the past century. This has artificially converted many shallow lakes into emergent marshes. Turbid lakes and lakes with many plant-eating fish tend to disappear more slowly. A "disappearing" lake (barely noticeable on a human timescale) typically has extensive plant mats at the water's edge. These become a new habitat for other plants, like peat moss when conditions are right, and animals, many of which are very rare. Gradually the lake closes and young peat may form, forming a fen. In lowland river valleys where a river can meander, the presence of peat is explained by the infilling of historical oxbow lakes. In the very last stages of succession, trees can grow in, eventually turning the wetland into a forest.
Some lakes can disappear seasonally. These are called intermittent lakes, ephemeral lakes, or seasonal lakes and can be found in karstic terrain. A prime example of an intermittent lake is Lake Cerknica in Slovenia or Lag Prau Pulte in Graubünden. Other intermittent lakes are only the result of above-average precipitation in a closed, or endorheic basin, usually filling dry lake beds. This can occur in some of the driest places on earth, like Death Valley. This occurred in the spring of 2005, after unusually heavy rains. The lake did not last into the summer, and was quickly evaporated (see photos to right). A more commonly filled lake of this type is Sevier Lake of west-central Utah.
Sometimes a lake will disappear quickly. On 3 June 2005, in Nizhny Novgorod Oblast, Russia, a lake called Lake Beloye vanished in a matter of minutes. News sources reported that government officials theorized that this strange phenomenon may have been caused by a shift in the soil underneath the lake that allowed its water to drain through channels leading to the Oka River.
The presence of ground permafrost is important to the persistence of some lakes. According to research published in the journal Science ("Disappearing Arctic Lakes", June 2005), thawing permafrost may explain the shrinking or disappearance of hundreds of large Arctic lakes across western Siberia. The idea here is that rising air and soil temperatures thaw permafrost, allowing the lakes to drain away into the ground.
Some lakes disappear because of human development factors. The shrinking Aral Sea is described as being "murdered" by the diversion for irrigation of the rivers feeding it.




Only one world other than Earth is known to harbor large lakes, Saturn's largest moon, Titan. Photographs and spectroscopic analysis by the Cassini–Huygens spacecraft show liquid ethane on the surface, which is thought to be mixed with liquid methane. The largest Titanean lake, Kraken Mare at 400,000 km2, is three-times the size of any lake on Earth, and even the second, Ligeia Mare, is estimated to be slightly larger than Earth's Lake Michigan–Huron.
Jupiter's large moon Io is volcanically active, and as a result Sulphur deposits have accumulated on the surface. Some photographs taken during the Galileo mission appear to show lakes of liquid sulfur in volcanic caldera, though these are more analogous to lake of lava than of water on Earth.
The planet Mars is too cold and has too little atmospheric pressure to permit the pooling of liquid water. Geologic evidence appears to confirm, however, that ancient lakes once formed on the surface. It is also possible that volcanic activity on Mars will occasionally melt subsurface ice, creating large temporary lakes. This water would quickly freeze and then sublimate, unless insulated in some manner, such as by a coating of volcanic ash.
There are dark basaltic plains on the Moon, similar to lunar maria but smaller, that are called lacus (singular lacus, Latin for "lake") because they were thought by early astronomers to be lakes of water.




The largest lake by surface area is Lake Michigan-Huron, which is hydrologically a single lake. Its surface area is 45,300 sq. mi./117,400 km2. For those who consider Lake Michigan-Huron to be separate lakes, Lake Superior would be the largest at 31,700 sq. mi./82,100 km2.
The deepest lake is Lake Baikal in Siberia, with a bottom at 1,637 metres (5,371 ft). Its mean depth is also the greatest in the world (749 metres (2,457 ft)).
It is also the world's largest lake by volume (23,600 cubic kilometres (5,700 cu mi), though smaller than the Caspian Sea at 78,200 cubic kilometres (18,800 cu mi)), and the second longest (about 630 kilometres (390 mi) from tip to tip).
The longest lake is Lake Tanganyika, with a length of about 660 kilometres (410 mi) (measured along the lake's center line).
It is also the second largest by volume and second deepest (1,470 metres (4,820 ft)) in the world, after lake Baikal.
The world's oldest lake is Lake Baikal, followed by Lake Tanganyika (Tanzania). Lake Maracaibo is considered by some to be the second-oldest lake on Earth, but since it lies at sea level and nowadays is a contiguous body of water with the sea, others consider that it has turned into a small bay.
The world's highest lake, if size is not a criterion, may be the crater lake of Ojos del Salado, at 6,390 metres (20,965 ft).
The highest large (greater than 250 square kilometres (97 sq mi)) lake in the world is the 290 square kilometres (110 sq mi) Pumoyong Tso (Pumuoyong Tso), in the Tibet Autonomous Region of China, at 28-34N 90-24E, 5,018 metres (16,463 ft) above sea level.
The world's highest commercially navigable lake is Lake Titicaca in Peru and Bolivia at 3,812 m (12,507 ft). It is also the largest lake in South America.
The world's lowest lake is the Dead Sea, bordering Israel and Jordan at 418 metres (1,371 ft) below sea level. It is also one of the lakes with highest salt concentration.
Lake Michigan–Huron has the longest lake coastline in the world: about 5,250 kilometres (3,260 mi), excluding the coastline of its many inner islands. Even if it is considered two lakes, Lake Huron alone would still have the longest coastline in the world at 2,980 kilometres (1,850 mi).
The largest island in a lake is Manitoulin Island in Lake Huron, with a surface area of 2,766 square kilometres (1,068 sq mi). Lake Manitou, on Manitoulin Island, is the largest lake on an island in a lake.
The largest lake on an island is Nettilling Lake on Baffin Island, with an area of 5,542 square kilometres (2,140 sq mi) and a maximum length of 123 kilometres (76 mi).
The largest lake in the world that drains naturally in two directions is Wollaston Lake.
Lake Toba on the island of Sumatra is in what is probably the largest resurgent caldera on Earth.
The largest lake completely within the boundaries of a single city is Lake Wanapitei in the city of Sudbury, Ontario, Canada. Before the current city boundaries came into effect in 2001, this status was held by Lake Ramsey, also in Sudbury.
Lake Enriquillo in Dominican Republic is the only saltwater lake in the world inhabited by crocodiles.
Lake Bernard, Ontario, Canada, claims to be the largest lake in the world with no islands.
The largest lake in one country is Lake Michigan, in the U.S.A. However, it is sometimes considered part of Lake Michigan-Huron, making the record go to Great Bear Lake, Northwest Territories, in Canada, the largest lake within one jurisdiction.
The largest lake on an island in a lake on an island is Crater Lake on Vulcano Island in Lake Taal on the island of Luzon, The Philippines.
The northernmost named lake on Earth is Upper Dumbell Lake in the Qikiqtaaluk Region of Nunavut, Canada at a latitude of 82°28'N. It is 5.2 kilometres (3.2 mi) southwest of Alert, the northernmost settlement in the world. There are also several small lakes north of Upper Dumbell Lake, but they are all unnamed and only appear on very detailed maps.



The largest lakes (surface area) by continent are:
Australia – Lake Eyre (salt lake)
Africa – Lake Victoria, also the third-largest freshwater lake on Earth. It is one of the Great Lakes of Africa.
Antarctica – Lake Vostok (subglacial)
Asia – Lake Baikal (if the Caspian Sea is considered a lake, it is the largest in Eurasia, but is divided between the two geographic continents)
Oceania – Lake Eyre when filled; the largest permanent (and freshwater) lake in Oceania is Lake Taupo.
Europe – Lake Ladoga, followed by Lake Onega, both in northwestern Russia.
North America – Lake Michigan-Huron, which is hydrologically a single lake. However, lakes Huron and Michigan are usually considered separate lakes, in which case Lake Superior would be the largest.
South America – Lake Titicaca, which is also the highest navigable body of water on Earth at 3,812 metres (12,507 ft) above sea level. The much larger Lake Maracaibo is much older, but perceived by some to no longer be genuinely a lake for multiple reasons.









World Lake Database
Global Lake DatabaseAn ocean (from Ancient Greek Ὠκεανός, transc. Okeanós, the sea of classical antiquity) is a body of saline water that composes much of a planet's hydrosphere. On Earth, an ocean is one of the major conventional divisions of the World Ocean, which covers almost 71% of its surface. These are, in descending order by area, the Pacific, Atlantic, Indian, Southern (Antarctic), and Arctic Oceans. The word sea is often used interchangeably with "ocean" in American English but, strictly speaking, a sea is a body of saline water (generally a division of the world ocean) partly or fully enclosed by land.
Saline water covers approximately 72% of the planet's surface (~3.6×108 km2) and is customarily divided into several principal oceans and smaller seas, with the ocean covering approximately 71% of Earth's surface and 90% of the Earth's biosphere. The ocean contains 97% of Earth's water, and oceanographers have stated that less than 5% of the World Ocean has been explored. The total volume is approximately 1.35 billion cubic kilometers (320 million cu mi) with an average depth of nearly 3,700 meters (12,100 ft).
As the world ocean is the principal component of Earth's hydrosphere, it is integral to all known life, forms part of the carbon cycle, and influences climate and weather patterns. The world ocean is the habitat of 230,000 known species, but because much of it is unexplored, the number of species that exist is much larger, possibly over two million. The origin of Earth's oceans remains unknown; oceans are thought to have formed in the Hadean period and may have been the impetus for the emergence of life.
Extraterrestrial oceans may be composed of water or other elements and compounds. The only confirmed large stable bodies of extraterrestrial surface liquids are the lakes of Titan, although there is evidence for the existence of oceans elsewhere in the Solar System. Early in their geologic histories, Mars and Venus are theorized to have had large water oceans. The Mars ocean hypothesis suggests that nearly a third of the surface of Mars was once covered by water, and a runaway greenhouse effect may have boiled away the global ocean of Venus. Compounds such as salts and ammonia dissolved in water lower its freezing point so that water might exist in large quantities in extraterrestrial environments as brine or convecting ice. Unconfirmed oceans are speculated beneath the surface of many dwarf planets and natural satellites; notably, the ocean of Europa is estimated to have over twice the water volume of Earth. The Solar System's giant planets are also thought to have liquid atmospheric layers of yet to be confirmed compositions. Oceans may also exist on exoplanets and exomoons, including surface oceans of liquid water within a circumstellar habitable zone. Ocean planets are a hypothetical type of planet with a surface completely covered with liquid.



The word « ocean » comes from the figure in classical antiquity, Oceanus (/oʊˈsiːənəs/; Greek: Ὠκεανός Ōkeanós, pronounced [ɔːkeanós]), the elder of the Titans in classical Greek mythology, believed by the ancient Greeks and Romans to be the divine personification of the sea, an enormous river encircling the world.







Though generally described as several separate oceans, these waters comprise one global, interconnected body of salt water sometimes referred to as the World Ocean or global ocean. This concept of a continuous body of water with relatively free interchange among its parts is of fundamental importance to oceanography.
The major oceanic divisions – listed below in descending order of area and volume – are defined in part by the continents, various archipelagos, and other criteria.

Oceans are fringed by smaller, adjoining bodies of water such as seas, gulfs, bays, bights, and straits.




The Mid-Oceanic Ridge of the World are connected and form the Ocean Ridge, a single global mid-oceanic ridge system that is part of every ocean, making it the longest mountain range in the world. The continuous mountain range is 65,000 km (40,400 mi) long (several times longer than the Andes, the longest continental mountain range), and the total length of the oceanic ridge system is 80,000 km (49,700 mi) long.




The total mass of the hydrosphere is about 1.4 quintillion metric tons (7018140000000000000♠1.4×1018 long tons or 7018150000000000000♠1.5×1018 short tons), which is about 0.023% of Earth's total mass. Less than 3% is freshwater; the rest is saltwater, almost all of which is in the ocean. The area of the World Ocean is about 361.9 million square kilometers (139.7 million square miles), which covers about 70.9% of Earth's surface, and its volume is approximately 1.335 billion cubic kilometers (320.3 million cubic miles). This can be thought of as a cube of water with an edge length of 1,101 kilometers (684 mi). Its average depth is about 3,688 meters (12,100 ft), and its maximum depth is 10,994 meters (6.831 mi) at the Mariana Trench. Nearly half of the world's marine waters are over 3,000 meters (9,800 ft) deep. The vast expanses of deep ocean (anything below 200 meters or 660 feet) cover about 66% of Earth's surface. This does not include seas not connected to the World Ocean, such as the Caspian Sea.
The bluish color of water is a composite of several contributing agents. Prominent contributors include dissolved organic matter and chlorophyll. Mariners and other seafarers have reported that the ocean often emits a visible glow which extends for miles at night. In 2005, scientists announced that for the first time, they had obtained photographic evidence of this glow. It is most likely caused by bioluminescence.




Oceanographers divide the ocean into different zones by physical and biological conditions. The pelagic zone includes all open ocean regions, and can be divided into further regions categorized by depth and light abundance. The photic zone includes the oceans from the surface to a depth of 200 m; it is the region where photosynthesis can occur and is, therefore, the most biodiverse. Because plants require photosynthesis, life found deeper than the photic zone must either rely on material sinking from above (see marine snow) or find another energy source. Hydrothermal vents are the primary source of energy in what is known as the aphotic zone (depths exceeding 200 m). The pelagic part of the photic zone is known as the epipelagic.
The pelagic part of the aphotic zone can be further divided into vertical regions according to temperature. The mesopelagic is the uppermost region. Its lowermost boundary is at a thermocline of 12 °C (54 °F), which, in the tropics generally lies at 700–1,000 meters (2,300–3,300 ft). Next is the bathypelagic lying between 10 and 4 °C (50 and 39 °F), typically between 700–1,000 meters (2,300–3,300 ft) and 2,000–4,000 meters (6,600–13,100 ft), lying along the top of the abyssal plain is the abyssopelagic, whose lower boundary lies at about 6,000 meters (20,000 ft). The last zone includes the deep oceanic trench, and is known as the hadalpelagic. This lies between 6,000–11,000 meters (20,000–36,000 ft) and is the deepest oceanic zone.
The benthic zones are aphotic and correspond to the three deepest zones of the deep-sea. The bathyal zone covers the continental slope down to about 4,000 meters (13,000 ft). The abyssal zone covers the abyssal plains between 4,000 and 6,000 m. Lastly, the hadal zone corresponds to the hadalpelagic zone, which is found in oceanic trenches.
The pelagic zone can be further subdivided into two subregions: the neritic zone and the oceanic zone. The neritic zone encompasses the water mass directly above the continental shelves whereas the oceanic zone includes all the completely open water.
In contrast, the littoral zone covers the region between low and high tide and represents the transitional area between marine and terrestrial conditions. It is also known as the intertidal zone because it is the area where tide level affects the conditions of the region.
The ocean can be divided into three density zones: the surface zone, the pycnocline, and the deep zone. The surface zone, also called the mixed layer, refers to the uppermost density zone of the ocean. Temperature and salinity are relatively constant with depth in this zone due to currents and wave action. The surface zone contains ocean water that is in contact with the atmosphere and within the photic zone. The surface zone has the ocean's least dense water and represents approximately 2% of the total volume of ocean water. The surface zone usually ranges between depths of 500 feet to 3,300 feet below ocean surface, but this can vary a great deal. In some cases, the surface zone can be entirely non-existent. The surface zone is typically thicker in the tropics than in regions of higher latitude. The transition to colder, denser water is more abrupt in the tropics than in regions of higher latitudes. The pycnocline refers to a zone wherein density substantially increases with depth due primarily to decreases in temperature. The pycnocline effectively separates the lower-density surface zone above from the higher-density deep zone below. The pycnocline represents approximately 18% of the total volume of ocean water. The deep zone refers to the lowermost density zone of the ocean. The deep zone usually begins at depths below 3,300 feet in mid-latitudes. The deep zone undergoes negligible changes in water density with depth. The deep zone represents approximately 80% of the total volume of ocean water. The deep zone contains relatively colder and stable water.
If a zone undergoes dramatic changes in temperature with depth, it contains a thermocline. The tropical thermocline is typically deeper than the thermocline at higher latitudes. Polar waters, which receive relatively little solar energy, are not stratified by temperature and generally lack a thermocline because surface water at polar latitudes are nearly as cold as water at greater depths. Below the thermocline, water is very cold, ranging from −1 °C to 3 °C. Because this deep and cold layer contains the bulk of ocean water, the average temperature of the world ocean is 3.9 °C. If a zone undergoes dramatic changes in salinity with depth, it contains a halocline. If a zone undergoes a strong, vertical chemistry gradient with depth, it contains a chemocline.
The halocline often coincides with the thermocline, and the combination produces a pronounced pycnocline.




Ocean travel by boat dates back to prehistoric times, but only in modern times has extensive underwater travel become possible.
The deepest point in the ocean is the Mariana Trench, located in the Pacific Ocean near the Northern Mariana Islands. Its maximum depth has been estimated to be 10,971 meters (35,994 ft) (plus or minus 11 meters; see the Mariana Trench article for discussion of the various estimates of the maximum depth.) The British naval vessel Challenger II surveyed the trench in 1951 and named the deepest part of the trench the "Challenger Deep". In 1960, the Trieste successfully reached the bottom of the trench, manned by a crew of two men.




Oceanic maritime currents have different origins. Tidal currents are in phase with the tide, hence are quasiperiodic, they may fomulate various knots in certain places, most notably around headlands. Non periodic currents have for origin the waves, wind and different densities.
The wind and waves create surface currents (designated as « drift currents »). These currents can decompose in one quasi permanent current (which varies within the hourly scale) and one movement of Stokes drift under the effect of rapid waves movement (at the echelon of a couple of seconds).). The quasi permanent current is accelerated by the breaking of waves, and in a lesser governing effect, by the friction of the wind on the surface.
This acceleration of the current takes place in the direction of waves and dominant wind. Accordingly, when the sea depth increases, the rotation of the earth changes the direction of currents, in proportion with the increase of depth while friction lowers their speed. At a certain sea depth, the current changes direction and is seen inverted in the opposite direction with speed current becoming nul: known as the Ekman spiral. The influence of these currents is mainly experienced at the mixed layer of the ocean surface, often from 400 to 800 meters of maximum depth. These currents can considerably alter, change and are dependent on the various yearly seasons. If the mixed layer is less thick (10 to 20 meters), the quasi permanent current at the surface adopts an extreme oblique direction in relation to the direction of the wind, becoming virtually homogeneous, until the Thermocline.
In the deep however, maritime currents are caused by the temperature gradients and the salinity between water density masses.
In Littoral zones, Breaking wave is so intense and the depth measurement so low, that maritime currents reach often 1 to 2 knots.




Ocean currents greatly affect Earth's climate by transferring heat from the tropics to the polar regions. Transferring warm or cold air and precipitation to coastal regions, winds may carry them inland. Surface heat and freshwater fluxes create global density gradients that drive the thermohaline circulation part of large-scale ocean circulation. It plays an important role in supplying heat to the polar regions, and thus in sea ice regulation. Changes in the thermohaline circulation are thought to have significant impacts on Earth's energy budget. In so far as the thermohaline circulation governs the rate at which deep waters reach the surface, it may also significantly influence atmospheric carbon dioxide concentrations.
For a discussion of the possibilities of changes to the thermohaline circulation under global warming, see shutdown of thermohaline circulation.
It is often stated that the thermohaline circulation is the primary reason that the climate of Western Europe is so temperate. An alternate hypothesis claims that this is largely incorrect, and that Europe is warm mostly because it lies downwind of an ocean basin, and because atmospheric waves bring warm air north from the subtropics.
The Antarctic Circumpolar Current encircles that continent, influencing the area's climate and connecting currents in several oceans.
One of the most dramatic forms of weather occurs over the oceans: tropical cyclones (also called "typhoons" and "hurricanes" depending upon where the system forms).




The ocean has a significant effect on the biosphere. Oceanic evaporation, as a phase of the water cycle, is the source of most rainfall, and ocean temperatures determine climate and wind patterns that affect life on land. Life within the ocean evolved 3 billion years prior to life on land. Both the depth and the distance from shore strongly influence the biodiversity of the plants and animals present in each region.
Lifeforms native to the ocean include:
Fish;
Radiata, such as jellyfish (Cnidaria);
Cetacea, such as whales, dolphins, and porpoises;
Cephalopods, such as octopus and squid;
Crustaceans, such as lobsters, shrimp, and krill;
Marine worms;
Plankton; and
Echinoderms, such as brittle stars, starfish, sea cucumbers, and sand dollars.
In addition, many land animals have adapted to living a major part of their life on the oceans. For instance, seabirds are a diverse group of birds that have adapted to a life mainly on the oceans. They feed on marine animals and spend most of their lifetime on water, many only going on land for breeding. Other birds that have adapted to oceans as their living space are penguins, seagulls and pelicans. Seven species of turtles, the sea turtles, also spend most of their time in the oceans.









The residence time is the amount of an element in the ocean divided by the rate at which that element is added to (or removed from) the ocean.
The mean oceanic mixing time is thought to be approximately 1,600 years. If a given element in the ocean stays in the ocean, on average, longer than the oceanic mixing time, then that element is assumed to be homogeneously spread throughout the ocean. As a result, because the major salts have a residence time that is longer than 1,600 years, the ratio of major salts is thought to be unchanging across the ocean. This constant ratio is often referred to as Forchhammer's principle or the principle of constant proportions.



A zone of rapid salinity increase with depth is called a halocline. The temperature of maximum density of seawater decreases as its salt content increases. Freezing temperature of water decreases with salinity, and boiling temperature of water increases with salinity. Typical seawater freezes at around −1.9 °C at atmospheric pressure. If precipitation exceeds evaporation, as is the case in polar and temperate regions, salinity will be lower. If evaporation exceeds precipitation, as is the case in tropical regions, salinity will be higher. Thus, oceanic waters in polar regions have lower salinity content than oceanic waters in temperate and tropical regions.
Salinity can be calculated using the chlorinity, which is a measure of the total mass of halogen ions (includes fluorine, chlorine, bromine, and iodine) in seawater. By international agreement, the following formula is used to determine salinity:
Salinity (in ‰)=1.80655 x Chlorinity (in ‰)
The average chlorinity is about 19.2‰, and, thus, the average salinity is around 34.7‰ 






The oceans are essential to transportation. This is because most of the world's goods move by ship between the world's seaports. Oceans are also the major supply source for the fishing industry. Some of the major harvests are shrimp, fish, crabs, and lobster.




The motions of the ocean surface, known as undulations or waves, are the partial and alternate rising and falling of the ocean surface. The series of mechanical waves that propagate along the interface between water and air is called swell.




Although Earth is the only known planet with large stable bodies of liquid water on its surface and the only one in the Solar System, other celestial bodies are thought to have large oceans.



The gas giants, Jupiter and Saturn, are thought to lack surfaces and instead have a stratum of liquid hydrogen, however their planetary geology is not well understood. The possibility of the ice giants Uranus and Neptune having hot, highly compressed, supercritical water under their thick atmospheres has been hypothesised. Although their composition is still not fully understood, a 2006 study by Wiktorowicz and Ingersall ruled out the possibility of such a water "ocean" existing on Neptune, though some studies have suggested that exotic oceans of liquid diamond are possible.
The Mars ocean hypothesis suggests that nearly a third of the surface of Mars was once covered by water, though the water on Mars is no longer oceanic (much of it residing in the ice caps). The possibility continues to be studied along with reasons for their apparent disappearance. Astronomers think that Venus had liquid water and perhaps oceans in its very early history. If they existed, all later vanished via resurfacing.



A global layer of liquid water thick enough to decouple the crust from the mantle is thought to be present on the natural satellites Titan, Europa, Enceladus and, with less certainty, Callisto, Ganymede and Triton. A magma ocean is thought to be present on Io. Geysers have been found on Saturn's moon Enceladus, possibly originating from about 10 kilometers (6.2 mi) deep ocean beneath an ice shell. Other icy moons may also have internal oceans, or may once have had internal oceans that have now frozen.
Large bodies of liquid hydrocarbons are thought to be present on the surface of Titan, although they are not large enough to be considered oceans and are sometimes referred to as lakes or seas. The Cassini–Huygens space mission initially discovered only what appeared to be dry lakebeds and empty river channels, suggesting that Titan had lost what surface liquids it might have had. Cassini's more recent fly-by of Titan offers radar images that strongly suggest hydrocarbon lakes exist near the colder polar regions. Titan is thought to have a subsurface liquid-water ocean under the ice and hydrocarbon mix that forms its outer crust.




Ceres appears to be differentiated into a rocky core and icy mantle and may harbour a liquid-water ocean under its surface.
Not enough is known of the larger trans-Neptunian objects to determine whether they are differentiated bodies capable of supporting oceans, although models of radioactive decay suggest that Pluto, Eris, Sedna, and Orcus have oceans beneath solid icy crusts approximately 100 to 180 km thick.




Some planets and natural satellites outside the Solar System are likely to have oceans, including possible water ocean planets similar to Earth in the habitable zone or "liquid-water belt". The detection of oceans, even through the spectroscopy method, however is likely extremely difficult and inconclusive.
Theoretical models have been used to predict with high probability that GJ 1214 b, detected by transit, is composed of exotic form of ice VII, making up 75% of its mass, making it an ocean planet.
Other possible candidates are merely speculated based on their mass and position in the habitable zone include planet though little is actually known of their composition. Some scientists speculate Kepler-22b may be an "ocean-like" planet. Models have been proposed for Gliese 581 d that could include surface oceans. Gliese 436 b is speculated to have an ocean of "hot ice". Exomoons orbiting planets, particularly gas giants within their parent star's habitable zone may theoretically have surface oceans.
Terrestrial planets will acquire water during their accretion, some of which will be buried in the magma ocean but most of it will go into a steam atmosphere, and when the atmosphere cools it will collapse on to the surface forming an ocean. There will also be outgassing of water from the mantle as the magma solidifies—this will happen even for planets with a low percentage of their mass composed of water, so "super-Earth exoplanets may be expected to commonly produce water oceans within tens to hundreds of millions of years of their last major accretionary impact."



Oceans, seas, lakes and other bodies of liquids can be composed of liquids other than water, for example the hydrocarbon lakes on Titan. The possibility of seas of nitrogen on Triton was also considered but ruled out. There is evidence that the icy surfaces of the moons Ganymede, Callisto, Europa, Titan and Enceladus are shells floating on oceans of very dense liquid water or water–ammonia. Earth is often called the ocean planet because it is 70% covered in water. Extrasolar terrestrial planets that are extremely close to their parent star will be tidally locked and so one half of the planet will be a magma ocean. It is also possible that terrestrial planets had magma oceans at some point during their formation as a result of giant impacts. Hot Neptunes close to their star could lose their atmospheres via hydrodynamic escape, leaving behind their cores with various liquids on the surface. Where there are suitable temperatures and pressures, volatile chemicals that might exist as liquids in abundant quantities on planets include ammonia, argon, carbon disulfide, ethane, hydrazine, hydrogen, hydrogen cyanide, hydrogen sulfide, methane, neon, nitrogen, nitric oxide, phosphine, silane, sulfuric acid, and water.
Supercritical fluids, although not liquids, do share various properties with liquids. Underneath the thick atmospheres of the planets Uranus and Neptune, it is expected that these planets are composed of oceans of hot high-density fluid mixtures of water, ammonia and other volatiles. The gaseous outer layers of Jupiter and Saturn transition smoothly into oceans of supercritical hydrogen. The atmosphere of Venus is 96.5% carbon dioxide, which is a supercritical fluid at its surface.









Matthias Tomczak and J. Stuart Godfrey. 2003. Regional Oceanography: an Introduction. (see the site)
Pope, F. 2009. From eternal darkness springs cast of angels and jellied jewels. in The Times. November 23. 2009 p. 16–17.



Oceans at DMOZ
Smithsonian Ocean Portal
NOAA – National Oceanic and Atmospheric Administration – Ocean
Ocean :: Science Daily
Ocean-bearing Planets: Looking For Extraterrestrial Life In All The Right Places
Titan Likely To Have Huge Underground Ocean | Mind Blowing Science
Origins of the oceans and continents". UN Atlas of the Oceans.The Pacific Ocean is the largest of the Earth's oceanic divisions. It extends from the Arctic Ocean in the north to the Southern Ocean (or, depending on definition, to Antarctica) in the south and is bounded by Asia and Australia in the west and the Americas in the east.
At 165.25 million square kilometers (63.8 million square miles) in area, this largest division of the World Ocean—and, in turn, the hydrosphere—covers about 46% of the Earth's water surface and about one-third of its total surface area, making it larger than all of the Earth's land area combined.
The equator subdivides it into the North Pacific Ocean and South Pacific Ocean, with two exceptions: the Galápagos and Gilbert Islands, while straddling the equator, are deemed wholly within the South Pacific. The Mariana Trench in the western North Pacific is the deepest point in the world, reaching a depth of 10,911 metres (35,797 ft).
Both the center of the Water Hemisphere and the Western Hemisphere are in the Pacific Ocean.
Though the peoples of Asia and Oceania have travelled the Pacific Ocean since prehistoric times, the eastern Pacific was first sighted by Europeans in the early 16th century when Spanish explorer Vasco Núñez de Balboa crossed the Isthmus of Panama in 1513 and discovered the great "southern sea" which he named Mar del Sur. The ocean's current name was coined by Portuguese explorer Ferdinand Magellan during the Spanish circumnavigation of the world in 1521, as he encountered favourable winds on reaching the ocean. He called it Mar Pacífico, which in both Portuguese and Spanish means "peaceful sea".







Important human migrations occurred in the Pacific in prehistoric times. About 3000 BC, the Austronesian peoples on the island of Taiwan mastered the art of long-distance canoe travel and spread themselves and their languages south to the Philippines, Indonesia, and maritime Southeast Asia; west towards Madagascar; southeast towards New Guinea and Melanesia (intermarrying with native Papuans); and east to the islands of Micronesia, Oceania and Polynesia.
Long-distance trade developed all along the coast from Mozambique to Japan. Trade, and therefore knowledge, extended to the Indonesian islands but apparently not Australia. By at least 878 when there was a significant Islamic settlement in Canton much of this trade was controlled by Arabs or Muslims. In 219 BC Xu Fu sailed out into the Pacific searching for the elixir of immortality. From 1404 to 1433 Zheng He led expeditions into the Indian Ocean.




The first contact of European navigators with the western edge of the Pacific Ocean was made by the Portuguese expeditions of António de Abreu and Francisco Serrão to the Maluku Islands in 1512, and with Jorge Álvares's expedition to southern China in 1513, both ordered by Afonso de Albuquerque.
The east side of the ocean was discovered by Spanish explorer Vasco Núñez de Balboa in 1513 after his expedition crossed the Isthmus of Panama and reached a new ocean. He named it Mar del Sur (literally, "Sea of the South" or "South Sea") because the ocean was to the south of the coast of the isthmus where he first observed the Pacific.
Later, Portuguese explorer Ferdinand Magellan sailed the Pacific on a Castilian (Spanish) expedition of world circumnavigation starting in 1519. Magellan called the ocean Pacífico (or "Pacific" meaning, "peaceful") because, after sailing through the stormy seas off Cape Horn, the expedition found calm waters. The ocean was often called the Sea of Magellan in his honor until the eighteenth century. Although Magellan himself died in the Philippines in 1521, Spanish Basque navigator Juan Sebastián Elcano led the expedition back to Spain across the Indian Ocean and round the Cape of Good Hope, completing the first world circumnavigation in 1522. Sailing around and east of the Moluccas, between 1525 and 1527, Portuguese expeditions discovered the Caroline Islands and Papua New Guinea. In 1542–43 the Portuguese also reached Japan.
In 1564, five Spanish ships consisting of 379 explorers crossed the ocean from Mexico led by Miguel López de Legazpi and sailed to the Philippines and Mariana Islands. For the remainder of the 16th century, Spanish influence was paramount, with ships sailing from Mexico and Peru across the Pacific Ocean to the Philippines, via Guam, and establishing the Spanish East Indies. The Manila galleons operated for two and a half centuries linking Manila and Acapulco, in one of the longest trade routes in history. Spanish expeditions also discovered Tuvalu, the Marquesas, the Cook Islands, the Solomon Islands, and the Admiralty Islands in the South Pacific.
Later, in the quest for Terra Australis (i.e., "the [great] Southern Land"), Spanish explorers in the 17th century discovered the Pitcairn and Vanuatu archipelagos, and sailed the Torres Strait between Australia and New Guinea, named after navigator Luís Vaz de Torres. Dutch explorers, sailing around southern Africa, also engaged in discovery and trade; Abel Janszoon Tasman discovered Tasmania and New Zealand in 1642.
In the 16th and 17th century Spain considered the Pacific Ocean a Mare clausum—a sea closed to other naval powers. As the only known entrance from the Atlantic the Strait of Magellan was at times patrolled by fleets sent to prevent entrance of non-Spanish ships. On the western end of the Pacific Ocean the Dutch threatened the Spanish Philippines.
The 18th century marked the beginning of major exploration by the Russians in Alaska and the Aleutian Islands. Spain also sent expeditions to the Pacific Northwest reaching Vancouver Island in southern Canada, and Alaska. The French explored and settled Polynesia, and the British made three voyages with James Cook to the South Pacific and Australia, Hawaii, and the North American Pacific Northwest. In 1768, Pierre-Antoine Véron, a young astronomer accompanying Louis Antoine de Bougainville on his voyage of exploration, established the width of the Pacific with precision for the first time in history. One of the earliest voyages of scientific exploration was organized by Spain in the Malaspina Expedition of 1789–1794. It sailed vast areas of the Pacific, from Cape Horn to Alaska, Guam and the Philippines, New Zealand, Australia, and the South Pacific.




Growing imperialism during the 19th century resulted in the occupation of much of Oceania by other European powers, and later, Japan and the United States. Significant contributions to oceanographic knowledge were made by the voyages of HMS Beagle in the 1830s, with Charles Darwin aboard; HMS Challenger during the 1870s; the USS Tuscarora (1873–76); and the German Gazelle (1874–76).

In Oceania, France got a leading position as imperial power after making Tahiti and New Caledonia protectorates in 1842 and 1853 respectively. After navy visits to Easter Island in 1875 and 1887, Chilean navy officer Policarpo Toro managed to negotiate an incorporation of the island into Chile with native Rapanui in 1888. By occupying Easter Island, Chile joined the imperial nations. By 1900 nearly all Pacific islands were in control of Britain, France, United States, Germany, Japan, and Chile.
Although the United States gained control of Guam and the Philippines from Spain in 1898, Japan controlled most of the western Pacific by 1914 and occupied many other islands during World War II. However, by the end of that war, Japan was defeated and the U.S. Pacific Fleet was the virtual master of the ocean. Since the end of World War II, many former colonies in the Pacific have become independent states.




The Pacific separates Asia and Australia from the Americas. It may be further subdivided by the equator into northern (North Pacific) and southern (South Pacific) portions. It extends from the Antarctic region in the South to the Arctic in the north. The Pacific Ocean encompasses approximately one-third of the Earth's surface, having an area of 165.2 million square kilometers (63.8 million square miles)—significantly larger than Earth's entire landmass of some 150 million square kilometers (58 million square miles).
Extending approximately 15,500 km (9,600 mi) from the Bering Sea in the Arctic to the northern extent of the circumpolar Southern Ocean at 60°S (older definitions extend it to Antarctica's Ross Sea), the Pacific reaches its greatest east-west width at about 5°N latitude, where it stretches approximately 19,800 km (12,300 mi) from Indonesia to the coast of Colombia—halfway around the world, and more than five times the diameter of the Moon. The lowest known point on Earth—the Mariana Trench—lies 10,911 m (35,797 ft; 5,966 fathoms) below sea level. Its average depth is 4,280 m (14,040 ft; 2,340 fathoms), putting the total water volume at 710,000,000 cubic kilometers.
Due to the effects of plate tectonics, the Pacific Ocean is currently shrinking by roughly 2.5 centimetres (0.98 in) per year on three sides, roughly averaging 0.52 square kilometres (0.20 sq mi) a year. By contrast, the Atlantic Ocean is increasing in size.
Along the Pacific Ocean's irregular western margins lie many seas, the largest of which are the Celebes Sea, Coral Sea, East China Sea, Philippine Sea, Sea of Japan, South China Sea, Sulu Sea, Tasman Sea, and Yellow Sea. The Indonesian Seaway (including the Strait of Malacca and Torres Strait) joins the Pacific and the Indian Ocean to the west, and Drake Passage and the Strait of Magellan link the Pacific with the Atlantic Ocean on the east. To the north, the Bering Strait connects the Pacific with the Arctic Ocean.

As the Pacific straddles the 180th meridian, the West Pacific (or western Pacific, near Asia) is in the Eastern Hemisphere, while the East Pacific (or eastern Pacific, near the Americas) is in the Western Hemisphere.
The Southern Pacific Ocean harbors the Southeast Indian Ridge crossing from south of Australia turning into the Pacific-Antarctic Ridge (north of the South Pole) and merges with another ridge (south of South American) to form the East Pacific Rise which also connects with another ridge (south of North America) which overlooks the Juan de Fuca Ridge.
For most of Magellan's voyage from the Strait of Magellan to the Philippines, the explorer indeed found the ocean peaceful. However, the Pacific is not always peaceful. Many tropical storms batter the islands of the Pacific. The lands around the Pacific Rim are full of volcanoes and often affected by earthquakes. Tsunamis, caused by underwater earthquakes, have devastated many islands and in some cases destroyed entire towns.
The Martin Waldseemüller map of 1507 was the first to show the Americas separating two distinct oceans. Later, the Diogo Ribeiro map of 1529 was the first to show the Pacific at about its proper size.







1 The status of Taiwan and China is disputed. For more information, see political status of Taiwan.







The islands entirely within the Pacific Ocean can be divided into three main groups known as Micronesia, Melanesia and Polynesia. Micronesia, which lies north of the equator and west of the International Date Line, includes the Mariana Islands in the northwest, the Caroline Islands in the center, the Marshall Islands to the west and the islands of Kiribati in the southwest.
Melanesia, to the southwest, includes New Guinea, the world's second largest island after Greenland and by far the largest of the Pacific islands. The other main Melanesian groups from north to south are the Bismarck Archipelago, the Solomon Islands, Santa Cruz, Vanuatu, Fiji and New Caledonia.
The largest area, Polynesia, stretching from Hawaii in the north to New Zealand in the south, also encompasses Tuvalu, Tokelau, Samoa, Tonga and the Kermadec Islands to the west, the Cook Islands, Society Islands and Austral Islands in the center, and the Marquesas Islands, Tuamotu, Mangareva Islands, and Easter Island to the east.
Islands in the Pacific Ocean are of four basic types: continental islands, high islands, coral reefs and uplifted coral platforms. Continental islands lie outside the andesite line and include New Guinea, the islands of New Zealand, and the Philippines. Some of these islands are structurally associated with nearby continents. High islands are of volcanic origin, and many contain active volcanoes. Among these are Bougainville, Hawaii, and the Solomon Islands.
The coral reefs of the South Pacific are low-lying structures that have built up on basaltic lava flows under the ocean's surface. One of the most dramatic is the Great Barrier Reef off northeastern Australia with chains of reef patches. A second island type formed of coral is the uplifted coral platform, which is usually slightly larger than the low coral islands. Examples include Banaba (formerly Ocean Island) and Makatea in the Tuamotu group of French Polynesia.




The volume of the Pacific Ocean, representing about 50.1 percent of the world's oceanic water, has been estimated at some 714 million cubic kilometres (171 million cubic miles). Surface water temperatures in the Pacific can vary from −1.4 °C (29.5 °F), the freezing point of sea water, in the poleward areas to about 30 °C (86 °F) near the equator. Salinity also varies latitudinally, reaching a maximum of 37 parts per thousand in the southeastern area. The water near the equator, which can have a salinity as low as 34 parts per thousand, is less salty than that found in the mid-latitudes because of abundant equatorial precipitation throughout the year. The lowest counts of less than 32 parts per thousand are found in the far north as less evaporation of seawater takes place in these frigid areas. The motion of Pacific waters is generally clockwise in the Northern Hemisphere (the North Pacific gyre) and counter-clockwise in the Southern Hemisphere. The North Equatorial Current, driven westward along latitude 15°N by the trade winds, turns north near the Philippines to become the warm Japan or Kuroshio Current.
Turning eastward at about 45°N, the Kuroshio forks and some water moves northward as the Aleutian Current, while the rest turns southward to rejoin the North Equatorial Current. The Aleutian Current branches as it approaches North America and forms the base of a counter-clockwise circulation in the Bering Sea. Its southern arm becomes the chilled slow, south-flowing California Current. The South Equatorial Current, flowing west along the equator, swings southward east of New Guinea, turns east at about 50°S, and joins the main westerly circulation of the South Pacific, which includes the Earth-circling Antarctic Circumpolar Current. As it approaches the Chilean coast, the South Equatorial Current divides; one branch flows around Cape Horn and the other turns north to form the Peru or Humboldt Current.




The climate patterns of the Northern and Southern Hemispheres generally mirror each other. The trade winds in the southern and eastern Pacific are remarkably steady while conditions in the North Pacific are far more varied with, for example, cold winter temperatures on the east coast of Russia contrasting with the milder weather off British Columbia during the winter months due to the preferred flow of ocean currents.
In the tropical and subtropical Pacific, the El Niño Southern Oscillation (ENSO) affects weather conditions. To determine the phase of ENSO, the most recent three-month sea surface temperature average for the area approximately 3,000 kilometres (1,900 mi) to the southeast of Hawaii is computed, and if the region is more than 0.5 °C (0.9 °F) above or below normal for that period, then an El Niño or La Niña is considered in progress.
In the tropical western Pacific, the monsoon and the related wet season during the summer months contrast with dry winds in the winter which blow over the ocean from the Asian landmass. Worldwide, tropical cyclone activity peaks in late summer, when the difference between temperatures aloft and sea surface temperatures is the greatest. However, each particular basin has its own seasonal patterns. On a worldwide scale, May is the least active month, while September is the most active month. November is the only month in which all the tropical cyclone basins are active. The Pacific hosts the two most active tropical cyclone basins, which are the northwestern Pacific and the eastern Pacific. Pacific hurricanes form south of Mexico, sometimes striking the western Mexican coast and occasionally the southwestern United States between June and October, while typhoons forming in the northwestern Pacific moving into southeast and east Asia from May to December. Tropical cyclones also form in the South Pacific basin, where they occasionally impact island nations.
In the arctic, icing from October to May can present a hazard for shipping while persistent fog occurs from June to December. A climatological low in the Gulf of Alaska keeps the southern coast wet and mild during the winter months. The Westerlies and associated jet stream within the Mid-Latitudes can be particularly strong, especially in the Southern Hemisphere, due to the temperature difference between the tropics and Antarctica, which records the coldest temperature readings on the planet. In the Southern hemisphere, because of the stormy and cloudy conditions associated with extratropical cyclones riding the jet stream, it is usual to refer to the Westerlies as the Roaring Forties, Furious Fifties and Shrieking Sixties according to the varying degrees of latitude.




The ocean was first mapped by Abraham Ortelius; he called it Maris Pacifici following Ferdinand Magellan's description of it as "a pacific sea" during his circumnavigation from 1519 to 1522. To Magellan, it seemed much more calm (pacific) than the Atlantic.
The andesite line is the most significant regional distinction in the Pacific. A petrologic boundary, it separates the deeper, mafic igneous rock of the Central Pacific Basin from the partially submerged continental areas of felsic igneous rock on its margins. The andesite line follows the western edge of the islands off California and passes south of the Aleutian arc, along the eastern edge of the Kamchatka Peninsula, the Kuril Islands, Japan, the Mariana Islands, the Solomon Islands, and New Zealand's North Island.
The dissimilarity continues northeastward along the western edge of the Andes Cordillera along South America to Mexico, returning then to the islands off California. Indonesia, the Philippines, Japan, New Guinea, and New Zealand lie outside the andesite line.
Within the closed loop of the andesite line are most of the deep troughs, submerged volcanic mountains, and oceanic volcanic islands that characterize the Pacific basin. Here basaltic lavas gently flow out of rifts to build huge dome-shaped volcanic mountains whose eroded summits form island arcs, chains, and clusters. Outside the andesite line, volcanism is of the explosive type, and the Pacific Ring of Fire is the world's foremost belt of explosive volcanism. The Ring of Fire is named after the several hundred active volcanoes that sit above the various subduction zones.
The Pacific Ocean is the only ocean which is almost totally bounded by subduction zones. Only the Antarctic and Australian coasts have no nearby subduction zones.



The Pacific Ocean was born 750 million years ago at the breakup of Rodinia, although it is generally called the Panthalassic Ocean until the breakup of Pangea, about 200 million years ago. The oldest Pacific Ocean floor is only around 180 Ma old, with older crust subducted by now.



The Pacific Ocean contains several long seamount chains, formed by hotspot volcanism. These include the Hawaiian–Emperor seamount chain and the Louisville seamount chain.



The exploitation of the Pacific's mineral wealth is hampered by the ocean's great depths. In shallow waters of the continental shelves off the coasts of Australia and New Zealand, petroleum and natural gas are extracted, and pearls are harvested along the coasts of Australia, Japan, Papua New Guinea, Nicaragua, Panama, and the Philippines, although in sharply declining volume in some cases.



Fish are an important economic asset in the Pacific. The shallower shoreline waters of the continents and the more temperate islands yield herring, salmon, sardines, snapper, swordfish, and tuna, as well as shellfish. Overfishing has become a serious problem in some areas. For example, catches in the rich fishing grounds of the Okhotsk Sea off the Russian coast have been reduced by at least half since the 1990s as a result of overfishing.




The quantity of small plastic fragments floating in the north-east Pacific Ocean increased a hundredfold between 1972 and 2012.
Marine pollution is a generic term for the harmful entry into the ocean of chemicals or particles. The main culprits are those using the rivers for disposing of their waste. The rivers then empty into the ocean, often also bringing chemicals used as fertilizers in agriculture. The excess of oxygen-depleting chemicals in the water leads to hypoxia and the creation of a dead zone.
Marine debris, also known as marine litter, is human-created waste that has ended up floating in a lake, sea, ocean, or waterway. Oceanic debris tends to accumulate at the center of gyres and coastlines, frequently washing aground where it is known as beach litter.
In addition, the Pacific Ocean has served as the crash site of satellites, including Mars 96, Fobos-Grunt, and Upper Atmosphere Research Satellite.















EPIC Pacific Ocean Data Collection Viewable on-line collection of observational data
NOAA In-situ Ocean Data Viewer plot and download ocean observations
NOAA PMEL Argo profiling floats Realtime Pacific Ocean data
NOAA TAO El Niño data Realtime Pacific Ocean El Niño buoy data
NOAA Ocean Surface Current Analyses—Realtime (OSCAR) Near-realtime Pacific Ocean Surface Currents derived from satellite altimeter and scatterometer dataThe Atlantic Ocean is the second largest of the world's oceans with a total area of about 106,460,000 square kilometres (41,100,000 sq mi). It covers approximately 20 percent of the Earth's surface and about 29 percent of its water surface area. It separates the "Old World" from the "New World".
The Atlantic Ocean occupies an elongated, S-shaped basin extending longitudinally between Eurasia and Africa to the east, and the Americas to the west. As one component of the interconnected global ocean, it is connected in the north to the Arctic Ocean, to the Pacific Ocean in the southwest, the Indian Ocean in the southeast, and the Southern Ocean in the south (other definitions describe the Atlantic as extending southward to Antarctica). The Equatorial Counter Current subdivides it into the North Atlantic Ocean and South Atlantic Ocean at about 8°N.
Scientific explorations of the Atlantic include the Challenger expedition, the German Meteor expedition, Columbia University's Lamont-Doherty Earth Observatory and the United States Navy Hydrographic Office.




The oldest known mention of "Atlantic" is in The Histories of Herodotus around 450 BC (Hdt. 1.202.4): Atlantis thalassa (Greek: Ἀτλαντὶς θάλασσα; English: Sea of Atlas) where the name refers to "the sea beyond the pillars of Heracles" which is said to be part of the ocean that surrounds all land. Thus, on one hand, the name refers to Atlas, the Titan of Greek mythology, who supported the heavens and who later appeared as a frontispiece in Medieval maps and also lend his name to modern atlases. On the other hand, to early Greek sailors and in Ancient Greek mythological literature such as the Iliad and the Odyssey, this all-encompassing ocean was instead known as Oceanus, the gigantic river that encircled the world; in contrast to the enclosed seas well-known to the Greeks: the Mediterranean and the Black Sea. In contrast, the term "Atlantic" originally referred specifically to the Atlas Mountains in Morocco and the sea off the Strait of Gibraltar and the North African coast. The Greek word thalassa has been reused by scientists for the huge Panthalassa ocean that surrounded the supercontinent Pangaea hundreds of million years ago.
The term "Aethiopian Ocean", derived from Ancient Ethiopia, was applied to the Southern Atlantic as late as the mid-19th century.



In modern times, some idioms refer to the ocean in a humorously diminutive way as "the Pond", describing both the geographical and cultural divide between North America and Europe, in particular between the English-speaking nations of both continents. Many Irish or British people refer to the United States and Canada as "across the pond", and vice versa.
The "Black Atlantic" refers to the role of this ocean in shaping black people's history, especially through the Atlantic slave trade. Irish migration to the US is meant when the term "The Green Atlantic" is used. The term "Red Atlantic" has been used in reference to the Marxian concept of an Atlantic working class, as well as to the Atlantic experience of indigenous Americans. 




The International Hydrographic Organization (IHO) defined the limits of the oceans and seas in 1953, but some of these definitions have been revised since then and some are not used by various authorities, institutions, and countries, see for example the CIA World Factbook. Correspondingly, the extent and number of oceans and seas varies.
The Atlantic Ocean is bounded on the west by North and South America. It connects to the Arctic Ocean through the Denmark Strait, Greenland Sea, Norwegian Sea and Barents Sea. To the east, the boundaries of the ocean proper are Europe: the Strait of Gibraltar (where it connects with the Mediterranean Sea–one of its marginal seas–and, in turn, the Black Sea, both of which also touch upon Asia) and Africa.
In the southeast, the Atlantic merges into the Indian Ocean. The 20° East meridian, running south from Cape Agulhas to Antarctica defines its border. In the 1953 definition it extends south to Antarctica, while in later maps it is bounded at the 60° parallel by the Southern Ocean.
The Atlantic has irregular coasts indented by numerous bays, gulfs, and seas. These include the Baltic Sea, Black Sea, Caribbean Sea, Davis Strait, Denmark Strait, part of the Drake Passage, Gulf of Mexico, Labrador Sea, Mediterranean Sea, North Sea, Norwegian Sea, almost all of the Scotia Sea, and other tributary water bodies. Including these marginal seas the coast line of the Atlantic measures 111,866 km (69,510 mi) compared to 135,663 km (84,297 mi) for the Pacific.
Including its marginal seas, the Atlantic covers an area of 106,460,000 km2 (41,100,000 sq mi) or 23.5% of the global ocean and has a volume of 310,410,900 km3 (74,471,500 cu mi) or 23.3%. Excluding its marginal seas, the Atlantic covers 81,760,000 km2 (31,570,000 sq mi) and has a volume of 305,811,900 km3 (73,368,200 cu mi). The North Atlantic covers 41,490,000 km2 (16,020,000 sq mi) (11.5%) and the South Atlantic 40,270,000 km2 (15,550,000 sq mi) (11.1%). The average depth is 3,646 m (11,962 ft) and the maximum depth, the Milwaukee Deep in the Puerto Rico Trench, is 8,486 m (27,841 ft).




The bathymetry of the Atlantic is dominated by a submarine mountain range called the Mid-Atlantic Ridge (MAR). It runs from 87°N or 300 km (190 mi) south of the North Pole to the subantarctic Bouvet Island at 42°S.




The MAR divides the Atlantic longitudinally into two halves, in each of which a series of basins are delimited by secondary, transverse ridges. The MAR reaches above 2000 m along most of its length, but is interrupted by larger transform faults at two places: the Romanche Trench near the Equator and the Gibbs Fracture Zone at 53°N. The MAR is a barrier for bottom water, but at these two transform faults deep water currents can pass from one side to the other.
The MAR rises 2–3 km (1.2–1.9 mi) above the surrounding ocean floor and its rift valley is the divergent boundary between the North American and Eurasian plates in the North Atlantic and the South American and African plates in the South Atlantic. The MAR produces basaltic volcanoes in Eyjafjallajökull, Iceland, and pillow lava on the ocean floor. The depth of water at the apex of the ridge is less than 2,700 metres (1,500 fathoms; 8,900 ft) in most places, while the bottom of the ridge is three times as deep.
The MAR is intersected by two perpendicular ridges: the Azores–Gibraltar Transform Fault, the boundary between the Nubian and Eurasian plates, intersects the MAR at the Azores Triple Junction, on either side of the Azores microplate, near the 40°N. A much vaguer, nameless boundary, between the North American and South American plates, intersects the MAR near or just north of the Fifteen-Twenty Fracture Zone, approximately at 16°N.
In the 1870s, the Challenger expedition discovered parts of what is now known as the Mid-Atlantic Ridge, or:

An elevated ridge rising to an average height of about 1,900 fathoms below the surface traverses the basins of the North and South Atlantic in a meridianal direction from Cape Farewell, probably its far south at least as Gough Island, following roughly the outlines of the coasts of the Old and the New Worlds.

The remainder of the ridge was discovered in the 1920s by the German Meteor expedition using echo-sounding equipment. The exploration of the MAR in the 1950s lead to the general acceptance of seafloor spreading and plate tectonics.
Most of the MAR runs under water but where it reaches the surfaces it has produced volcanic islands. While nine of these have collectively been nominated a World Heritage Site for their geological value, four of them are considered of "Outstanding Universal Value" based on their cultural and natural criteria: Þingvellir, Iceland; Landscape of the Pico Island Vineyard Culture, Portugal; Gough and Inaccessible Islands, United Kingdom; and Brazilian Atlantic Islands: Fernando de Noronha and Atol das Rocas Reserves, Brazil.




Continental shelves in the Atlantic are wide off Newfoundland, southern-most South America, and north-eastern Europe. In the western Atlantic carbonate platforms dominate large areas, for example the Blake Plateau and Bermuda Rise. The Atlantic is surrounded by passive margins except at a few locations where active margins form deep trenches: the Puerto Rico Trench (8,414 m (27,605 ft) maximum depth) in the western Pacific and South Sandwich Trench (8,264 m (27,113 ft)) in the South Atlantic. There are numerous submarine canyons off north-eastern North America, western Europe, and north-western Africa. Some of these canyons extend along the continental rises and farther into the abyssal plains as deep-sea channels.
The deep ocean floor is thought to be fairly flat with occasional deeps, abyssal plains, trenches, seamounts, basins, plateaus, canyons, and some guyots. Various shelves along the margins of the continents constitute about 11% of the bottom topography with few deep channels cut across the continental rise.
The mean depth between 60°N and 60°S is 3,730 m (12,240 ft), or close to the average for the global ocean, with a modal depth between 4,000 and 5,000 m (13,000 and 16,000 ft).
In the South Atlantic the Walvis Ridge and Rio Grande Rise form barriers to ocean currents. The Laurentian Abyss is found off the eastern coast of Canada.




Surface water temperatures, which vary with latitude, current systems, and season and reflect the latitudinal distribution of solar energy, range from below −2 °C (28 °F) to over 30 °C (86 °F). Maximum temperatures occur north of the equator, and minimum values are found in the polar regions. In the middle latitudes, the area of maximum temperature variations, values may vary by 7–8 °C (13–14 °F).
From October to June the surface is usually covered with sea ice in the Labrador Sea, Denmark Strait, and Baltic Sea.
The Coriolis effect circulates North Atlantic water in a clockwise direction, whereas South Atlantic water circulates counter-clockwise. The south tides in the Atlantic Ocean are semi-diurnal; that is, two high tides occur during each 24 lunar hours. In latitudes above 40° North some east-west oscillation, known as the North Atlantic Oscillation, occurs.



On average, the Atlantic is the saltiest major ocean; surface water salinity in the open ocean ranges from 33 to 37 parts per thousand (3.3 – 3.7%) by mass and varies with latitude and season. Evaporation, precipitation, river inflow and sea ice melting influence surface salinity values. Although the lowest salinity values are just north of the equator (because of heavy tropical rainfall), in general the lowest values are in the high latitudes and along coasts where large rivers enter. Maximum salinity values occur at about 25° north and south, in subtropical regions with low rainfall and high evaporation.
The high surface salinity in the Atlantic, on which the Atlantic thermohaline circulation is dependent, is maintained by two processes: the Agulhas Leakage/Rings, which brings salty Indian Ocean waters into the South Atlantic, and the "Atmospheric Bridge", which evaporates subtropical Atlantic waters and exports it to the Pacific.



The Atlantic Ocean consists of four major, upper water masses with distinct temperature and salinity. The Atlantic Subarctic Upper Water in the northern-most North Atlantic is the source for Subarctic Intermediate Water and North Atlantic Intermediate Water. North Atlantic Central Water can be divided into the Eastern and Western North Atlantic central Water since the western part is strongly affected by the Gulf Stream and therefore the upper layer is closer to underlying fresher subpolar intermediate water. The eastern water is saltier because of its proximity to Mediterranean Water. North Atlantic Central Water flows into South Atlantic Central Water at 15°N.
There are five intermediate waters: four low-salinity waters formed at subpolar latitudes and one high-salinity formed through evaporation. Arctic Intermediate Water, flows from north to become the source for North Atlantic Deep Water south of the Greenland-Scotland sill. These two intermediate waters have different salinity in the western and eastern basins. The wide range of salinities in the North Atlantic is caused by the asymmetry of the northern subtropical gyre and the large number of contributions from a wide range of sources: Labrador Sea, Norwegian-Greenland Sea, Mediterranean, and South Atlantic Intermediate Water.
The North Atlantic Deep Water (NADW) is a complex of four water masses, two that form by deep convection in the open ocean — Classical and Upper Labrador Sea Water — and two that form from the inflow of dense water across the Greenland-Iceland-Scotland sill — Denmark Strait and Iceland-Scotland Overflow Water. Along its path across Earth the composition of the NADW is affected by other water masses, especially Antarctic Bottom Water and Mediterranean Overflow Water. The NADW is fed by a flow of warm shallow water into the northern North Atlantic which is responsible for the anomalous warm climate in Europe. Changes in the formation of NADW have been linked to global climate changes in the past. Since man-made substances were introduced into the environment, the path of the NADW can be traced throughout its course by measuring tritium and radiocarbon from nuclear weapon tests in the 1960s and CFCs.




The clockwise warm-water North Atlantic Gyre occupies the northern Atlantic, and the counter-clockwise warm-water South Atlantic Gyre appears in the southern Atlantic.
In the North Atlantic surface circulation is dominated by three inter-connected currents: the Gulf Stream which flows north-east from the North American coast at Cape Hatteras; the North Atlantic Current, a branch of the Gulf Stream which flows northward from the Grand Banks; and the Subpolar Front, an extension of the North Atlantic Current, a wide, vaguely defined region separating the subtropical gyre from the subpolar gyre. This system of currents transport warm water into the North Atlantic, without which temperatures in the North Atlantic and Europe would plunge dramatically.

North of the North Atlantic Gyre, the cyclonic North Atlantic Subpolar Gyre plays a key role in climate variability. It is governed by ocean currents from marginal seas and regional topography, rather than being steered by wind, both in the deep ocean and at sea level. The subpolar gyre forms an important part of the global thermohaline circulation. Its eastern portion includes eddying branches of the North Atlantic Current which transport warm, saline waters from the subtropics to the north-eastern Atlantic. There this water is cooled during winter and forms return currents that merge along the eastern continental slope of Greenland where they form an intense (40–50 Sv) current which flows around the continental margins of the Labrador Sea. A third of this water become parts of the deep portion of the North Atlantic Deep Water (NADW). The NADW, in its turn, feed the meridional overturning circulation (MOC), the northward heat transport of which is threatened by anthropogenic climate change. Large variations in the subpolar gyre on a decade-century scale, associated with the North Atlantic Oscillation, are especially pronounced in Labrador Sea Water, the upper layers of the MOC.
The South Atlantic is dominated by the anti-cyclonic southern subtropical gyre. The South Atlantic Central Water originates in this gyre, while Antarctic Intermediate Water originates in the upper layers of the circumpolar region, near the Drake Passage and Falkland Islands. Both these currents receive some contribution from the Indian Ocean. On the African east coast the small cyclonic Angola Gyre lies embedded in the large subtropical gyre. The southern subtropical gyre is partly masked by a wind-induced Ekman layer. The residence time of the gyre is 4.4–8.5 years. North Atlantic Deep Water flows southerward below the thermocline of the subtropical gyre.




The Sargasso Sea in the western North Atlantic can be defined as the area where two species of Sargassum (S. fluitans and natans) float, an area 4,000 km (2,500 mi) wide and encircled by the Gulf Stream, North Atlantic Drift, and North Equatorial Current. This population of seaweed probably originated from Tertiary ancestors on the European shores of the former Tethys Ocean and has, if so, maintained itself by vegetative growth, floating in the ocean for millions of years.

Other species endemic to the Sargasso Sea include the sargassum fish, a predator with algae-like appendages who hovers motionless among the Sargassum. Fossils of similar fishes have been found in fossil bays of the former Tethys Ocean, in what is now the Carpathian region, that were similar to the Sargasso Sea. It is possible that the population in the Sargasso Sea migrated to the Atlantic as the Tethys closed at the end of the Miocene around 17 Ma. The origin of the Sargasso fauna and flora remained enigmatic for centuries. The fossils found in the Carpathians in the mid-20th century, often called the "quasi-Sargasso assemblage", finally showed that this assemblage originated in the Carpathian Basin from were it migrated over Sicily to the Central Atlantic where it evolved into modern species of the Sargasso Sea.
The location of the spawning ground for European eels remained unknown for decades. In the early 19th century it was discovered that the southern Sargasso Sea is the spawning ground for both the European and American eel and that the former migrate more than 5,000 km (3,100 mi) and the latter 2,000 km (1,200 mi). Ocean currents such as the Gulf Stream transport eel larvae from the Sargasso Sea to foraging areas in North America, Europe, and Northern Africa.




Climate is influenced by the temperatures of the surface waters and water currents as well as winds. Because of the ocean's great capacity to store and release heat, maritime climates are more moderate and have less extreme seasonal variations than inland climates. Precipitation can be approximated from coastal weather data and air temperature from water temperatures.
The oceans are the major source of the atmospheric moisture that is obtained through evaporation. Climatic zones vary with latitude; the warmest zones stretch across the Atlantic north of the equator. The coldest zones are in high latitudes, with the coldest regions corresponding to the areas covered by sea ice. Ocean currents influence climate by transporting warm and cold waters to other regions. The winds that are cooled or warmed when blowing over these currents influence adjacent land areas.
The Gulf Stream and its northern extension towards Europe, the North Atlantic Drift, for example, warms the atmosphere of the British Isles and north-western Europe and influences weather and climate as far south as the northern Mediterranean. The cold water currents contribute to heavy fog off the coast of eastern Canada (the Grand Banks of Newfoundland area) and Africa's north-western coast. In general, winds transport moisture and air over land areas. More local particular weather examples could be found in examples such as the Azores High, Benguela Current, and Nor'easter.




Icebergs are common from February to August in the Davis Strait, Denmark Strait, and the northwestern Atlantic and have been spotted as far south as Bermuda and Madeira. Ships are subject to superstructure icing in the extreme north from October to May. Persistent fog can be a maritime hazard from May to September, as can hurricanes north of the equator (May to December).
The United States' southeast coast, especially the Virginia and North Carolina coasts, has a long history of shipwrecks due to its many shoals and reefs.
The Bermuda Triangle is popularly believed to be the site of numerous aviation and shipping incidents because of unexplained and supposedly mysterious causes, but Coast Guard records do not support this belief.
Hurricanes are also a natural hazard in the Atlantic, but mainly in the northern part of the ocean, rarely tropical cyclones form in the southern parts. Hurricanes usually form annually between June and November.






The break-up of Pangaea began in the Central Atlantic, between North America and Northwest Africa, where rift basins opened during the Late Triassic and Early Jurassic. This period also saw the first stages of the uplift of the Atlas Mountains. The exact timing is controversial with estimates ranging from 200 to 170 Ma.
The opening of the Atlantic Ocean coincided with the initial break-up of the supercontinent Pangaea, both of which were initiated by the eruption of the Central Atlantic Magmatic Province (CAMP), one of the most extensive and voluminous large igneous provinces in Earth's history associated with the Triassic–Jurassic extinction event, one of Earth's major extinction events. Theoliitic dikes, sills, and lava flows from the CAMP eruption at 200 Ma have been found in West Africa, eastern North America, and northern South America. The extent of the volcanism has been estimated to 4.5×106 km2 (1.7×106 sq mi) of which 2.5×106 km2 (9.7×105 sq mi) covered what is now northern and central Brazil.
The formation of the Central American Isthmus closed the Central American Seaway at the end of the Pliocene 2.8 Ma ago. The formation of the isthmus resulted in the migration and extinction of many land-living animals, known as the Great American Interchange, but the closure of the seaway resulted in a "Great American Schism" as it affected ocean currents, salinity, and temperatures in both the Atlantic and Pacific. Marine organisms on both sides of the isthmus became isolated and either diverged or went extinct.




Geologically the Northern Atlantic is the area delimited to the south by two conjugate margins, Newfoundland and Iberia, and to the north by the Arctic Eurasian Basin. The opening of the Northern Atlantic closely followed the margins of its predecessor, the Iapetus Ocean, and spread from the Central Atlantic in six stages: Iberia–Newfoundland, Porcupine–North America, Eurasia–Greenland, Eurasia–North America. Active and inactive spreading systems in this area are marked by the interaction with the Iceland hotspot.



West Gondwana (South America and Africa) broke up in the Early Cretaceous to form the South Atlantic. The apparent fit between the coastlines of the two continents was noted on the first maps that included the South Atlantic and it was also the subject of the first computer-assisted plate tectonic reconstructions in 1965. This magnificent fit, however, has since then proven problematic and later reconstructions have introduced various deformation zones along the shorelines to accommodate the northward-propagating break-up. Intra-continental rifts and deformations have also been introduced to subdivide both continental plates into sub-plates.
Geologically the South Atlantic can be divided into four segments: Equatorial segment, from 10°N to the Romanche Fracture Zone (RFZ);; Central segment, from RFZ to Florianopolis Fracture Zone (FFZ, north of Walvis Ridge and Rio Grande Rise); Southern segment, from FFZ to the Agulhas-Falkland Fracture Zone (AFFZ); and Falkland segment, south of AFFZ.
In the southern segment the Early Cretaceous (133–130 Ma) intensive magmatism of the Paraná–Etendeka Large Igneous Province produced by the Tristan hotspot resulted in an estimated volume of 1.5×106 to 2.0×106 km3 (3.6×105 to 4.8×105 cu mi). It covered an area of 1.2×106 to 1.6×106 km2 (4.6×105 to 6.2×105 sq mi) in Brazil, Paraguay, and Uruguay and 0.8×105 km2 (3.1×104 sq mi) in Africa. Dyke swarms in Brazil, Angola, eastern Paraguay, and Namibia, however, suggest the LIP originally covered a much larger area and also indicate failed rifts in all these areas. Associated offshore basaltic flows reach as far south as the Falkland Islands and South Africa. Traces of magmatism in both offshore and onshore basins in the central and southern segments have been dated to 147–49 Ma with two peaks between 143–121 Ma and 90–60 Ma.
In the Falkland segment rifting began with dextral movements between the Patagonia and Colorado sub-plates between the Early Jurassic (190 Ma) and the Early Cretaceous (126.7 Ma). Around 150 Ma sea-floor spreading propagated northward into the southern segment. No later than 130 Ma rifting had reached the Walvis Ridge–Rio Grande Rise.
In the central segment rifting started to break Africa in two by opening the Benue Trough around 118 Ma. Rifting in the central segment, however, coincided with the Cretaceous Normal Superchron (also known as the Cretaceous quiet period), a 40 Ma period without magnetic reversals, which makes it difficult to date sea-floor spreading in this segment.
The equatorial segment is the last phase of the break-up, but, because it is located on the Equator, magnetic anomalies cannot be used for dating. Various estimates date the propagation of sea-floor spreading in this segment to the period 120–96 Ma. This final stage, nevertheless, coincided with or resulted in the end of continental extension in Africa.
About 50 Ma the opening of the Drake Passage resulted from a change in the motions and separation rate of the South American and Antarctic plates. First small ocean basins opened and a shallow gateway appeared during the Middle Eocene. 34–30 Ma a deeper seaway developed, followed by an Eocene–Oligocene climatic deterioration and the growth of the Antarctic ice sheet.



An embryonic subduction margin is potentially developing west of Gibraltar. The Gibraltar Arc in the western Mediterranean is migrating westward into the Central Atlantic where it joins the converging African and Eurasian plates. Together these three tectonic forces are slowly developing into a new subduction system in the eastern Atlantic Basin. Meanwhile, the Scotia Arc and Caribbean Plate in the western Atlantic Basin are eastward-propagating subduction systems that might, together with the Gibraltar system, represent the beginning of the closure of the Atlantic Ocean and the final stage of the Atlantic Wilson Cycle.






Humans evolved in Africa; first by diverging from other apes around 7 Ma; then developing stone tools around 2.6 Ma; to finally evolve as modern humans around 100 kya. The earliest evidences for the complex behaviour associated with this behavioral modernity has been found in the Greater Cape Floristic Region (GCFR) along the coast of South Africa. During the latest glacial stages the now-submerged plains of the Agulhas Bank were exposed above sea level, extending the South African coastline farther south by hundreds of kilometres. A small population of modern humans — probably fewer than a thousand reproducing individuals — survived glacial maxima by exploring the high diversity offered by these Palaeo-Agulhas plains. The GCFR is delimited to the north by the Cape Fold Belt and the limited space south of it resulted in the development of social networks out of which complex Stone Age technologies emerged. Human history thus begins on the coasts of South Africa where the Atlantic Benguela Upwelling and Indian Ocean Agulhas Current meet to produce an intertidal zone on which shellfish, fur seal, fishes and sea birds provided the necessary protein sources. The African origin of this modern behaviour is evidenced by 70,000 years-old engravings from Blombos Cave, South Africa.



Mitochondrial DNA (mtDNA) studies indicate that 80–60,000 years ago a major demographic expansion within Africa, derived from a single, small population, coincided with the emergence of behavioural complexity and the rapid MIS 5–4 environmental changes. This group of people not only expanded over the whole of Africa, but also started to disperse out of Africa into Asia, Europe, and Australasia around 65.000 years ago and quickly replaced the archaic humans in these regions. During the Last Glacial Maximum (LGM) 20,000 years ago humans had to abandon their initial settlements along the European North Atlantic coast and retreat to the Mediterranean. Following rapid climate changes at the end of the LGM this region was repopulated by Magdalenian culture. Other hunter-gatherers followed in waves interrupted by large-scale hazards such as the Laacher See volcanic eruption, the inundation of Doggerland (now the North Sea), and the formation of the Baltic Sea. The European coasts of the North Atlantic were permanently populated about 9–8.5 thousand years ago.
This human dispersal left abundant traces along the coasts of the Atlantic Ocean. 50 ka-old, deeply stratified shell middens found in Ysterfontein on the western coast of South Africa are associated with the Middle Stone Age (MSA). The MSA population was small and dispersed and the rate of their reproduction and exploitation was less intense than those of later generations. While their middens resemble 12-11 ka-old Late Stone Age (LSA) middens found on every inhabited continent, the 50-45 ka-old Enkapune Ya Muto in Kenya probably represents the oldest traces of the first modern humans to disperse out of Africa.

The same development can be seen in Europe. In La Riera Cave (23-13 ka) in Asturias, Spain, only some 26,600 molluscs were deposited over 10 ka. In contrast, 8-7 ka-old shell middens in Portugal, Denmark, and Brazil generated thousands of tonnes of debris and artefacts. The Ertebølle middens in Denmark, for example, accumulated 2,000 m3 (71,000 cu ft) of shell deposits representing some 50 million molluscs over only a thousand years. This intensification in the exploitation of marine resources has been described as accompanied by new technologies — such as boats, harpoons, and fish-hooks — because many caves found in the Mediterranean and on the European Atlantic coast have increased quantities of marine shells in their upper levels and reduced quantities in their lower. The earliest exploitation, however, took place on the now submerged shelves, and most settlements now excavated were then located several kilometres from these shelves. The reduced quantities of shells in the lower levels can represent the few shells that were exported inland.



During the LGM the Laurentide Ice Sheet covered most of northern North America while Beringia connected Siberia to Alaska. In 1973 late U.S. geoscientist Paul S. Martin proposed a "blitzkrieg" colonization of America by which Clovis hunters migrated into North America around 13,000 years ago in a single wave through an ice-free corridor in the ice sheet and "spread southward explosively, briefly attaining a density sufficiently large to overkill much of their prey." Others later proposed a "three-wave" migration over the Bering Land Bridge. These hypotheses remained the long-held view regarding the settlement of the Americas, a view challenged by more recent archaeological discoveries: the oldest archaeological sites in the Americas have been found in South America; sites in north-east Siberia report virtually no human presence there during the LGM; and most Clovis artefacts have been found in eastern North America along the Atlantic coast. Furthermore, colonisation models based on mtDNA, yDNA, and atDNA data respectively support neither the "blitzkrieg" nor the "three-wave" hypotheses but they also deliver mutually ambiguous results. Contradictory data from archaeology and genetics will most likely deliver future hypotheses that will, eventually, confirm each other. A proposed route across the Pacific to South America could explain early South American finds and another hypothesis proposes a northern path, through the Canadian Arctic and down the North American Atlantic coast. Early settlements across the Atlantic have been suggested by alternative theories, ranging from purely hypothetical to mostly disputed, including the Solutrean hypothesis and some of the Pre-Columbian trans-oceanic contact theories.

The Norse settlement of the Faroe Islands and Iceland began during the 9th and 10th centuries. A settlement on Greenland was established before 1000 CE, but contact with it was lost in 1409 and it was finally abandoned during the early Little Ice Age. This setback was caused by a range of factors: an unsustainable economy resulted in erosion and denudation, while conflicts with the local Inuit resulted in the failure to adapt their Arctic technologies; a colder climate resulted in starvation; and the colony got economically marginalised as the Great Plague and Barbary pirates harvested its victims on Iceland in the 15th century. Iceland was initially settled 865–930 CE following a warm period when winter temperatures hovered around 2 °C (36 °F) which made farming favourable at high latitudes. This did not last, however, and temperatures quickly dropped; at 1080 CE summer temperatures had reached a maximum of 5 °C (41 °F). The Landnámabók (Book of Settlement) records disastrous famines during the first century of settlement — "men ate foxes and ravens" and "the old and helpless were killed and thrown over cliffs" — and by the early 1200s hay had to be abandoned for short-season crops such as barley.




Christopher Columbus discovered the Americas in 1492 under Spanish flag. Six years later Vasco da Gama reached India under Portuguese flag, by navigating south around the Cape of Good Hope, thus proving that the Atlantic and Indian Oceans are connected. In 1500, in his voyage to India following Vasco da Gama, Pedro Alvares Cabral reached Brazil, taken by the currents of the South Atlantic Gyre. Following these explorations, Spain and Portugal quickly conquered and colonized large territories in the New World and forced the Native American population into slavery in order to explore the vast quantities of silver and gold they found. Spain and Portugal monopolised this trade in order to keep other European nations out, but conflicting interests nevertheless lead to a series of Spanish-Portuguese wars. A peace treaty mediated by the Pope divided the conquered territories into Spanish and Portuguese sectors while keeping other colonial powers away. England, France, and the Dutch Republic enviously watched the Spanish and Portuguese wealth grow and allied themselves with pirates such as Henry Mainwaring and Alexandre Exquemelin. They could explore the convoys leaving America because prevailing winds and currents made the transport of heavy metals slow and predictable.

In the American colonies depredation, disease, and slavery quickly reduced the indigenous American population to the extent that the Atlantic slave trade had to be introduced to replace them — a trade that became norm and an integral part of the colonisation. Between the 15th century and 1888, when Brazil became the last part of America to end slave trade, an estimated ten million Africans were exported as slaves, most of them destined for agricultural labour. The slave trade was officially abolished in the British Empire and the United States in 1808, and slavery itself was abolished in the British Empire in 1838 and in the U.S. in 1865 after the Civil War.
From Columbus to the Industrial revolution Trans-Atlantic trade, including colonialism and slavery, became crucial for Western Europe. For European countries with a direct access to the Atlantic (including Britain, France, the Netherlands, Portugal, and Spain) 1500–1800 was a period of sustained growth during which these countries grew richer than those in Eastern Europe and Asia. Colonialism evolved as part of the Trans-Atlantic trade, but this trade also strengthened the position of merchant groups at the expense of monarchs. Growth was more rapid in non-absolutist countries, such as Britain and the Netherlands, and more limited in absolutist monarchies, such as Portugal, Spain, and France, where profit mostly or exclusively benefited the monarchy and its allies.
Trans-Atlantic trade also resulted in an increasing urbanisation: in European countries facing the Atlantic urbanisation grew from 8% in 1300, 10.1% in 1500, to 24.5% in 1850; in other European countries from 10% in 1300, 11.4% in 1500, to 17% in 1850. Likewise, GDP doubled in Atlantic countries but rose by only 30% in the rest of Europe. By end of the 17th century the volume of the Trans-Atlantic trade had surpassed that of the Mediterranean trade.



The Atlantic has contributed significantly to the development and economy of surrounding countries. Besides major transatlantic transportation and communication routes, the Atlantic offers abundant petroleum deposits in the sedimentary rocks of the continental shelves.
The Atlantic harbours petroleum and gas fields, fish, marine mammals (seals and whales), sand and gravel aggregates, placer deposits, polymetallic nodules, and precious stones. Gold deposits are a mile or two under water on the ocean floor, however the deposits are also encased in rock that must be mined through. Currently, there is no cost-effective way to mine or extract gold from the ocean to make a profit.
Various international treaties attempt to reduce pollution caused by environmental threats such as oil spills, marine debris, and the incineration of toxic wastes at sea.



The shelves of the Atlantic hosts one of the world's richest fishing resources. The most productive areas include the Grand Banks of Newfoundland, the Scotian Shelf, Georges Bank off Cape Cod, the Bahama Banks, the waters around Iceland, the Irish Sea, the Dogger Bank of the North Sea, and the Falkland Banks. Fisheries have, however, undergone significant changes since the 1950s and global catches can now be divided into three groups of which only two are observed in the Atlantic: fisheries in the Eastern Central and South-West Atlantic oscillate around a globally stable value, the rest of the Atlantic is in overall decline following historical peaks. The third group, "continuously increasing trend since 1950", is only found in the Indian Ocean and Western Pacific.

In the North-East Atlantic total catches decreased between the mid-1970s and the 1990s and reached 8.7 million tonnes in 2013. Blue whiting reached a 2.4 million tonnes peak in 2004 but was down to 628,000 tonnes in 2013. Recovery plans for cod, sole, and plaice have reduced mortality in these species. Arctic cod reached its lowest levels in the 1960s-1980s but is now recovered. Arctic saithe and haddock are considered fully fished; Sand eel is overfished as was capelin which has now recovered to fully fished. Limited data makes the state of redfishes and deep-water species difficult to assess but most likely they remain vulnerable to overfishing. Stocks of northern shrimp and Norwegian lobster are in good condition. In the North-East Atlantic 21% of stocks are considered overfished.

In the North-West Atlantic landings have decreased from 4.2 million tonnes in the early 1970s to 1.9 million tonnes in 2013. During the 21th century some species have shown weak signs of recovery, including Greenland halibut, yellowtail flounder, Atlantic halibut, haddock, spiny dogfish, while other stocks shown no such signs, including cod, witch flounder, and redfish. Stocks of invertebrates, in contrast, remain at record levels of abundance. 31% of stocks are overfished in the North-west Atlantic.

In 1497 John Cabot became the first to explore mainland North America and one of his major discoveries was the abundant resources of Atlantic cod off Newfoundland. Referred to as "Newfoundland Currency" this discovery supplied mankind with some 200 million tonnes of fish over five centuries. In the late 19th and early 20th centuries new fisheries started to exploit haddock, mackerel, and lobster. From the 1950s to the 1970s the introduction of European and Asian distant-water fleets in the area dramatically increased the fishing capacity and number of exploited species. It also expanded the exploited areas from near-shore to the open sea and to great depths to include deep-water species such as redfish, Greenland halibut, witch flounder, and grenadiers. Overfishing in the area was recognised as early as the 1960s but, because this was occurring on international waters, it took until the late 1970s before any attempts to regulate was made. In the early 1990s this finally resulted in the collapse of the Atlantic northwest cod fishery. The population of a number of deep-sea fishes also collapsed in the process, including American plaice, redfish, and Greenland halibut, together with flounder and grenadier.
In the Eastern Central Atlantic small pelagic fishes constitute about 50% of landings with sardine reaching 0.6–1.0 million tonnes per year. Pelagic fish stocks are considered fully fishes or overfished, with sardines south of Cape Bojador the notable exception. Almost half of stocks are fished at biologically unsustainable levels. Total catches have been fluctuating since the 1970s; reaching 3.9 million tonnes in 2013 or slightly less than the peak production in 2010.

In the Western Central Atlantic catches have been decreasing since 2000 and reached 1.3 million tonnes in 2013. The most important species in the area, Gulf menhaden, reached a million tonnes in the mid-1980s but only half a million tonnes in 2013 and is now considered fully fished. Round sardinella was an important species in the 1990s but is now considered overfished. Groupers and snappers are overfished and northern brown shrimp and American cupped oyster are considered fully fished approaching overfished. 44% of stocks are being fished at unsustainable levels.

In the South-East Atlantic catches have decreased from 3.3 million tonnes in the early 1970s to 1.3 million tonnes in 2013. Horse mackerel and hake are the most important species, together representing almost half of the landings. Off South Africa and Namibia deep-water hake and shallow-water Cape hake have recovered to sustainable levels since regulations were introduced in 2006 and the states of Southern African pilchard and anchovy have improved to fully fished in 2013.
In the South-West Atlantic a peak was reached in the mid-1980s and catches now fluctuate between 1.7 and 2.6 million tonnes. The most important species, the Argentine shortfin squid, which reached half a million tonnes in 2013 or half the peak value, is considered fully fished to overfished. Another important species was the Brazilian sardinella, with a production of 100,000 tonnes in 2013 it is now considered overfished. Half the stocks in this area are being fished at unsustainable levels: Whitehead’s round herring has not yet reached fully fished but Cunene horse mackerel is overfished. The sea snail perlemoen abalone is targeted by illegal fishing and remain overfished.




Endangered marine species include the manatee, seals, sea lions, turtles, and whales. Drift net fishing can kill dolphins, albatrosses and other seabirds (petrels, auks), hastening the fish stock decline and contributing to international disputes. Municipal pollution comes from the eastern United States, southern Brazil, and eastern Argentina; oil pollution in the Caribbean Sea, Gulf of Mexico, Lake Maracaibo, Mediterranean Sea, and North Sea; and industrial waste and municipal sewage pollution in the Baltic Sea, North Sea, and Mediterranean Sea.
North Atlantic hurricane activity has increased over past decades because of increased sea surface temperature (SST) at tropical latitudes, changes that can be attributed to either the natural Atlantic Multidecadal Oscillation (AMO) or to anthropogenic climate change. A 2005 report indicated that the Atlantic meridional overturning circulation (AMOC) slowed down by 30% between 1957 and 2004. If the AMO was responsible for SST variability then the AMOC would have increased in strength, which is apparently not the case. Furthermore, it is clear from statistical analyses of annual tropical cyclones that these changes do not display multidecadal cyclicity. Therefore, these changes in SST must be caused by human activities.
The ocean mixed layer plays an important role heat storage over seasonal and decadal time-scales, whereas deeper layers are affected over millennia and has a heat capacity about 50 times that of the mixed layer. This heat uptake provides a time-lag for climate change but it also results in a thermal expansion of the oceans which contribute to sea-level rise. 21st century global warming will probably result in an equilibrium sea-level rise five times greater than today, whilst melting of glaciers, including that of the Greenland ice-sheet, expected to have virtually no effect during the 21st century, will probably result in a sea-level rise of 3–6 m over a millennium.
On 7 June 2006, Florida's wildlife commission voted to take the manatee off the state's endangered species list. Some environmentalists worry that this could erode safeguards for the popular sea creature.
Marine pollution is a generic term for the entry into the ocean of potentially hazardous chemicals or particles. The biggest culprits are rivers and with them many agriculture fertilizer chemicals as well as livestock and human waste. The excess of oxygen-depleting chemicals leads to hypoxia and the creation of a dead zone.
Marine debris, which is also known as marine litter, describes human-created waste floating in a body of water. Oceanic debris tends to accumulate at the centre of gyres and coastlines, frequently washing aground where it is known as beach litter.




List of countries and territories bordering the Atlantic Ocean
Seven Seas
Gulf Stream shutdown
Shipwrecks in the Atlantic Ocean
Atlantic hurricanes
Transatlantic crossing












Winchester, Simon (2010). Atlantic: A Vast Ocean of a Million Stories. HarperCollins UK. ISBN 978-0-00-734137-5. 




Atlantic Ocean
"Map of Atlantic Coast of North America from the Chesapeake Bay to Florida" from 1639 via the World Digital LibraryEarth, otherwise known as the world, (Greek: Γαῖα Gaia; Latin: Terra) is the third planet from the Sun and the only object in the Universe known to harbor life. It is the densest planet in the Solar System and the largest of the four terrestrial planets.
According to radiometric dating and other sources of evidence, Earth formed about 4.54 billion years ago. Earth's gravity interacts with other objects in space, especially the Sun and the Moon, Earth's only natural satellite. During one orbit around the Sun, Earth rotates about its axis over 365 times, thus an Earth year is about 365.26 days long. Earth's axis of rotation is tilted, producing seasonal variations on the planet's surface. The gravitational interaction between the Earth and Moon causes ocean tides, stabilizes the Earth's orientation on its axis, and gradually slows its rotation.
Earth's lithosphere is divided into several rigid tectonic plates that migrate across the surface over periods of many millions of years. About 71% of Earth's surface is covered with water, mostly by its oceans. The remaining 29% is land consisting of continents and islands that together have many lakes, rivers and other sources of water that contribute to the hydrosphere. The majority of Earth's polar regions are covered in ice, including the Antarctic ice sheet and the sea ice of the Arctic ice pack. Earth's interior remains active with a solid iron inner core, a liquid outer core that generates the Earth's magnetic field, and a convecting mantle that drives plate tectonics.
Within the first billion years of Earth's history, life appeared in the oceans and began to affect the Earth's atmosphere and surface, leading to the proliferation of aerobic and anaerobic organisms. Some geological evidence indicates that life may have arisen as much as 4.1 billion years ago. Since then, the combination of Earth's distance from the Sun, physical properties, and geological history have allowed life to evolve and thrive. In the history of the Earth, biodiversity has gone through long periods of expansion, occasionally punctuated by mass extinction events. Over 99% of all species that ever lived on Earth are extinct. Estimates of the number of species on Earth today vary widely; most species have not been described. Over 7.4 billion humans live on Earth and depend on its biosphere and minerals for their survival. Humans have developed diverse societies and cultures; politically, the world has about 200 sovereign states.



The modern English word  Earth developed from a wide variety of Middle English forms, which derived from an Old English noun most often spelled eorðe. It has cognates in every Germanic language, and their proto-Germanic root has been reconstructed as *erþō. In its earliest appearances, eorðe was already being used to translate the many senses of Latin terra and Greek γῆ (gē): the ground, its soil, dry land, the human world, the surface of the world (including the sea), and the globe itself. As with Terra and Gaia, Earth was a personified goddess in Germanic paganism: the Angles were listed by Tacitus as among the devotees of Nerthus, and later Norse mythology included Jörð, a giantess often given as the mother of Thor.
Originally, earth was written in lowercase, and from early Middle English, its definite sense as "the globe" was expressed as the earth. By early Modern English, many nouns were capitalized, and the earth became (and often remained) the Earth, particularly when referenced along with other heavenly bodies. More recently, the name is sometimes simply given as Earth, by analogy with the names of the other planets. House styles now vary: Oxford spelling recognizes the lowercase form as the most common, with the capitalized form an acceptable variant. Another convention capitalizes "Earth" when appearing as a name (e.g. "Earth's atmosphere") but writes it in lowercase when preceded by the (e.g. "the atmosphere of the earth"). It almost always appears in lowercase in colloquial expressions such as "what on earth are you doing?"







The oldest material found in the Solar System is dated to 7000456720000000000♠4.5672±0.0006 billion years ago (Gya). By 7000454000000000000♠4.54±0.04 Gya the primordial Earth had formed. The formation and evolution of Solar System bodies occurred along with the Sun. In theory, a solar nebula partitions a volume out of a molecular cloud by gravitational collapse, which begins to spin and flatten into a circumstellar disk, and then the planets grow out of that disk along with the Sun. A nebula contains gas, ice grains, and dust (including primordial nuclides). According to nebular theory, planetesimals formed by accretion, with the primordial Earth taking 10–7001200000000000000♠20 million years (Ma) to form.
A subject of on-going research is the formation of the Moon, some 4.53 billion years ago. A working hypothesis is that it was formed by accretion from material loosed from Earth after a Mars-sized object, named Theia, impacted Earth. In this scenario, the mass of Theia was approximately 10% of that of Earth, it impacted Earth with a glancing blow, and some of its mass merged with Earth. Between approximately 4.1 and 7000380000000000000♠3.8 Gya, numerous asteroid impacts during the Late Heavy Bombardment caused significant changes to the greater surface environment of the Moon, and by inference, to that of Earth.




Earth's atmosphere and oceans were formed by volcanic activity and outgassing that included water vapor. The origin of the world's oceans was condensation augmented by water and ice delivered by asteroids, protoplanets, and comets. In this model, atmospheric "greenhouse gases" kept the oceans from freezing when the newly forming Sun had only 70% of its current luminosity. By 7000350000000000000♠3.5 Gya, Earth's magnetic field was established, which helped prevent the atmosphere from being stripped away by the solar wind.
A crust formed when the molten outer layer of Earth cooled to form a solid. The two models that explain land mass propose either a steady growth to the present-day forms or, more likely, a rapid growth early in Earth history followed by a long-term steady continental area. Continents formed by plate tectonics, a process ultimately driven by the continuous loss of heat from Earth's interior. On time scales lasting hundreds of millions of years, the supercontinents have assembled and broken apart. Roughly 7016236682000000000♠750 mya (million years ago), one of the earliest known supercontinents, Rodinia, began to break apart. The continents later recombined to form Pannotia, 600–7016170411040000000♠540 mya, then finally Pangaea, which also broke apart 7015568036800000000♠180 mya.
The present pattern of ice ages began about 7015126230400000000♠40 mya and then intensified during the Pleistocene about 7013946728000000000♠3 mya. High-latitude regions have since undergone repeated cycles of glaciation and thaw, repeating about every 40,000–7012315576000000000♠100000 years. The last continental glaciation ended 10,000 years ago.




Chemical reactions led to the first self–replicating molecules about four billion years ago. A half billion years later, the last common ancestor of all life arose. The evolution of photosynthesis allowed the Sun's energy to be harvested directly by life forms. The resultant molecular oxygen (O2) accumulated in the atmosphere and due to interaction with ultraviolet solar radiation, formed a protective ozone layer (O3) in the upper atmosphere. The incorporation of smaller cells within larger ones resulted in the development of complex cells called eukaryotes. True multicellular organisms formed as cells within colonies became increasingly specialized. Aided by the absorption of harmful ultraviolet radiation by the ozone layer, life colonized Earth's surface. Among the earliest fossil evidence for life is microbial mat fossils found in 3.48 billion-year-old sandstone in Western Australia, biogenic graphite found in 3.7 billion-year-old metasedimentary rocks in Western Greenland, remains of biotic material found in 4.1 billion-year-old rocks in Western Australia.
During the Neoproterozoic, 7016236682000000000♠750 to 580 mya, much of Earth might have been covered in ice. This hypothesis has been termed "Snowball Earth", and it is of particular interest because it preceded the Cambrian explosion, when multicellular life forms significantly increased in complexity. Following the Cambrian explosion, 7016168833160000000♠535 mya, there have been five major mass extinctions. The most recent such event was 7015208280160000000♠66 mya, when an asteroid impact triggered the extinction of the non-avian dinosaurs and other large reptiles, but spared some small animals such as mammals, which then resembled shrews. Over the past 7015208280160000000♠66 Ma, mammalian life has diversified, and several million years ago an African ape-like animal such as Orrorin tugenensis gained the ability to stand upright. This facilitated tool use and encouraged communication that provided the nutrition and stimulation needed for a larger brain, which allowed the evolution of humans. The development of agriculture, and then civilization, led to humans having an influence on Earth and the nature and quantity of other life forms that continues today.




Earth's long-term future is closely tied to that of the Sun. Over the next 7016347133600000000♠1.1 Ga, solar luminosity will increase by 10%, and over the next 7017110451600000000♠3.5 Ga by 40%. The Earth's increasing surface temperature will accelerate the inorganic CO2 cycle, reducing its concentration to levels lethally low for plants (6995099999999999999♠10 ppm for C4 photosynthesis) in approximately 500–7016284018400000000♠900 Ma. The lack of vegetation will result in the loss of oxygen in the atmosphere, and animal life will become extinct. After another billion years all surface water will have disappeared and the mean global temperature will reach 7002343150000000000♠70 °C (7002343150000000000♠158 °F). From that point, the Earth is expected to be habitable for another 7016157788000000000♠500 Ma, possibly up to 7016725824800000000♠2.3 Ga if nitrogen is removed from the atmosphere. Even if the Sun were eternal and stable, 27% of the water in the modern oceans will descend to the mantle in one billion years, due to reduced steam venting from mid-ocean ridges.
The Sun will evolve to become a red giant in about 7017157788000000000♠5 Ga. Models predict that the Sun will expand to roughly 1 AU (150,000,000 km), which is about 250 times its present radius. Earth's fate is less clear. As a red giant, the Sun will lose roughly 30% of its mass, so, without tidal effects, Earth will move to an orbit 1.7 AU from the Sun when the star reaches its maximum radius. Most, if not all, remaining life will be destroyed by the Sun's increased luminosity (peaking at about 5,000 times its present level). A 2008 simulation indicates that Earth's orbit will eventually decay due to tidal effects and drag, causing it to enter the Sun's atmosphere and be vaporized.







The shape of Earth is approximately oblate spheroidal. Due to rotation, the Earth is flattened along the geographic axis and bulging around the equator. The diameter of the Earth at the equator is 43 kilometres (27 mi) larger than the pole-to-pole diameter. Thus the point on the surface farthest from Earth's center of mass is the summit of the equatorial Chimborazo volcano in Ecuador. The average diameter of the reference spheroid is 12,742 kilometres (7,918 mi). Local topography deviates from this idealized spheroid, although on a global scale these deviations are small compared to Earth's radius: The maximum deviation of only 0.17% is at the Mariana Trench (10,911 metres (35,797 ft) below local sea level), whereas Mount Everest (8,848 metres (29,029 ft) above local sea level) represents a deviation of 0.14%.




Earth's mass is approximately 7024597000000000000♠5.97×1024 kg (5,970 Yg). It is composed mostly of iron (32.1%), oxygen (30.1%), silicon (15.1%), magnesium (13.9%), sulfur (2.9%), nickel (1.8%), calcium (1.5%), and aluminium (1.4%), with the remaining 1.2% consisting of trace amounts of other elements. Due to mass segregation, the core region is estimated to be primarily composed of iron (88.8%), with smaller amounts of nickel (5.8%), sulfur (4.5%), and less than 1% trace elements.
A little more than 47% of Earth's crust consists of oxygen. The most common rock constituents of the crust are nearly all oxides: chlorine, sulfur, and fluorine are the important exceptions to this and their total amount in any rock is usually much less than 1%. The principal oxides are silica, alumina, iron oxides, lime, magnesia, potash, and soda. The silica functions principally as an acid, forming silicates, and all the most common minerals of igneous rocks are of this nature. 99.22% of all rocks are composed of 11 oxides (see the table at right), with the other constituents occurring in minute quantities.




Earth's interior, like that of the other terrestrial planets, is divided into layers by their chemical or physical (rheological) properties. The outer layer is a chemically distinct silicate solid crust, which is underlain by a highly viscous solid mantle. The crust is separated from the mantle by the Mohorovičić discontinuity. The thickness of the crust varies from about 7003600000000000000♠6 km (kilometers) under the oceans to 30–50 km for the continents. The crust and the cold, rigid, top of the upper mantle are collectively known as the lithosphere, and it is of the lithosphere that the tectonic plates are composed. Beneath the lithosphere is the asthenosphere, a relatively low-viscosity layer on which the lithosphere rides. Important changes in crystal structure within the mantle occur at 410 and 7005660000000000000♠660 km below the surface, spanning a transition zone that separates the upper and lower mantle. Beneath the mantle, an extremely low viscosity liquid outer core lies above a solid inner core. The Earth's inner core might rotate at a slightly higher angular velocity than the remainder of the planet, advancing by 0.1–0.5° per year. The radius of the inner core is about one fifth of that of Earth.




Earth's internal heat comes from a combination of residual heat from planetary accretion (about 20%) and heat produced through radioactive decay (80%). The major heat-producing isotopes within Earth are potassium-40, uranium-238, and thorium-232. At the center, the temperature may be up to 6,000 °C (10,830 °F), and the pressure could reach 360 GPa. Because much of the heat is provided by radioactive decay, scientists postulate that early in Earth's history, before isotopes with short half-lives were depleted, Earth's heat production was much higher. At approximately 7016946728000000000♠3 Ga, twice the present-day heat would have been produced, increasing the rates of mantle convection and plate tectonics, and allowing the production of uncommon igneous rocks such as komatiites that are rarely formed today.
The mean heat loss from Earth is 87 mW m−2, for a global heat loss of 4.42 × 1013 W. A portion of the core's thermal energy is transported toward the crust by mantle plumes, a form of convection consisting of upwellings of higher-temperature rock. These plumes can produce hotspots and flood basalts. More of the heat in Earth is lost through plate tectonics, by mantle upwelling associated with mid-ocean ridges. The final major mode of heat loss is through conduction through the lithosphere, the majority of which occurs under the oceans because the crust there is much thinner than that of the continents.




The mechanically rigid outer layer of Earth, the lithosphere, is divided into pieces called tectonic plates. These plates are rigid segments that move in relation to one another at one of three types of plate boundaries: convergent boundaries, at which two plates come together, divergent boundaries, at which two plates are pulled apart, and transform boundaries, in which two plates slide past one another laterally. Earthquakes, volcanic activity, mountain-building, and oceanic trench formation can occur along these plate boundaries. The tectonic plates ride on top of the asthenosphere, the solid but less-viscous part of the upper mantle that can flow and move along with the plates.
As the tectonic plates migrate, oceanic crust is subducted under the leading edges of the plates at convergent boundaries. At the same time, the upwelling of mantle material at divergent boundaries creates mid-ocean ridges. The combination of these processes recycles the oceanic crust back into the mantle. Due to this recycling, most of the ocean floor is less than 7015315576000000000♠100 Ma old in age. The oldest oceanic crust is located in the Western Pacific and has an estimated age of 7015631152000000000♠200 Ma. By comparison, the oldest dated continental crust is 7017127177128000000♠4030 Ma.
The seven major plates are the Pacific, North American, Eurasian, African, Antarctic, Indo-Australian, and South American. Other notable plates include the Arabian Plate, the Caribbean Plate, the Nazca Plate off the west coast of South America and the Scotia Plate in the southern Atlantic Ocean. The Australian Plate fused with the Indian Plate between 50 and 7015173566800000000♠55 mya. The fastest-moving plates are the oceanic plates, with the Cocos Plate advancing at a rate of 75 mm/year and the Pacific Plate moving 52–69 mm/year. At the other extreme, the slowest-moving plate is the Eurasian Plate, progressing at a typical rate of 21 mm/year.




The total surface area of the Earth is about 7008510000000000000♠510 million km2 (197 million sq mi). Of this, 70.8%, or 7008361130000000000♠361.13 million km2 (139.43 million sq mi), is below sea level and covered by ocean water. Below the ocean's surface are much of the continental shelf, mountains, volcanoes, oceanic trenches, submarine canyons, oceanic plateaus, abyssal plains, and a globe-spanning mid-ocean ridge system. The remaining 29.2% (7008148940000000000♠148.94 million km2, or 57.51 million sq mi) not covered by water has terrain that varies greatly from place to place and consists of mountains, deserts, plains, plateaus, and other landforms. Tectonics and erosion, volcanic eruptions, flooding, weathering, glaciation, the growth of coral reefs, and meteorite impacts are among the processes that constantly reshape the Earth's surface over geological time.
The continental crust consists of lower density material such as the igneous rocks granite and andesite. Less common is basalt, a denser volcanic rock that is the primary constituent of the ocean floors. Sedimentary rock is formed from the accumulation of sediment that becomes buried and compacted together. Nearly 75% of the continental surfaces are covered by sedimentary rocks, although they form about 5% of the crust. The third form of rock material found on Earth is metamorphic rock, which is created from the transformation of pre-existing rock types through high pressures, high temperatures, or both. The most abundant silicate minerals on Earth's surface include quartz, feldspars, amphibole, mica, pyroxene and olivine. Common carbonate minerals include calcite (found in limestone) and dolomite.
The elevation of the land surface varies from the low point of −418 m at the Dead Sea, to a maximum altitude of 8,848 m at the top of Mount Everest. The mean height of land above sea level is 840 m.
The pedosphere is the outermost layer of Earth's continental surface and is composed of soil and subject to soil formation processes. The total arable land is 10.9% of the land surface, with 1.3% being permanent cropland. Close to 40% of Earth's land surface is used for cropland and pasture, or an estimated 1.3×107 km2 of cropland and 3.4×107 km2 of pastureland.




The abundance of water on Earth's surface is a unique feature that distinguishes the "Blue Planet" from other planets in the Solar System. Earth's hydrosphere consists chiefly of the oceans, but technically includes all water surfaces in the world, including inland seas, lakes, rivers, and underground waters down to a depth of 2,000 m. The deepest underwater location is Challenger Deep of the Mariana Trench in the Pacific Ocean with a depth of 10,911.4 m.
The mass of the oceans is approximately 1.35×1018 metric tons or about 1/4400 of Earth's total mass. The oceans cover an area of 7014361800000000000♠3.618×108 km2 with a mean depth of 7003368200000000000♠3682 m, resulting in an estimated volume of 7018133200000000000♠1.332×109 km3. If all of Earth's crustal surface were at the same elevation as a smooth sphere, the depth of the resulting world ocean would be 2.7 to 2.8 km.
About 97.5% of the water is saline; the remaining 2.5% is fresh water. Most fresh water, about 68.7%, is present as ice in ice caps and glaciers.
The average salinity of Earth's oceans is about 35 grams of salt per kilogram of sea water (3.5% salt). Most of this salt was released from volcanic activity or extracted from cool igneous rocks. The oceans are also a reservoir of dissolved atmospheric gases, which are essential for the survival of many aquatic life forms. Sea water has an important influence on the world's climate, with the oceans acting as a large heat reservoir. Shifts in the oceanic temperature distribution can cause significant weather shifts, such as the El Niño-Southern Oscillation.




The atmospheric pressure on Earth's surface averages 101.325 kPa, with a scale height of about 8.5 km. It has a composition of 78% nitrogen and 21% oxygen, with trace amounts of water vapor, carbon dioxide, and other gaseous molecules. The height of the troposphere varies with latitude, ranging between 8 km at the poles to 17 km at the equator, with some variation resulting from weather and seasonal factors.
Earth's biosphere has significantly altered its atmosphere. Oxygenic photosynthesis evolved 7000270000000000000♠2.7 Gya, forming the primarily nitrogen–oxygen atmosphere of today. This change enabled the proliferation of aerobic organisms and, indirectly, the formation of the ozone layer due to the subsequent conversion of atmospheric O2 into O3. The ozone layer blocks ultraviolet solar radiation, permitting life on land. Other atmospheric functions important to life include transporting water vapor, providing useful gases, causing small meteors to burn up before they strike the surface, and moderating temperature. This last phenomenon is known as the greenhouse effect: trace molecules within the atmosphere serve to capture thermal energy emitted from the ground, thereby raising the average temperature. Water vapor, carbon dioxide, methane, and ozone are the primary greenhouse gases in the atmosphere. Without this heat-retention effect, the average surface temperature would be −18 °C, in contrast to the current +15 °C, and life would likely not exist.




Earth's atmosphere has no definite boundary, slowly becoming thinner and fading into outer space. Three-quarters of the atmosphere's mass is contained within the first 11 km of the surface. This lowest layer is called the troposphere. Energy from the Sun heats this layer, and the surface below, causing expansion of the air. This lower-density air then rises and is replaced by cooler, higher-density air. The result is atmospheric circulation that drives the weather and climate through redistribution of thermal energy.
The primary atmospheric circulation bands consist of the trade winds in the equatorial region below 30° latitude and the westerlies in the mid-latitudes between 30° and 60°. Ocean currents are also important factors in determining climate, particularly the thermohaline circulation that distributes thermal energy from the equatorial oceans to the polar regions.
Water vapor generated through surface evaporation is transported by circulatory patterns in the atmosphere. When atmospheric conditions permit an uplift of warm, humid air, this water condenses and falls to the surface as precipitation. Most of the water is then transported to lower elevations by river systems and usually returned to the oceans or deposited into lakes. This water cycle is a vital mechanism for supporting life on land and is a primary factor in the erosion of surface features over geological periods. Precipitation patterns vary widely, ranging from several meters of water per year to less than a millimeter. Atmospheric circulation, topographic features, and temperature differences determine the average precipitation that falls in each region.
The amount of solar energy reaching Earth's surface decreases with increasing latitude. At higher latitudes, the sunlight reaches the surface at lower angles, and it must pass through thicker columns of the atmosphere. As a result, the mean annual air temperature at sea level decreases by about 0.4 °C (0.7 °F) per degree of latitude from the equator. Earth's surface can be subdivided into specific latitudinal belts of approximately homogeneous climate. Ranging from the equator to the polar regions, these are the tropical (or equatorial), subtropical, temperate and polar climates.
This latitudinal rule has several anomalies:
Proximity to oceans moderates the climate. For example, the Scandinavian peninsula has more moderate climate than similarly northern latitudes of northern Canada.
The wind enables this moderating effect. The windward side of a land mass experiences more moderation than the leeward side. In the Northern Hemisphere, the prevailing wind is west-to-east, and western coasts tend to be milder than eastern coasts. This is seen in Eastern North America and Western Europe, where rough continental climates appear on the east coast on parallels with mild climates on the other side of the ocean. In the Southern Hemisphere, the prevailing wind is east-to-west, and the eastern coasts are milder.
The distance from the Earth to the Sun varies. The Earth is closest to the Sun (at perihelion) in January, which is summer in the Southern Hemisphere. It is furthest away (at aphelion) in July, which is summer in the Northern Hemisphere, and only 93.55% of the solar radiation from the Sun falls on a given square area of land than at perihelion. Despite this, there are larger land masses in the Northern Hemisphere, which are easier to heat than the seas. Consequently, summers are 2.3 °C (4 °F) warmer in the Northern Hemisphere than in the Southern Hemisphere under similar conditions.
The climate is colder at high altitudes than at sea level because of the decreased air density.
The commonly used Köppen climate classification system has five broad groups (humid tropics, arid, humid middle latitudes, continental and cold polar), which are further divided into more specific subtypes. The Köppen system rates regions of terrain based on observed temperature and precipitation.
The highest air temperature ever measured on Earth was 56.7 °C (134.1 °F) in Furnace Creek, California, in Death Valley, in 1913. The lowest air temperature ever directly measured on Earth was −89.2 °C (−128.6 °F) at Vostok Station in 1983, but satellites have used remote sensing to measure temperatures as low as −94.7 °C (−138.5 °F) in East Antarctica. These temperature records are only measurements made with modern instruments from the 20th century onwards and likely do not reflect the full range of temperature on Earth.




Above the troposphere, the atmosphere is usually divided into the stratosphere, mesosphere, and thermosphere. Each layer has a different lapse rate, defining the rate of change in temperature with height. Beyond these, the exosphere thins out into the magnetosphere, where the geomagnetic fields interact with the solar wind. Within the stratosphere is the ozone layer, a component that partially shields the surface from ultraviolet light and thus is important for life on Earth. The Kármán line, defined as 100 km above Earth's surface, is a working definition for the boundary between the atmosphere and outer space.
Thermal energy causes some of the molecules at the outer edge of the atmosphere to increase their velocity to the point where they can escape from Earth's gravity. This causes a slow but steady loss of the atmosphere into space. Because unfixed hydrogen has a low molecular mass, it can achieve escape velocity more readily, and it leaks into outer space at a greater rate than other gases. The leakage of hydrogen into space contributes to the shifting of Earth's atmosphere and surface from an initially reducing state to its current oxidizing one. Photosynthesis provided a source of free oxygen, but the loss of reducing agents such as hydrogen is thought to have been a necessary precondition for the widespread accumulation of oxygen in the atmosphere. Hence the ability of hydrogen to escape from the atmosphere may have influenced the nature of life that developed on Earth. In the current, oxygen-rich atmosphere most hydrogen is converted into water before it has an opportunity to escape. Instead, most of the hydrogen loss comes from the destruction of methane in the upper atmosphere.




The main part of Earth's magnetic field is generated in the core, the site of a dynamo process that converts the kinetic energy of thermally and compositionally driven convection into electrical and magnetic field energy. The field extends outwards from the core, through the mantle, and up to Earth's surface, where it is, approximately, a dipole. The poles of the dipole are located close to Earth's geographic poles. At the equator of the magnetic field, the magnetic-field strength at the surface is 3.05 × 10−5 T, with global magnetic dipole moment of 7.91 × 1015 T m3. The convection movements in the core are chaotic; the magnetic poles drift and periodically change alignment. This causes secular variation of the main field and field reversals at irregular intervals averaging a few times every million years. The most recent reversal occurred approximately 700,000 years ago.




The extent of Earth's magnetic field in space defines the magnetosphere. Ions and electrons of the solar wind are deflected by the magnetosphere; solar wind pressure compresses the dayside of the magnetosphere, to about 10 Earth radii, and extends the nightside magnetosphere into a long tail. Because the velocity of the solar wind is greater than the speed at which wave propagate through the solar wind, a supersonic bowshock precedes the dayside magnetosphere within the solar wind. Charged particles are contained within the magnetosphere; the plasmasphere is defined by low-energy particles that essentially follow magnetic field lines as Earth rotates; the ring current is defined by medium-energy particles that drift relative to the geomagnetic field, but with paths that are still dominated by the magnetic field, and the Van Allen radiation belt are formed by high-energy particles whose motion is essentially random, but otherwise contained by the magnetosphere.
During magnetic storms and substorms, charged particles can be deflected from the outer magnetosphere and especially the magnetotail, directed along field lines into Earth's ionosphere, where atmospheric atoms can be excited and ionized, causing the aurora.







Earth's rotation period relative to the Sun—its mean solar day—is 86,400 seconds of mean solar time (86,400.0025 SI seconds). Because Earth's solar day is now slightly longer than it was during the 19th century due to tidal deceleration, each day varies between 0 and 2 SI ms longer.
Earth's rotation period relative to the fixed stars, called its stellar day by the International Earth Rotation and Reference Systems Service (IERS), is 86,164.098903691 seconds of mean solar time (UT1), or 23h 56m 4.098903691s. Earth's rotation period relative to the precessing or moving mean vernal equinox, misnamed its sidereal day, is 86,164.09053083288 seconds of mean solar time (UT1) (23h 56m 4.09053083288s) as of 1982. Thus the sidereal day is shorter than the stellar day by about 8.4 ms. The length of the mean solar day in SI seconds is available from the IERS for the periods 1623–2005 and 1962–2005.
Apart from meteors within the atmosphere and low-orbiting satellites, the main apparent motion of celestial bodies in Earth's sky is to the west at a rate of 15°/h = 15'/min. For bodies near the celestial equator, this is equivalent to an apparent diameter of the Sun or the Moon every two minutes; from Earth's surface, the apparent sizes of the Sun and the Moon are approximately the same.




Earth orbits the Sun at an average distance of about 150 million km (93 million mi) every 365.2564 mean solar days, or one sidereal year. This gives an apparent movement of the Sun eastward with respect to the stars at a rate of about 1°/day, which is one apparent Sun or Moon diameter every 12 hours. Due to this motion, on average it takes 24 hours—a solar day—for Earth to complete a full rotation about its axis so that the Sun returns to the meridian. The orbital speed of Earth averages about 29.78 km/s (107,200 km/h; 66,600 mph), which is fast enough to travel a distance equal to Earth's diameter, about 12,742 km (7,918 mi), in seven minutes, and the distance to the Moon, 384,000 km (239,000 mi), in about 3.5 hours.
The Moon and Earth orbit a common barycenter every 27.32 days relative to the background stars. When combined with the Earth–Moon system's common orbit around the Sun, the period of the synodic month, from new moon to new moon, is 29.53 days. Viewed from the celestial north pole, the motion of Earth, the Moon, and their axial rotations are all counterclockwise. Viewed from a vantage point above the north poles of both the Sun and Earth, Earth orbits in a counterclockwise direction about the Sun. The orbital and axial planes are not precisely aligned: Earth's axis is tilted some 23.44 degrees from the perpendicular to the Earth–Sun plane (the ecliptic), and the Earth–Moon plane is tilted up to ±5.1 degrees against the Earth–Sun plane. Without this tilt, there would be an eclipse every two weeks, alternating between lunar eclipses and solar eclipses.
The Hill sphere, or the sphere of gravitational influence, of the Earth is about 1.5 million kilometres (930,000 mi) in radius. This is the maximum distance at which the Earth's gravitational influence is stronger than the more distant Sun and planets. Objects must orbit the Earth within this radius, or they can become unbound by the gravitational perturbation of the Sun.
Earth, along with the Solar System, is situated in the Milky Way and orbits about 28,000 light-years from its center. It is about 20 light-years above the galactic plane in the Orion Arm.




The axial tilt of the Earth is approximately 23.439281° with the axis of its orbit plane, always pointing towards the Celestial Poles. Due to Earth's axial tilt, the amount of sunlight reaching any given point on the surface varies over the course of the year. This causes the seasonal change in climate, with summer in the Northern Hemisphere occurring when the Tropic of Cancer is facing the Sun, and winter taking place when the Tropic of Capricorn in the Southern Hemisphere faces the Sun. During the summer, the day lasts longer, and the Sun climbs higher in the sky. In winter, the climate becomes cooler and the days shorter. In northern temperate latitudes, the Sun rises north of true east during the summer solstice, and sets north of true west, reversing in the winter. The Sun rises south of true east in the summer for the southern temperate zone and sets south of true west.
Above the Arctic Circle, an extreme case is reached where there is no daylight at all for part of the year, up to six months at the North Pole itself, a polar night. In the Southern Hemisphere, the situation is exactly reversed, with the South Pole oriented opposite the direction of the North Pole. Six months later, this pole will experience a midnight sun, a day of 24 hours, again reversing with the South Pole.
By astronomical convention, the four seasons can be determined by the solstices—the points in the orbit of maximum axial tilt toward or away from the Sun—and the equinoxes, when the direction of the tilt and the direction to the Sun are perpendicular. In the Northern Hemisphere, winter solstice currently occurs around 21 December; summer solstice is near 21 June, spring equinox is around 20 March and autumnal equinox is about 22 or 23 September. In the Southern Hemisphere, the situation is reversed, with the summer and winter solstices exchanged and the spring and autumnal equinox dates swapped.
The angle of Earth's axial tilt is relatively stable over long periods of time. Its axial tilt does undergo nutation; a slight, irregular motion with a main period of 18.6 years. The orientation (rather than the angle) of Earth's axis also changes over time, precessing around in a complete circle over each 25,800 year cycle; this precession is the reason for the difference between a sidereal year and a tropical year. Both of these motions are caused by the varying attraction of the Sun and the Moon on Earth's equatorial bulge. The poles also migrate a few meters across Earth's surface. This polar motion has multiple, cyclical components, which collectively are termed quasiperiodic motion. In addition to an annual component to this motion, there is a 14-month cycle called the Chandler wobble. Earth's rotational velocity also varies in a phenomenon known as length-of-day variation.
In modern times, Earth's perihelion occurs around 3 January, and its aphelion around 4 July. These dates change over time due to precession and other orbital factors, which follow cyclical patterns known as Milankovitch cycles. The changing Earth–Sun distance causes an increase of about 6.9% in solar energy reaching Earth at perihelion relative to aphelion. Because the Southern Hemisphere is tilted toward the Sun at about the same time that Earth reaches the closest approach to the Sun, the Southern Hemisphere receives slightly more energy from the Sun than does the northern over the course of a year. This effect is much less significant than the total energy change due to the axial tilt, and most of the excess energy is absorbed by the higher proportion of water in the Southern Hemisphere.




A planet that can sustain life is termed habitable, even if life did not originate there. Earth provides liquid water—an environment where complex organic molecules can assemble and interact, and sufficient energy to sustain metabolism. The distance of Earth from the Sun, as well as its orbital eccentricity, rate of rotation, axial tilt, geological history, sustaining atmosphere, and magnetic field all contribute to the current climatic conditions at the surface.




A planet's life forms inhabit ecosystems, whose total is sometimes said to form a "biosphere". Earth's biosphere is thought to have begun evolving about 7000350000000000000♠3.5 Gya. The biosphere is divided into a number of biomes, inhabited by broadly similar plants and animals. On land, biomes are separated primarily by differences in latitude, height above sea level and humidity. Terrestrial biomes lying within the Arctic or Antarctic Circles, at high altitudes or in extremely arid areas are relatively barren of plant and animal life; species diversity reaches a peak in humid lowlands at equatorial latitudes.
In July 2016, scientists reported identifying a set of 355 genes from the Last Universal Common Ancestor (LUCA) of all organisms living on Earth.




Earth has resources that have been exploited by humans. Those termed non-renewable resources, such as fossil fuels, only renew over geological timescales.
Large deposits of fossil fuels are obtained from Earth's crust, consisting of coal, petroleum, and natural gas. These deposits are used by humans both for energy production and as feedstock for chemical production. Mineral ore bodies have also been formed within the crust through a process of ore genesis, resulting from actions of magmatism, erosion, and plate tectonics. These bodies form concentrated sources for many metals and other useful elements.
Earth's biosphere produces many useful biological products for humans, including food, wood, pharmaceuticals, oxygen, and the recycling of many organic wastes. The land-based ecosystem depends upon topsoil and fresh water, and the oceanic ecosystem depends upon dissolved nutrients washed down from the land. In 1980, 5,053 Mha (50.53 million km2) of Earth's land surface consisted of forest and woodlands, 6,788 Mha (67.88 million km2) was grasslands and pasture, and 1,501 Mha (15.01 million km2) was cultivated as croplands. The estimated amount of irrigated land in 1993 was 2,481,250 square kilometres (958,020 sq mi). Humans also live on the land by using building materials to construct shelters.




Large areas of Earth's surface are subject to extreme weather such as tropical cyclones, hurricanes, or typhoons that dominate life in those areas. From 1980 to 2000, these events caused an average of 11,800 human deaths per year. Many places are subject to earthquakes, landslides, tsunamis, volcanic eruptions, tornadoes, sinkholes, blizzards, floods, droughts, wildfires, and other calamities and disasters.
Many localized areas are subject to human-made pollution of the air and water, acid rain and toxic substances, loss of vegetation (overgrazing, deforestation, desertification), loss of wildlife, species extinction, soil degradation, soil depletion and erosion.
There is a scientific consensus linking human activities to global warming due to industrial carbon dioxide emissions. This is predicted to produce changes such as the melting of glaciers and ice sheets, more extreme temperature ranges, significant changes in weather and a global rise in average sea levels.




Cartography, the study and practice of map-making, and geography, the study of the lands, features, inhabitants and phenomena on Earth, have historically been the disciplines devoted to depicting Earth. Surveying, the determination of locations and distances, and to a lesser extent navigation, the determination of position and direction, have developed alongside cartography and geography, providing and suitably quantifying the requisite information.
Earth's human population reached approximately seven billion on 31 October 2011. Projections indicate that the world's human population will reach 9.2 billion in 2050. Most of the growth is expected to take place in developing nations. Human population density varies widely around the world, but a majority live in Asia. By 2020, 60% of the world's population is expected to be living in urban, rather than rural, areas.
It is estimated that one-eighth of Earth's surface is suitable for humans to live on – three-quarters of Earth's surface is covered by oceans, leaving one-quarter as land. Half of that land area is desert (14%), high mountains (27%), or other unsuitable terrains. The northernmost permanent settlement in the world is Alert, on Ellesmere Island in Nunavut, Canada. (82°28′N) The southernmost is the Amundsen–Scott South Pole Station, in Antarctica, almost exactly at the South Pole. (90°S)
Independent sovereign nations claim the planet's entire land surface, except for some parts of Antarctica, a few land parcels along the Danube river's western bank, and the unclaimed area of Bir Tawil between Egypt and Sudan. As of 2015, there are 193 sovereign states that are member states of the United Nations, plus two observer states and 72 dependent territories and states with limited recognition. Earth has never had a sovereign government with authority over the entire globe, although some nation-states have striven for world domination and failed.
The United Nations is a worldwide intergovernmental organization that was created with the goal of intervening in the disputes between nations, thereby avoiding armed conflict. The U.N. serves primarily as a forum for international diplomacy and international law. When the consensus of the membership permits, it provides a mechanism for armed intervention.
The first human to orbit Earth was Yuri Gagarin on 12 April 1961. In total, about 487 people have visited outer space and reached orbit as of 30 July 2010, and, of these, twelve have walked on the Moon. Normally, the only humans in space are those on the International Space Station. The station's crew, made up of six people, is usually replaced every six months. The farthest that humans have traveled from Earth is 400,171 km, achieved during the Apollo 13 mission in 1970.




The Moon is a relatively large, terrestrial, planet-like natural satellite, with a diameter about one-quarter of Earth's. It is the largest moon in the Solar System relative to the size of its planet, although Charon is larger relative to the dwarf planet Pluto. The natural satellites of other planets are also referred to as "moons", after Earth's.
The gravitational attraction between Earth and the Moon causes tides on Earth. The same effect on the Moon has led to its tidal locking: its rotation period is the same as the time it takes to orbit Earth. As a result, it always presents the same face to the planet. As the Moon orbits Earth, different parts of its face are illuminated by the Sun, leading to the lunar phases; the dark part of the face is separated from the light part by the solar terminator.

Due to their tidal interaction, the Moon recedes from Earth at the rate of approximately 38 mm/yr. Over millions of years, these tiny modifications—and the lengthening of Earth's day by about 23 µs/yr—add up to significant changes. During the Devonian period, for example, (approximately 7016129386160000000♠410 mya) there were 400 days in a year, with each day lasting 21.8 hours.
The Moon may have dramatically affected the development of life by moderating the planet's climate. Paleontological evidence and computer simulations show that Earth's axial tilt is stabilized by tidal interactions with the Moon. Some theorists think that without this stabilization against the torques applied by the Sun and planets to Earth's equatorial bulge, the rotational axis might be chaotically unstable, exhibiting chaotic changes over millions of years, as appears to be the case for Mars.
Viewed from Earth, the Moon is just far enough away to have almost the same apparent-sized disk as the Sun. The angular size (or solid angle) of these two bodies match because, although the Sun's diameter is about 400 times as large as the Moon's, it is also 400 times more distant. This allows total and annular solar eclipses to occur on Earth.
The most widely accepted theory of the Moon's origin, the giant-impact hypothesis, states that it formed from the collision of a Mars-size protoplanet called Theia with the early Earth. This hypothesis explains (among other things) the Moon's relative lack of iron and volatile elements and the fact that its composition is nearly identical to that of Earth's crust.




Earth has at least five co-orbital asteroids, including 3753 Cruithne and 2002 AA29. A trojan asteroid companion, 2010 TK7, is librating around the leading Lagrange triangular point, L4, in the Earth's orbit around the Sun.
The tiny near-Earth asteroid 2006 RH120 makes close approaches to the Earth–Moon system roughly every twenty years. During these approaches, it can orbit Earth for brief periods of time.
As of June 2016, there were 1,419 operational, human-made satellites orbiting Earth. There are also inoperative satellites, including Vanguard 1, the oldest satellite currently in orbit, and over 16,000 pieces of tracked space debris. Earth's largest artificial satellite is the International Space Station.




The standard astronomical symbol of Earth consists of a cross circumscribed by a circle, , representing the four quadrants of the world.
Human cultures have developed many views of the planet. Earth is sometimes personified as a deity. In many cultures it is a mother goddess that is also the primary fertility deity, and by the mid-20th century, the Gaia Principle compared Earth's environments and life as a single self-regulating organism leading to broad stabilization of the conditions of habitability. Creation myths in many religions involve the creation of Earth by a supernatural deity or deities.
Scientific investigation has resulted in several culturally transformative shifts in our view of the planet. In the West, belief in a flat Earth was displaced by the idea of spherical Earth, credited to Pythagoras in the 6th century BC. Earth was further believed to be the center of the universe until the 16th century when scientists first theorized that it was a moving object, comparable to the other planets in the Solar System. Due to the efforts of influential Christian scholars and clerics such as James Ussher, who sought to determine the age of Earth through analysis of genealogies in Scripture, Westerners before the 19th century generally believed Earth to be a few thousand years old at most. It was only during the 19th century that geologists realized Earth's age was at least many millions of years. Lord Kelvin used thermodynamics to estimate the age of Earth to be between 20 million and 400 million years in 1864, sparking a vigorous debate on the subject; it was only when radioactivity and radioactive dating were discovered in the late 19th and early 20th centuries that a reliable mechanism for determining Earth's age was established, proving the planet to be billions of years old. The perception of Earth shifted again in the 20th century when humans first viewed it from orbit, and especially with photographs of Earth returned by the Apollo program.



Celestial sphere
Earth physical characteristics tables
Earth science
Earth system science
Timeline of the far future









Comins, Neil F. (2001). Discovering the Essential Universe (2nd ed.). New York: W. H. Freeman. Bibcode:2003deu..book.....C. ISBN 0-7167-5804-0. OCLC 52082611. 




National Geographic encyclopedic entry about Earth
Earth – Profile – Solar System Exploration – NASA
Earth – Climate Changes Cause Shape to Change – NASA
United States Geological Survey – USGS
Earth – Astronaut Photography Gateway – NASA
Earth Observatory – NASA
Earth – Audio (29:28) – Cain/Gay – Astronomy Cast (2007)
Earth – Videos – International Space Station:
Video (01:02) – Earth (time-lapse)
Video (00:27) – Earth and Auroras (time-lapse)Climate is the statistics of weather, usually over a 30-year interval. It is measured by assessing the patterns of variation in temperature, humidity, atmospheric pressure, wind, precipitation, atmospheric particle count and other meteorological variables in a given region over long periods of time. Climate differs from weather, in that weather only describes the short-term conditions of these variables in a given region.
A region's climate is generated by the climate system, which has five components: atmosphere, hydrosphere, cryosphere, lithosphere, and biosphere.
The climate of a location is affected by its latitude, terrain, and altitude, as well as nearby water bodies and their currents. Climates can be classified according to the average and the typical ranges of different variables, most commonly temperature and precipitation. The most commonly used classification scheme was the Köppen climate classification. The Thornthwaite system, in use since 1948, incorporates evapotranspiration along with temperature and precipitation information and is used in studying biological diversity and how climate change effects it. The Bergeron and Spatial Synoptic Classification systems focus on the origin of air masses that define the climate of a region.
Paleoclimatology is the study of ancient climates. Since direct observations of climate are not available before the 19th century, paleoclimates are inferred from proxy variables that include non-biotic evidence such as sediments found in lake beds and ice cores, and biotic evidence such as tree rings and coral. Climate models are mathematical models of past, present and future climates. Climate change may occur over long and short timescales from a variety of factors; recent warming is discussed in global warming. Global warming results in redistributions. For example, "a 3°C change in mean annual temperature corresponds to a shift in isotherms of approximately 300–400 km in latitude (in the temperate zone) or 500 m in elevation. Therefore, species are expected to move upwards in elevation or towards the poles in latitude in response to shifting climate zones".



Climate (from Ancient Greek klima, meaning inclination) is commonly defined as the weather averaged over a long period. The standard averaging period is 30 years, but other periods may be used depending on the purpose. Climate also includes statistics other than the average, such as the magnitudes of day-to-day or year-to-year variations. The Intergovernmental Panel on Climate Change (IPCC) 2001 glossary definition is as follows:

Climate in a narrow sense is usually defined as the "average weather," or more rigorously, as the statistical description in terms of the mean and variability of relevant quantities over a period ranging from months to thousands or millions of years. The classical period is 30 years, as defined by the World Meteorological Organization (WMO). These quantities are most often surface variables such as temperature, precipitation, and wind. Climate in a wider sense is the state, including a statistical description, of the climate system.

The World Meteorological Organization (WMO) describes climate "normals" as "reference points used by climatologists to compare current climatological trends to that of the past or what is considered 'normal'. A Normal is defined as the arithmetic average of a climate element (e.g. temperature) over a 30-year period. A 30 year period is used, as it is long enough to filter out any interannual variation or anomalies, but also short enough to be able to show longer climatic trends." The WMO originated from the International Meteorological Organization which set up a technical commission for climatology in 1929. At its 1934 Wiesbaden meeting the technical commission designated the thirty-year period from 1901 to 1930 as the reference time frame for climatological standard normals. In 1982 the WMO agreed to update climate normals, and in these were subsequently completed on the basis of climate data from 1 January 1961 to 31 December 1990.
The difference between climate and weather is usefully summarized by the popular phrase "Climate is what you expect, weather is what you get." Over historical time spans there are a number of nearly constant variables that determine climate, including latitude, altitude, proportion of land to water, and proximity to oceans and mountains. These change only over periods of millions of years due to processes such as plate tectonics. Other climate determinants are more dynamic: the thermohaline circulation of the ocean leads to a 5 °C (9 °F) warming of the northern Atlantic Ocean compared to other ocean basins. Other ocean currents redistribute heat between land and water on a more regional scale. The density and type of vegetation coverage affects solar heat absorption, water retention, and rainfall on a regional level. Alterations in the quantity of atmospheric greenhouse gases determines the amount of solar energy retained by the planet, leading to global warming or global cooling. The variables which determine climate are numerous and the interactions complex, but there is general agreement that the broad outlines are understood, at least insofar as the determinants of historical climate change are concerned.




There are several ways to classify climates into similar regimes. Originally, climes were defined in Ancient Greece to describe the weather depending upon a location's latitude. Modern climate classification methods can be broadly divided into genetic methods, which focus on the causes of climate, and empiric methods, which focus on the effects of climate. Examples of genetic classification include methods based on the relative frequency of different air mass types or locations within synoptic weather disturbances. Examples of empiric classifications include climate zones defined by plant hardiness, evapotranspiration, or more generally the Köppen climate classification which was originally designed to identify the climates associated with certain biomes. A common shortcoming of these classification schemes is that they produce distinct boundaries between the zones they define, rather than the gradual transition of climate properties more common in nature.




The simplest classification is that involving air masses. The Bergeron classification is the most widely accepted form of air mass classification. Air mass classification involves three letters. The first letter describes its moisture properties, with c used for continental air masses (dry) and m for maritime air masses (moist). The second letter describes the thermal characteristic of its source region: T for tropical, P for polar, A for Arctic or Antarctic, M for monsoon, E for equatorial, and S for superior air (dry air formed by significant downward motion in the atmosphere). The third letter is used to designate the stability of the atmosphere. If the air mass is colder than the ground below it, it is labeled k. If the air mass is warmer than the ground below it, it is labeled w. While air mass identification was originally used in weather forecasting during the 1950s, climatologists began to establish synoptic climatologies based on this idea in 1973.
Based upon the Bergeron classification scheme is the Spatial Synoptic Classification system (SSC). There are six categories within the SSC scheme: Dry Polar (similar to continental polar), Dry Moderate (similar to maritime superior), Dry Tropical (similar to continental tropical), Moist Polar (similar to maritime polar), Moist Moderate (a hybrid between maritime polar and maritime tropical), and Moist Tropical (similar to maritime tropical, maritime monsoon, or maritime equatorial).




The Köppen classification depends on average monthly values of temperature and precipitation. The most commonly used form of the Köppen classification has five primary types labeled A through E. These primary types are A) tropical, B) dry, C) mild mid-latitude, D) cold mid-latitude, and E) polar. The five primary classifications can be further divided into secondary classifications such as rain forest, monsoon, tropical savanna, humid subtropical, humid continental, oceanic climate, Mediterranean climate, desert, steppe, subarctic climate, tundra, and polar ice cap.
Rain forests are characterized by high rainfall, with definitions setting minimum normal annual rainfall between 1,750 millimetres (69 in) and 2,000 millimetres (79 in). Mean monthly temperatures exceed 18 °C (64 °F) during all months of the year.
A monsoon is a seasonal prevailing wind which lasts for several months, ushering in a region's rainy season. Regions within North America, South America, Sub-Saharan Africa, Australia and East Asia are monsoon regimes.

A tropical savanna is a grassland biome located in semiarid to semi-humid climate regions of subtropical and tropical latitudes, with average temperatures remain at or above 18 °C (64 °F) year round and rainfall between 750 millimetres (30 in) and 1,270 millimetres (50 in) a year. They are widespread on Africa, and are found in India, the northern parts of South America, Malaysia, and Australia.

The humid subtropical climate zone where winter rainfall (and sometimes snowfall) is associated with large storms that the westerlies steer from west to east. Most summer rainfall occurs during thunderstorms and from occasional tropical cyclones. Humid subtropical climates lie on the east side continents, roughly between latitudes 20° and 40° degrees away from the equator.

A humid continental climate is marked by variable weather patterns and a large seasonal temperature variance. Places with more than three months of average daily temperatures above 10 °C (50 °F) and a coldest month temperature below −3 °C (27 °F) and which do not meet the criteria for an arid or semiarid climate, are classified as continental.
An oceanic climate is typically found along the west coasts at the middle latitudes of all the world's continents, and in southeastern Australia, and is accompanied by plentiful precipitation year-round.
The Mediterranean climate regime resembles the climate of the lands in the Mediterranean Basin, parts of western North America, parts of Western and South Australia, in southwestern South Africa and in parts of central Chile. The climate is characterized by hot, dry summers and cool, wet winters.
A steppe is a dry grassland with an annual temperature range in the summer of up to 40 °C (104 °F) and during the winter down to −40 °C (−40 °F).
A subarctic climate has little precipitation, and monthly temperatures which are above 10 °C (50 °F) for one to three months of the year, with permafrost in large parts of the area due to the cold winters. Winters within subarctic climates usually include up to six months of temperatures averaging below 0 °C (32 °F).

Tundra occurs in the far Northern Hemisphere, north of the taiga belt, including vast areas of northern Russia and Canada.
A polar ice cap, or polar ice sheet, is a high-latitude region of a planet or moon that is covered in ice. Ice caps form because high-latitude regions receive less energy as solar radiation from the sun than equatorial regions, resulting in lower surface temperatures.
A desert is a landscape form or region that receives very little precipitation. Deserts usually have a large diurnal and seasonal temperature range, with high or low, depending on location daytime temperatures (in summer up to 45 °C or 113 °F), and low nighttime temperatures (in winter down to 0 °C or 32 °F) due to extremely low humidity. Many deserts are formed by rain shadows, as mountains block the path of moisture and precipitation to the desert.




Devised by the American climatologist and geographer C. W. Thornthwaite, this climate classification method monitors the soil water budget using evapotranspiration. It monitors the portion of total precipitation used to nourish vegetation over a certain area. It uses indices such as a humidity index and an aridity index to determine an area's moisture regime based upon its average temperature, average rainfall, and average vegetation type. The lower the value of the index in any given area, the drier the area is.
The moisture classification includes climatic classes with descriptors such as hyperhumid, humid, subhumid, subarid, semi-arid (values of −20 to −40), and arid (values below −40). Humid regions experience more precipitation than evaporation each year, while arid regions experience greater evaporation than precipitation on an annual basis. A total of 33 percent of the Earth's landmass is considered either arid or semi-arid, including southwest North America, southwest South America, most of northern and a small part of southern Africa, southwest and portions of eastern Asia, as well as much of Australia. Studies suggest that precipitation effectiveness (PE) within the Thornthwaite moisture index is overestimated in the summer and underestimated in the winter. This index can be effectively used to determine the number of herbivore and mammal species numbers within a given area. The index is also used in studies of climate change.
Thermal classifications within the Thornthwaite scheme include microthermal, mesothermal, and megathermal regimes. A microthermal climate is one of low annual mean temperatures, generally between 0 °C (32 °F) and 14 °C (57 °F) which experiences short summers and has a potential evaporation between 14 centimetres (5.5 in) and 43 centimetres (17 in). A mesothermal climate lacks persistent heat or persistent cold, with potential evaporation between 57 centimetres (22 in) and 114 centimetres (45 in). A megathermal climate is one with persistent high temperatures and abundant rainfall, with potential annual evaporation in excess of 114 centimetres (45 in).







Details of the modern climate record are known through the taking of measurements from such weather instruments as thermometers, barometers, and anemometers during the past few centuries. The instruments used to study weather over the modern time scale, their known error, their immediate environment, and their exposure have changed over the years, which must be considered when studying the climate of centuries past.




Paleoclimatology is the study of past climate over a great period of the Earth's history. It uses evidence from ice sheets, tree rings, sediments, coral, and rocks to determine the past state of the climate. It demonstrates periods of stability and periods of change and can indicate whether changes follow patterns such as regular cycles.




Climate change is the variation in global or regional climates over time. It reflects changes in the variability or average state of the atmosphere over time scales ranging from decades to millions of years. These changes can be caused by processes internal to the Earth, external forces (e.g. variations in sunlight intensity) or, more recently, human activities.

In recent usage, especially in the context of environmental policy, the term "climate change" often refers only to changes in modern climate, including the rise in average surface temperature known as global warming. In some cases, the term is also used with a presumption of human causation, as in the United Nations Framework Convention on Climate Change (UNFCCC). The UNFCCC uses "climate variability" for non-human caused variations.
Earth has undergone periodic climate shifts in the past, including four major ice ages. These consisting of glacial periods where conditions are colder than normal, separated by interglacial periods. The accumulation of snow and ice during a glacial period increases the surface albedo, reflecting more of the Sun's energy into space and maintaining a lower atmospheric temperature. Increases in greenhouse gases, such as by volcanic activity, can increase the global temperature and produce an interglacial period. Suggested causes of ice age periods include the positions of the continents, variations in the Earth's orbit, changes in the solar output, and volcanism.




Climate models use quantitative methods to simulate the interactions of the atmosphere, oceans, land surface and ice. They are used for a variety of purposes; from the study of the dynamics of the weather and climate system, to projections of future climate. All climate models balance, or very nearly balance, incoming energy as short wave (including visible) electromagnetic radiation to the earth with outgoing energy as long wave (infrared) electromagnetic radiation from the earth. Any imbalance results in a change in the average temperature of the earth.
The most talked-about applications of these models in recent years have been their use to infer the consequences of increasing greenhouse gases in the atmosphere, primarily carbon dioxide (see greenhouse gas). These models predict an upward trend in the global mean surface temperature, with the most rapid increase in temperature being projected for the higher latitudes of the Northern Hemisphere.
Models can range from relatively simple to quite complex:
Simple radiant heat transfer model that treats the earth as a single point and averages outgoing energy
this can be expanded vertically (radiative-convective models), or horizontally
finally, (coupled) atmosphere–ocean–sea ice global climate models discretise and solve the full equations for mass and energy transfer and radiant exchange.
Climate forecasting is a way by some scientists are using to predict climate change. In 1997 the prediction division of the International Research Institute for Climate and Society at Columbia University began generating seasonal climate forecasts on a real-time basis. To produce these forecasts an extensive suite of forecasting tools was developed, including a multimodel ensemble approach that required thorough validation of each model's accuracy level in simulating interannual climate variability.









The Study of Climate on Alien Worlds; Characterizing atmospheres beyond our Solar System is now within our reach Kevin Heng July–August 2012 American Scientist
Reumert, Johannes: "Vahls climatic divisions. An explanation" (Geografisk Tidsskrift, Band 48; 1946)



NOAA Climate Services Portal
NOAA State of the Climate
NASA's Climate change and global warming portal
Climate Models and modeling groups
Climate Prediction Project
ESPERE Climate Encyclopaedia
Climate index and mode information – Arctic
A current view of the Bering Sea Ecosystem and Climate
Climate: Data and charts for world and US locations
MIL-HDBK-310, Global Climate Data U.S. Department of Defense – Aid to derive natural environmental design criteria
IPCC Data Distribution Centre – Climate data and guidance on use.
HistoricalClimatology.com – Past, present and future climates – 2013.
Globalclimatemonitor – Contains climatic information from 1901.
ClimateCharts – Webapplication to generate climate charts for recent and historical data.
International Disaster Database
Paris Climate ConferenceWeather is the state of the atmosphere, to the degree that it is hot or cold, wet or dry, calm or stormy, clear or cloudy. Most weather phenomena occur in the troposphere, just below the stratosphere. Weather refers to day-to-day temperature and precipitation activity, whereas climate is the term for the statistics of atmospheric conditions over longer periods of time. When used without qualification, "weather" is generally understood to mean the weather of Earth.
Weather is driven by air pressure, temperature and moisture differences between one place and another. These differences can occur due to the sun's angle at any particular spot, which varies by latitude from the tropics. The strong temperature contrast between polar and tropical air gives rise to the jet stream. Weather systems in the mid-latitudes, such as extratropical cyclones, are caused by instabilities of the jet stream flow. Because the Earth's axis is tilted relative to its orbital plane, sunlight is incident at different angles at different times of the year. On Earth's surface, temperatures usually range ±40 °C (−40 °F to 100 °F) annually. Over thousands of years, changes in Earth's orbit can affect the amount and distribution of solar energy received by the Earth, thus influencing long-term climate and global climate change.
Surface temperature differences in turn cause pressure differences. Higher altitudes are cooler than lower altitudes due to differences in compressional heating. Weather forecasting is the application of science and technology to predict the state of the atmosphere for a future time and a given location. The system is a chaotic system; so small changes to one part of the system can grow to have large effects on the system as a whole. Human attempts to control the weather have occurred throughout human history, and there is evidence that human activities such as agriculture and industry have modified weather patterns.
Studying how the weather works on other planets has been helpful in understanding how weather works on Earth. A famous landmark in the Solar System, Jupiter's Great Red Spot, is an anticyclonic storm known to have existed for at least 300 years. However, weather is not limited to planetary bodies. A star's corona is constantly being lost to space, creating what is essentially a very thin atmosphere throughout the Solar System. The movement of mass ejected from the Sun is known as the solar wind.




On Earth, the common weather phenomena include wind, cloud, rain, snow, fog and dust storms. Less common events include natural disasters such as tornadoes, hurricanes, typhoons and ice storms. Almost all familiar weather phenomena occur in the troposphere (the lower part of the atmosphere). Weather does occur in the stratosphere and can affect weather lower down in the troposphere, but the exact mechanisms are poorly understood.
Weather occurs primarily due to air pressure, temperature and moisture differences between one place to another. These differences can occur due to the sun angle at any particular spot, which varies by latitude from the tropics. In other words, the farther from the tropics one lies, the lower the sun angle is, which causes those locations to be cooler due to the indirect sunlight. The strong temperature contrast between polar and tropical air gives rise to the jet stream. Weather systems in the mid-latitudes, such as extratropical cyclones, are caused by instabilities of the jet stream flow (see baroclinity). Weather systems in the tropics, such as monsoons or organized thunderstorm systems, are caused by different processes.

Because the Earth's axis is tilted relative to its orbital plane, sunlight is incident at different angles at different times of the year. In June the Northern Hemisphere is tilted towards the sun, so at any given Northern Hemisphere latitude sunlight falls more directly on that spot than in December (see Effect of sun angle on climate). This effect causes seasons. Over thousands to hundreds of thousands of years, changes in Earth's orbital parameters affect the amount and distribution of solar energy received by the Earth and influence long-term climate. (See Milankovitch cycles).
The uneven solar heating (the formation of zones of temperature and moisture gradients, or frontogenesis) can also be due to the weather itself in the form of cloudiness and precipitation. Higher altitudes are typically cooler than lower altitudes, which is explained by the lapse rate. In some situations, the temperature actually increases with height. This phenomenon is known as an inversion and can cause mountaintops to be warmer than the valleys below. Inversions can lead to the formation of fog and often act as a cap that suppresses thunderstorm development. On local scales, temperature differences can occur because different surfaces (such as oceans, forests, ice sheets, or man-made objects) have differing physical characteristics such as reflectivity, roughness, or moisture content.
Surface temperature differences in turn cause pressure differences. A hot surface warms the air above it causing it to expand and lower the density and the resulting surface air pressure. The resulting horizontal pressure gradient moves the air from higher to lower pressure regions, creating a wind, and the Earth's rotation then causes deflection of this air flow due to the Coriolis effect. The simple systems thus formed can then display emergent behaviour to produce more complex systems and thus other weather phenomena. Large scale examples include the Hadley cell while a smaller scale example would be coastal breezes.
The atmosphere is a chaotic system, so small changes to one part of the system can grow to have large effects on the system as a whole. This makes it difficult to accurately predict weather more than a few days in advance, though weather forecasters are continually working to extend this limit through the scientific study of weather, meteorology. It is theoretically impossible to make useful day-to-day predictions more than about two weeks ahead, imposing an upper limit to potential for improved prediction skill.




Weather is one of the fundamental processes that shape the Earth. The process of weathering breaks down the rocks and soils into smaller fragments and then into their constituent substances. During rains precipitation, the water droplets absorb and dissolve carbon dioxide from the surrounding air. This causes the rainwater to be slightly acidic, which aids the erosive properties of water. The released sediment and chemicals are then free to take part in chemical reactions that can affect the surface further (such as acid rain), and sodium and chloride ions (salt) deposited in the seas/oceans. The sediment may reform in time and by geological forces into other rocks and soils. In this way, weather plays a major role in erosion of the surface.



EUMETSAT created "A Year in Weather 2015" a narrated video of the earth's weather photographed from weather satellites for the entire year 2015. Geostationary satellite photographs from EUMETSAT, the Japan Meteorological Agency and the National Oceanic and Atmospheric Administration were assembled to show weather changing on earth for 365 days in a time lapse video.







Weather, seen from an anthropological perspective, is something all humans in the world constantly experience through their senses, at least while being outside. There are socially and scientifically constructed understandings of what weather is, what makes it change, the effect it has on humans in different situations, etc. Therefore, weather is something people often communicate about.




Weather has played a large and sometimes direct part in human history. Aside from climatic changes that have caused the gradual drift of populations (for example the desertification of the Middle East, and the formation of land bridges during glacial periods), extreme weather events have caused smaller scale population movements and intruded directly in historical events. One such event is the saving of Japan from invasion by the Mongol fleet of Kublai Khan by the Kamikaze winds in 1281. French claims to Florida came to an end in 1565 when a hurricane destroyed the French fleet, allowing Spain to conquer Fort Caroline. More recently, Hurricane Katrina redistributed over one million people from the central Gulf coast elsewhere across the United States, becoming the largest diaspora in the history of the United States.
The Little Ice Age caused crop failures and famines in Europe. The 1690s saw the worst famine in France since the Middle Ages. Finland suffered a severe famine in 1696–1697, during which about one-third of the Finnish population died.




Weather forecasting is the application of science and technology to predict the state of the atmosphere for a future time and a given location. Human beings have attempted to predict the weather informally for millennia, and formally since at least the nineteenth century. Weather forecasts are made by collecting quantitative data about the current state of the atmosphere and using scientific understanding of atmospheric processes to project how the atmosphere will evolve.
Once an all-human endeavor based mainly upon changes in barometric pressure, current weather conditions, and sky condition, forecast models are now used to determine future conditions. Human input is still required to pick the best possible forecast model to base the forecast upon, which involves pattern recognition skills, teleconnections, knowledge of model performance, and knowledge of model biases. The chaotic nature of the atmosphere, the massive computational power required to solve the equations that describe the atmosphere, error involved in measuring the initial conditions, and an incomplete understanding of atmospheric processes mean that forecasts become less accurate as the difference in current time and the time for which the forecast is being made (the range of the forecast) increases. The use of ensembles and model consensus helps to narrow the error and pick the most likely outcome.
There are a variety of end users to weather forecasts. Weather warnings are important forecasts because they are used to protect life and property. Forecasts based on temperature and precipitation are important to agriculture, and therefore to commodity traders within stock markets. Temperature forecasts are used by utility companies to estimate demand over coming days. On an everyday basis, people use weather forecasts to determine what to wear on a given day. Since outdoor activities are severely curtailed by heavy rain, snow and the wind chill, forecasts can be used to plan activities around these events, and to plan ahead and survive them.



The aspiration to control the weather is evident throughout human history: from ancient rituals intended to bring rain for crops to the U.S. Military Operation Popeye, an attempt to disrupt supply lines by lengthening the North Vietnamese monsoon. The most successful attempts at influencing weather involve cloud seeding; they include the fog- and low stratus dispersion techniques employed by major airports, techniques used to increase winter precipitation over mountains, and techniques to suppress hail. A recent example of weather control was China's preparation for the 2008 Summer Olympic Games. China shot 1,104 rain dispersal rockets from 21 sites in the city of Beijing in an effort to keep rain away from the opening ceremony of the games on 8 August 2008. Guo Hu, head of the Beijing Municipal Meteorological Bureau (BMB), confirmed the success of the operation with 100 millimeters falling in Baoding City of Hebei Province, to the southwest and Beijing's Fangshan District recording a rainfall of 25 millimeters.
Whereas there is inconclusive evidence for these techniques' efficacy, there is extensive evidence that human activity such as agriculture and industry results in inadvertent weather modification:
Acid rain, caused by industrial emission of sulfur dioxide and nitrogen oxides into the atmosphere, adversely affects freshwater lakes, vegetation, and structures.
Anthropogenic pollutants reduce air quality and visibility.
Climate change caused by human activities that emit greenhouse gases into the air is expected to affect the frequency of extreme weather events such as drought, extreme temperatures, flooding, high winds, and severe storms.
Heat, generated by large metropolitan areas have been shown to minutely affect nearby weather, even at distances as far as 1,600 kilometres (990 mi).
The effects of inadvertent weather modification may pose serious threats to many aspects of civilization, including ecosystems, natural resources, food and fiber production, economic development, and human health.



Microscale meteorology is the study of short-lived atmospheric phenomena smaller than mesoscale, about 1 km or less. These two branches of meteorology are sometimes grouped together as "mesoscale and microscale meteorology" (MMM) and together study all phenomena smaller than synoptic scale; that is they study features generally too small to be depicted on a weather map. These include small and generally fleeting cloud "puffs" and other small cloud features.




On Earth, temperatures usually range ±40 °C (100 °F to −40 °F) annually. The range of climates and latitudes across the planet can offer extremes of temperature outside this range. The coldest air temperature ever recorded on Earth is −89.2 °C (−128.6 °F), at Vostok Station, Antarctica on 21 July 1983. The hottest air temperature ever recorded was 57.7 °C (135.9 °F) at 'Aziziya, Libya, on 13 September 1922, but that reading is queried. The highest recorded average annual temperature was 34.4 °C (93.9 °F) at Dallol, Ethiopia. The coldest recorded average annual temperature was −55.1 °C (−67.2 °F) at Vostok Station, Antarctica.
The coldest average annual temperature in a permanently inhabited location is at Eureka, Nunavut, in Canada, where the annual average temperature is −19.7 °C (−3.5 °F).




Studying how the weather works on other planets has been seen as helpful in understanding how it works on Earth. Weather on other planets follows many of the same physical principles as weather on Earth, but occurs on different scales and in atmospheres having different chemical composition. The Cassini–Huygens mission to Titan discovered clouds formed from methane or ethane which deposit rain composed of liquid methane and other organic compounds. Earth's atmosphere includes six latitudinal circulation zones, three in each hemisphere. In contrast, Jupiter's banded appearance shows many such zones, Titan has a single jet stream near the 50th parallel north latitude, and Venus has a single jet near the equator.
One of the most famous landmarks in the Solar System, Jupiter's Great Red Spot, is an anticyclonic storm known to have existed for at least 300 years. On other gas giants, the lack of a surface allows the wind to reach enormous speeds: gusts of up to 600 metres per second (about 2,100 km/h or 1,300 mph) have been measured on the planet Neptune. This has created a puzzle for planetary scientists. The weather is ultimately created by solar energy and the amount of energy received by Neptune is only about  1⁄900 of that received by Earth, yet the intensity of weather phenomena on Neptune is far greater than on Earth. The strongest planetary winds discovered so far are on the extrasolar planet HD 189733 b, which is thought to have easterly winds moving at more than 9,600 kilometres per hour (6,000 mph).




Weather is not limited to planetary bodies. Like all stars, the sun's corona is constantly being lost to space, creating what is essentially a very thin atmosphere throughout the Solar System. The movement of mass ejected from the Sun is known as the solar wind. Inconsistencies in this wind and larger events on the surface of the star, such as coronal mass ejections, form a system that has features analogous to conventional weather systems (such as pressure and wind) and is generally known as space weather. Coronal mass ejections have been tracked as far out in the solar system as Saturn. The activity of this system can affect planetary atmospheres and occasionally surfaces. The interaction of the solar wind with the terrestrial atmosphere can produce spectacular aurorae, and can play havoc with electrically sensitive systems such as electricity grids and radio signals.



Weather station
Outline of meteorology





In meteorology, a cloud is an aerosol comprising a visible mass of minute liquid droplets or frozen crystals, both of which are made of water or various chemicals. The droplets or particles are suspended in the atmosphere above the surface of a planetary body. On Earth, clouds are formed by the saturation of air in the homosphere (which includes the troposphere, stratosphere, and mesosphere). The air may be cooled to its dew point by a variety of atmospheric processes or it may gain moisture (usually in the form of water vapor) from an adjacent source. Nephology is the science of clouds which is undertaken in the cloud physics branch of meteorology.
Cloud types in the troposphere, the atmospheric layer closest to Earth's surface, have Latin names due to the universal adaptation of Luke Howard's nomenclature. It was formally proposed in December 1802 and published for the first time the following year. It became the basis of a modern international system that classifies these tropospheric aerosols into five physical forms and three altitude levels or étages. These physical types, in approximate ascending order of convective activity, include stratiform sheets, cirriform wisps and patches, stratocumuliform layers (mainly structured as rolls, ripples, and patches), cumuliform heaps and tufts, and very large cumulonimbiform heaps that often show complex structure. The physical forms are cross-classified by altitude levels to produce ten basic genus-types or genera. Some of these basic types are common to more than one form or more than one étage, as illustrated in the stratocumuliform and cumuliform columns of the classification table below. Most genera can be divided into species, some of which are common to more than one genus. These can be subdivided into varieties, some of which are common to more than one genus or species.
Cirriform clouds that form higher up in the stratosphere and mesosphere have common names for their main types, but are sub-classified alpha-numerically rather than with the elaborate system of Latin names given to cloud types in the troposphere. They are relatively uncommon and are mostly seen in the polar regions of Earth. Clouds have been observed in the atmospheres of other planets and moons in the Solar System and beyond. However, due to their different temperature characteristics, they are often composed of other substances such as methane, ammonia, and sulfuric acid as well as water.






The origin of the term cloud can be found in the old English clud or clod, meaning a hill or a mass of rock. Around the beginning of the 13th century, it was extended as a metaphor to include rain clouds as masses of evaporated water in the sky because of the similarity in appearance between a mass of rock and a cumulus heap cloud. Over time, the metaphoric term replaced the original old English weolcan to refer to clouds in general.



Ancient cloud studies were not made in isolation, but were observed in combination with other weather elements and even other natural sciences. In about 340 BC the Greek philosopher Aristotle wrote Meteorologica, a work which represented the sum of knowledge of the time about natural science, including weather and climate. For the first time, precipitation and the clouds from which precipitation fell were called meteors, which originate from the Greek word meteoros, meaning 'high in the sky'. From that word came the modern term meteorology, the study of clouds and weather. Meteorologica was based on intuition and simple observation, but not on what is now considered the scientific method. Nevertheless, it was the first known work that attempted to treat a broad range of meteorological topics.
The magazine De Mundo (attributed to Pseudo-Aristotle) noted:

Cloud is a vaporous mass, concentrated and producing water. Rain is produced from the compression of a closely condensed cloud, varying according to the pressure exerted on the cloud; when the pressure is slight it scatters gentle drops; when it is great it produces a more violent fall, and we call this a shower, being heavier than ordinary rain, and forming continuous masses of water falling over earth. Snow is produced by the breaking up of condensed clouds, the cleavage taking place before the change into water; it is the process of cleavage which causes its resemblance to foam and its intense whiteness, while the cause of its coldness is the congelation of the moisture in it before it is dispersed or rarefied. When snow is violent and falls heavily we call it a blizzard. Hail is produced when snow becomes densified and acquires impetus for a swifter fall from its close mass; the weight becomes greater and the fall more violent in proportion to the size of the broken fragments of cloud. Such then are the phenomena which occur as the result of moist exhalation.

Several years after Aristotle's book, his pupil Theophrastus put together a book on weather forecasting called The Book of Signs. Various indicators such as solar and lunar halos formed by high clouds were presented as ways to forecast the weather. The combined works of Aristotle and Theophrastus had such authority they became the main influence in the study of clouds, weather and weather forecasting for nearly 2000 years.



After centuries of speculative theories about the formation and behavior of clouds, the first truly scientific studies were undertaken by Luke Howard in England and Jean-Baptiste Lamarck in France. Howard was a methodical observer with a strong grounding in the Latin language and used his background to classify the various tropospheric cloud types during 1802. He believed that the changing cloud forms in the sky could unlock the key to weather forecasting. Lamarck had worked independently on cloud classification the same year and had come up with a different naming scheme that failed to make an impression even in his home country of France because it used unusual French names for cloud types. His system of nomenclature included twelve categories of clouds, with such names as (translated from French) hazy clouds, dappled clouds and broom-like clouds. By contrast, Howard used universally accepted Latin, which caught on quickly after it was published in 1803. As a sign of the popularity of the naming scheme, the German dramatist and poet Johann Wolfgang von Goethe composed four poems about clouds, dedicating them to Howard. An elaboration of Howard's system was eventually formally adopted by the International Meteorological Conference in 1891.

Howard's original system established three physical categories or forms based on appearance and process of formation: cirriform (mainly detached and wispy), cumuliform or convective (mostly detached and heaped, rolled, or rippled), and non-convective stratiform (mainly continuous layers in sheets). These were cross-classified into lower and upper étages. Cumuliform clouds forming in the lower level were given the genus name cumulus from the Latin word for heap, while low stratiform clouds took the genus name stratus from the Latin word for a flattened or spread out sheet. Cirriform clouds were identified as always upper level and given the genus name cirrus from the Latin for hair. From this genus name, the prefix cirro- was derived and attached to the names of upper level cumulus and stratus, yielding the names cirrocumulus, and cirrostratus.
In addition to these individual cloud types; Howard added two names to designate cloud systems consisting of more than one form joined together or located in very close proximity. Cumulostratus described large cumulus clouds blended with stratiform layers in the lower or upper levels. The term nimbus, taken from the Latin word for rain cloud, was given to complex systems of cirriform, cumuliform, and stratiform clouds with sufficient vertical development to produce significant precipitation, and it came to be identified as a distinct nimbiform physical category.



In 1840, German meteorologist Ludwig Kaemtz added stratocumulus to Howard's canon as a mostly detached low-étage genus of limited convection. It was defined as having cumuliform and stratiform characteristics integrated into a single layer (in contrast to cumulostratus which was deemed to be composite in nature and could be structured into more than one layer). This led to the recognition of a stratocumuliform physical category that included rolled and rippled clouds classified separately from the more freely convective heaped cumuliform clouds.

During the mid 1850s, Emilien Renou, director of the Parc Saint-Maur and Montsouris observatories, began work on an elaboration of Howard's classifications that would lead to the introduction during the 1870s of a newly defined middle étage . Clouds in this altitude range were given the prefix alto- derived from the Latin word altum pertaining to height above the low-level clouds. This resulted in the genus name altocumulus for mid-level cumuliform and stratocumuliform types and altostratus for stratiform types in the same altitude range.
In 1880, Philip Weilbach, secretary and librarian at the Art Academy in Copenhagen, and like Luke Howard, an amateur meteorologist, unsuccessfully proposed an alternative to Howard's classification. However, he also proposed and had accepted by the permanent committee of the International Meteorological Organization (IMO), a forerunner of the present-day World Meteorological Organization (WMO), the designation of a new free-convective vertical or multi-étage genus type, cumulonimbus (heaped rain cloud), which would be distinct from cumulus and nimbus and identifiable by its often very complex structure (frequently including a cirriform top and what are now recognized as multiple accessory clouds), and its ability to produce thunder. With this addition, a canon of ten tropospheric cloud genera was established that came to be officially and universally accepted. Howard's cumulostratus was not included as a distinct type, having effectively been reclassified into its component cumuliform and stratiform genus types already included in the new canon.
In 1890, Otto Jesse revealed the discovery and identification of the first clouds known to form above the troposphere. He proposed the name noctilucent which is Latin for night shining. Because of the extremely high altitudes of these clouds in what is now known to be the mesosphere, they could become illuminated by the a sun's rays when the sky was nearly dark after sunset and before sunrise. Three years later, Henrik Mohn revealed a similar discovery of nacreous clouds in what is now considered the stratosphere.
In 1896, the first cloud atlas sanctioned by the IMO was produced by Teisserenc de Borte based on collaborations with Hugo H. Hildebrandsson. The latter had become the first researcher to use photography for the study and classification of clouds in 1879.
Alternatives to Howard's classification system were proposed throughout the 19th century. Heinrich Dove of Germany and Elias Loomis of the United States came up with other schemes in 1828 and 1841 respectively, but neither met with international success. Additional proposals were made by Andre Poey (1863), Clemment Ley (1894), and H.H. Clayton (1896), but their systems, like earlier alternative schemes, differed too much from Howard's to have any success beyond the adoption of some secondary cloud types. However, Clayton's idea to formalize the division of clouds by their physical structures into cirriform, stratiform, "flocciform" (stratocumuliform) and cumuliform (with the later addition of cumulonimbiform), eventually found favor as an aid in the analysis of satellite cloud images.



A further modification of the genus classification system came when an IMC commission for the study of clouds put forward a refined and more restricted definition of the genus nimbus which was effectively reclassified as a stratiform cloud type. It was then renamed nimbostratus (flattened or spread out rain cloud) and published with the new name in the 1932 edition of the International Atlas of Clouds and of States of the Sky. This left cumulonimbus as the only nimbiform type as indicated by its root-name.
On April 1, 1960, the first successful weather satellite, TIROS-1 (Television Infrared Observation Satellite), was launched from Cape Canaveral, Florida by the National Aeronautics and Space Administration (NASA) with the participation of The US Army Signal Research and Development Lab, RCA, the US Weather Bureau, and the US Naval Photographic Center. During its 78-day mission, it relayed thousands of pictures showing the structure of large-scale cloud regimes, and proved that satellites could provide useful surveillance of global weather conditions from space.
In 1976, the United Kingdom Department of Industry published a modification of the international cloud classification system adapted for satellite cloud observations. It was co-sponsored by NASA and showed a change in name of the nimbiform type to cumulonimbiform, although the earlier name and original meaning pertaining to all rain clouds can still be found in some classifications.



Tropospheric classification is based on a hierarchy of cloud types with physical forms and étages at the top. These are divided into a total of ten genus types which are derived by a cross-classification of the forms and étages. The genera can be subdivided into species and further subdivided into varieties which are at the bottom of the hierarchy.




Clouds in the troposphere comprise five physical forms based on structure and process of formation. These forms are commonly used for the purpose of satellite analysis. They are given below in approximate ascending order of instability or convective activity. Two of the forms can each be divided into several genera that are differentiated mainly by altitude range or étage. The other three forms each comprise just one genus type.



Non-convective stratiform clouds appear in stable airmass conditions and, in general, have flat sheet-like structures that can form at any altitude in the troposphere. Very low stratiform cloud results when advection fog is lifted above surface level during breezy conditions. The stratiform group is divided by altitude range into the genera cirrostratus (high-étage), altostratus (middle-étage), stratus (low-étage), and nimbostratus (multi-étage).



Cirriform clouds are generally of the genus cirrus and have the appearance of detached or semi-merged filaments. They form at high tropospheric altitudes in air that is mostly stable with little or no convective activity, although denser patches may occasionally show buildups caused by limited high-level convection where the air is partly unstable.



Clouds of this structure have both cumuliform and stratiform characteristics in the form of rolls, ripples, or patches. They generally form as a result of limited convection in an otherwise mostly stable airmass topped by an inversion layer. If the inversion layer is absent or higher in the troposphere, increased convective activity may cause the cloud layers to develop tops in the form of turrets consisting of embedded cumuliform buildups. The stratocumuliform group is divided into layered cirrocumulus (high-étage), layered altocumulus (middle-étage), and stratocumulus (low-étage).



Cumuliform clouds generally appear in isolated heaps or tufts. They are the product of localized but generally free-convective lift where there are no inversion layers in the atmosphere to limit vertical growth. In general, small cumuliform clouds tend to indicate comparatively weak instability. Larger cumuliform types are a sign of moderate to strong atmospheric instability and convective activity. Depending on their vertical size, clouds of the cumulus genus-type may be low-level single-étage or multi-étage with moderate to towering vertical extent. Tufted altocumulus and cirrocumulus genera in the middle and high étages are also considered cumuliform because they have a more detached heaped structure than their layered stratocumuliform variants.




The largest free-convective clouds comprise the genus cumulonimbus which are multi-étage because of their towering vertical extent. They occur in highly unstable air and often have complex structures that include cirriform tops and multiple accessory clouds.




Tropospheric clouds form in any of three étages based on altitude range above the Earth's surface. The grouping of clouds into étages is commonly done for the purposes of cloud atlases, surface weather observations and weather maps. Each altitude range comprises two or three genus types differentiated mainly by physical form.
The base-height range for each étage varies depending on the latitudinal geographical zone. A consensus exists as to the designation of high, middle, and low étages in the troposphere, and the classification of most genus types found in each altitude range. There is less consensus regarding the classification nimbostratus that has significant vertical extent and can occupy more than one étage. The World Meteorological Organization (WMO) classifies this cloud as middle étage for synoptic purposes while physically characterizing it as multi-étage. Independent meteorologists and educators appear split between those who largely follow the WMO model and those who classify nimbostratus as low-étage, despite its considerable vertical extent and it's usual formation in the middle étage.
The standard étages and genus-types are summarised below in approximate descending order of the altitude at which each is normally based. Multi-étage clouds with significant vertical extent are separately listed and summarised in approximate ascending order of instability or convective activity.



Clouds of the high étage form at altitudes of 3,000 to 7,600 m (10,000 to 25,000 ft) in the polar regions, 5,000 to 12,200 m (16,500 to 40,000 ft) in the temperate regions and 6,100 to 18,300 m (20,000 to 60,000 ft) in the tropical region. All cirriform clouds are classified as high and thus constitute a single genus cirrus (Ci). Stratocumuliform and stratiform clouds in the high étage carry the prefix cirro-, yielding the respective genus names cirrocumulus (Cc) and cirrostratus (Cs). When comparatively low-resolution satellite images of high clouds are analized without supporting data from direct human observations, it becomes impossible to distinguish between individual genus types which are then collectively identified as cirrus-type.

Genus cirrus (Ci):
These are mostly fibrous wisps of delicate white cirriform ice crystal cloud that show up clearly against the blue sky. Cirrus are generally non-convective except castellanus and floccus subtypes which show limited convection. They often form along a high altitude jetstream and at the very leading edge of a frontal or low-pressure disturbance where they may merge into cirrostratus. These high clouds do not produce precipitation.
Genus cirrocumulus (Cc):

This is most commonly a pure white high-étage stratocumuliform layer of limited convection. It is composed of ice crystals or supercooled water droplets appearing as small unshaded round masses or flakes in groups or lines with ripples like sand on a beach. Cirrocumulus occasionally forms alongside cirrus and may be accompanied or replaced by cirrostratus clouds at the very leading edge of an active weather system. Tufted cirrocumulus forms in more isolated heaps than the layered variant and can therefore be considered a cumuliform cloud which retains its pure white coloration.
Genus cirrostratus (Cs):
Cirrostratus is a thin non-convective stratiform ice crystal veil that typically gives rise to halos caused by refraction of the sun's rays. The sun and moon are visible in clear outline. Cirrostratus often thickens into altostratus ahead of a warm front or low-pressure area.



Non-vertical clouds in the middle étage are prefixed by alto-, yielding the genus names altocumulus (Ac) and altostratus (As). These clouds can form as low as 2,000 m (6,500 ft) above surface at any latitude, but may be based as high as 4,000 m (13,000 ft) near the poles, 7,000 m (23,000 ft) at mid latitudes, and 7,600 m (25,000 ft) in the tropics. As with high clouds, it is not always possible to distinguish between individual genera using satellite photography alone. Without the addition of human observations, these clouds are usually collectively identified as 'middle-type' on satellite images.

Genus altocumulus (Ac):
This is most commonly a middle-étage stratocumuliform cloud layer of limited convection that is usually appears in the form of irregular patches or more extensive sheets arranges in groups, lines, or waves. High altocumulus may resemble cirrocumulus but is usually thicker and composed of water droplets so that the bases show at least some light-grey shading. Opaque altocumulus associated with a weak frontal or low-pressure disturbance can produce virga, very light intermittent precipitation that evaporates before reaching the ground. If the altocumulus is mixed with moisture-laden altostratus, the precipitation may reach the ground. As with cirrocumulus, tufted altocumulus in isolated heaps can be considered a cumuliform rather than a stratocumuliform cloud.
Genus altostratus (As):

Altostratus is a mid-level opaque or translucent stratiform or non-convective veil of grey/blue-grey cloud that often forms along warm fronts and around low-pressure areas. Altostratus is usually composed of water droplets but may be mixed with ice crystals at higher altitudes. Widespread opaque altostratus can produce light continuous or intermittent precipitation. Precipitation commonly becomes heavier and more widespread if it thickens into nimbostratus.



Low-étage clouds are found from near surface up to 2,000 m (6,500 ft). Genus types in this étage either have no prefix or carry one that refers to a characteristic other than altitude.

Genus stratocumulus (Sc):
This genus type is a stratocumuliform cloud layer of limited convection, usually in the form of irregular patches or more extensive sheets similar to altocumulus but having larger elements with deeper-gray shading. Opaque stratocumulus can produce very light intermittent precipitation. This cloud often forms under a precipitating deck of altostratus or high-based nimbostratus associated with a well-developed warm front, slow-moving cold front, or low-pressure area. This can create the illusion of continuous precipitation of more than very light intensity falling from stratocumulus.
Genus cumulus (Cu) – little vertical extent:

These are small detached fair-weather cumuliform clouds that have nearly horizontal bases and flattened tops, and do not produce rain showers.
Genus stratus (St):

This is a flat or sometimes ragged non-convective stratiform type that sometimes resembles elevated fog. Only very weak precipitation can fall from this cloud (usually drizzle or snow grains), although heavier rain or snow may fall through a stratus layer from a higher precipitating cloud deck. When a low stratiform cloud contacts the ground, it is called fog if the prevailing surface visibility is less than 1 kilometer, although radiation and advection types of fog tend to form in clear air rather than from stratus layers. If the visibility increases to 1 kilometer or higher in any kind of fog, the visible condensation is termed mist.



These clouds have low to middle-étage bases that form anywhere from near surface to about 2,400 m (8,000 ft) and tops that can extend into the high étage. Nimbostratus and some cumulus in this group usually achieve moderate or deep vertical extent, but without towering structure. However, with sufficient airmass instability, upward-growing cumuliform clouds can grow to high towering proportions. Although genus types with vertical extent are often considered a single group, the International Civil Aviation Organization (ICAO) further distinguishes towering vertical clouds as a separate group or sub-group. It is specified that these very large cumuliform and cumulonimbiform types must be identified by their standard names or abbreviations in all aviation observations (METARS) and forecasts (TAFS) to warn pilots of possible severe weather and turbulence.
Moderate and deep vertical

Genus nimbostratus (Ns):
This is a diffuse dark-grey non-convective stratiform layer with great horizontal extent and moderate to deep vertical development. It lacks towering structure and looks feebly illuminated from the inside. Nimbostratus normally forms from middle-étage altostratus, and develops at least moderate vertical extent when the base subsides into the low étage during precipitation that can reach moderate to heavy intensity. It commonly achieves deep vertical development when it simultaneously grows upward into the high étage due to large scale frontal or cyclonic lift. The nimbo- prefix refers to its ability to produce continuous rain or snow over a wide area, especially ahead of a warm front. Nimbostratus may be accompanied by embedded towering cumuliform or cumulonimbiform types.
Genus cumulus (Cu) – moderate vertical extent:
These cumuliform clouds of free convection have clear-cut medium-grey flat bases and white domed tops in the form of small sproutings and generally do not produce precipitation. They usually form in the low étage except during conditions of very low relative humidity when the clouds bases can rise into the middle altitude range.
Towering vertical

These clouds are sometimes classified separately from the other vertical or multi-étage types because of their ability to produce severe turbulence.
Genus cumulus (Cu) – great vertical extent:
Increasing airmass instability can cause free-convective cumulus to grow very tall to the extent that the vertical height from base to top is greater than the base-width of the cloud. The cloud base takes on a darker grey coloration and the top commonly resembles a cauliflower. This cloud type can produce moderate to heavy showers and is designated Towering cumulus (Tcu) by ICAO.
Genus cumulonimbus (Cb):

This genus type is a heavy towering cumulonimbiform mass of free convective cloud with a dark-grey to nearly black base and a very high top in the form of a mountain or huge tower. Cumulonimbus can produce thunderstorms, local very heavy downpours of rain that may cause flash floods, and a variety of types of lightning including cloud-to-ground that can cause wildfires. Other convective severe weather may or may not be associated with thunderstorms and include heavy snow showers, hail, strong wind shear, downbursts, and tornadoes. Of all these possible cumulonimbus-related events, lightning is the only one of these that requires a thunderstorm to be taking place since it is the lightning that creates the thunder. Cumulonimbus clouds can form in unstable airmass conditions, but tend to be more concentrated and intense when they are associated with unstable cold fronts.




Genus types are commonly divided into subtypes called species that indicate specific structural details which can vary according to the stability and windshear characteristics of the atmosphere at any given time and location. Despite this hierarchy, a particular species may be a subtype of more than one genus, especially if the genera are of the same physical form and are differentiated from each other mainly by altitude or étage. There are a few species, each of which can a subtype of more than one genus, each of which can be associated with a different physical form. The species types are grouped below according to the physical forms and genera with which each is normally associated. The forms, genera, and species are listed in approximate ascending order of instability or convective activity.
Genus and species types are further subdivided into varieties whose names can appear after the species name to provide a fuller description of a cloud. Some cloud varieties are not restricted to a specific étage or form, and can therefore be common to more than one genus or species.



Stable or mostly stable Of the stratiform group, high-level cirrostratus comprises two species. Cirrostratus nebulosus has a rather diffuse appearance lacking in structural detail. Cirrostratus fibratus is a species made of semi-merged filaments that are transitional to or from cirrus. Mid-level altostratus and multi-level nimbostratus always have a flat or diffuse appearance and are therefore not subdivided into species. Low-étage stratus is of the species nebulosus except when broken up into ragged sheets of stratus fractus (see below).
Cirriform clouds have three non-convective species that can form in mostly stable airmass conditions. Cirrus fibratus comprise filaments that may be straight, wavy, or occasionally twisted by non-convective wind shear. The species uncinus is similar but has upturned hooks at the ends. Cirrus spissatus appear as opaque patches that can show light grey shading.
Stratocumuliform genus-types (cirrocumulus, altocumulus, and stratocumulus) that appear in mostly stable air have two species each that can form in the high, middle, or low étages of the troposphere. The stratiformis species normally occur in extensive sheets or in smaller patches where there is only minimal convective activity. Clouds of the lenticularis species tend to have lens-like shapes tapered at the ends. They are most commonly seen as orographic mountain-wave clouds, but can occur anywhere in the troposphere where there is strong wind shear combined with sufficient airmass stability to maintain a generally flat cloud structure.
Ragged The species fractus shows variable instability because it can be a subdivision of genus-types of different physical forms that have different stability characteristics. This subtype can be in the form of ragged but mostly stable stratiform sheets (stratus fractus) or small ragged cumuliform heaps with somewhat greater instability (cumulus fractus). When they form at low altitudes, stratiform and cumuliform genus-types can be torn up into shreds by brisk low level winds that create mechanical turbulence against the ground. Fractus clouds can form in precipitation at low altitudes, with or without brisk or gusty winds. They are closely associated with precipitating cloud systems of considerable vertical and sometimes horizontal extent, so they are also classified as accessory clouds under the name pannus (see section on supplementary features).
Partly unstable These species are subdivisions of genus types that occur in partly unstable air. The species castellanus appears when a mostly stable stratocumuliform or cirriform layer becomes disturbed by localized areas of airmass instability. This results in the formation of cumuliform buildups arising from a common stratiform base. Castellanus resembles the turrets of a castle when viewed from the side, and can be found with stratocumuliform genera at any tropospheric altitude level and with limited-convective patches of high-étage cirrus. Clouds of the more detached tufted floccus species are subdivisions of genus-types which may be cirriform or cumuliform in overall structure. They are sometimes seen with cirrus, and with tufted cirrocumulus, and altocumulus. However floccus species are not generally found in the low étage, an altitude range where their place is taken by the fractus and humilis species of the cumulus genus.
Unstable or mostly unstable More general airmass instability in the troposphere tends to produce clouds of the more freely convective cumulus genus type, whose species are mainly indicators of degrees of atmospheric instability and resultant vertical development of the clouds. A cumulus cloud initially forms in the low étage as a cloudlet of the species humilis that shows only slight vertical development. If the air becomes more unstable, the cloud tends to grow vertically into the species mediocris, then congestus, the tallest cumulus species which is the same type that the International Civil Aviation Organization refers to as 'towering cumulus'.
With highly unstable atmospheric conditions, large cumulus may continue to grow into cumulonimbus calvus (essentially a very tall congestus cloud that produces thunder), then ultimately into the species capillatus when supercooled water droplets at the top of the cloud turn into ice crystals giving it a cirriform appearance.



Opacity-based All cloud varieties fall into one of two main groups. One group identifies the opacities of particular low and middle étage cloud structures and comprises the varieties translucidus (thin translucent), perlucidus (thick opaque with translucent breaks), and opacus (thick opaque). These varieties are always identifiable for cloud genera and species with variable opacity. All three are associated with the stratiformis species of altocumulus and stratocumulus. However, only two varieties are seen with altostratus and stratus nebulosus whose uniform structures prevent the formation of a perlucidus variety. Opacity-based varieties are not applied to high-étage clouds because they are always translucent, or in the case of cirrus spissatus, always opaque. Similarly, these varieties are also not associated with moderate and towering vertical clouds because they are always opaque.
Pattern-based

A second group describes the occasional arrangements of cloud structures into particular patterns that are discernible by a surface-based observer (cloud fields usually being visible only from a significant altitude above the formations). These varieties are not always present with the genera and species with which they are otherwise associated, but only appear when atmospheric conditions favor their formation. Intortus and vertebratus varieties occur on occasion with cirrus fibratus. They are respectively filaments twisted into irregular shapes, and those that are arranged in fishbone patterns, usually by uneven wind currents that favor the formation of these varieties. The variety radiatus is associated with cloud rows of a particular type that appear to converge at the horizon. It is sometimes seen with the fibratus and uncinus species of cirrus, the stratiformis species of altocumulus and stratocumulus, the mediocris and sometimes humilis species of cumulus, and with the genus altostratus.
Another variety, duplicatus (closely spaced layers of the same type, one above the other), is sometimes found with cirrus of both the fibratus and uncinus species, and with altocumulus and stratocumulus of the species stratiformis and lenticularis. The variety undulatus (having a wavy undulating base) can occur with any clouds of the species stratiformis or lenticularis, and with altostratus. It is only rarely observed with stratus nebulosus. The variety lacunosus is caused by localized downdrafts that create circular holes in the form of a honeycomb or net. It is occasionally seen with cirrocumulus and altocumulus of the species stratiformis, castellanus, and floccus, and with stratocumulus of the species stratiformis and castellanus.
Combinations It is possible for some species to show combined varieties at one time, especially if one variety is opacity-based and the other is pattern-based. An example of this would be a layer of altocumulus stratiformis arranged in seemingly converging rows separated by small breaks. The full technical name of a cloud in this configuration would be altocumulus stratiformis radiatus perlucidus, which would identify respectively its genus, species, and two combined varieties.




Supplementary features and accessory clouds are not further subdivisions of cloud types below the species and variety level. Rather, they are either hydrometeors or special cloud types with their own Latin names that form in association with certain cloud genera, species, and varieties. Supplementary features, whether in the form of clouds or precipitation, are directly attached to the main genus-cloud. Accessory clouds, by contrast, are generally detached from the main cloud.



One group of supplementary features are not actual cloud formations, but precipitation that falls when water droplets or ice crystals that make up visible clouds have grown too heavy to remain aloft. Virga is a feature seen with clouds producing precipitation that evaporates before reaching the ground, these being of the genera cirrocumulus, altocumulus, altostratus, nimbostratus, stratocumulus, cumulus, and cumulonimbus.
When the precipitation reaches the ground without completely evaporating, it is designated as the feature praecipitatio. This normally occurs with altostratus opacus, which can produce widespread but usually light precipitation, and with thicker clouds that show significant vertical development. Of the latter, upward-growing cumulus mediocris produces only isolated light showers, while downward growing nimbostratus is capable of heavier, more extensive precipitation. Towering vertical clouds have the greatest ability to produce intense precipitation events, but these tend to be localized unless organized along fast-moving cold fronts. Showers of moderate to heavy intensity can fall from cumulus congestus clouds. Cumulonimbus, the largest of all cloud genera, has the capacity to produce very heavy showers. Low stratus clouds usually produce only light precipitation, but this always occurs as the feature praecipitatio due to the fact this cloud genus lies too close to the ground to allow for the formation of virga.



Incus is the most type-specific supplementary feature, seen only with cumulonimbus of the species capillatus. A cumulonimbus incus cloud top is one that has spread out into a clear anvil shape as a result of rising air currents hitting the stability layer at the tropopause where the air no longer continues to get colder with increasing altitude.
The mamma feature forms on the bases of clouds as downward-facing bubble-like protuberances caused by localized downdrafts within the cloud. It is also sometimes called mammatus, an earlier version of the term used before a standardization of Latin nomenclature brought about by the World Meterorological Organization during the 20th century. The best-known is cumulonimbus with mammatus, but the mamma feature is also seen occasionally with cirrus, cirrocumulus, altocumulus, altostratus, and stratocumulus.
A tuba feature is a cloud column that may hang from the bottom of a cumulus or cumulonimbus. A newly formed or poorly organized column might be comparatively benign, but can quickly intensify into a funnel cloud or tornado.
An arcus feature is a roll cloud with ragged edges attached to the lower front part of cumulus congestus or cumulonimbus that forms along the leading edge of a squall line or thunderstorm outflow. A large arcus formation can have the appearance of a dark menacing arch.
There are some arcus-like clouds that form as a consequence of interactions with specific geographical features rather than with a parent cloud. Perhaps the strangest geographically specific cloud of this type is the Morning Glory, a rolling cylindrical cloud that appears unpredictably over the Gulf of Carpentaria in Northern Australia. Associated with a powerful "ripple" in the atmosphere, the cloud may be "surfed" in glider aircraft. It has been officially suggested that roll clouds of this type that are not attached to a parent cloud be reclassified as a new species of stratocumulus, possibly with the Latin name volutus.



Supplementary cloud formations detached from the main cloud are known as accessory clouds. The heavier precipitating clouds, nimbostratus, towering cumulus (cumulus congestus), and cumulonimbus typically see the formation in precipitation of the pannus feature, low ragged clouds of the genera and species cumulus fractus or stratus fractus.
After the pannus types, the remaining accessory clouds comprise formations that are associated mainly with upward-growing cumuliform and cumulonimbiform clouds of free convection. Pileus is a cap cloud that can form over a cumulonimbus or large cumulus cloud, whereas a velum feature is a thin horizontal sheet that sometimes forms like an apron around the middle or in front of the parent cloud.
Under conditions of strong atmospheric wind shear and instability, wave-like undulatus formations may break into regularly spaced crests. This variant has no separate WMO Latin designation, but is sometimes known informally as a Kelvin–Helmholtz (wave) cloud. This phenomenon has also been observed in cloud formations over other planets and even in the sun's atmosphere. It has been formally suggested that this wave cloud be classified as a supplementary feature, possibly with the Latin name fluctus. Another wave-like cloud feature that is distinct from the variety undulatus has been given the Latin name asperatus. It has been recommended for formal classification as a supplementary feature using its suggested Latin name.
A circular fall-streak hole occasionally forms in a thin layer of supercooled altocumulus or cirrocumulus. Fall streaks consisting of virga or wisps of cirrus are usually seen beneath the hole as ice crystals fall out to a lower altitude. This type of hole is usually larger than typical lacunosus holes, and a formal recommendation has been made to classify it as a supplementary feature, possibly with the Latin name cavus.




Clouds initially form in clear air or become clouds when fog rises above surface level. The genus of a newly formed cloud is determined mainly by air mass characteristics such as stability and moisture content. If these characteristics change over time, the genus tends to change accordingly. When this happens, the original genus is called a mother cloud. If the mother cloud retains much of its original form after the appearance of the new genus, it is termed a genitus cloud. One example of this is stratocumulus cumulogenitus, a stratocumulus cloud formed by the partial spreading of a cumulus type when there is a loss of convective lift. If the mother cloud undergoes a complete change in genus, it is considered to be a mutatus cloud.
It has been officially recommended that the genitus category be expanded to include certain types that do not originate from pre-existing clouds or as the result of any natural atmospheric processes. Among vertically developed clouds, these may include flammagenitus for cumulus congestus or cumulonimbus that are formed by large scale fires or volcanic eruptions. Smaller low-étage "pyrocumulus" or "fumulus" clouds formed by contained industrial activity could be classified as cumulus homogenitus. Contrails formed from the exhaust of aircraft flying in the high étage can persist and spread into formations resembling any of the high cloud genus-types. These variants have no special WMO designations, but are sometimes given the faux-Latin name Aviaticus. Persistent contrails have been identified as candidates for possible inclusion in the genitus category as cirrus, cirrostratus, or cirrocumulus homogenitus



Stratocumulus clouds can be organized into "fields" that take on certain specially classified shapes and characteristics. In general, these fields are more discernible from high altitudes than from ground level. They can often be found in the following forms:
Actinoform, which resembles a leaf or a spoked wheel.
Closed cell, which is cloudy in the center and clear on the edges, similar to a filled honeycomb.
Open cell, which resembles an empty honeycomb, with clouds around the edges and clear, open space in the middle.




These patterns are formed from a phenomenon known as a Kármán vortex which is named after the engineer and fluid dynamicist Theodore von Kármán,. When wind driven clouds are forced through a mountain range, or when ocean wind driven clouds encounter a high elevation island, they can begin to circle the mountain or high land mass. They can form at any altitude in the troposphere and are not restricted to any particular cloud type.







Air can become saturated as a result of being cooled to its dew point or by having moisture added from an adjacent source. Adiabatic cooling occurs when one or more of three possible lifting agents - cyclonic/frontal, convective, or orographic — causes air containing invisible water vapor to rise and cool to its dew point, the temperature at which the air becomes saturated. The main mechanism behind this process is adiabatic cooling. If the air is cooled to its dew point and becomes saturated, it normally sheds vapor it can no longer retain, which condenses into cloud. Water vapor in saturated air is normally attracted to condensation nuclei such as dust and salt particles that are small enough to be held aloft by normal circulation of the air.
Frontal and cyclonic lift occur when stable air is forced aloft at weather fronts and around centers of low pressure. Warm fronts associated with extratropical cyclones tend to generate mostly cirriform and stratiform clouds over a wide area unless the approaching warm airmass is unstable, in which case cumulus congestus or cumulonimbus clouds will usually be embedded in the main precipitating cloud layer. Cold fronts are usually faster moving and generate a narrower line of clouds which are mostly stratocumuliform, cumuliform, or cumulonimbiform depending on the stability of the warm air mass just ahead of the front.
Another agent is the convective upward motion of air caused by daytime solar heating at surface level. Airmass instability allows for the formation of cumuliform clouds that can produce showers if the air is sufficiently moist. On comparatively rare occasions, convective lift can be powerful enough to penetrate the tropopause and push the cloud top into the stratosphere.
A third source of lift is wind circulation forcing air over a physical barrier such as a mountain (orographic lift). If the air is generally stable, nothing more than lenticular cap clouds will form. However, if the air becomes sufficiently moist and unstable, orographic showers or thunderstorms may appear.

Along with adiabatic cooling that requires a lifting agent, there are three major non-adiabatic mechanisms for lowering the temperature of the air to its dew point. Conductive, radiational, and evaporative cooling require no lifting mechanism and can cause condensation at surface level resulting in the formation of fog.
There are several main sources of water vapor that can be added to the air as a way of achieving saturation without any cooling process: Water or moist ground, precipitation or virga, and transpiration from plants




Although the local distribution of clouds can be significantly influenced by topography, the global prevalence of cloud cover tends to vary more by latitude. It is most prevalent globally in and along low pressure zones of surface atmospheric convergence which encircle the Earth close to the equator and near the 50th parallels of latitude in the northern and southern hemispheres. The adiabatic cooling processes that lead to the creation of clouds by way of lifting agents are all associated with convergence; a process that involves the horizontal inflow and accumulation of air at a given location, as well as the rate at which this happens. Near the equator, increased cloudiness is due to the presence of the low-pressure Intertropical Convergence Zone (ITCZ) where very warm and unstable air promotes mostly cumuliform and cumulonimbiform clouds. Clouds of virtually any type can form along the mid-latitude convergence zones depending on the stability and moisture content of the air. These extratropical convergence zones are occupied by the polar fronts where air masses of polar origin meet and clash with those of tropical or subtropical origin. This leads to the formation of weather-making extratropical cyclones composed of cloud systems that may be stable or unstable to varying degrees according to the stability characteristics of the various airmasses that are in conflict.




Divergence is the opposite of convergence. In the Earth's atmosphere, it involves the horizontal outflow of air from the upper part of a rising column of air, or from the lower part of a subsiding column often associated with an area or ridge of high pressure. Cloudiness tends to be least prevalent near the poles and in the subtropics close to the 20th parallels, north and south. The latter are sometimes referred to as the horse latitudes. The presence of a large-scale high-pressure subtropical ridge on each side of the equator reduces cloudiness at these low latitudes. Similar patterns also occur at higher latitudes in both hemispheres.




The luminance or brightness of a cloud is determined by how light is reflected, scattered, and transmitted by the cloud's particles. Its brightness may also be affected by the presence of haze or photometeors such as halos and rainbows. In the troposphere, dense, deep clouds exhibit a high reflectance (70% to 95%) throughout the visible spectrum. Tiny particles of water are densely packed and sunlight cannot penetrate far into the cloud before it is reflected out, giving a cloud its characteristic white color, especially when viewed from the top. Cloud droplets tend to scatter light efficiently, so that the intensity of the solar radiation decreases with depth into the gases. As a result, the cloud base can vary from a very light to very-dark-grey depending on the cloud's thickness and how much light is being reflected or transmitted back to the observer. High thin tropospheric clouds reflect less light because of the comparatively low concentration of constituent ice crystals or supercooled water droplets which results in a slightly off-white appearance. However, a thick dense ice-crystal cloud appears brilliant white with pronounced grey shading because of its greater reflectivity.
As a tropospheric cloud matures, the dense water droplets may combine to produce larger droplets. If the droplets become too large and heavy to be kept aloft by the air circulation, they will fall from the cloud as rain. By this process of accumulation, the space between droplets becomes increasingly larger, permitting light to penetrate farther into the cloud. If the cloud is sufficiently large and the droplets within are spaced far enough apart, a percentage of the light that enters the cloud is not reflected back out but is absorbed giving the cloud a darker look. A simple example of this is one's being able to see farther in heavy rain than in heavy fog. This process of reflection/absorption is what causes the range of cloud color from white to black.

Striking cloud colorations can be seen at any altitude, with the color of a cloud usually being the same as the incident light. During daytime when the sun is relatively high in the sky, tropospheric clouds generally appear bright white on top with varying shades of grey underneath. Thin clouds may look white or appear to have acquired the color of their environment or background. Red, orange, and pink clouds occur almost entirely at sunrise/sunset and are the result of the scattering of sunlight by the atmosphere. When the sun is just below the horizon, low-etage clouds are gray, middle clouds appear rose-colored, and high-etage clouds are white or off-white. Clouds at night are black or dark grey in a moonless sky, or whitish when illuminated by the moon. They may also reflect the colors of large fires, city lights, or auroras that might be present.
A cumulonimbus cloud that appears to have a greenish/bluish tint is a sign that it contains extremely high amounts of water; hail or rain which scatter light in a way that gives the cloud a blue color. A green colorization occurs mostly late in the day when the sun is comparatively low in the sky and the incident sunlight has a reddish tinge that appears green when illuminating a very tall bluish cloud. Supercell type storms are more likely to be characterized by this but any storm can appear this way. Coloration such as this does not directly indicate that it is a severe thunderstorm, it only confirms its potential. Since a green/blue tint signifies copious amounts of water, a strong updraft to support it, high winds from the storm raining out, and wet hail; all elements that improve the chance for it to become severe, can all be inferred from this. In addition, the stronger the updraft is, the more likely the storm is to undergo tornadogenesis and to produce large hail and high winds.
Yellowish clouds may be seen in the troposphere in the late spring through early fall months during forest fire season. The yellow color is due to the presence of pollutants in the smoke. Yellowish clouds caused by the presence of nitrogen dioxide are sometimes seen in urban areas with high air pollution levels.




The role of tropospheric clouds in regulating weather and climate remains a leading source of uncertainty in projections of global warming. This uncertainty arises because of the delicate balance of processes related to clouds, spanning scales from millimeters to planetary. Hence, interactions between large-scale weather events (synoptic meteorology) and clouds becomes difficult to represent in global models.
The complexity and diversity of clouds, as outlined above, adds to the problem. On the one hand, white-colored cloud tops promote cooling of Earth's surface by reflecting short-wave radiation from the sun. Most of the sunlight that reaches the ground is absorbed, warming the surface, which emits radiation upward at longer, infrared, wavelengths. At these wavelengths, however, water in the clouds acts as an efficient absorber. The water reacts by radiating, also in the infrared, both upward and downward, and the downward long-wave radiation results in some warming at the surface. This is analogous to the greenhouse effect of greenhouse gases and water vapor.
High-étage genus-types particularly show this duality with both short-wave albedo cooling and long-wave greenhouse warming effects. On the whole though, ice-crystal clouds in the upper troposphere tend to favor net warming. However, the cooling effect is dominant with mid-level and low clouds made of very small water droplets with an average radius of about 0.002 mm (0.00008 in)., especially when they form in extensive sheets that block out more of the sun. Small-droplet aerosols are not good at absorbing long-wave radiation reflected back from Earth, so there is a net cooling with almost no long-wave effect. This effect is particularly pronounced with low clouds that form over water. Measurements taken by NASA indicate that on the whole, the effects of low and middle étage clouds that tend to promote cooling are outweighing the warming effects of high layers and the variable outcomes associated with or vertically developed clouds.
Low and vertical heaps of cumulus, towering cumulus, and cumulonimbus are made of larger water droplets ranging in radius from 0.005 to about 0.015 mm. Nimbostratus cloud droplets can also be quite large, up to 0.015 mm radius. These larger droplets associated with vertically developed clouds are better able to trap the long-wave radiation thus mitigating the cooling effect to some degree. However, these large often precipitating clouds are variable or unpredictable in their overall effect because of variations in their concentration, distribution, and vertical extent.
As difficult as it is to evaluate the effects of current cloud cover characteristics on climate change, it is even more problematic to predict the outcome of this change with respect to future cloud patterns and events. As a consequence, much research has focused on the response of low and vertical clouds to a changing climate. Leading global models can produce quite different results, however, with some showing increasing low-étage clouds and others showing decreases.




Polar stratospheric clouds show little variation in structure and are limited to a single very high range of altitude of about 15,000–25,000 m (49,200–82,000 ft), so they are not classified into étages, genus types, species, or varieties in the manner of tropospheric clouds. Instead, the classification is alpha-numeric and is based on chemical makeup rather than variations in physical appearance.






Type 1 (Non-nacreous): This type contains frozen or supercooled nitric acid and water droplets and lacks any special coloration. It is dividable into subtype 1A which is mostly made up of water ice crystals and frozen nitric acid, 1B which consists of supercooled droplets of nitric and sulfuric acid, and subtype 1C which comprises small particles of nitric acid. Nacreous type 2 is sometimes associated or embedded. Type I non-nacreous clouds are known to have harmful effects over the polar regions of Earth. They become catalysts which convert relatively benign man-made chlorine into active free radicals like chlorine monoxide which are destructive of the stratospheric ozone layer.
Type 2 (Nacreous): Nacreous polar stratospheric cloud consists of ice crystals only and generally shows mother-of-pearl colors. This is due to the refraction and diffusion of the sun's rays through thin clouds with supercooled droplets that often contain compounds other than water.



Polar stratospheric clouds form in the lowest part of the stratosphere during the winter, at the altitude and during the season that produces the coldest temperatures and therefore the best chances of triggering condensation caused by adiabatic cooling. They are typically very thin with an undulating cirriform appearance. Moisture is scarce in the stratosphere, so nacreous and non-nacreous cloud at this altitude range is rare and is usually restricted to polar regions in the winter where the air is coldest.




Polar mesospheric clouds form at a single extreme altitude range of about 80 to 85 km (50 to 53 mi) and are consequently not classified into more than one étage. They are given the Latin name noctilucent because of their illumination well after sunset and before sunrise. They typically have a bluish or silvery white coloration that can resemble brightly illuminated cirrus. Noctilucent clouds may occasionally take on more of a red or orange hue. They are not common or widespread enough to have a significant effect on climate. However, an increasing frequency of occurrence of noctilucent clouds since the 19th century may be the result of climate change.An alpha-numeric classification is used to identify variations in physical appearance.






Type 1: The first type is characterized by very tenuous filaments resembling cirrus fibratus.
Type 2: This type comprises bands in the form of long streaks, often in groups or interwoven at small angles, similar to cirrus intortus. It is dividable into two subtypes; 2A where the streaks have diffuse, blurred edges, and 2B where they have sharply defined edges.
Type 3: Billows in the form of short streaks can be seen that are clearly spaced and roughly parallel. Subtype 3A has short, straight, narrow streaks while 3B has wave-like streaks similar to cirrus undulatus.
Type 4: This shows whirls in the form of partial or rarely complete rings with dark centers. With subtype 4A, the whirls are of small angular radius and have a similar appearance to surface water ripples. 4B is characterized by simple curves of medium angular radius with one or more bands. Subtype 4C has whirls with large-scale ring structure.



Polar mesospheric clouds are the highest in the atmosphere and form near the top of the mesosphere at about ten times the altitude of tropospheric high clouds. From ground level, they can occasionally be seen illuminated by the sun during deep twilight. Ongoing research indicates that convective lift in the mesosphere is strong enough during the polar summer to cause adiabatic cooling of small amount of water vapour to the point of saturation. This tends to produce the coldest temperatures in the entire atmosphere just below the mesopause. These conditions result in the best environment for the formation of polar mesospheric clouds. There is also evidence that smoke particles from burnt-up meteors provide much of the condensation nuclei required for the formation of noctilucent cloud.
Distribution in the mesosphere is similar to the stratosphere except at much higher altitudes. Because of the need for maximum cooling of the water vapor to produce noctilucent clouds, their distribution tends to be restricted to polar regions of Earth. A major seasonal difference is that convective lift from below the mesosphere pushes very scarce water vapor to higher colder altitudes required for cloud formation during the respective summer seasons in the northern and southern hemispheres. Sightings are rare more than 45 degrees south of the north pole or north of the south pole.




Cloud cover has been seen on most other planets in the solar system. Venus's thick clouds are composed of sulfur dioxide and appear to be almost entirely stratiform. They are arranged in three main layers at altitudes of 45 to 65 km that obscure the planet's surface and can produce virga. No embedded cumuliform types have been identified, but broken stratocumuliform wave formations are sometimes seen in the top layer that reveal more continuous layer clouds underneath. On Mars, noctilucent, cirrus, cirrocumulus and stratocumulus composed of water-ice have been detected mostly near the poles. Water-ice fogs have also been detected on this planet.
Both Jupiter and Saturn have an outer cirriform cloud deck composed of ammonia, an intermediate stratiform haze-cloud layer made of ammonium hydrosulfide, and an inner deck of cumulus water clouds. Embedded cumulonimbus are known to exist near the Great Red Spot on Jupiter. The same category-types can be found covering Uranus, and Neptune, but are all composed of methane. Saturn's moon Titan has cirrus clouds believed to be composed largely of methane. The Cassini–Huygens Saturn mission uncovered evidence of polar stratospheric clouds and a fluid cycle on Titan, including lakes near the poles and fluvial channels on the surface of the moon.
Some planets outside the solar system are known to have atmospheric clouds. In October 2013, the detection of high altitude optically thick clouds in the atmosphere of exoplanet Kepler-7b was announced, and, in December 2013, also in the atmospheres of GJ 436 b and GJ 1214 b.









Ackerman, Steven A. (2011). Meteorology: Clouds and the Greenhouse Effect. Jones & Bartlett. ISBN 0-7637-8927-5. 
Dunlop, Storm (June 2003). The Weather Identification Handbook. Lyons Press. ISBN 978-1-58574-857-0. 




Could Reducing Global Dimming Mean a Hotter, Dryer World?
BadMeteorology's explanation of why clouds form
Monthly maps of global cloud cover, from NASA's Earth Observatory
Introduction to Clouds: Sky Watcher Chart National Oceanic and Atmospheric Administration and National Aeronautics and Space Administration using pre-1956 classification for nimbostratus
Cloud Appreciation Society Aesthetics of clouds
Shuttle Views the Earth: Clouds from Space
Details of selected main cloud types and sub types
USA Today Understanding clouds and Fog
clouds that look as if they were sculpted out of the sky
Clouds 365 Project Year-long photographic experiment shooting clouds everyday
The Function of Clouds
The short film Know Your Clouds (1 January 1967) is available for free download at the Internet Archive
Clouds and global cloud cover — Astronoo
CLOUD. YouTube Video of Michael Leuschner and Meinolf WewelChemistry is a branch of physical science that studies the composition, structure, properties and change of matter. Chemistry includes topics such as the properties of individual atoms, how atoms form chemical bonds to create chemical compounds, the interactions of substances through intermolecular forces that give matter its general properties, and the interactions between substances through chemical reactions to form different substances.
Chemistry is sometimes called the central science because it bridges other natural sciences, including physics, geology and biology. For the differences between chemistry and physics see comparison of chemistry and physics.
Scholars disagree about the etymology of the word chemistry. The history of chemistry can be traced to alchemy, which had been practiced for several millennia in various parts of the world.



The word chemistry comes from alchemy, which referred to an earlier set of practices that encompassed elements of chemistry, metallurgy, philosophy, astrology, astronomy, mysticism and medicine. It is often seen as linked to the quest to turn lead or another common starting material into gold, though in ancient times the study encompassed many of the questions of modern chemistry being defined as the study of the composition of waters, movement, growth, embodying, disembodying, drawing the spirits from bodies and bonding the spirits within bodies by the early 4th century Greek-Egyptian alchemist Zosimos. An alchemist was called a 'chemist' in popular speech, and later the suffix "-ry" was added to this to describe the art of the chemist as "chemistry".
The modern word alchemy in turn is derived from the Arabic word al-kīmīā (الکیمیاء). In origin, the term is borrowed from the Greek χημία or χημεία. This may have Egyptian origins since al-kīmīā is derived from the Greek χημία, which is in turn derived from the word Chemi or Kimi, which is the ancient name of Egypt in Egyptian. Alternately, al-kīmīā may derive from χημεία, meaning "cast together".



In retrospect, the definition of chemistry has changed over time, as new discoveries and theories add to the functionality of the science. The term "chymistry", in the view of noted scientist Robert Boyle in 1661, meant the subject of the material principles of mixed bodies. In 1663 the chemist Christopher Glaser described "chymistry" as a scientific art, by which one learns to dissolve bodies, and draw from them the different substances on their composition, and how to unite them again, and exalt them to a higher perfection.
The 1730 definition of the word "chemistry", as used by Georg Ernst Stahl, meant the art of resolving mixed, compound, or aggregate bodies into their principles; and of composing such bodies from those principles. In 1837, Jean-Baptiste Dumas considered the word "chemistry" to refer to the science concerned with the laws and effects of molecular forces. This definition further evolved until, in 1947, it came to mean the science of substances: their structure, their properties, and the reactions that change them into other substances - a characterization accepted by Linus Pauling. More recently, in 1998, Professor Raymond Chang broadened the definition of "chemistry" to mean the study of matter and the changes it undergoes.




Early civilizations, such as the Egyptians Babylonians, Indians amassed practical knowledge concerning the arts of metallurgy, pottery and dyes, but didn't develop a systematic theory.
A basic chemical hypothesis first emerged in Classical Greece with the theory of four elements as propounded definitively by Aristotle stating that fire, air, earth and water were the fundamental elements from which everything is formed as a combination. Greek atomism dates back to 440 BC, arising in works by philosophers such as Democritus and Epicurus. In 50 BC, the Roman philosopher Lucretius expanded upon the theory in his book De rerum natura (On The Nature of Things). Unlike modern concepts of science, Greek atomism was purely philosophical in nature, with little concern for empirical observations and no concern for chemical experiments.
In the Hellenistic world the art of alchemy first proliferated, mingling magic and occultism into the study of natural substances with the ultimate goal of transmuting elements into gold and discovering the elixir of eternal life. Work, particularly the development of distillation, continued in the early Byzantine period with the most famous practitioner being the 4th century Greek-Egyptian Zosimos of Panopolis. Alchemy continued to be developed and practised throughout the Arab world after the Muslim conquests, and from there, and from the Byzantine remnants, diffused into medieval and Renaissance Europe through Latin translations. Some influential Muslim chemists, Abū al-Rayhān al-Bīrūnī, Avicenna and Al-Kindi refuted the theories of alchemy, particularly the theory of the transmutation of metals; and al-Tusi described a version of the conservation of mass, noting that a body of matter is able to change but is not able to disappear.




The development of the modern scientific method was slow and arduous, but an early scientific method for chemistry began emerging among early Muslim chemists, beginning with the 9th century Persian or Arabian chemist Jābir ibn Hayyān (known as "Geber" in Europe), who is sometimes referred to as "the father of chemistry". He introduced a systematic and experimental approach to scientific research based in the laboratory, in contrast to the ancient Greek and Egyptian alchemists whose works were largely allegorical and often unintelligble. Under the influence of the new empirical methods propounded by Sir Francis Bacon and others, a group of chemists at Oxford, Robert Boyle, Robert Hooke and John Mayow began to reshape the old alchemical traditions into a scientific discipline. Boyle in particular is regarded as the founding father of chemistry due to his most important work, the classic chemistry text The Sceptical Chymist where the differentiation is made between the claims of alchemy and the empirical scientific discoveries of the new chemistry. He formulated Boyle's law, rejected the classical "four elements" and proposed a mechanistic alternative of atoms and chemical reactions that could be subject to rigorous experiment.

The theory of phlogiston (a substance at the root of all combustion) was propounded by the German Georg Ernst Stahl in the early 18th century and was only overturned by the end of the century by the French chemist Antoine Lavoisier, the chemical analogue of Newton in physics; who did more than any other to establish the new science on proper theoretical footing, by elucidating the principle of conservation of mass and developing a new system of chemical nomenclature used to this day.
Before his work, though, many important discoveries had been made, specifically relating to the nature of 'air' which was discovered to be composed of many different gases. The Scottish chemist Joseph Black (the first experimental chemist) and the Dutchman J. B. van Helmont discovered carbon dioxide, or what Black called 'fixed air' in 1754; Henry Cavendish discovered hydrogen and elucidated its properties and Joseph Priestley and, independently, Carl Wilhelm Scheele isolated pure oxygen.

English scientist John Dalton proposed the modern theory of atoms; that all substances are composed of indivisible 'atoms' of matter and that different atoms have varying atomic weights.
The development of the electrochemical theory of chemical combinations occurred in the early 19th century as the result of the work of two scientists in particular, J. J. Berzelius and Humphry Davy, made possible by the prior invention of the voltaic pile by Alessandro Volta. Davy discovered nine new elements including the alkali metals by extracting them from their oxides with electric current.
British William Prout first proposed ordering all the elements by their atomic weight as all atoms had a weight that was an exact multiple of the atomic weight of hydrogen. J. A. R. Newlands devised an early table of elements, which was then developed into the modern periodic table of elements in the 1860s by Dmitri Mendeleev and independently by several other scientists including Julius Lothar Meyer. The inert gases, later called the noble gases were discovered by William Ramsay in collaboration with Lord Rayleigh at the end of the century, thereby filling in the basic structure of the table.
Organic chemistry was developed by Justus von Liebig and others, following Friedrich Wöhler's synthesis of urea which proved that living organisms were, in theory, reducible to chemistry. Other crucial 19th century advances were; an understanding of valence bonding (Edward Frankland in 1852) and the application of thermodynamics to chemistry (J. W. Gibbs and Svante Arrhenius in the 1870s).




At the turn of the twentieth century the theoretical underpinnings of chemistry were finally understood due to a series of remarkable discoveries that succeeded in probing and discovering the very nature of the internal structure of atoms. In 1897, J. J. Thomson of Cambridge University discovered the electron and soon after the French scientist Becquerel as well as the couple Pierre and Marie Curie investigated the phenomenon of radioactivity. In a series of pioneering scattering experiments Ernest Rutherford at the University of Manchester discovered the internal structure of the atom and the existence of the proton, classified and explained the different types of radioactivity and successfully transmuted the first element by bombarding nitrogen with alpha particles.
His work on atomic structure was improved on by his students, the Danish physicist Niels Bohr and Henry Moseley. The electronic theory of chemical bonds and molecular orbitals was developed by the American scientists Linus Pauling and Gilbert N. Lewis.
The year 2011 was declared by the United Nations as the International Year of Chemistry. It was an initiative of the International Union of Pure and Applied Chemistry, and of the United Nations Educational, Scientific, and Cultural Organization and involves chemical societies, academics, and institutions worldwide and relied on individual initiatives to organize local and regional activities.




The current model of atomic structure is the quantum mechanical model. Traditional chemistry starts with the study of elementary particles, atoms, molecules, substances, metals, crystals and other aggregates of matter. This matter can be studied in solid, liquid, or gas states, in isolation or in combination. The interactions, reactions and transformations that are studied in chemistry are usually the result of interactions between atoms, leading to rearrangements of the chemical bonds which hold atoms together. Such behaviors are studied in a chemistry laboratory.
The chemistry laboratory stereotypically uses various forms of laboratory glassware. However glassware is not central to chemistry, and a great deal of experimental (as well as applied/industrial) chemistry is done without it.
A chemical reaction is a transformation of some substances into one or more different substances. The basis of such a chemical transformation is the rearrangement of electrons in the chemical bonds between atoms. It can be symbolically depicted through a chemical equation, which usually involves atoms as subjects. The number of atoms on the left and the right in the equation for a chemical transformation is equal. (When the number of atoms on either side is unequal, the transformation is referred to as a nuclear reaction or radioactive decay.) The type of chemical reactions a substance may undergo and the energy changes that may accompany it are constrained by certain basic rules, known as chemical laws.
Energy and entropy considerations are invariably important in almost all chemical studies. Chemical substances are classified in terms of their structure, phase, as well as their chemical compositions. They can be analyzed using the tools of chemical analysis, e.g. spectroscopy and chromatography. Scientists engaged in chemical research are known as chemists. Most chemists specialize in one or more sub-disciplines. Several concepts are essential for the study of chemistry; some of them are:




In chemistry, matter is defined as anything that has rest mass and volume (it takes up space) and is made up of particles. The particles that make up matter have rest mass as well - not all particles have rest mass, such as the photon. Matter can be a pure chemical substance or a mixture of substances.




The atom is the basic unit of chemistry. It consists of a dense core called the atomic nucleus surrounded by a space called the electron cloud. The nucleus is made up of positively charged protons and uncharged neutrons (together called nucleons), while the electron cloud consists of negatively charged electrons which orbit the nucleus. In a neutral atom, the negatively charged electrons balance out the positive charge of the protons. The nucleus is dense; the mass of a nucleon is 1,836 times that of an electron, yet the radius of an atom is about 10,000 times that of its nucleus.
The atom is also the smallest entity that can be envisaged to retain the chemical properties of the element, such as electronegativity, ionization potential, preferred oxidation state(s), coordination number, and preferred types of bonds to form (e.g., metallic, ionic, covalent).




A chemical element is a pure substance which is composed of a single type of atom, characterized by its particular number of protons in the nuclei of its atoms, known as the atomic number and represented by the symbol Z. The mass number is the sum of the number of protons and neutrons in a nucleus. Although all the nuclei of all atoms belonging to one element will have the same atomic number, they may not necessarily have the same mass number; atoms of an element which have different mass numbers are known as isotopes. For example, all atoms with 6 protons in their nuclei are atoms of the chemical element carbon, but atoms of carbon may have mass numbers of 12 or 13.
The standard presentation of the chemical elements is in the periodic table, which orders elements by atomic number. The periodic table is arranged in groups, or columns, and periods, or rows. The periodic table is useful in identifying periodic trends.




A compound is a pure chemical substance composed of more than one element. The properties of a compound bear little similarity to those of its elements. The standard nomenclature of compounds is set by the International Union of Pure and Applied Chemistry (IUPAC). Organic compounds are named according to the organic nomenclature system. Inorganic compounds are named according to the inorganic nomenclature system. In addition the Chemical Abstracts Service has devised a method to index chemical substances. In this scheme each chemical substance is identifiable by a number known as its CAS registry number.




A molecule is the smallest indivisible portion of a pure chemical substance that has its unique set of chemical properties, that is, its potential to undergo a certain set of chemical reactions with other substances. However, this definition only works well for substances that are composed of molecules, which is not true of many substances (see below). Molecules are typically a set of atoms bound together by covalent bonds, such that the structure is electrically neutral and all valence electrons are paired with other electrons either in bonds or in lone pairs.
Thus, molecules exist as electrically neutral units, unlike ions. When this rule is broken, giving the "molecule" a charge, the result is sometimes named a molecular ion or a polyatomic ion. However, the discrete and separate nature of the molecular concept usually requires that molecular ions be present only in well-separated form, such as a directed beam in a vacuum in a mass spectrometer. Charged polyatomic collections residing in solids (for example, common sulfate or nitrate ions) are generally not considered "molecules" in chemistry.

The "inert" or noble gas elements (helium, neon, argon, krypton, xenon and radon) are composed of lone atoms as their smallest discrete unit, but the other isolated chemical elements consist of either molecules or networks of atoms bonded to each other in some way. Identifiable molecules compose familiar substances such as water, air, and many organic compounds like alcohol, sugar, gasoline, and the various pharmaceuticals.
However, not all substances or chemical compounds consist of discrete molecules, and indeed most of the solid substances that make up the solid crust, mantle, and core of the Earth are chemical compounds without molecules. These other types of substances, such as ionic compounds and network solids, are organized in such a way as to lack the existence of identifiable molecules per se. Instead, these substances are discussed in terms of formula units or unit cells as the smallest repeating structure within the substance. Examples of such substances are mineral salts (such as table salt), solids like carbon and diamond, metals, and familiar silica and silicate minerals such as quartz and granite.
One of the main characteristics of a molecule is its geometry often called its structure. While the structure of diatomic, triatomic or tetra atomic molecules may be trivial, (linear, angular pyramidal etc.) the structure of polyatomic molecules, that are constituted of more than six atoms (of several elements) can be crucial for its chemical nature.



A chemical substance is a kind of matter with a definite composition and set of properties. A collection of substances is called a mixture. Examples of mixtures are air and alloys.




The mole is a unit of measurement that denotes an amount of substance (also called chemical amount). The mole is defined as the number of atoms found in exactly 0.012 kilogram (or 12 grams) of carbon-12, where the carbon-12 atoms are unbound, at rest and in their ground state. The number of entities per mole is known as the Avogadro constant, and is determined empirically to be approximately 6.022×1023 mol−1. Molar concentration is the amount of a particular substance per volume of solution, and is commonly reported in moldm−3.




In addition to the specific chemical properties that distinguish different chemical classifications, chemicals can exist in several phases. For the most part, the chemical classifications are independent of these bulk phase classifications; however, some more exotic phases are incompatible with certain chemical properties. A phase is a set of states of a chemical system that have similar bulk structural properties, over a range of conditions, such as pressure or temperature.
Physical properties, such as density and refractive index tend to fall within values characteristic of the phase. The phase of matter is defined by the phase transition, which is when energy put into or taken out of the system goes into rearranging the structure of the system, instead of changing the bulk conditions.
Sometimes the distinction between phases can be continuous instead of having a discrete boundary, in this case the matter is considered to be in a supercritical state. When three states meet based on the conditions, it is known as a triple point and since this is invariant, it is a convenient way to define a set of conditions.
The most familiar examples of phases are solids, liquids, and gases. Many substances exhibit multiple solid phases. For example, there are three phases of solid iron (alpha, gamma, and delta) that vary based on temperature and pressure. A principal difference between solid phases is the crystal structure, or arrangement, of the atoms. Another phase commonly encountered in the study of chemistry is the aqueous phase, which is the state of substances dissolved in aqueous solution (that is, in water).
Less familiar phases include plasmas, Bose–Einstein condensates and fermionic condensates and the paramagnetic and ferromagnetic phases of magnetic materials. While most familiar phases deal with three-dimensional systems, it is also possible to define analogs in two-dimensional systems, which has received attention for its relevance to systems in biology.




Atoms sticking together in molecules or crystals are said to be bonded with one another. A chemical bond may be visualized as the multipole balance between the positive charges in the nuclei and the negative charges oscillating about them. More than simple attraction and repulsion, the energies and distributions characterize the availability of an electron to bond to another atom.
A chemical bond can be a covalent bond, an ionic bond, a hydrogen bond or just because of Van der Waals force. Each of these kinds of bonds is ascribed to some potential. These potentials create the interactions which hold atoms together in molecules or crystals. In many simple compounds, valence bond theory, the Valence Shell Electron Pair Repulsion model (VSEPR), and the concept of oxidation number can be used to explain molecular structure and composition.
An ionic bond is formed when a metal loses one or more of its electrons, becoming a positively charged cation, and the electrons are then gained by the non-metal atom, becoming a negatively charged anion. The two oppositely charged ions attract one another, and the ionic bond is the electrostatic force of attraction between them. For example, sodium (Na), a metal, loses one electron to become an Na+ cation while chlorine (Cl), a non-metal, gains this electron to become Cl−. The ions are held together due to electrostatic attraction, and that compound sodium chloride (NaCl), or common table salt, is formed.

In a covalent bond, one or more pairs of valence electrons are shared by two atoms: the resulting electrically neutral group of bonded atoms is termed a molecule. Atoms will share valence electrons in such a way as to create a noble gas electron configuration (eight electrons in their outermost shell) for each atom. Atoms that tend to combine in such a way that they each have eight electrons in their valence shell are said to follow the octet rule. However, some elements like hydrogen and lithium need only two electrons in their outermost shell to attain this stable configuration; these atoms are said to follow the duet rule, and in this way they are reaching the electron configuration of the noble gas helium, which has two electrons in its outer shell.
Similarly, theories from classical physics can be used to predict many ionic structures. With more complicated compounds, such as metal complexes, valence bond theory is less applicable and alternative approaches, such as the molecular orbital theory, are generally used. See diagram on electronic orbitals.




In the context of chemistry, energy is an attribute of a substance as a consequence of its atomic, molecular or aggregate structure. Since a chemical transformation is accompanied by a change in one or more of these kinds of structures, it is invariably accompanied by an increase or decrease of energy of the substances involved. Some energy is transferred between the surroundings and the reactants of the reaction in the form of heat or light; thus the products of a reaction may have more or less energy than the reactants.
A reaction is said to be exergonic if the final state is lower on the energy scale than the initial state; in the case of endergonic reactions the situation is the reverse. A reaction is said to be exothermic if the reaction releases heat to the surroundings; in the case of endothermic reactions, the reaction absorbs heat from the surroundings.
Chemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy. The speed of a chemical reaction (at given temperature T) is related to the activation energy E, by the Boltzmann's population factor 
  
    
      
        
          e
          
            −
            E
            
              /
            
            k
            T
          
        
      
    
    {\displaystyle e^{-E/kT}}
   - that is the probability of a molecule to have energy greater than or equal to E at the given temperature T. This exponential dependence of a reaction rate on temperature is known as the Arrhenius equation. The activation energy necessary for a chemical reaction to occur can be in the form of heat, light, electricity or mechanical force in the form of ultrasound.
A related concept free energy, which also incorporates entropy considerations, is a very useful means for predicting the feasibility of a reaction and determining the state of equilibrium of a chemical reaction, in chemical thermodynamics. A reaction is feasible only if the total change in the Gibbs free energy is negative, 
  
    
      
        Δ
        G
        ≤
        0
        
      
    
    {\displaystyle \Delta G\leq 0\,}
  ; if it is equal to zero the chemical reaction is said to be at equilibrium.
There exist only limited possible states of energy for electrons, atoms and molecules. These are determined by the rules of quantum mechanics, which require quantization of energy of a bound system. The atoms/molecules in a higher energy state are said to be excited. The molecules/atoms of substance in an excited energy state are often much more reactive; that is, more amenable to chemical reactions.
The phase of a substance is invariably determined by its energy and the energy of its surroundings. When the intermolecular forces of a substance are such that the energy of the surroundings is not sufficient to overcome them, it occurs in a more ordered phase like liquid or solid as is the case with water (H2O); a liquid at room temperature because its molecules are bound by hydrogen bonds. Whereas hydrogen sulfide (H2S) is a gas at room temperature and standard pressure, as its molecules are bound by weaker dipole-dipole interactions.
The transfer of energy from one chemical substance to another depends on the size of energy quanta emitted from one substance. However, heat energy is often transferred more easily from almost any substance to another because the phonons responsible for vibrational and rotational energy levels in a substance have much less energy than photons invoked for the electronic energy transfer. Thus, because vibrational and rotational energy levels are more closely spaced than electronic energy levels, heat is more easily transferred between substances relative to light or other forms of electronic energy. For example, ultraviolet electromagnetic radiation is not transferred with as much efficacy from one substance to another as thermal or electrical energy.
The existence of characteristic energy levels for different chemical substances is useful for their identification by the analysis of spectral lines. Different kinds of spectra are often used in chemical spectroscopy, e.g. IR, microwave, NMR, ESR, etc. Spectroscopy is also used to identify the composition of remote objects - like stars and distant galaxies - by analyzing their radiation spectra.

The term chemical energy is often used to indicate the potential of a chemical substance to undergo a transformation through a chemical reaction or to transform other chemical substances.




When a chemical substance is transformed as a result of its interaction with another substance or with energy, a chemical reaction is said to have occurred. A chemical reaction is therefore a concept related to the "reaction" of a substance when it comes in close contact with another, whether as a mixture or a solution; exposure to some form of energy, or both. It results in some energy exchange between the constituents of the reaction as well as with the system environment, which may be designed vessels—often laboratory glassware.
Chemical reactions can result in the formation or dissociation of molecules, that is, molecules breaking apart to form two or more smaller molecules, or rearrangement of atoms within or across molecules. Chemical reactions usually involve the making or breaking of chemical bonds. Oxidation, reduction, dissociation, acid-base neutralization and molecular rearrangement are some of the commonly used kinds of chemical reactions.
A chemical reaction can be symbolically depicted through a chemical equation. While in a non-nuclear chemical reaction the number and kind of atoms on both sides of the equation are equal, for a nuclear reaction this holds true only for the nuclear particles viz. protons and neutrons.
The sequence of steps in which the reorganization of chemical bonds may be taking place in the course of a chemical reaction is called its mechanism. A chemical reaction can be envisioned to take place in a number of steps, each of which may have a different speed. Many reaction intermediates with variable stability can thus be envisaged during the course of a reaction. Reaction mechanisms are proposed to explain the kinetics and the relative product mix of a reaction. Many physical chemists specialize in exploring and proposing the mechanisms of various chemical reactions. Several empirical rules, like the Woodward–Hoffmann rules often come in handy while proposing a mechanism for a chemical reaction.
According to the IUPAC gold book, a chemical reaction is "a process that results in the interconversion of chemical species." Accordingly, a chemical reaction may be an elementary reaction or a stepwise reaction. An additional caveat is made, in that this definition includes cases where the interconversion of conformers is experimentally observable. Such detectable chemical reactions normally involve sets of molecular entities as indicated by this definition, but it is often conceptually convenient to use the term also for changes involving single molecular entities (i.e. 'microscopic chemical events').




An ion is a charged species, an atom or a molecule, that has lost or gained one or more electrons. When an atom loses an electron and thus has more protons than electrons, the atom is a positively charged ion or cation. When an atom gains an electron and thus has more electrons than protons, the atom is a negatively charged ion or anion. Cations and anions can form a crystalline lattice of neutral salts, such as the Na+ and Cl− ions forming sodium chloride, or NaCl. Examples of polyatomic ions that do not split up during acid-base reactions are hydroxide (OH−) and phosphate (PO43−).
Plasma is composed of gaseous matter that has been completely ionized, usually through high temperature.




A substance can often be classified as an acid or a base. There are several different theories which explain acid-base behavior. The simplest is Arrhenius theory, which states than an acid is a substance that produces hydronium ions when it is dissolved in water, and a base is one that produces hydroxide ions when dissolved in water. According to Brønsted–Lowry acid–base theory, acids are substances that donate a positive hydrogen ion to another substance in a chemical reaction; by extension, a base is the substance which receives that hydrogen ion.
A third common theory is Lewis acid-base theory, which is based on the formation of new chemical bonds. Lewis theory explains that an acid is a substance which is capable of accepting a pair of electrons from another substance during the process of bond formation, while a base is a substance which can provide a pair of electrons to form a new bond. According to this theory, the crucial things being exchanged are charges. There are several other ways in which a substance may be classified as an acid or a base, as is evident in the history of this concept.
Acid strength is commonly measured by two methods. One measurement, based on the Arrhenius definition of acidity, is pH, which is a measurement of the hydronium ion concentration in a solution, as expressed on a negative logarithmic scale. Thus, solutions that have a low pH have a high hydronium ion concentration, and can be said to be more acidic. The other measurement, based on the Brønsted–Lowry definition, is the acid dissociation constant (Ka), which measures the relative ability of a substance to act as an acid under the Brønsted–Lowry definition of an acid. That is, substances with a higher Ka are more likely to donate hydrogen ions in chemical reactions than those with lower Ka values.




Redox (reduction-oxidation) reactions include all chemical reactions in which atoms have their oxidation state changed by either gaining electrons (reduction) or losing electrons (oxidation). Substances that have the ability to oxidize other substances are said to be oxidative and are known as oxidizing agents, oxidants or oxidizers. An oxidant removes electrons from another substance. Similarly, substances that have the ability to reduce other substances are said to be reductive and are known as reducing agents, reductants, or reducers.
A reductant transfers electrons to another substance, and is thus oxidized itself. And because it "donates" electrons it is also called an electron donor. Oxidation and reduction properly refer to a change in oxidation number—the actual transfer of electrons may never occur. Thus, oxidation is better defined as an increase in oxidation number, and reduction as a decrease in oxidation number.




Although the concept of equilibrium is widely used across sciences, in the context of chemistry, it arises whenever a number of different states of the chemical composition are possible, as for example, in a mixture of several chemical compounds that can react with one another, or when a substance can be present in more than one kind of phase.
A system of chemical substances at equilibrium, even though having an unchanging composition, is most often not static; molecules of the substances continue to react with one another thus giving rise to a dynamic equilibrium. Thus the concept describes the state in which the parameters such as chemical composition remain unchanged over time.




Chemical reactions are governed by certain laws, which have become fundamental concepts in chemistry. Some of them are:






Chemistry is typically divided into several major sub-disciplines. There are also several main cross-disciplinary and more specialized fields of chemistry.
Analytical chemistry is the analysis of material samples to gain an understanding of their chemical composition and structure. Analytical chemistry incorporates standardized experimental methods in chemistry. These methods may be used in all subdisciplines of chemistry, excluding purely theoretical chemistry.
Biochemistry is the study of the chemicals, chemical reactions and chemical interactions that take place in living organisms. Biochemistry and organic chemistry are closely related, as in medicinal chemistry or neurochemistry. Biochemistry is also associated with molecular biology and genetics.
Inorganic chemistry is the study of the properties and reactions of inorganic compounds. The distinction between organic and inorganic disciplines is not absolute and there is much overlap, most importantly in the sub-discipline of organometallic chemistry.
Materials chemistry is the preparation, characterization, and understanding of substances with a useful function. The field is a new breadth of study in graduate programs, and it integrates elements from all classical areas of chemistry with a focus on fundamental issues that are unique to materials. Primary systems of study include the chemistry of condensed phases (solids, liquids, polymers) and interfaces between different phases.
Neurochemistry is the study of neurochemicals; including transmitters, peptides, proteins, lipids, sugars, and nucleic acids; their interactions, and the roles they play in forming, maintaining, and modifying the nervous system.
Nuclear chemistry is the study of how subatomic particles come together and make nuclei. Modern Transmutation is a large component of nuclear chemistry, and the table of nuclides is an important result and tool for this field.
Organic chemistry is the study of the structure, properties, composition, mechanisms, and reactions of organic compounds. An organic compound is defined as any compound based on a carbon skeleton.
Physical chemistry is the study of the physical and fundamental basis of chemical systems and processes. In particular, the energetics and dynamics of such systems and processes are of interest to physical chemists. Important areas of study include chemical thermodynamics, chemical kinetics, electrochemistry, statistical mechanics, spectroscopy, and more recently, astrochemistry. Physical chemistry has large overlap with molecular physics. Physical chemistry involves the use of infinitesimal calculus in deriving equations. It is usually associated with quantum chemistry and theoretical chemistry. Physical chemistry is a distinct discipline from chemical physics, but again, there is very strong overlap.
Theoretical chemistry is the study of chemistry via fundamental theoretical reasoning (usually within mathematics or physics). In particular the application of quantum mechanics to chemistry is called quantum chemistry. Since the end of the Second World War, the development of computers has allowed a systematic development of computational chemistry, which is the art of developing and applying computer programs for solving chemical problems. Theoretical chemistry has large overlap with (theoretical and experimental) condensed matter physics and molecular physics.
Other disciplines within chemistry are traditionally grouped by the type of matter being studied or the kind of study. These include inorganic chemistry, the study of inorganic matter; organic chemistry, the study of organic (carbon-based) matter; biochemistry, the study of substances found in biological organisms; physical chemistry, the study of chemical processes using physical concepts such as thermodynamics and quantum mechanics; and analytical chemistry, the analysis of material samples to gain an understanding of their chemical composition and structure. Many more specialized disciplines have emerged in recent years, e.g. neurochemistry the chemical study of the nervous system (see subdisciplines).
Other fields include agrochemistry, astrochemistry (and cosmochemistry), atmospheric chemistry, chemical engineering, chemical biology, chemo-informatics, electrochemistry, environmental chemistry, femtochemistry, flavor chemistry, flow chemistry, geochemistry, green chemistry, histochemistry, history of chemistry, hydrogenation chemistry, immunochemistry, marine chemistry, materials science, mathematical chemistry, mechanochemistry, medicinal chemistry, molecular biology, molecular mechanics, nanotechnology, natural product chemistry, oenology, organometallic chemistry, petrochemistry, pharmacology, photochemistry, physical organic chemistry, phytochemistry, polymer chemistry, radiochemistry, solid-state chemistry, sonochemistry, supramolecular chemistry, surface chemistry, synthetic chemistry, thermochemistry, and many others.




The chemical industry represents an important economic activity worldwide. The global top 50 chemical producers in 2013 had sales of US$980.5 billion with a profit margin of 10.3%.







Outline of chemistry
Glossary of chemistry terms
Common chemicals
International Year of Chemistry
List of chemists
List of compounds
List of important publications in chemistry
Comparison of software for molecular mechanics modeling
List of unsolved problems in chemistry
Periodic systems of small molecules
Philosophy of chemistry






Atkins, Peter; de Paula, Julio (2009) [1992]. Elements of Physical Chemistry (5th ed.). New York: Oxford University Press. ISBN 978-0-19-922672-6. 
Burrows, Andrew; Holman, John; Parsons, Andrew; Pilling, Gwen; Price, Gareth (2009). Chemistry3. Italy: Oxford University Press. ISBN 978-0-19-927789-6. 
Housecroft, Catherine E.; Sharpe, Alan G. (2008) [2001]. Inorganic Chemistry (3rd ed.). Harlow, Essex: Pearson Education. ISBN 978-0-13-175553-6. 




Popular reading
Atkins, P.W. Galileo's Finger (Oxford University Press) ISBN 0-19-860941-8
Atkins, P.W. Atkins' Molecules (Cambridge University Press) ISBN 0-521-82397-8
Kean, Sam. The Disappearing Spoon - and other true tales from the Periodic Table (Black Swan) London, 2010 ISBN 978-0-552-77750-6
Levi, Primo The Periodic Table (Penguin Books) [1975] translated from the Italian by Raymond Rosenthal (1984) ISBN 978-0-14-139944-7
Stwertka, A. A Guide to the Elements (Oxford University Press) ISBN 0-19-515027-9
"Dictionary of the History of Ideas". Archived from the original on March 10, 2008. 
 "Chemistry". Encyclopædia Britannica. 6 (11th ed.). 1911. pp. 33–76. 
Introductory undergraduate text books
Atkins, P.W., Overton, T., Rourke, J., Weller, M. and Armstrong, F. Shriver and Atkins inorganic chemistry (4th edition) 2006 (Oxford University Press) ISBN 0-19-926463-5
Chang, Raymond. Chemistry 6th ed. Boston: James M. Smith, 1998. ISBN 0-07-115221-0.
Clayden, Jonathan; Greeves, Nick; Warren, Stuart; Wothers, Peter (2001). Organic Chemistry (1st ed.). Oxford University Press. ISBN 978-0-19-850346-0. 
Voet and Voet Biochemistry (Wiley) ISBN 0-471-58651-X
Advanced undergraduate-level or graduate text books
Atkins, P.W. Physical Chemistry (Oxford University Press) ISBN 0-19-879285-9
Atkins, P.W. et al. Molecular Quantum Mechanics (Oxford University Press)
McWeeny, R. Coulson's Valence (Oxford Science Publications) ISBN 0-19-855144-4
Pauling, L. The Nature of the chemical bond (Cornell University Press) ISBN 0-8014-0333-2
Pauling, L., and Wilson, E. B. Introduction to Quantum Mechanics with Applications to Chemistry (Dover Publications) ISBN 0-486-64871-0
Smart and Moore Solid State Chemistry: An Introduction (Chapman and Hall) ISBN 0-412-40040-5
Stephenson, G. Mathematical Methods for Science Students (Longman) ISBN 0-582-44416-0



General Chemistry principles, patterns and applications.A chemical compound (or just compound if used in the context of chemistry) is an entity consisting of two or more atoms, at least two from different elements, which associate via chemical bonds. There are four types of compounds, depending on how the constituent atoms are held together: molecules held together by covalent bonds, ionic compounds held together by ionic bonds, intermetallic compounds held together by metallic bonds, and certain complexes held together by coordinate covalent bonds. Many chemical compounds have a unique numerical identifier assigned by the Chemical Abstracts Service (CAS): its CAS number.
A chemical formula is a way of expressing information about the proportions of atoms that constitute a particular chemical compound, using the standard abbreviations for the chemical elements, and subscripts to indicate the number of atoms involved. For example, water is composed of two hydrogen atoms bonded to one oxygen atom: the chemical formula is H2O.
A compound can be converted to a different chemical composition by interaction with a second chemical compound via a chemical reaction. In this process, bonds between atoms are broken in both of the interacting compounds, and then bonds are reformed so that new associations are made between atoms. Schematically, this reaction could be described as AB + CD --> AC + BD, where A, B, C, and D are each unique atoms; and AB, CD, AC, and BD are each unique compounds.
A chemical element bonded to an identical chemical element is not a chemical compound since only one element, not two different elements, is involved. Examples are the diatomic molecule hydrogen (H2) and the polyatomic molecule sulfur (S8).



Any substance consisting of two or more different types of atoms (chemical elements) in a fixed proportion of its atoms (i.e., stoichiometry) can be termed a chemical compound; the concept is most readily understood when considering pure chemical substances.  It follows from their being composed of fixed proportions of two or more types of atoms that chemical compounds can be converted, via chemical reaction, into compounds or substances each having fewer atoms. In the case of non-stoichiometric compounds, the proportions may be reproducible with regard to their preparation, and give fixed proportions of their component elements, but proportions that are not integral [e.g., for palladium hydride, PdHx (0.02 < x < 0.58)]. Chemical compounds have a unique and defined chemical structure held together in a defined spatial arrangement by chemical bonds. Chemical compounds can be molecular compounds held together by covalent bonds, salts held together by ionic bonds, intermetallic compounds held together by metallic bonds, or the subset of chemical complexes that are held together by coordinate covalent bonds. Pure chemical elements are generally not considered chemical compounds, failing the two or more atom requirement, though they often consist of molecules composed of multiple atoms (such as in the diatomic molecule H2, or the polyatomic molecule S8, etc.).
There is varying and sometimes inconsistent nomenclature differentiating substances, which include truly non-stoichiometric examples, from chemical compounds, which require the fixed ratios. Many solid chemical substances—for example many silicate minerals—are chemical substances, but do not have simple formulae reflecting chemically bonding of elements to one another in fixed ratios; even so, these crystalline substances are often called "non-stoichiometric compounds". It may be argued that they are related to, rather than being chemical compounds, insofar as the variability in their compositions is often due to either the presence of foreign elements trapped within the crystal structure of an otherwise known true chemical compound, or due to perturbations in structure relative to the known compound that arise because of an excess of deficit of the constituent elements at places in its structure; such non-stoichiometric substances form most of the crust and mantle of the Earth. Other compounds regarded as chemically identical may have varying amounts of heavy or light isotopes of the constituent elements, which changes the ratio of elements by mass slightly.



Characteristic properties of compounds include that elements in a compound are present in a definite proportion. For example, the molecule of the compound water is composed of hydrogen and oxygen in a ratio of 2:1. In addition, compounds have a definite set of properties, and the elements that comprise a compound do not retain their original properties. For example, hydrogen, which is combustible and non-supportive of combustion, combines with oxygen, which is non-combustible and supportive of combustion, to produce the compound water, which is non-combustible and non-supportive of combustion.



The physical and chemical properties of compounds differ from those of their constituent elements. This is one of the main criteria that distinguish a compound from a mixture of elements or other substances—in general, a mixture's properties are closely related to, and depend on, the properties of its constituents. Another criterion that distinguishes a compound from a mixture is that constituents of a mixture can usually be separated by simple mechanical means, such as filtering, evaporation, or magnetic force, but components of a compound can be separated only by a chemical reaction. However, mixtures can be created by mechanical means alone, but a compound can be created (either from elements or from other compounds, or a combination of the two) only by a chemical reaction.
Some mixtures are so intimately combined that they have some properties similar to compounds and may easily be mistaken for compounds. One example is alloys. Alloys are made mechanically, most commonly by heating the constituent metals to a liquid state, mixing them thoroughly, and then cooling the mixture quickly so that the constituents are trapped in the base metal. Other examples of compound-like mixtures include intermetallic compounds and solutions of alkali metals in a liquid form of ammonia.




A chemical formula is a way of expressing information about the proportions of atoms that constitute a particular chemical compound, using a single line of chemical element symbols, numbers, and sometimes also other symbols, such as parentheses, dashes, brackets, commas and plus (+) and minus (−) signs.
Compounds may be described using formulas in various formats. For compounds that exist as molecules, the formula for the molecular unit is shown. For polymeric materials, such as minerals and many metal oxides, the empirical formula is normally given, e.g. NaCl for table salt.
The elements in a chemical formula are normally listed in a specific order, called the Hill system. In this system, the carbon atoms (if there are any) are usually listed first, any hydrogen atoms are listed next, and all other elements follow in alphabetical order. If the formula contains no carbon, then all of the elements, including hydrogen, are listed alphabetically. There are, however, several important exceptions to the normal rules. For ionic compounds, the positive ion is almost always listed first and the negative ion is listed second. For oxides, oxygen is usually listed last.
In general, organic acids follow the normal rules with C and H coming first in the formula. For example, the formula for trifluoroacetic acid is usually written as C2HF3O2. More descriptive formulas can convey structural information, such as writing the formula for trifluoroacetic acid as CF3CO2H. On the other hand, the chemical formulas for most inorganic acids and bases are exceptions to the normal rules. They are written according to the rules for ionic compounds (positive first, negative second), but they also follow rules that emphasize their Arrhenius definitions. To be specific, the formula for most inorganic acids begins with hydrogen and the formula for most bases ends with the hydroxide ion (OH−). Formulas for inorganic compounds do not often convey structural information, as illustrated by the common use of the formula H2SO4 for a molecule (sulfuric acid) that contains no H-S bonds. A more descriptive presentation would be O2S(OH)2, but it is almost never written this way.



Compounds may have several possible phases. All compounds can exist as solids, at least at low enough temperatures. Molecular compounds may also exist as liquids, gases, and, in some cases, even plasmas. All compounds decompose upon applying heat. The temperature at which such fragmentation occurs is often called the decomposition temperature. Decomposition temperatures are not sharp and depend on pressure, temperature, and the concentration of each species in the compound.



Chemical element
Chemical revolution
Chemical structure
IUPAC nomenclature
Dictionary of chemical formulas
List of compounds
Addition to pi ligands






Robert Siegfried (2002), From elements to atoms: a history of chemical composition, American Philosophical Society, ISBN 978-0-87169-924-4 


A molecule is an electrically neutral group of two or more atoms held together by chemical bonds. Molecules are distinguished from ions by their lack of electrical charge. However, in quantum physics, organic chemistry, and biochemistry, the term molecule is often used less strictly, also being applied to polyatomic ions.
In the kinetic theory of gases, the term molecule is often used for any gaseous particle regardless of its composition. According to this definition, noble gas atoms are considered molecules as they are in fact monoatomic molecules.
A molecule may be homonuclear, that is, it consists of atoms of one chemical element, as with oxygen (O2); or it may be heteronuclear, a chemical compound composed of more than one element, as with water (H2O). Atoms and complexes connected by non-covalent interactions, such as hydrogen bonds or ionic bonds, are generally not considered single molecules.
Molecules as components of matter are common in organic substances (and therefore biochemistry). They also make up most of the oceans and atmosphere. However, the majority of familiar solid substances on Earth, including most of the minerals that make up the crust, mantle, and core of the Earth, contain many chemical bonds, but are not made of identifiable molecules. Also, no typical molecule can be defined for ionic crystals (salts) and covalent crystals (network solids), although these are often composed of repeating unit cells that extend either in a plane (such as in graphene) or three-dimensionally (such as in diamond, quartz, or sodium chloride). The theme of repeated unit-cellular-structure also holds for most condensed phases with metallic bonding, which means that solid metals are also not made of molecules. In glasses (solids that exist in a vitreous disordered state), atoms may also be held together by chemical bonds with no presence of any definable molecule, nor any of the regularity of repeating units that characterizes crystals.



The science of molecules is called molecular chemistry or molecular physics, depending on whether the focus is on chemistry or physics. Molecular chemistry deals with the laws governing the interaction between molecules that results in the formation and breakage of chemical bonds, while molecular physics deals with the laws governing their structure and properties. In practice, however, this distinction is vague. In molecular sciences, a molecule consists of a stable system (bound state) composed of two or more atoms. Polyatomic ions may sometimes be usefully thought of as electrically charged molecules. The term unstable molecule is used for very reactive species, i.e., short-lived assemblies (resonances) of electrons and nuclei, such as radicals, molecular ions, Rydberg molecules, transition states, van der Waals complexes, or systems of colliding atoms as in Bose–Einstein condensate.




According to Merriam-Webster and the Online Etymology Dictionary, the word "molecule" derives from the Latin "moles" or small unit of mass.
Molecule (1794) – "extremely minute particle", from French molécule (1678), from New Latin molecula, diminutive of Latin moles "mass, barrier". A vague meaning at first; the vogue for the word (used until the late 18th century only in Latin form) can be traced to the philosophy of Descartes.
The definition of the molecule has evolved as knowledge of the structure of molecules has increased. Earlier definitions were less precise, defining molecules as the smallest particles of pure chemical substances that still retain their composition and chemical properties. This definition often breaks down since many substances in ordinary experience, such as rocks, salts, and metals, are composed of large crystalline networks of chemically bonded atoms or ions, but are not made of discrete molecules.



Molecules are held together by either covalent bonding or ionic bonding. Several types of non-metal elements exist only as molecules in the environment. For example, hydrogen only exists as hydrogen molecule. A molecule of a compound is made out of two or more elements.




A covalent bond is a chemical bond that involves the sharing of electron pairs between atoms. These electron pairs are termed shared pairs or bonding pairs, and the stable balance of attractive and repulsive forces between atoms, when they share electrons, is termed covalent bonding.




Ionic bonding is a type of chemical bond that involves the electrostatic attraction between oppositely charged ions, and is the primary interaction occurring in ionic compounds. The ions are atoms that have lost one or more electrons (termed cations) and atoms that have gained one or more electrons (termed anions). This transfer of electrons is termed electrovalence in contrast to covalence. In the simplest case, the cation is a metal atom and the anion is a nonmetal atom, but these ions can be of a more complicated nature, e.g. molecular ions like NH4+ or SO42−. In simpler words, an ionic bond is the transfer of electrons from a metal to a non-metal for both atoms to obtain a full valence shell.




Most molecules are far too small to be seen with the naked eye, but there are exceptions. DNA, a macromolecule, can reach macroscopic sizes, as can molecules of many polymers. Molecules commonly used as building blocks for organic synthesis have a dimension of a few angstroms (Å) to several dozen Å, or around one billionth of a meter. Single molecules cannot usually be observed by light (as noted above), but small molecules and even the outlines of individual atoms may be traced in some circumstances by use of an atomic force microscope. Some of the largest molecules are macromolecules or supermolecules.
The smallest molecule is the diatomic hydrogen (H2), with a bond length of 0.74 Å.
Effective molecular radius is the size a molecule displays in solution. The table of permselectivity for different substances contains examples.







The chemical formula for a molecule uses one line of chemical element symbols, numbers, and sometimes also other symbols, such as parentheses, dashes, brackets, and plus (+) and minus (−) signs. These are limited to one typographic line of symbols, which may include subscripts and superscripts.
A compound's empirical formula is a very simple type of chemical formula. It is the simplest integer ratio of the chemical elements that constitute it. For example, water is always composed of a 2:1 ratio of hydrogen to oxygen atoms, and ethyl alcohol or ethanol is always composed of carbon, hydrogen, and oxygen in a 2:6:1 ratio. However, this does not determine the kind of molecule uniquely – dimethyl ether has the same ratios as ethanol, for instance. Molecules with the same atoms in different arrangements are called isomers. Also carbohydrates, for example, have the same ratio (carbon:hydrogen:oxygen= 1:2:1) (and thus the same empirical formula) but different total numbers of atoms in the molecule.
The molecular formula reflects the exact number of atoms that compose the molecule and so characterizes different molecules. However different isomers can have the same atomic composition while being different molecules.
The empirical formula is often the same as the molecular formula but not always. For example, the molecule acetylene has molecular formula C2H2, but the simplest integer ratio of elements is CH.
The molecular mass can be calculated from the chemical formula and is expressed in conventional atomic mass units equal to 1/12 of the mass of a neutral carbon-12 (12C isotope) atom. For network solids, the term formula unit is used in stoichiometric calculations.




For molecules with a complicated 3-dimensional structure, especially involving atoms bonded to four different substituents, a simple molecular formula or even semi-structural chemical formula may not be enough to completely specify the molecule. In this case, a graphical type of formula called a structural formula may be needed. Structural formulas may in turn be represented with a one-dimensional chemical name, but such chemical nomenclature requires many words and terms which are not part of chemical formulas.




Molecules have fixed equilibrium geometries—bond lengths and angles— about which they continuously oscillate through vibrational and rotational motions. A pure substance is composed of molecules with the same average geometrical structure. The chemical formula and the structure of a molecule are the two important factors that determine its properties, particularly its reactivity. Isomers share a chemical formula but normally have very different properties because of their different structures. Stereoisomers, a particular type of isomer, may have very similar physico-chemical properties and at the same time different biochemical activities.




Molecular spectroscopy deals with the response (spectrum) of molecules interacting with probing signals of known energy (or frequency, according to Planck's formula). Molecules have quantized energy levels that can be analyzed by detecting the molecule's energy exchange through absorbance or emission. Spectroscopy does not generally refer to diffraction studies where particles such as neutrons, electrons, or high energy X-rays interact with a regular arrangement of molecules (as in a crystal).
Microwave spectroscopy commonly measures changes in the rotation of molecules, and can be used to identify molecules in outer space. Infrared spectroscopy measures changes in vibration of molecules, including stretching, bending or twisting motions. It is commonly used to identify the kinds of bonds or functional groups in molecules. Changes in the arrangements of electrons yield absorption or emission lines in ultraviolet, visible or near infrared light, and result in colour. Nuclear resonance spectroscopy actually measures the environment of particular nuclei in the molecule, and can be used to characterise the numbers of atoms in different positions in a molecule.



The study of molecules by molecular physics and theoretical chemistry is largely based on quantum mechanics and is essential for the understanding of the chemical bond. The simplest of molecules is the hydrogen molecule-ion, H2+, and the simplest of all the chemical bonds is the one-electron bond. H2+ is composed of two positively charged protons and one negatively charged electron, which means that the Schrödinger equation for the system can be solved more easily due to the lack of electron–electron repulsion. With the development of fast digital computers, approximate solutions for more complicated molecules became possible and are one of the main aspects of computational chemistry.
When trying to define rigorously whether an arrangement of atoms is sufficiently stable to be considered a molecule, IUPAC suggests that it "must correspond to a depression on the potential energy surface that is deep enough to confine at least one vibrational state". This definition does not depend on the nature of the interaction between the atoms, but only on the strength of the interaction. In fact, it includes weakly bound species that would not traditionally be considered molecules, such as the helium dimer, He2, which has one vibrational bound state and is so loosely bound that it is only likely to be observed at very low temperatures.
Whether or not an arrangement of atoms is sufficiently stable to be considered a molecule is inherently an operational definition. Philosophically, therefore, a molecule is not a fundamental entity (in contrast, for instance, to an elementary particle); rather, the concept of a molecule is the chemist's way of making a useful statement about the strengths of atomic-scale interactions in the world that we observe.










Molecule of the Month – School of Chemistry, University of BristolThe scientific method is a body of techniques for investigating phenomena, acquiring new knowledge, or correcting and integrating previous knowledge. To be termed scientific, a method of inquiry is commonly based on empirical or measurable evidence subject to specific principles of reasoning. The Oxford Dictionaries Online define the scientific method as "a method or procedure that has characterized natural science since the 17th century, consisting in systematic observation, measurement, and experiment, and the formulation, testing, and modification of hypotheses". Experiments need to be designed to test hypotheses. The most important part of the scientific method is the experiment.
The scientific method is a continuous process, which usually begins with observations about the natural world. Human beings are naturally inquisitive, so they often come up with questions about things they see or hear and often develop ideas (hypotheses) about why things are the way they are. The best hypotheses lead to predictions that can be tested in various ways, including making further observations about nature. In general, the strongest tests of hypotheses come from carefully controlled and replicated experiments that gather empirical data. Depending on how well the tests match the predictions, the original hypothesis may require refinement, alteration, expansion or even rejection. If a particular hypothesis becomes very well supported a general theory may be developed.
Although procedures vary from one field of inquiry to another, identifiable features are frequently shared in common between them. The overall process of the scientific method involves making conjectures (hypotheses), deriving predictions from them as logical consequences, and then carrying out experiments based on those predictions. A hypothesis is a conjecture, based on knowledge obtained while formulating the question. The hypothesis might be very specific or it might be broad. Scientists then test hypotheses by conducting experiments. Under modern interpretations, a scientific hypothesis must be falsifiable, implying that it is possible to identify a possible outcome of an experiment that conflicts with predictions deduced from the hypothesis; otherwise, the hypothesis cannot be meaningfully tested.
The purpose of an experiment is to determine whether observations agree with or conflict with the predictions derived from a hypothesis. Experiments can take place anywhere from a college lab to CERN's Large Hadron Collider. There are difficulties in a formulaic statement of method, however. Though the scientific method is often presented as a fixed sequence of steps, it represents rather a set of general principles. Not all steps take place in every scientific inquiry (or to the same degree), and are not always in the same order. Some philosophers and scientists have argued that there is no scientific method, such as Lee Smolin and Paul Feyerabend (in his Against Method). Nola and Sankey remark that "For some, the whole idea of a theory of scientific method is yester-year's debate".



The DNA example below is a synopsis of this method
The scientific method is the process by which science is carried out. As in other areas of inquiry, science (through the scientific method) can build on previous knowledge and develop a more sophisticated understanding of its topics of study over time. This model can be seen to underlay the scientific revolution. One thousand years ago, Alhazen argued the importance of forming questions and subsequently testing them, an approach which was advocated by Galileo in 1638 with the publication of Two New Sciences. The current method is based on a hypothetico-deductive model formulated in the 20th century, although it has undergone significant revision since first proposed (for a more formal discussion, see below).



The overall process involves making conjectures (hypotheses), deriving predictions from them as logical consequences, and then carrying out experiments based on those predictions to determine whether the original conjecture was correct. There are difficulties in a formulaic statement of method, however. Though the scientific method is often presented as a fixed sequence of steps, they are better considered as general principles. Not all steps take place in every scientific inquiry (or to the same degree), and are not always in the same order. As noted by William Whewell (1794–1866), "invention, sagacity, [and] genius" are required at every step.



The question can refer to the explanation of a specific observation, as in "Why is the sky blue?", but can also be open-ended, as in "How can I design a drug to cure this particular disease?" This stage frequently involves finding and evaluating evidence from previous experiments, personal scientific observations or assertions, and/or the work of other scientists. If the answer is already known, a different question that builds on the previous evidence can be posed. When applying the scientific method to scientific research, determining a good question can be very difficult and affects the final outcome of the investigation.



A hypothesis is a conjecture, based on knowledge obtained while formulating the question, that may explain the observed behavior of a part of our universe. The hypothesis might be very specific, e.g., Einstein's equivalence principle or Francis Crick's "DNA makes RNA makes protein", or it might be broad, e.g., unknown species of life dwell in the unexplored depths of the oceans. A statistical hypothesis is a conjecture about some population. For example, the population might be people with a particular disease. The conjecture might be that a new drug will cure the disease in some of those people. Terms commonly associated with statistical hypotheses are null hypothesis and alternative hypothesis. A null hypothesis is the conjecture that the statistical hypothesis is false, e.g., that the new drug does nothing and that any cures are due to chance effects. Researchers normally want to show that the null hypothesis is false. The alternative hypothesis is the desired outcome, e.g., that the drug does better than chance. A final point: a scientific hypothesis must be falsifiable, meaning that one can identify a possible outcome of an experiment that conflicts with predictions deduced from the hypothesis; otherwise, it cannot be meaningfully tested.



This step involves determining the logical consequences of the hypothesis. One or more predictions are then selected for further testing. The more unlikely that a prediction would be correct simply by coincidence, then the more convincing it would be if the prediction were fulfilled; evidence is also stronger if the answer to the prediction is not already known, due to the effects of hindsight bias (see also postdiction). Ideally, the prediction must also distinguish the hypothesis from likely alternatives; if two hypotheses make the same prediction, observing the prediction to be correct is not evidence for either one over the other. (These statements about the relative strength of evidence can be mathematically derived using Bayes' Theorem).



This is an investigation of whether the real world behaves as predicted by the hypothesis. Scientists (and other people) test hypotheses by conducting experiments. The purpose of an experiment is to determine whether observations of the real world agree with or conflict with the predictions derived from a hypothesis. If they agree, confidence in the hypothesis increases; otherwise, it decreases. Agreement does not assure that the hypothesis is true; future experiments may reveal problems. Karl Popper advised scientists to try to falsify hypotheses, i.e., to search for and test those experiments that seem most doubtful. Large numbers of successful confirmations are not convincing if they arise from experiments that avoid risk. Experiments should be designed to minimize possible errors, especially through the use of appropriate scientific controls. For example, tests of medical treatments are commonly run as double-blind tests. Test personnel, who might unwittingly reveal to test subjects which samples are the desired test drugs and which are placebos, are kept ignorant of which are which. Such hints can bias the responses of the test subjects. Furthermore, failure of an experiment does not necessarily mean the hypothesis is false. Experiments always depend on several hypotheses, e.g., that the test equipment is working properly, and a failure may be a failure of one of the auxiliary hypotheses. (See the Duhem–Quine thesis.) Experiments can be conducted in a college lab, on a kitchen table, at CERN's Large Hadron Collider, at the bottom of an ocean, on Mars (using one of the working rovers), and so on. Astronomers do experiments, searching for planets around distant stars. Finally, most individual experiments address highly specific topics for reasons of practicality. As a result, evidence about broader topics is usually accumulated gradually.



This involves determining what the results of the experiment show and deciding on the next actions to take. The predictions of the hypothesis are compared to those of the null hypothesis, to determine which is better able to explain the data. In cases where an experiment is repeated many times, a statistical analysis such as a chi-squared test may be required. If the evidence has falsified the hypothesis, a new hypothesis is required; if the experiment supports the hypothesis but the evidence is not strong enough for high confidence, other predictions from the hypothesis must be tested. Once a hypothesis is strongly supported by evidence, a new question can be asked to provide further insight on the same topic. Evidence from other scientists and experience are frequently incorporated at any stage in the process. Depending on the complexity of the experiment, many iterations may be required to gather sufficient evidence to answer a question with confidence, or to build up many answers to highly specific questions in order to answer a single broader question.



The discovery became the starting point for many further studies involving the genetic material, such as the field of molecular genetics, and it was awarded the Nobel Prize in 1962. Each step of the example is examined in more detail later in the article.



The scientific method also includes other components required even when all the iterations of the steps above have been completed:



If an experiment cannot be repeated to produce the same results, this implies that the original results might have been in error. As a result, it is common for a single experiment to be performed multiple times, especially when there are uncontrolled variables or other indications of experimental error. For significant or surprising results, other scientists may also attempt to replicate the results for themselves, especially if those results would be important to their own work.



The process of peer review involves evaluation of the experiment by experts, who typically give their opinions anonymously. Some journals request that the experimenter provide lists of possible peer reviewers, especially if the field is highly specialized. Peer review does not certify correctness of the results, only that, in the opinion of the reviewer, the experiments themselves were sound (based on the description supplied by the experimenter). If the work passes peer review, which occasionally may require new experiments requested by the reviewers, it will be published in a peer-reviewed scientific journal. The specific journal that publishes the results indicates the perceived quality of the work.



Scientists typically are careful in recording their data, a requirement promoted by Ludwik Fleck (1896–1961) and others. Though not typically required, they might be requested to supply this data to other scientists who wish to replicate their original results (or parts of their original results), extending to the sharing of any experimental samples that may be difficult to obtain.



Scientific inquiry generally aims to obtain knowledge in the form of testable explanations that scientists can use to predict the results of future experiments. This allows scientists to gain a better understanding of the topic under study, and later to use that understanding to intervene in its causal mechanisms (such as to cure disease). The better an explanation is at making predictions, the more useful it frequently can be, and the more likely it will continue to explain a body of evidence better than its alternatives. The most successful explanations - those which explain and make accurate predictions in a wide range of circumstances - are often called scientific theories.
Most experimental results do not produce large changes in human understanding; improvements in theoretical scientific understanding typically result from a gradual process of development over time, sometimes across different domains of science. Scientific models vary in the extent to which they have been experimentally tested and for how long, and in their acceptance in the scientific community. In general, explanations become accepted over time as evidence accumulates on a given topic, and the explanation in question proves more powerful than its alternatives at explaining the evidence. Often subsequent researchers re-formulate the explanations over time, or combined explanations to produce new explanations.
Tow sees the scientific method in terms of an evolutionary algorithm applied to science and technology.



Scientific knowledge is closely tied to empirical findings, and can remain subject to falsification if new experimental observation incompatible with it is found. That is, no theory can ever be considered final, since new problematic evidence might be discovered. If such evidence is found, a new theory may be proposed, or (more commonly) it is found that modifications to the previous theory are sufficient to explain the new evidence. The strength of a theory can be argued to relate to how long it has persisted without major alteration to its core principles.
Theories can also become subsumed by other theories. For example, Newton's laws explained thousands of years of scientific observations of the planets almost perfectly. However, these laws were then determined to be special cases of a more general theory (relativity), which explained both the (previously unexplained) exceptions to Newton's laws and predicted and explained other observations such as the deflection of light by gravity. Thus, in certain cases independent, unconnected, scientific observations can be connected to each other, unified by principles of increasing explanatory power.
Since new theories might be more comprehensive than what preceded them, and thus be able to explain more than previous ones, successor theories might be able to meet a higher standard by explaining a larger body of observations than their predecessors. For example, the theory of evolution explains the diversity of life on Earth, how species adapt to their environments, and many other patterns observed in the natural world; its most recent major modification was unification with genetics to form the modern evolutionary synthesis. In subsequent modifications, it has also subsumed aspects of many other fields such as biochemistry and molecular biology.




Scientific methodology often directs that hypotheses be tested in controlled conditions wherever possible. This is frequently possible in certain areas, such as in the biological sciences, and more difficult in other areas, such as in astronomy. The practice of experimental control and reproducibility can have the effect of diminishing the potentially harmful effects of circumstance, and to a degree, personal bias. For example, pre-existing beliefs can alter the interpretation of results, as in confirmation bias; this is a heuristic that leads a person with a particular belief to see things as reinforcing their belief, even if another observer might disagree (in other words, people tend to observe what they expect to observe).
A historical example is the belief that the legs of a galloping horse are splayed at the point when none of the horse's legs touches the ground, to the point of this image being included in paintings by its supporters. However, the first stop-action pictures of a horse's gallop by Eadweard Muybridge showed this to be false, and that the legs are instead gathered together. Another important human bias that plays a role is a preference for new, surprising statements (see appeal to novelty), which can result in a search for evidence that the new is true. In contrast to this standard in the scientific method, poorly attested beliefs can be believed and acted upon via a less rigorous heuristic, sometimes taking advantage of the narrative fallacy that when narrative is constructed its elements become easier to believe. Sometimes, these have their elements assumed a priori, or contain some other logical or methodological flaw in the process that ultimately produced them.



There are different ways of outlining the basic method used for scientific inquiry. The scientific community and philosophers of science generally agree on the following classification of method components. These methodological elements and organization of procedures tend to be more characteristic of natural sciences than social sciences. Nonetheless, the cycle of formulating hypotheses, testing and analyzing the results, and formulating new hypotheses, will resemble the cycle described below.
Four essential elements of the scientific method are iterations, recursions, interleavings, or orderings of the following:
Characterizations (observations, definitions, and measurements of the subject of inquiry)
Hypotheses (theoretical, hypothetical explanations of observations and measurements of the subject)
Predictions (reasoning including deductive reasoning from the hypothesis or theory)
Experiments (tests of all of the above)

Each element of the scientific method is subject to peer review for possible mistakes. These activities do not describe all that scientists do (see below) but apply mostly to experimental sciences (e.g., physics, chemistry, and biology). The elements above are often taught in the educational system as "the scientific method".
The scientific method is not a single recipe: it requires intelligence, imagination, and creativity. In this sense, it is not a mindless set of standards and procedures to follow, but is rather an ongoing cycle, constantly developing more useful, accurate and comprehensive models and methods. For example, when Einstein developed the Special and General Theories of Relativity, he did not in any way refute or discount Newton's Principia. On the contrary, if the astronomically large, the vanishingly small, and the extremely fast are removed from Einstein's theories – all phenomena Newton could not have observed – Newton's equations are what remain. Einstein's theories are expansions and refinements of Newton's theories and, thus, increase our confidence in Newton's work.
A linearized, pragmatic scheme of the four points above is sometimes offered as a guideline for proceeding:
Define a question
Gather information and resources (observe)
Form an explanatory hypothesis
Test the hypothesis by performing an experiment and collecting data in a reproducible manner
Analyze the data
Interpret the data and draw conclusions that serve as a starting point for new hypothesis
Publish results
Retest (frequently done by other scientists)
The iterative cycle inherent in this step-by-step method goes from point 3 to 6 back to 3 again.
While this schema outlines a typical hypothesis/testing method, it should also be noted that a number of philosophers, historians and sociologists of science (perhaps most notably Paul Feyerabend) claim that such descriptions of scientific method have little relation to the ways that science is actually practiced.



The scientific method depends upon increasingly sophisticated characterizations of the subjects of investigation. (The subjects can also be called unsolved problems or the unknowns.) For example, Benjamin Franklin conjectured, correctly, that St. Elmo's fire was electrical in nature, but it has taken a long series of experiments and theoretical changes to establish this. While seeking the pertinent properties of the subjects, careful thought may also entail some definitions and observations; the observations often demand careful measurements and/or counting.
The systematic, careful collection of measurements or counts of relevant quantities is often the critical difference between pseudo-sciences, such as alchemy, and science, such as chemistry or biology. Scientific measurements are usually tabulated, graphed, or mapped, and statistical manipulations, such as correlation and regression, performed on them. The measurements might be made in a controlled setting, such as a laboratory, or made on more or less inaccessible or unmanipulatable objects such as stars or human populations. The measurements often require specialized scientific instruments such as thermometers, spectroscopes, particle accelerators, or voltmeters, and the progress of a scientific field is usually intimately tied to their invention and improvement.

I am not accustomed to saying anything with certainty after only one or two observations.



Measurements in scientific work are also usually accompanied by estimates of their uncertainty. The uncertainty is often estimated by making repeated measurements of the desired quantity. Uncertainties may also be calculated by consideration of the uncertainties of the individual underlying quantities used. Counts of things, such as the number of people in a nation at a particular time, may also have an uncertainty due to data collection limitations. Or counts may represent a sample of desired quantities, with an uncertainty that depends upon the sampling method used and the number of samples taken.



Measurements demand the use of operational definitions of relevant quantities. That is, a scientific quantity is described or defined by how it is measured, as opposed to some more vague, inexact or "idealized" definition. For example, electric current, measured in amperes, may be operationally defined in terms of the mass of silver deposited in a certain time on an electrode in an electrochemical device that is described in some detail. The operational definition of a thing often relies on comparisons with standards: the operational definition of "mass" ultimately relies on the use of an artifact, such as a particular kilogram of platinum-iridium kept in a laboratory in France.
The scientific definition of a term sometimes differs substantially from its natural language usage. For example, mass and weight overlap in meaning in common discourse, but have distinct meanings in mechanics. Scientific quantities are often characterized by their units of measure which can later be described in terms of conventional physical units when communicating the work.
New theories are sometimes developed after realizing certain terms have not previously been sufficiently clearly defined. For example, Albert Einstein's first paper on relativity begins by defining simultaneity and the means for determining length. These ideas were skipped over by Isaac Newton with, "I do not define time, space, place and motion, as being well known to all." Einstein's paper then demonstrates that they (viz., absolute time and length independent of motion) were approximations. Francis Crick cautions us that when characterizing a subject, however, it can be premature to define something when it remains ill-understood. In Crick's study of consciousness, he actually found it easier to study awareness in the visual system, rather than to study free will, for example. His cautionary example was the gene; the gene was much more poorly understood before Watson and Crick's pioneering discovery of the structure of DNA; it would have been counterproductive to spend much time on the definition of the gene, before them.




The history of the discovery of the structure of DNA is a classic example of the elements of the scientific method: in 1950 it was known that genetic inheritance had a mathematical description, starting with the studies of Gregor Mendel, and that DNA contained genetic information (Oswald Avery's transforming principle). But the mechanism of storing genetic information (i.e., genes) in DNA was unclear. Researchers in Bragg's laboratory at Cambridge University made X-ray diffraction pictures of various molecules, starting with crystals of salt, and proceeding to more complicated substances. Using clues painstakingly assembled over decades, beginning with its chemical composition, it was determined that it should be possible to characterize the physical structure of DNA, and the X-ray images would be the vehicle. ..2. DNA-hypotheses




The characterization element can require extended and extensive study, even centuries. It took thousands of years of measurements, from the Chaldean, Indian, Persian, Greek, Arabic and European astronomers, to fully record the motion of planet Earth. Newton was able to include those measurements into consequences of his laws of motion. But the perihelion of the planet Mercury's orbit exhibits a precession that cannot be fully explained by Newton's laws of motion (see diagram to the right), as Leverrier pointed out in 1859. The observed difference for Mercury's precession between Newtonian theory and observation was one of the things that occurred to Einstein as a possible early test of his theory of General Relativity. His relativistic calculations matched observation much more closely than did Newtonian theory. The difference is approximately 43 arc-seconds per century.




A hypothesis is a suggested explanation of a phenomenon, or alternately a reasoned proposal suggesting a possible correlation between or among a set of phenomena.
Normally hypotheses have the form of a mathematical model. Sometimes, but not always, they can also be formulated as existential statements, stating that some particular instance of the phenomenon being studied has some characteristic and causal explanations, which have the general form of universal statements, stating that every instance of the phenomenon has a particular characteristic.
Scientists are free to use whatever resources they have – their own creativity, ideas from other fields, inductive reasoning, Bayesian inference, and so on – to imagine possible explanations for a phenomenon under study. Charles Sanders Peirce, borrowing a page from Aristotle (Prior Analytics, 2.25) described the incipient stages of inquiry, instigated by the "irritation of doubt" to venture a plausible guess, as abductive reasoning. The history of science is filled with stories of scientists claiming a "flash of inspiration", or a hunch, which then motivated them to look for evidence to support or refute their idea. Michael Polanyi made such creativity the centerpiece of his discussion of methodology.
William Glen observes that
the success of a hypothesis, or its service to science, lies not simply in its perceived "truth", or power to displace, subsume or reduce a predecessor idea, but perhaps more in its ability to stimulate the research that will illuminate ... bald suppositions and areas of vagueness.
In general scientists tend to look for theories that are "elegant" or "beautiful". In contrast to the usual English use of these terms, they here refer to a theory in accordance with the known facts, which is nevertheless relatively simple and easy to handle. Occam's Razor serves as a rule of thumb for choosing the most desirable amongst a group of equally explanatory hypotheses.




Linus Pauling proposed that DNA might be a triple helix. This hypothesis was also considered by Francis Crick and James D. Watson but discarded. When Watson and Crick learned of Pauling's hypothesis, they understood from existing data that Pauling was wrong and that Pauling would soon admit his difficulties with that structure. So, the race was on to figure out the correct structure (except that Pauling did not realize at the time that he was in a race) ..3. DNA-predictions




Any useful hypothesis will enable predictions, by reasoning including deductive reasoning. It might predict the outcome of an experiment in a laboratory setting or the observation of a phenomenon in nature. The prediction can also be statistical and deal only with probabilities.
It is essential that the outcome of testing such a prediction be currently unknown. Only in this case does a successful outcome increase the probability that the hypothesis is true. If the outcome is already known, it is called a consequence and should have already been considered while formulating the hypothesis.
If the predictions are not accessible by observation or experience, the hypothesis is not yet testable and so will remain to that extent unscientific in a strict sense. A new technology or theory might make the necessary experiments feasible. Thus, much scientifically based speculation might convince one (or many) that the hypothesis that other intelligent species exist is true. But since there no experiment now known which can test this hypothesis, science itself can have little to say about the possibility. In future, some new technique might lead to an experimental test and the speculation would then become part of accepted science.




James D. Watson, Francis Crick, and others hypothesized that DNA had a helical structure. This implied that DNA's X-ray diffraction pattern would be 'x shaped'. This prediction followed from the work of Cochran, Crick and Vand (and independently by Stokes). The Cochran-Crick-Vand-Stokes theorem provided a mathematical explanation for the empirical observation that diffraction from helical structures produces x shaped patterns.
In their first paper, Watson and Crick also noted that the double helix structure they proposed provided a simple mechanism for DNA replication, writing, "It has not escaped our notice that the specific pairing we have postulated immediately suggests a possible copying mechanism for the genetic material". ..4. DNA-experiments




Einstein's theory of General Relativity makes several specific predictions about the observable structure of space-time, such as that light bends in a gravitational field, and that the amount of bending depends in a precise way on the strength of that gravitational field. Arthur Eddington's observations made during a 1919 solar eclipse supported General Relativity rather than Newtonian gravitation.




Once predictions are made, they can be sought by experiments. If the test results contradict the predictions, the hypotheses which entailed them are called into question and become less tenable. Sometimes the experiments are conducted incorrectly or are not very well designed, when compared to a crucial experiment. If the experimental results confirm the predictions, then the hypotheses are considered more likely to be correct, but might still be wrong and continue to be subject to further testing. The experimental control is a technique for dealing with observational error. This technique uses the contrast between multiple samples (or observations) under differing conditions to see what varies or what remains the same. We vary the conditions for each measurement, to help isolate what has changed. Mill's canons can then help us figure out what the important factor is. Factor analysis is one technique for discovering the important factor in an effect.
Depending on the predictions, the experiments can have different shapes. It could be a classical experiment in a laboratory setting, a double-blind study or an archaeological excavation. Even taking a plane from New York to Paris is an experiment which tests the aerodynamical hypotheses used for constructing the plane.
Scientists assume an attitude of openness and accountability on the part of those conducting an experiment. Detailed record keeping is essential, to aid in recording and reporting on the experimental results, and supports the effectiveness and integrity of the procedure. They will also assist in reproducing the experimental results, likely by others. Traces of this approach can be seen in the work of Hipparchus (190–120 BCE), when determining a value for the precession of the Earth, while controlled experiments can be seen in the works of Jābir ibn Hayyān (721–815 CE), al-Battani (853–929) and Alhazen (965–1039).




Watson and Crick showed an initial (and incorrect) proposal for the structure of DNA to a team from Kings College – Rosalind Franklin, Maurice Wilkins, and Raymond Gosling. Franklin immediately spotted the flaws which concerned the water content. Later Watson saw Franklin's detailed X-ray diffraction images which showed an X-shape and was able to confirm the structure was helical. This rekindled Watson and Crick's model building and led to the correct structure. ..1. DNA-characterizations



The scientific method is iterative. At any stage it is possible to refine its accuracy and precision, so that some consideration will lead the scientist to repeat an earlier part of the process. Failure to develop an interesting hypothesis may lead a scientist to re-define the subject under consideration. Failure of a hypothesis to produce interesting and testable predictions may lead to reconsideration of the hypothesis or of the definition of the subject. Failure of an experiment to produce interesting results may lead a scientist to reconsider the experimental method, the hypothesis, or the definition of the subject.
Other scientists may start their own research and enter the process at any stage. They might adopt the characterization and formulate their own hypothesis, or they might adopt the hypothesis and deduce their own predictions. Often the experiment is not done by the person who made the prediction, and the characterization is based on experiments done by someone else. Published results of experiments can also serve as a hypothesis predicting their own reproducibility.




After considerable fruitless experimentation, being discouraged by their superior from continuing, and numerous false starts, Watson and Crick were able to infer the essential structure of DNA by concrete modeling of the physical shapes of the nucleotides which comprise it. They were guided by the bond lengths which had been deduced by Linus Pauling and by Rosalind Franklin's X-ray diffraction images. ..DNA Example



Science is a social enterprise, and scientific work tends to be accepted by the scientific community when it has been confirmed. Crucially, experimental and theoretical results must be reproduced by others within the scientific community. Researchers have given their lives for this vision; Georg Wilhelm Richmann was killed by ball lightning (1753) when attempting to replicate the 1752 kite-flying experiment of Benjamin Franklin.
To protect against bad science and fraudulent data, government research-granting agencies such as the National Science Foundation, and science journals, including Nature and Science, have a policy that researchers must archive their data and methods so that other researchers can test the data and methods and build on the research that has gone before. Scientific data archiving can be done at a number of national archives in the U.S. or in the World Data Center.






The classical model of scientific inquiry derives from Aristotle, who distinguished the forms of approximate and exact reasoning, set out the threefold scheme of abductive, deductive, and inductive inference, and also treated the compound forms such as reasoning by analogy.




In 1877, Charles Sanders Peirce (/ˈpɜːrs/ like "purse"; 1839–1914) characterized inquiry in general not as the pursuit of truth per se but as the struggle to move from irritating, inhibitory doubts born of surprises, disagreements, and the like, and to reach a secure belief, belief being that on which one is prepared to act. He framed scientific inquiry as part of a broader spectrum and as spurred, like inquiry generally, by actual doubt, not mere verbal or hyperbolic doubt, which he held to be fruitless. He outlined four methods of settling opinion, ordered from least to most successful:
The method of tenacity (policy of sticking to initial belief) – which brings comforts and decisiveness but leads to trying to ignore contrary information and others' views as if truth were intrinsically private, not public. It goes against the social impulse and easily falters since one may well notice when another's opinion is as good as one's own initial opinion. Its successes can shine but tend to be transitory.
The method of authority – which overcomes disagreements but sometimes brutally. Its successes can be majestic and long-lived, but it cannot operate thoroughly enough to suppress doubts indefinitely, especially when people learn of other societies present and past.
The method of the a priori – which promotes conformity less brutally but fosters opinions as something like tastes, arising in conversation and comparisons of perspectives in terms of "what is agreeable to reason." Thereby it depends on fashion in paradigms and goes in circles over time. It is more intellectual and respectable but, like the first two methods, sustains accidental and capricious beliefs, destining some minds to doubt it.
The scientific method – the method wherein inquiry regards itself as fallible and purposely tests itself and criticizes, corrects, and improves itself.
Peirce held that slow, stumbling ratiocination can be dangerously inferior to instinct and traditional sentiment in practical matters, and that the scientific method is best suited to theoretical research, which in turn should not be trammeled by the other methods and practical ends; reason's "first rule" is that, in order to learn, one must desire to learn and, as a corollary, must not block the way of inquiry. The scientific method excels the others by being deliberately designed to arrive – eventually – at the most secure beliefs, upon which the most successful practices can be based. Starting from the idea that people seek not truth per se but instead to subdue irritating, inhibitory doubt, Peirce showed how, through the struggle, some can come to submit to truth for the sake of belief's integrity, seek as truth the guidance of potential practice correctly to its given goal, and wed themselves to the scientific method.
For Peirce, rational inquiry implies presuppositions about truth and the real; to reason is to presuppose (and at least to hope), as a principle of the reasoner's self-regulation, that the real is discoverable and independent of our vagaries of opinion. In that vein he defined truth as the correspondence of a sign (in particular, a proposition) to its object and, pragmatically, not as actual consensus of some definite, finite community (such that to inquire would be to poll the experts), but instead as that final opinion which all investigators would reach sooner or later but still inevitably, if they were to push investigation far enough, even when they start from different points. In tandem he defined the real as a true sign's object (be that object a possibility or quality, or an actuality or brute fact, or a necessity or norm or law), which is what it is independently of any finite community's opinion and, pragmatically, depends only on the final opinion destined in a sufficient investigation. That is a destination as far, or near, as the truth itself to you or me or the given finite community. Thus, his theory of inquiry boils down to "Do the science." Those conceptions of truth and the real involve the idea of a community both without definite limits (and thus potentially self-correcting as far as needed) and capable of definite increase of knowledge. As inference, "logic is rooted in the social principle" since it depends on a standpoint that is, in a sense, unlimited.
Paying special attention to the generation of explanations, Peirce outlined the scientific method as a coordination of three kinds of inference in a purposeful cycle aimed at settling doubts, as follows (in §III–IV in "A Neglected Argument" except as otherwise noted):
Abduction (or retroduction). Guessing, inference to explanatory hypotheses for selection of those best worth trying. From abduction, Peirce distinguishes induction as inferring, on the basis of tests, the proportion of truth in the hypothesis. Every inquiry, whether into ideas, brute facts, or norms and laws, arises from surprising observations in one or more of those realms (and for example at any stage of an inquiry already underway). All explanatory content of theories comes from abduction, which guesses a new or outside idea so as to account in a simple, economical way for a surprising or complicative phenomenon. Oftenest, even a well-prepared mind guesses wrong. But the modicum of success of our guesses far exceeds that of sheer luck and seems born of attunement to nature by instincts developed or inherent, especially insofar as best guesses are optimally plausible and simple in the sense, said Peirce, of the "facile and natural", as by Galileo's natural light of reason and as distinct from "logical simplicity". Abduction is the most fertile but least secure mode of inference. Its general rationale is inductive: it succeeds often enough and, without it, there is no hope of sufficiently expediting inquiry (often multi-generational) toward new truths. Coordinative method leads from abducing a plausible hypothesis to judging it for its testability and for how its trial would economize inquiry itself. Peirce calls his pragmatism "the logic of abduction". His pragmatic maxim is: "Consider what effects that might conceivably have practical bearings you conceive the objects of your conception to have. Then, your conception of those effects is the whole of your conception of the object". His pragmatism is a method of reducing conceptual confusions fruitfully by equating the meaning of any conception with the conceivable practical implications of its object's conceived effects—a method of experimentational mental reflection hospitable to forming hypotheses and conducive to testing them. It favors efficiency. The hypothesis, being insecure, needs to have practical implications leading at least to mental tests and, in science, lending themselves to scientific tests. A simple but unlikely guess, if uncostly to test for falsity, may belong first in line for testing. A guess is intrinsically worth testing if it has instinctive plausibility or reasoned objective probability, while subjective likelihood, though reasoned, can be misleadingly seductive. Guesses can be chosen for trial strategically, for their caution (for which Peirce gave as example the game of Twenty Questions), breadth, and incomplexity. One can hope to discover only that which time would reveal through a learner's sufficient experience anyway, so the point is to expedite it; the economy of research is what demands the leap, so to speak, of abduction and governs its art.
Deduction. Two stages:
Explication. Unclearly premissed, but deductive, analysis of the hypothesis in order to render its parts as clear as possible.
Demonstration: Deductive Argumentation, Euclidean in procedure. Explicit deduction of hypothesis's consequences as predictions, for induction to test, about evidence to be found. Corollarial or, if needed, theorematic.

Induction. The long-run validity of the rule of induction is deducible from the principle (presuppositional to reasoning in general) that the real is only the object of the final opinion to which adequate investigation would lead; anything to which no such process would ever lead would not be real. Induction involving ongoing tests or observations follows a method which, sufficiently persisted in, will diminish its error below any predesignate degree. Three stages:
Classification. Unclearly premissed, but inductive, classing of objects of experience under general ideas.
Probation: direct inductive argumentation. Crude (the enumeration of instances) or gradual (new estimate of proportion of truth in the hypothesis after each test). Gradual induction is qualitative or quantitative; if qualitative, then dependent on weightings of qualities or characters; if quantitative, then dependent on measurements, or on statistics, or on countings.
Sentential Induction. "...which, by inductive reasonings, appraises the different probations singly, then their combinations, then makes self-appraisal of these very appraisals themselves, and passes final judgment on the whole result".




Frequently the scientific method is employed not only by a single person, but also by several people cooperating directly or indirectly. Such cooperation can be regarded as an important element of a scientific community. Various standards of scientific methodology are used within such an environment.



Scientific journals use a process of peer review, in which scientists' manuscripts are submitted by editors of scientific journals to (usually one to three) fellow (usually anonymous) scientists familiar with the field for evaluation. In certain journals, the journal itself selects the referees; while in others (especially journals that are extremely specialized), the manuscript author might recommend referees. The referees may or may not recommend publication, or they might recommend publication with suggested modifications, or sometimes, publication in another journal. This standard is practiced to various degrees by different journals, and can have the effect of keeping the literature free of obvious errors and to generally improve the quality of the material, especially in the journals who use the standard most rigorously. The peer review process can have limitations when considering research outside the conventional scientific paradigm: problems of "groupthink" can interfere with open and fair deliberation of some new research.




Sometimes experimenters may make systematic errors during their experiments, veer from standard methods and practices (Pathological science) for various reasons, or, in rare cases, deliberately report false results. Occasionally because of this then, other scientists might attempt to repeat the experiments in order to duplicate the results.



Researchers sometimes practice scientific data archiving, such as in compliance with the policies of government funding agencies and scientific journals. In these cases, detailed records of their experimental procedures, raw data, statistical analyses and source code can be preserved in order to provide evidence of the methodology and practice of the procedure and assist in any potential future attempts to reproduce the result. These procedural records may also assist in the conception of new experiments to test the hypothesis, and may prove useful to engineers who might examine the potential practical applications of a discovery.



When additional information is needed before a study can be reproduced, the author of the study might be asked to provide it. They might provide it, or if the author refuses to share data, appeals can be made to the journal editors who published the study or to the institution which funded the research.



Since it is impossible for a scientist to record everything that took place in an experiment, facts selected for their apparent relevance are reported. This may lead, unavoidably, to problems later if some supposedly irrelevant feature is questioned. For example, Heinrich Hertz did not report the size of the room used to test Maxwell's equations, which later turned out to account for a small deviation in the results. The problem is that parts of the theory itself need to be assumed in order to select and report the experimental conditions. The observations are hence sometimes described as being 'theory-laden'.




The primary constraints on contemporary science are:
Publication, i.e. Peer review
Resources (mostly funding)
It has not always been like this: in the old days of the "gentleman scientist" funding (and to a lesser extent publication) were far weaker constraints.
Both of these constraints indirectly require scientific method – work that violates the constraints will be difficult to publish and difficult to get funded. Journals require submitted papers to conform to "good scientific practice" and to a degree this can be enforced by peer review. Originality, importance and interest are more important – see for example the author guidelines for Nature.
Smaldino and McElreath 2016 have noted that our need to reward scientific understanding is being nullified by poor research design and poor data analysis, which is leading to false-positive findings.




Philosophy of science looks at the underpinning logic of the scientific method, at what separates science from non-science, and the ethic that is implicit in science. There are basic assumptions, derived from philosophy by at least one prominent scientist, that form the base of the scientific method – namely, that reality is objective and consistent, that humans have the capacity to perceive reality accurately, and that rational explanations exist for elements of the real world. These assumptions from methodological naturalism form a basis on which science may be grounded. Logical Positivist, empiricist, falsificationist, and other theories have criticized these assumptions and given alternative accounts of the logic of science, but each has also itself been criticized. More generally, the scientific method can be recognized as an idealization.
Thomas Kuhn examined the history of science in his The Structure of Scientific Revolutions, and found that the actual method used by scientists differed dramatically from the then-espoused method. His observations of science practice are essentially sociological and do not speak to how science is or can be practiced in other times and other cultures.
Norwood Russell Hanson, Imre Lakatos and Thomas Kuhn have done extensive work on the "theory laden" character of observation. Hanson (1958) first coined the term for the idea that all observation is dependent on the conceptual framework of the observer, using the concept of gestalt to show how preconceptions can affect both observation and description. He opens Chapter 1 with a discussion of the Golgi bodies and their initial rejection as an artefact of staining technique, and a discussion of Brahe and Kepler observing the dawn and seeing a "different" sun rise despite the same physiological phenomenon. Kuhn and Feyerabend acknowledge the pioneering significance of his work.
Kuhn (1961) said the scientist generally has a theory in mind before designing and undertaking experiments so as to make empirical observations, and that the "route from theory to measurement can almost never be traveled backward". This implies that the way in which theory is tested is dictated by the nature of the theory itself, which led Kuhn (1961, p. 166) to argue that "once it has been adopted by a profession ... no theory is recognized to be testable by any quantitative tests that it has not already passed".
Paul Feyerabend similarly examined the history of science, and was led to deny that science is genuinely a methodological process. In his book Against Method he argues that scientific progress is not the result of applying any particular method. In essence, he says that for any specific method or norm of science, one can find a historic episode where violating it has contributed to the progress of science. Thus, if believers in scientific method wish to express a single universally valid rule, Feyerabend jokingly suggests, it should be 'anything goes'. Criticisms such as his led to the strong programme, a radical approach to the sociology of science.
The postmodernist critiques of science have themselves been the subject of intense controversy. This ongoing debate, known as the science wars, is the result of conflicting values and assumptions between the postmodernist and realist camps. Whereas postmodernists assert that scientific knowledge is simply another discourse (note that this term has special meaning in this context) and not representative of any form of fundamental truth, realists in the scientific community maintain that scientific knowledge does reveal real and fundamental truths about reality. Many books have been written by scientists which take on this problem and challenge the assertions of the postmodernists while defending science as a legitimate method of deriving truth.




Somewhere between 33% and 50% of all scientific discoveries are estimated to have been stumbled upon, rather than sought out. This may explain why scientists so often express that they were lucky. Louis Pasteur is credited with the famous saying that "Luck favours the prepared mind", but some psychologists have begun to study what it means to be 'prepared for luck' in the scientific context. Research is showing that scientists are taught various heuristics that tend to harness chance and the unexpected. This is what Nassim Nicholas Taleb calls "Anti-fragility"; while some systems of investigation are fragile in the face of human error, human bias, and randomness, the scientific method is more than resistant or tough – it actually benefits from such randomness in many ways (it is anti-fragile). Taleb believes that the more anti-fragile the system, the more it will flourish in the real world.
Psychologist Kevin Dunbar says the process of discovery often starts with researchers finding bugs in their experiments. These unexpected results lead researchers to try to fix what they think is an error in their method. Eventually, the researcher decides the error is too persistent and systematic to be a coincidence. The highly controlled, cautious and curious aspects of the scientific method are thus what make it well suited for identifying such persistent systematic errors. At this point, the researcher will begin to think of theoretical explanations for the error, often seeking the help of colleagues across different domains of expertise.




The history of scientific method considers changes in the methodology of scientific inquiry, as distinct from the history of science itself. The development of rules for scientific reasoning has not been straightforward; scientific method has been the subject of intense and recurring debate throughout the history of science, and eminent natural philosophers and scientists have argued for the primacy of one or another approach to establishing scientific knowledge. Despite the disagreements about approaches, scientific method has advanced in definite steps. Rationalist explanations of nature, including atomism, appeared both in ancient Greece in the thought of Leucippus and Democritus, and in ancient India, in the Nyaya, Vaisesika and Buddhist schools, while Charvaka materialism rejected inference as a source of knowledge in favour of an empiricism that was always subject to doubt. Aristotle pioneered scientific method in ancient Greece alongside his empirical biology and his work on logic, rejecting a purely deductive framework in favour of generalisations made from observations of nature. Important debates in the history of scientific method center on rationalism, especially as advocated by René Descartes, inductivism, which rose to particular prominence with Isaac Newton and his followers, and hypothetico-deductivism, which came to the fore in the early 19th century. In the late 19th and early 20th centuries, a debate over realism vs. antirealism was conducted as powerful scientific theories extended beyond the realm of the observable, while in the mid-20th century, prominent philosophers such as Paul Feyerabend argued against any universal rules of science at all.



Science is the process of gathering, comparing, and evaluating proposed models against observables. A model can be a simulation, mathematical or chemical formula, or set of proposed steps. Science is like mathematics in that researchers in both disciplines can clearly distinguish what is known from what is unknown at each stage of discovery. Models, in both science and mathematics, need to be internally consistent and also ought to be falsifiable (capable of disproof). In mathematics, a statement need not yet be proven; at such a stage, that statement would be called a conjecture. But when a statement has attained mathematical proof, that statement gains a kind of immortality which is highly prized by mathematicians, and for which some mathematicians devote their lives.
Mathematical work and scientific work can inspire each other. For example, the technical concept of time arose in science, and timelessness was a hallmark of a mathematical topic. But today, the Poincaré conjecture has been proven using time as a mathematical concept in which objects can flow (see Ricci flow).
Nevertheless, the connection between mathematics and reality (and so science to the extent it describes reality) remains obscure. Eugene Wigner's paper, The Unreasonable Effectiveness of Mathematics in the Natural Sciences, is a very well known account of the issue from a Nobel Prize-winning physicist. In fact, some observers (including some well known mathematicians such as Gregory Chaitin, and others such as Lakoff and Núñez) have suggested that mathematics is the result of practitioner bias and human limitation (including cultural ones), somewhat like the post-modernist view of science.
George Pólya's work on problem solving, the construction of mathematical proofs, and heuristic show that the mathematical method and the scientific method differ in detail, while nevertheless resembling each other in using iterative or recursive steps.
In Pólya's view, understanding involves restating unfamiliar definitions in your own words, resorting to geometrical figures, and questioning what we know and do not know already; analysis, which Pólya takes from Pappus, involves free and heuristic construction of plausible arguments, working backward from the goal, and devising a plan for constructing the proof; synthesis is the strict Euclidean exposition of step-by-step details of the proof; review involves reconsidering and re-examining the result and the path taken to it.
Gauss, when asked how he came about his theorems, once replied "durch planmässiges Tattonieren" (through systematic palpable experimentation).
Imre Lakatos argued that mathematicians actually use contradiction, criticism and revision as principles for improving their work. In like manner to science, where truth is sought, but certainty is not found, in Proofs and refutations (1976), what Lakatos tried to establish was that no theorem of informal mathematics is final or perfect. This means that we should not think that a theorem is ultimately true, only that no counterexample has yet been found. Once a counterexample, i.e. an entity contradicting/not explained by the theorem is found, we adjust the theorem, possibly extending the domain of its validity. This is a continuous way our knowledge accumulates, through the logic and process of proofs and refutations. (If axioms are given for a branch of mathematics, however, Lakatos claimed that proofs from those axioms were tautological, i.e. logically true, by rewriting them, as did Poincaré (Proofs and Refutations, 1976).)
Lakatos proposed an account of mathematical knowledge based on Polya's idea of heuristics. In Proofs and Refutations, Lakatos gave several basic rules for finding proofs and counterexamples to conjectures. He thought that mathematical 'thought experiments' are a valid way to discover mathematical conjectures and proofs.



The scientific method has been extremely successful in bringing the world out of medieval thinking, especially once it was combined with industrial processes. However, when the scientific method employs statistics as part of its arsenal, there are mathematical and practical issues that can have a deleterious effect on the reliability of the output of scientific methods. This is described in a popular 2005 scientific paper "Why Most Published Research Findings Are False" by John Ioannidis.
The particular points raised are statistical ("The smaller the studies conducted in a scientific field, the less likely the research findings are to be true" and "The greater the flexibility in designs, definitions, outcomes, and analytical modes in a scientific field, the less likely the research findings are to be true.") and economical ("The greater the financial and other interests and prejudices in a scientific field, the less likely the research findings are to be true" and "The hotter a scientific field (with more scientific teams involved), the less likely the research findings are to be true.") Hence: "Most research findings are false for most research designs and for most fields" and "As shown, the majority of modern biomedical research is operating in areas with very low pre- and poststudy probability for true findings." However: "Nevertheless, most new discoveries will continue to stem from hypothesis-generating research with low or very low pre-study odds," which means that *new* discoveries will come from research that, when that research started, had low or very low odds (a low or very low chance) of succeeding. Hence, if the scientific method is used to expand the frontiers of knowledge, research into areas that are outside the mainstream will yield most new discoveries.





















Andersen, Anne; Hepburn, Brian. "Scientific Method". Stanford Encyclopedia of Philosophy. 
"Confirmation and Induction". Internet Encyclopedia of Philosophy. 
Scientific method at PhilPapers
Scientific method at the Indiana Philosophy Ontology Project
An Introduction to Science: Scientific Thinking and a scientific method by Steven D. Schafersman.
Introduction to the scientific method at the University of Rochester
Theory-ladenness by Paul Newall at The Galilean Library
Lecture on Scientific Method by Greg Anderson
Using the scientific method for designing science fair projects
SCIENTIFIC METHODS an online book by Richard D. Jarrard
Richard Feynman on the Key to Science (one minute, three seconds), from the Cornell Lectures.
Lectures on the Scientific Method by Nick Josh Karean, Kevin Padian, Michael Shermer and Richard DawkinsA computer network or data network is a telecommunications network which allows nodes to share resources. In computer networks, networked computing devices exchange data with each other using a data link. The connections between nodes are established using either cable media or wireless media. The best-known computer network is the Internet.
Network computer devices that originate, route and terminate the data are called network nodes. Nodes can include hosts such as personal computers, phones, servers as well as networking hardware. Two such devices can be said to be networked together when one device is able to exchange information with the other device, whether or not they have a direct connection to each other.
Computer networks differ in the transmission medium used to carry their signals, communications protocols to organize network traffic, the network's size, topology and organizational intent.
Computer networks support an enormous number of applications and services such as access to the World Wide Web, digital video, digital audio, shared use of application and storage servers, printers, and fax machines, and use of email and instant messaging applications as well as many others. In most cases, application-specific communications protocols are layered (i.e. carried as payload) over other more general communications protocols.




The chronology of significant computer-network developments includes:
In the late 1950s, early networks of computers included the military radar system Semi-Automatic Ground Environment (SAGE).
In 1959, Anatolii Ivanovich Kitov proposed to the Central Committee of the Communist Party of the Soviet Union a detailed plan for the re-organisation of the control of the Soviet armed forces and of the Soviet economy on the basis of a network of computing centres.
In 1960, the commercial airline reservation system semi-automatic business research environment (SABRE) went online with two connected mainframes.
In 1962, J.C.R. Licklider developed a working group he called the "Intergalactic Computer Network", a precursor to the ARPANET, at the Advanced Research Projects Agency (ARPA).
In 1964, researchers at Dartmouth College developed the Dartmouth Time Sharing System for distributed users of large computer systems. The same year, at Massachusetts Institute of Technology, a research group supported by General Electric and Bell Labs used a computer to route and manage telephone connections.
Throughout the 1960s, Leonard Kleinrock, Paul Baran, and Donald Davies independently developed network systems that used packets to transfer information between computers over a network.
In 1965, Thomas Marill and Lawrence G. Roberts created the first wide area network (WAN). This was an immediate precursor to the ARPANET, of which Roberts became program manager.
Also in 1965, Western Electric introduced the first widely used telephone switch that implemented true computer control.
In 1969, the University of California at Los Angeles, the Stanford Research Institute, the University of California at Santa Barbara, and the University of Utah became connected as the beginning of the ARPANET network using 50 kbit/s circuits.
In 1972, commercial services using X.25 were deployed, and later used as an underlying infrastructure for expanding TCP/IP networks.
In 1973, Robert Metcalfe wrote a formal memo at Xerox PARC describing Ethernet, a networking system that was based on the Aloha network, developed in the 1960s by Norman Abramson and colleagues at the University of Hawaii. In July 1976, Robert Metcalfe and David Boggs published their paper "Ethernet: Distributed Packet Switching for Local Computer Networks" and collaborated on several patents received in 1977 and 1978. In 1979, Robert Metcalfe pursued making Ethernet an open standard.
In 1976, John Murphy of Datapoint Corporation created ARCNET, a token-passing network first used to share storage devices.
In 1995, the transmission speed capacity for Ethernet increased from 10 Mbit/s to 100 Mbit/s. By 1998, Ethernet supported transmission speeds of a Gigabit. Subsequently, higher speeds of up to 100 Gbit/s were added (as of 2016). The ability of Ethernet to scale easily (such as quickly adapting to support new fiber optic cable speeds) is a contributing factor to its continued use.



Computer networking may be considered a branch of electrical engineering, telecommunications, computer science, information technology or computer engineering, since it relies upon the theoretical and practical application of the related disciplines.
A computer network facilitates interpersonal communications allowing users to communicate efficiently and easily via various means: email, instant messaging, chat rooms, telephone, video telephone calls, and video conferencing. Providing access to information on shared storage devices is an important feature of many networks. A network allows sharing of files, data, and other types of information giving authorized users the ability to access information stored on other computers on the network. A network allows sharing of network and computing resources. Users may access and use resources provided by devices on the network, such as printing a document on a shared network printer. Distributed computing uses computing resources across a network to accomplish tasks. A computer network may be used by computer crackers to deploy computer viruses or computer worms on devices connected to the network, or to prevent these devices from accessing the network via a denial of service attack.




Computer communication links that do not support packets, such as traditional point-to-point telecommunication links, simply transmit data as a bit stream. However, most information in computer networks is carried in packets. A network packet is a formatted unit of data (a list of bits or bytes, usually a few tens of bytes to a few kilobytes long) carried by a packet-switched network.
In packet networks, the data is formatted into packets that are sent through the network to their destination. Once the packets arrive they are reassembled into their original message. With packets, the bandwidth of the transmission medium can be better shared among users than if the network were circuit switched. When one user is not sending packets, the link can be filled with packets from other users, and so the cost can be shared, with relatively little interference, provided the link isn't overused.
Packets consist of two kinds of data: control information, and user data (payload). The control information provides data the network needs to deliver the user data, for example: source and destination network addresses, error detection codes, and sequencing information. Typically, control information is found in packet headers and trailers, with payload data in between.
Often the route a packet needs to take through a network is not immediately available. In that case the packet is queued and waits until a link is free.




The physical layout of a network is usually less important than the topology that connects network nodes. Most diagrams that describe a physical network are therefore topological, rather than geographic. The symbols on these diagrams usually denote network links and network nodes.



The transmission media (often referred to in the literature as the physical media) used to link devices to form a computer network include electrical cable (Ethernet, HomePNA, power line communication, G.hn), optical fiber (fiber-optic communication), and radio waves (wireless networking). In the OSI model, these are defined at layers 1 and 2 — the physical layer and the data link layer.
A widely adopted family of transmission media used in local area network (LAN) technology is collectively known as Ethernet. The media and protocol standards that enable communication between networked devices over Ethernet are defined by IEEE 802.3. Ethernet transmits data over both copper and fiber cables. Wireless LAN standards (e.g. those defined by IEEE 802.11) use radio waves, or others use infrared signals as a transmission medium. Power line communication uses a building's power cabling to transmit data.




The orders of the following wired technologies are, roughly, from slowest to fastest transmission speed.
Coaxial cable is widely used for cable television systems, office buildings, and other work-sites for local area networks. The cables consist of copper or aluminum wire surrounded by an insulating layer (typically a flexible material with a high dielectric constant), which itself is surrounded by a conductive layer. The insulation helps minimize interference and distortion. Transmission speed ranges from 200 million bits per second to more than 500 million bits per second.
ITU-T G.hn technology uses existing home wiring (coaxial cable, phone lines and power lines) to create a high-speed (up to 1 Gigabit/s) local area network
Twisted pair wire is the most widely used medium for all telecommunication. Twisted-pair cabling consist of copper wires that are twisted into pairs. Ordinary telephone wires consist of two insulated copper wires twisted into pairs. Computer network cabling (wired Ethernet as defined by IEEE 802.3) consists of 4 pairs of copper cabling that can be utilized for both voice and data transmission. The use of two wires twisted together helps to reduce crosstalk and electromagnetic induction. The transmission speed ranges from 2 million bits per second to 10 billion bits per second. Twisted pair cabling comes in two forms: unshielded twisted pair (UTP) and shielded twisted-pair (STP). Each form comes in several category ratings, designed for use in various scenarios.

An optical fiber is a glass fiber. It carries pulses of light that represent data. Some advantages of optical fibers over metal wires are very low transmission loss and immunity from electrical interference. Optical fibers can simultaneously carry multiple wavelengths of light, which greatly increases the rate that data can be sent, and helps enable data rates of up to trillions of bits per second. Optic fibers can be used for long runs of cable carrying very high data rates, and are used for undersea cables to interconnect continents.
Price is a main factor distinguishing wired- and wireless-technology options in a business. Wireless options command a price premium that can make purchasing wired computers, printers and other devices a financial benefit. Before making the decision to purchase hard-wired technology products, a review of the restrictions and limitations of the selections is necessary. Business and employee needs may override any cost considerations.




Terrestrial microwave – Terrestrial microwave communication uses Earth-based transmitters and receivers resembling satellite dishes. Terrestrial microwaves are in the low-gigahertz range, which limits all communications to line-of-sight. Relay stations are spaced approximately 48 km (30 mi) apart.
Communications satellites – Satellites communicate via microwave radio waves, which are not deflected by the Earth's atmosphere. The satellites are stationed in space, typically in geosynchronous orbit 35,400 km (22,000 mi) above the equator. These Earth-orbiting systems are capable of receiving and relaying voice, data, and TV signals.
Cellular and PCS systems use several radio communications technologies. The systems divide the region covered into multiple geographic areas. Each area has a low-power transmitter or radio relay antenna device to relay calls from one area to the next area.
Radio and spread spectrum technologies – Wireless local area networks use a high-frequency radio technology similar to digital cellular and a low-frequency radio technology. Wireless LANs use spread spectrum technology to enable communication between multiple devices in a limited area. IEEE 802.11 defines a common flavor of open-standards wireless radio-wave technology known as Wifi.
Free-space optical communication uses visible or invisible light for communications. In most cases, line-of-sight propagation is used, which limits the physical positioning of communicating devices.



There have been various attempts at transporting data over exotic media:
IP over Avian Carriers was a humorous April fool's Request for Comments, issued as RFC 1149. It was implemented in real life in 2001.
Extending the Internet to interplanetary dimensions via radio waves, the Interplanetary Internet.
Both cases have a large round-trip delay time, which gives slow two-way communication, but doesn't prevent sending large amounts of information.




Apart from any physical transmission medium there may be, networks comprise additional basic system building blocks, such as network interface controller (NICs), repeaters, hubs, bridges, switches, routers, modems, and firewalls.




A network interface controller (NIC) is computer hardware that provides a computer with the ability to access the transmission media, and has the ability to process low-level network information. For example, the NIC may have a connector for accepting a cable, or an aerial for wireless transmission and reception, and the associated circuitry.
The NIC responds to traffic addressed to a network address for either the NIC or the computer as a whole.
In Ethernet networks, each network interface controller has a unique Media Access Control (MAC) address—usually stored in the controller's permanent memory. To avoid address conflicts between network devices, the Institute of Electrical and Electronics Engineers (IEEE) maintains and administers MAC address uniqueness. The size of an Ethernet MAC address is six octets. The three most significant octets are reserved to identify NIC manufacturers. These manufacturers, using only their assigned prefixes, uniquely assign the three least-significant octets of every Ethernet interface they produce.



A repeater is an electronic device that receives a network signal, cleans it of unnecessary noise and regenerates it. The signal is retransmitted at a higher power level, or to the other side of an obstruction, so that the signal can cover longer distances without degradation. In most twisted pair Ethernet configurations, repeaters are required for cable that runs longer than 100 meters. With fiber optics, repeaters can be tens or even hundreds of kilometers apart.
A repeater with multiple ports is known as a hub. Repeaters work on the physical layer of the OSI model. Repeaters require a small amount of time to regenerate the signal. This can cause a propagation delay that affects network performance. As a result, many network architectures limit the number of repeaters that can be used in a row, e.g., the Ethernet 5-4-3 rule.
Hubs have been mostly obsoleted by modern switches; but repeaters are used for long distance links, notably undersea cabling.



A network bridge connects and filters traffic between two network segments at the data link layer (layer 2) of the OSI model to form a single network. This breaks the network's collision domain but maintains a unified broadcast domain. Network segmentation breaks down a large, congested network into an aggregation of smaller, more efficient networks.
Bridges come in three basic types:
Local bridges: Directly connect LANs
Remote bridges: Can be used to create a wide area network (WAN) link between LANs. Remote bridges, where the connecting link is slower than the end networks, largely have been replaced with routers.
Wireless bridges: Can be used to join LANs or connect remote devices to LANs.



A network switch is a device that forwards and filters OSI layer 2 datagrams (frames) between ports based on the destination MAC address in each frame. A switch is distinct from a hub in that it only forwards the frames to the physical ports involved in the communication rather than all ports connected. It can be thought of as a multi-port bridge. It learns to associate physical ports to MAC addresses by examining the source addresses of received frames. If an unknown destination is targeted, the switch broadcasts to all ports but the source. Switches normally have numerous ports, facilitating a star topology for devices, and cascading additional switches.
Multi-layer switches are capable of routing based on layer 3 addressing or additional logical levels. The term switch is often used loosely to include devices such as routers and bridges, as well as devices that may distribute traffic based on load or based on application content (e.g., a Web URL identifier).




A router is an internetworking device that forwards packets between networks by processing the routing information included in the packet or datagram (Internet protocol information from layer 3). The routing information is often processed in conjunction with the routing table (or forwarding table). A router uses its routing table to determine where to forward packets. A destination in a routing table can include a "null" interface, also known as the "black hole" interface because data can go into it, however, no further processing is done for said data, i.e. the packets are dropped.



Modems (MOdulator-DEModulator) are used to connect network nodes via wire not originally designed for digital network traffic, or for wireless. To do this one or more carrier signals are modulated by the digital signal to produce an analog signal that can be tailored to give the required properties for transmission. Modems are commonly used for telephone lines, using a Digital Subscriber Line technology.



A firewall is a network device for controlling network security and access rules. Firewalls are typically configured to reject access requests from unrecognized sources while allowing actions from recognized ones. The vital role firewalls play in network security grows in parallel with the constant increase in cyber attacks.



Network topology is the layout or organizational hierarchy of interconnected nodes of a computer network. Different network topologies can affect throughput, but reliability is often more critical. With many technologies, such as bus networks, a single failure can cause the network to fail entirely. In general the more interconnections there are, the more robust the network is; but the more expensive it is to install.




Common layouts are:
A bus network: all nodes are connected to a common medium along this medium. This was the layout used in the original Ethernet, called 10BASE5 and 10BASE2.
A star network: all nodes are connected to a special central node. This is the typical layout found in a Wireless LAN, where each wireless client connects to the central Wireless access point.
A ring network: each node is connected to its left and right neighbour node, such that all nodes are connected and that each node can reach each other node by traversing nodes left- or rightwards. The Fiber Distributed Data Interface (FDDI) made use of such a topology.
A mesh network: each node is connected to an arbitrary number of neighbours in such a way that there is at least one traversal from any node to any other.
A fully connected network: each node is connected to every other node in the network.
A tree network: nodes are arranged hierarchically.
Note that the physical layout of the nodes in a network may not necessarily reflect the network topology. As an example, with FDDI, the network topology is a ring (actually two counter-rotating rings), but the physical topology is often a star, because all neighboring connections can be routed via a central physical location.




An overlay network is a virtual computer network that is built on top of another network. Nodes in the overlay network are connected by virtual or logical links. Each link corresponds to a path, perhaps through many physical links, in the underlying network. The topology of the overlay network may (and often does) differ from that of the underlying one. For example, many peer-to-peer networks are overlay networks. They are organized as nodes of a virtual system of links that run on top of the Internet.
Overlay networks have been around since the invention of networking when computer systems were connected over telephone lines using modems, before any data network existed.
The most striking example of an overlay network is the Internet itself. The Internet itself was initially built as an overlay on the telephone network. Even today, each Internet node can communicate with virtually any other through an underlying mesh of sub-networks of wildly different topologies and technologies. Address resolution and routing are the means that allow mapping of a fully connected IP overlay network to its underlying network.
Another example of an overlay network is a distributed hash table, which maps keys to nodes in the network. In this case, the underlying network is an IP network, and the overlay network is a table (actually a map) indexed by keys.
Overlay networks have also been proposed as a way to improve Internet routing, such as through quality of service guarantees to achieve higher-quality streaming media. Previous proposals such as IntServ, DiffServ, and IP Multicast have not seen wide acceptance largely because they require modification of all routers in the network. On the other hand, an overlay network can be incrementally deployed on end-hosts running the overlay protocol software, without cooperation from Internet service providers. The overlay network has no control over how packets are routed in the underlying network between two overlay nodes, but it can control, for example, the sequence of overlay nodes that a message traverses before it reaches its destination.
For example, Akamai Technologies manages an overlay network that provides reliable, efficient content delivery (a kind of multicast). Academic research includes end system multicast, resilient routing and quality of service studies, among others.




A communications protocol is a set of rules for exchanging information over network links. In a protocol stack (also see the OSI model), each protocol leverages the services of the protocol below it. An important example of a protocol stack is HTTP (the World Wide Web protocol) running over TCP over IP (the Internet protocols) over IEEE 802.11 (the Wi-Fi protocol). This stack is used between the wireless router and the home user's personal computer when the user is surfing the web.
Whilst the use of protocol layering is today ubiquitous across the field of computer networking, it has been historically criticized by many researchers for two principal reasons. Firstly, abstracting the protocol stack in this way may cause a higher layer to duplicate functionality of a lower layer, a prime example being error recovery on both a per-link basis and an end-to-end basis. Secondly, it is common that a protocol implementation at one layer may require data, state or addressing information that is only present at another layer, thus defeating the point of separating the layers in the first place. For example, TCP uses the ECN field in the IPv4 header as an indication of congestion; IP is a network layer protocol whereas TCP is a transport layer protocol.
Communication protocols have various characteristics. They may be connection-oriented or connectionless, they may use circuit mode or packet switching, and they may use hierarchical addressing or flat addressing.
There are many communication protocols, a few of which are described below.



IEEE 802 is a family of IEEE standards dealing with local area networks and metropolitan area networks. The complete IEEE 802 protocol suite provides a diverse set of networking capabilities. The protocols have a flat addressing scheme. They operate mostly at levels 1 and 2 of the OSI model.
For example, MAC bridging (IEEE 802.1D) deals with the routing of Ethernet packets using a Spanning Tree Protocol. IEEE 802.1Q describes VLANs, and IEEE 802.1X defines a port-based Network Access Control protocol, which forms the basis for the authentication mechanisms used in VLANs (but it is also found in WLANs) – it is what the home user sees when the user has to enter a "wireless access key".



Ethernet, sometimes simply called LAN, is a family of protocols used in wired LANs, described by a set of standards together called IEEE 802.3 published by the Institute of Electrical and Electronics Engineers.



Wireless LAN, also widely known as WLAN or WiFi, is probably the most well-known member of the IEEE 802 protocol family for home users today. It is standarized by IEEE 802.11 and shares many properties with wired Ethernet.



The Internet Protocol Suite, also called TCP/IP, is the foundation of all modern networking. It offers connection-less as well as connection-oriented services over an inherently unreliable network traversed by data-gram transmission at the Internet protocol (IP) level. At its core, the protocol suite defines the addressing, identification, and routing specifications for Internet Protocol Version 4 (IPv4) and for IPv6, the next generation of the protocol with a much enlarged addressing capability.



Synchronous optical networking (SONET) and Synchronous Digital Hierarchy (SDH) are standardized multiplexing protocols that transfer multiple digital bit streams over optical fiber using lasers. They were originally designed to transport circuit mode communications from a variety of different sources, primarily to support real-time, uncompressed, circuit-switched voice encoded in PCM (Pulse-Code Modulation) format. However, due to its protocol neutrality and transport-oriented features, SONET/SDH also was the obvious choice for transporting Asynchronous Transfer Mode (ATM) frames.



Asynchronous Transfer Mode (ATM) is a switching technique for telecommunication networks. It uses asynchronous time-division multiplexing and encodes data into small, fixed-sized cells. This differs from other protocols such as the Internet Protocol Suite or Ethernet that use variable sized packets or frames. ATM has similarity with both circuit and packet switched networking. This makes it a good choice for a network that must handle both traditional high-throughput data traffic, and real-time, low-latency content such as voice and video. ATM uses a connection-oriented model in which a virtual circuit must be established between two endpoints before the actual data exchange begins.
While the role of ATM is diminishing in favor of next-generation networks, it still plays a role in the last mile, which is the connection between an Internet service provider and the home user.



A network can be characterized by its physical capacity or its organizational purpose. Use of the network, including user authorization and access rights, differ accordingly.
Nanoscale network
A nanoscale communication network has key components implemented at the nanoscale including message carriers and leverages physical principles that differ from macroscale communication mechanisms. Nanoscale communication extends communication to very small sensors and actuators such as those found in biological systems and also tends to operate in environments that would be too harsh for classical communication.
Personal area network
A personal area network (PAN) is a computer network used for communication among computer and different information technological devices close to one person. Some examples of devices that are used in a PAN are personal computers, printers, fax machines, telephones, PDAs, scanners, and even video game consoles. A PAN may include wired and wireless devices. The reach of a PAN typically extends to 10 meters. A wired PAN is usually constructed with USB and FireWire connections while technologies such as Bluetooth and infrared communication typically form a wireless PAN.
Local area network
A local area network (LAN) is a network that connects computers and devices in a limited geographical area such as a home, school, office building, or closely positioned group of buildings. Each computer or device on the network is a node. Wired LANs are most likely based on Ethernet technology. Newer standards such as ITU-T G.hn also provide a way to create a wired LAN using existing wiring, such as coaxial cables, telephone lines, and power lines.
The defining characteristics of a LAN, in contrast to a wide area network (WAN), include higher data transfer rates, limited geographic range, and lack of reliance on leased lines to provide connectivity. Current Ethernet or other IEEE 802.3 LAN technologies operate at data transfer rates up to 100 Gbit/s, standarized by IEEE in 2010. Currently, 400 Gbit/s Ethernet is being developed.
A LAN can be connected to a WAN using a router.
Home area network
A home area network (HAN) is a residential LAN used for communication between digital devices typically deployed in the home, usually a small number of personal computers and accessories, such as printers and mobile computing devices. An important function is the sharing of Internet access, often a broadband service through a cable TV or digital subscriber line (DSL) provider.
Storage area network
A storage area network (SAN) is a dedicated network that provides access to consolidated, block level data storage. SANs are primarily used to make storage devices, such as disk arrays, tape libraries, and optical jukeboxes, accessible to servers so that the devices appear like locally attached devices to the operating system. A SAN typically has its own network of storage devices that are generally not accessible through the local area network by other devices. The cost and complexity of SANs dropped in the early 2000s to levels allowing wider adoption across both enterprise and small to medium-sized business environments.
Campus area network
A campus area network (CAN) is made up of an interconnection of LANs within a limited geographical area. The networking equipment (switches, routers) and transmission media (optical fiber, copper plant, Cat5 cabling, etc.) are almost entirely owned by the campus tenant / owner (an enterprise, university, government, etc.).
For example, a university campus network is likely to link a variety of campus buildings to connect academic colleges or departments, the library, and student residence halls.
Backbone network
A backbone network is part of a computer network infrastructure that provides a path for the exchange of information between different LANs or sub-networks. A backbone can tie together diverse networks within the same building, across different buildings, or over a wide area.
For example, a large company might implement a backbone network to connect departments that are located around the world. The equipment that ties together the departmental networks constitutes the network backbone. When designing a network backbone, network performance and network congestion are critical factors to take into account. Normally, the backbone network's capacity is greater than that of the individual networks connected to it.
Another example of a backbone network is the Internet backbone, which is the set of wide area networks (WANs) and core routers that tie together all networks connected to the Internet.
Metropolitan area network
A Metropolitan area network (MAN) is a large computer network that usually spans a city or a large campus.
Wide area network
A wide area network (WAN) is a computer network that covers a large geographic area such as a city, country, or spans even intercontinental distances. A WAN uses a communications channel that combines many types of media such as telephone lines, cables, and air waves. A WAN often makes use of transmission facilities provided by common carriers, such as telephone companies. WAN technologies generally function at the lower three layers of the OSI reference model: the physical layer, the data link layer, and the network layer.
Enterprise private network
An enterprise private network is a network that a single organization builds to interconnect its office locations (e.g., production sites, head offices, remote offices, shops) so they can share computer resources.
Virtual private network
A virtual private network (VPN) is an overlay network in which some of the links between nodes are carried by open connections or virtual circuits in some larger network (e.g., the Internet) instead of by physical wires. The data link layer protocols of the virtual network are said to be tunneled through the larger network when this is the case. One common application is secure communications through the public Internet, but a VPN need not have explicit security features, such as authentication or content encryption. VPNs, for example, can be used to separate the traffic of different user communities over an underlying network with strong security features.
VPN may have best-effort performance, or may have a defined service level agreement (SLA) between the VPN customer and the VPN service provider. Generally, a VPN has a topology more complex than point-to-point.
Global area network
A global area network (GAN) is a network used for supporting mobile across an arbitrary number of wireless LANs, satellite coverage areas, etc. The key challenge in mobile communications is handing off user communications from one local coverage area to the next. In IEEE Project 802, this involves a succession of terrestrial wireless LANs.



Networks are typically managed by the organizations that own them. Private enterprise networks may use a combination of intranets and extranets. They may also provide network access to the Internet, which has no single owner and permits virtually unlimited global connectivity.



An intranet is a set of networks that are under the control of a single administrative entity. The intranet uses the IP protocol and IP-based tools such as web browsers and file transfer applications. The administrative entity limits use of the intranet to its authorized users. Most commonly, an intranet is the internal LAN of an organization. A large intranet typically has at least one web server to provide users with organizational information. An intranet is also anything behind the router on a local area network.



An extranet is a network that is also under the administrative control of a single organization, but supports a limited connection to a specific external network. For example, an organization may provide access to some aspects of its intranet to share data with its business partners or customers. These other entities are not necessarily trusted from a security standpoint. Network connection to an extranet is often, but not always, implemented via WAN technology.



An internetwork is the connection of multiple computer networks via a common routing technology using routers.




The Internet is the largest example of an internetwork. It is a global system of interconnected governmental, academic, corporate, public, and private computer networks. It is based on the networking technologies of the Internet Protocol Suite. It is the successor of the Advanced Research Projects Agency Network (ARPANET) developed by DARPA of the United States Department of Defense. The Internet is also the communications backbone underlying the World Wide Web (WWW).
Participants in the Internet use a diverse array of methods of several hundred documented, and often standardized, protocols compatible with the Internet Protocol Suite and an addressing system (IP addresses) administered by the Internet Assigned Numbers Authority and address registries. Service providers and large enterprises exchange information about the reachability of their address spaces through the Border Gateway Protocol (BGP), forming a redundant worldwide mesh of transmission paths.



A darknet is an overlay network, typically running on the internet, that is only accessible through specialized software. A darknet is an anonymizing network where connections are made only between trusted peers — sometimes called "friends" (F2F) — using non-standard protocols and ports.
Darknets are distinct from other distributed peer-to-peer networks as sharing is anonymous (that is, IP addresses are not publicly shared), and therefore users can communicate with little fear of governmental or corporate interference.




Routing is the process of selecting network paths to carry network traffic. Routing is performed for many kinds of networks, including circuit switching networks and packet switched networks.
In packet switched networks, routing directs packet forwarding (the transit of logically addressed network packets from their source toward their ultimate destination) through intermediate nodes. Intermediate nodes are typically network hardware devices such as routers, bridges, gateways, firewalls, or switches. General-purpose computers can also forward packets and perform routing, though they are not specialized hardware and may suffer from limited performance. The routing process usually directs forwarding on the basis of routing tables, which maintain a record of the routes to various network destinations. Thus, constructing routing tables, which are held in the router's memory, is very important for efficient routing.
There are usually multiple routes that can be taken, and to choose between them, different elements can be considered to decide which routes get installed into the routing table, such as (sorted by priority):
Prefix-Length: where longer subnet masks are preferred (independent if it is within a routing protocol or over different routing protocol)
Metric: where a lower metric/cost is preferred (only valid within one and the same routing protocol)
Administrative distance: where a lower distance is preferred (only valid between different routing protocols)
Most routing algorithms use only one network path at a time. Multipath routing techniques enable the use of multiple alternative paths.
Routing, in a more narrow sense of the term, is often contrasted with bridging in its assumption that network addresses are structured and that similar addresses imply proximity within the network. Structured addresses allow a single routing table entry to represent the route to a group of devices. In large networks, structured addressing (routing, in the narrow sense) outperforms unstructured addressing (bridging). Routing has become the dominant form of addressing on the Internet. Bridging is still widely used within localized environments.



Network services are applications hosted by servers on a computer network, to provide some functionality for members or users of the network, or to help the network itself to operate.
The World Wide Web, E-mail, printing and network file sharing are examples of well-known network services. Network services such as DNS (Domain Name System) give names for IP and MAC addresses (people remember names like “nm.lan” better than numbers like “210.121.67.18”), and DHCP to ensure that the equipment on the network has a valid IP address.
Services are usually based on a service protocol that defines the format and sequencing of messages between clients and servers of that network service.






Depending on the installation requirements, network performance is usually measured by the quality of service of a telecommunications product. The parameters that affect this typically can include throughput, jitter, bit error rate and latency.
The following list gives examples of network performance measures for a circuit-switched network and one type of packet-switched network, viz. ATM:
Circuit-switched networks: In circuit switched networks, network performance is synonymous with the grade of service. The number of rejected calls is a measure of how well the network is performing under heavy traffic loads. Other types of performance measures can include the level of noise and echo.
ATM: In an Asynchronous Transfer Mode (ATM) network, performance can be measured by line rate, quality of service (QoS), data throughput, connect time, stability, technology, modulation technique and modem enhancements.
There are many ways to measure the performance of a network, as each network is different in nature and design. Performance can also be modelled instead of measured. For example, state transition diagrams are often used to model queuing performance in a circuit-switched network. The network planner uses these diagrams to analyze how the network performs in each state, ensuring that the network is optimally designed.



Network congestion occurs when a link or node is carrying so much data that its quality of service deteriorates. Typical effects include queueing delay, packet loss or the blocking of new connections. A consequence of these latter two is that incremental increases in offered load lead either only to small increase in network throughput, or to an actual reduction in network throughput.
Network protocols that use aggressive retransmissions to compensate for packet loss tend to keep systems in a state of network congestion—even after the initial load is reduced to a level that would not normally induce network congestion. Thus, networks using these protocols can exhibit two stable states under the same level of load. The stable state with low throughput is known as congestive collapse.
Modern networks use congestion control and congestion avoidance techniques to try to avoid congestion collapse. These include: exponential backoff in protocols such as 802.11's CSMA/CA and the original Ethernet, window reduction in TCP, and fair queueing in devices such as routers. Another method to avoid the negative effects of network congestion is implementing priority schemes, so that some packets are transmitted with higher priority than others. Priority schemes do not solve network congestion by themselves, but they help to alleviate the effects of congestion for some services. An example of this is 802.1p. A third method to avoid network congestion is the explicit allocation of network resources to specific flows. One example of this is the use of Contention-Free Transmission Opportunities (CFTXOPs) in the ITU-T G.hn standard, which provides high-speed (up to 1 Gbit/s) Local area networking over existing home wires (power lines, phone lines and coaxial cables).
For the Internet RFC 2914 addresses the subject of congestion control in detail.



Network resilience is "the ability to provide and maintain an acceptable level of service in the face of faults and challenges to normal operation.”






Network security consists of provisions and policies adopted by the network administrator to prevent and monitor unauthorized access, misuse, modification, or denial of the computer network and its network-accessible resources. Network security is the authorization of access to data in a network, which is controlled by the network administrator. Users are assigned an ID and password that allows them access to information and programs within their authority. Network security is used on a variety of computer networks, both public and private, to secure daily transactions and communications among businesses, government agencies and individuals.



Network surveillance is the monitoring of data being transferred over computer networks such as the Internet. The monitoring is often done surreptitiously and may be done by or at the behest of governments, by corporations, criminal organizations, or individuals. It may or may not be legal and may or may not require authorization from a court or other independent agency.
Computer and network surveillance programs are widespread today, and almost all Internet traffic is or could potentially be monitored for clues to illegal activity.
Surveillance is very useful to governments and law enforcement to maintain social control, recognize and monitor threats, and prevent/investigate criminal activity. With the advent of programs such as the Total Information Awareness program, technologies such as high speed surveillance computers and biometrics software, and laws such as the Communications Assistance For Law Enforcement Act, governments now possess an unprecedented ability to monitor the activities of citizens.
However, many civil rights and privacy groups—such as Reporters Without Borders, the Electronic Frontier Foundation, and the American Civil Liberties Union—have expressed concern that increasing surveillance of citizens may lead to a mass surveillance society, with limited political and personal freedoms. Fears such as this have led to numerous lawsuits such as Hepting v. AT&T. The hacktivist group Anonymous has hacked into government websites in protest of what it considers "draconian surveillance".



End-to-end encryption (E2EE) is a digital communications paradigm of uninterrupted protection of data traveling between two communicating parties. It involves the originating party encrypting data so only the intended recipient can decrypt it, with no dependency on third parties. End-to-end encryption prevents intermediaries, such as Internet providers or application service providers, from discovering or tampering with communications. End-to-end encryption generally protects both confidentiality and integrity.
Examples of end-to-end encryption include PGP for email, OTR for instant messaging, ZRTP for telephony, and TETRA for radio.
Typical server-based communications systems do not include end-to-end encryption. These systems can only guarantee protection of communications between clients and servers, not between the communicating parties themselves. Examples of non-E2EE systems are Google Talk, Yahoo Messenger, Facebook, and Dropbox. Some such systems, for example LavaBit and SecretInk, have even described themselves as offering "end-to-end" encryption when they do not. Some systems that normally offer end-to-end encryption have turned out to contain a back door that subverts negotiation of the encryption key between the communicating parties, for example Skype or Hushmail.
The end-to-end encryption paradigm does not directly address risks at the communications endpoints themselves, such as the technical exploitation of clients, poor quality random number generators, or key escrow. E2EE also does not address traffic analysis, which relates to things such as the identities of the end points and the times and quantities of messages that are sent.



Users and network administrators typically have different views of their networks. Users can share printers and some servers from a workgroup, which usually means they are in the same geographic location and are on the same LAN, whereas a Network Administrator is responsible to keep that network up and running. A community of interest has less of a connection of being in a local area, and should be thought of as a set of arbitrarily located users who share a set of servers, and possibly also communicate via peer-to-peer technologies.
Network administrators can see networks from both physical and logical perspectives. The physical perspective involves geographic locations, physical cabling, and the network elements (e.g., routers, bridges and application layer gateways) that interconnect via the transmission media. Logical networks, called, in the TCP/IP architecture, subnets, map onto one or more transmission media. For example, a common practice in a campus of buildings is to make a set of LAN cables in each building appear to be a common subnet, using virtual LAN (VLAN) technology.
Both users and administrators are aware, to varying extents, of the trust and scope characteristics of a network. Again using TCP/IP architectural terminology, an intranet is a community of interest under private administration usually by an enterprise, and is only accessible by authorized users (e.g. employees). Intranets do not have to be connected to the Internet, but generally have a limited connection. An extranet is an extension of an intranet that allows secure communications to users outside of the intranet (e.g. business partners, customers).
Unofficially, the Internet is the set of users, enterprises, and content providers that are interconnected by Internet Service Providers (ISP). From an engineering viewpoint, the Internet is the set of subnets, and aggregates of subnets, which share the registered IP address space and exchange information about the reachability of those IP addresses using the Border Gateway Protocol. Typically, the human-readable names of servers are translated to IP addresses, transparently to users, via the directory function of the Domain Name System (DNS).
Over the Internet, there can be business-to-business (B2B), business-to-consumer (B2C) and consumer-to-consumer (C2C) communications. When money or sensitive information is exchanged, the communications are apt to be protected by some form of communications security mechanism. Intranets and extranets can be securely superimposed onto the Internet, without any access by general Internet users and administrators, using secure Virtual Private Network (VPN) technology.



Comparison of network diagram software
Cyberspace
History of the Internet
Network simulation
Network planning and design




 This article incorporates public domain material from the General Services Administration document "Federal Standard 1037C".



Shelly, Gary, et al. "Discovering Computers" 2003 Edition.
Wendell Odom, Rus Healy, Denise Donohue. (2010) CCIE Routing and Switching. Indianapolis, IN: Cisco Press
Kurose James F and Keith W. Ross : Computer Networking: A Top-Down Approach Featuring the Internet, Pearson Education 2005.
William Stallings, Computer Networking with Internet Protocols and Technology, Pearson Education 2004.
Important publications in computer networks
Network Communication Architecture and Protocols: OSI Network Architecture 7 Layers Model
Dimitri Bertsekas, and Robert Gallager, "Data Networks," Prentice Hall, 1992.



Networking at DMOZ
IEEE Ethernet manufacturer information
A computer networking acronym guideLogic programming is a type of programming paradigm which is largely based on formal logic. Any program written in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain. Major logic programming language families include Prolog, Answer set programming (ASP) and Datalog. In all of these languages, rules are written in the form of clauses:
H :- B1, …, Bn.
and are read declaratively as logical implications:
H if B1 and … and Bn.
H is called the head of the rule and B1, …, Bn is called the body. Facts are rules that have no body, and are written in the simplified form:
H.
In the simplest case in which H, B1, …, Bn are all atomic formulae, these clauses are called definite clauses or Horn clauses. However, there exist many extensions of this simple case, the most important one being the case in which conditions in the body of a clause can also be negations of atomic formulae. Logic programming languages that include this extension have the knowledge representation capabilities of a non-monotonic logic.
In ASP and Datalog, logic programs have only a declarative reading, and their execution is performed by means of a proof procedure or model generator whose behaviour is not meant to be under the control of the programmer. However, in the Prolog family of languages, logic programs also have a procedural interpretation as goal-reduction procedures:
to solve H, solve B1, and ... and solve Bn.
Consider, for example, the following clause:
fallible(X) :- human(X).
based on an example used by Terry Winograd  to illustrate the programming language Planner. As a clause in a logic program, it can be used both as a procedure to test whether X is fallible by testing whether X is human, and as a procedure to find an X that is fallible by finding an X that is human. Even facts have a procedural interpretation. For example, the clause:
human(socrates).
can be used both as a procedure to show that socrates is human, and as a procedure to find an X that is human by "assigning" socrates to X.
The declarative reading of logic programs can be used by a programmer to verify their correctness. Moreover, logic-based program transformation techniques can also be used to transform logic programs into logically equivalent programs that are more efficient. In the Prolog family of logic programming languages, the programmer can also use the known problem-solving behaviour of the execution mechanism to improve the efficiency of programs.



The use of mathematical logic to represent and execute computer programs is also a feature of the lambda calculus, developed by Alonzo Church in the 1930s. However, the first proposal to use the clausal form of logic for representing computer programs was made by Cordell Green. This used an axiomatization of a subset of LISP, together with a representation of an input-output relation, to compute the relation by simulating the execution of the program in LISP. Foster and Elcock's Absys, on the other hand, employed a combination of equations and lambda calculus in an assertional programming language which places no constraints on the order in which operations are performed.
Logic programming in its present form can be traced back to debates in the late 1960s and early 1970s about declarative versus procedural representations of knowledge in Artificial Intelligence. Advocates of declarative representations were notably working at Stanford, associated with John McCarthy, Bertram Raphael and Cordell Green, and in Edinburgh, with John Alan Robinson (an academic visitor from Syracuse University), Pat Hayes, and Robert Kowalski. Advocates of procedural representations were mainly centered at MIT, under the leadership of Marvin Minsky and Seymour Papert.
Although it was based on the proof methods of logic, Planner, developed at MIT, was the first language to emerge within this proceduralist paradigm. Planner featured pattern-directed invocation of procedural plans from goals (i.e. goal-reduction or backward chaining) and from assertions (i.e. forward chaining). The most influential implementation of Planner was the subset of Planner, called Micro-Planner, implemented by Gerry Sussman, Eugene Charniak and Terry Winograd. It was used to implement Winograd's natural-language understanding program SHRDLU, which was a landmark at that time. To cope with the very limited memory systems at the time, Planner used a backtracking control structure so that only one possible computation path had to be stored at a time. Planner gave rise to the programming languages QA-4, Popler, Conniver, QLISP, and the concurrent language Ether.
Hayes and Kowalski in Edinburgh tried to reconcile the logic-based declarative approach to knowledge representation with Planner's procedural approach. Hayes (1973) developed an equational language, Golux, in which different procedures could be obtained by altering the behavior of the theorem prover. Kowalski, on the other hand, developed SLD resolution, a variant of SL-resolution, and showed how it treats implications as goal-reduction procedures. Kowalski collaborated with Colmerauer in Marseille, who developed these ideas in the design and implementation of the programming language Prolog.
The Association for Logic Programming was founded to promote Logic Programming in 1986.
Prolog gave rise to the programming languages ALF, Fril, Gödel, Mercury, Oz, Ciao, Visual Prolog, XSB, and λProlog, as well as a variety of concurrent logic programming languages, constraint logic programming languages and datalog.







Logic programming can be viewed as controlled deduction. An important concept in logic programming is the separation of programs into their logic component and their control component. With pure logic programming languages, the logic component alone determines the solutions produced. The control component can be varied to provide alternative ways of executing a logic program. This notion is captured by the slogan
Algorithm = Logic + Control
where "Logic" represents a logic program and "Control" represents different theorem-proving strategies.



In the simplified, propositional case in which a logic program and a top-level atomic goal contain no variables, backward reasoning determines an and-or tree, which constitutes the search space for solving the goal. The top-level goal is the root of the tree. Given any node in the tree and any clause whose head matches the node, there exists a set of child nodes corresponding to the sub-goals in the body of the clause. These child nodes are grouped together by an "and". The alternative sets of children corresponding to alternative ways of solving the node are grouped together by an "or".
Any search strategy can be used to search this space. Prolog uses a sequential, last-in-first-out, backtracking strategy, in which only one alternative and one sub-goal is considered at a time. Other search strategies, such as parallel search, intelligent backtracking, or best-first search to find an optimal solution, are also possible.
In the more general case, where sub-goals share variables, other strategies can be used, such as choosing the subgoal that is most highly instantiated or that is sufficiently instantiated so that only one procedure applies. Such strategies are used, for example, in concurrent logic programming.




For most practical applications, as well as for applications that require non-monotonic reasoning in artificial intelligence, Horn clause logic programs need to be extended to normal logic programs, with negative conditions. A clause in a normal logic program has the form:
H :- A1, …, An, not B1, …, not Bn.
and is read declaratively as a logical implication:
H if A1 and … and An and not B1 and … and not Bn.
where H and all the Ai and Bi are atomic formulas. The negation in the negative literals not Bi is commonly referred to as "negation as failure", because in most implementations, a negative condition not Bi is shown to hold by showing that the positive condition Bi fails to hold. For example:

Given the goal of finding something that can fly:

there are two candidate solutions, which solve the first subgoal bird(X), namely X  mary is the only solution of the goal.
Micro-Planner had a construct, called "thnot", which when applied to an expression returns the value true if (and only if) the evaluation of the expression fails. An equivalent operator is normally built-in in modern Prolog's implementations. It is normally written as not(Goal) or \+ Goal, where Goal is some goal (proposition) to be proved by the program. This operator differs from negation in first-order logic: a negation such as \+ X  1 can succeed, binding X to 1, depending on whether X was initially bound (note that standard Prolog executes goals in left-to-right order).
The logical status of negation as failure was unresolved until Keith Clark [1978] showed that, under certain natural conditions, it is a correct (and sometimes complete) implementation of classical negation with respect to the completion of the program. Completion amounts roughly to regarding the set of all the program clauses with the same predicate on the left hand side, say
H :- Body1.
…
H :- Bodyk.
as a definition of the predicate
H iff (Body1 or … or Bodyk)
where "iff" means "if and only if". Writing the completion also requires explicit use of the equality predicate and the inclusion of a set of appropriate axioms for equality. However, the implementation of negation by failure needs only the if-halves of the definitions without the axioms of equality.
For example, the completion of the program above is:
canfly(X) iff bird(X), not abnormal(X).
abnormal(X) iff wounded(X).
bird(X) iff X  mary.
X = X.
not john = mary.
not mary = john.
The notion of completion is closely related to McCarthy's circumscription semantics for default reasoning, and to the closed world assumption.
As an alternative to the completion semantics, negation as failure can also be interpreted epistemically, as in the stable model semantics of answer set programming. In this interpretation not(Bi) means literally that Bi is not known or not believed. The epistemic interpretation has the advantage that it can be combined very simply with classical negation, as in "extended logic programming", to formalise such phrases as "the contrary can not be shown", where "contrary" is classical negation and "can not be shown" is the epistemic interpretation of negation as failure.



The fact that Horn clauses can be given a procedural interpretation and, vice versa, that goal-reduction procedures can be understood as Horn clauses + backward reasoning means that logic programs combine declarative and procedural representations of knowledge. The inclusion of negation as failure means that logic programming is a kind of non-monotonic logic.
Despite its simplicity compared with classical logic, this combination of Horn clauses and negation as failure has proved to be surprisingly expressive. For example, it provides a natural representation for the common-sense laws of cause and effect, as formalised by both the situation calculus and event calculus. It has also been shown to correspond quite naturally to the semi-formal language of legislation. In particular, Prakken and Sartor  credit the representation of the British Nationality Act as a logic program  with being "hugely influential for the development of computational representations of legislation, showing how logic programming enables intuitively appealing representations that can be directly deployed to generate automatic inferences".







The programming language Prolog was developed in 1972 by Alain Colmerauer. It emerged from a collaboration between Colmerauer in Marseille and Robert Kowalski in Edinburgh. Colmerauer was working on natural language understanding, using logic to represent semantics and using resolution for question-answering. During the summer of 1971, Colmerauer and Kowalski discovered that the clausal form of logic could be used to represent formal grammars and that resolution theorem provers could be used for parsing. They observed that some theorem provers, like hyper-resolution, behave as bottom-up parsers and others, like SL-resolution (1971), behave as top-down parsers.
It was in the following summer of 1972, that Kowalski, again working with Colmerauer, developed the procedural interpretation of implications. This dual declarative/procedural interpretation later became formalised in the Prolog notation
H :- B1, …, Bn.
which can be read (and used) both declaratively and procedurally. It also became clear that such clauses could be restricted to definite clauses or Horn clauses, where H, B1, …, Bn are all atomic predicate logic formulae, and that SL-resolution could be restricted (and generalised) to LUSH or SLD-resolution. Kowalski's procedural interpretation and LUSH were described in a 1973 memo, published in 1974.
Colmerauer, with Philippe Roussel, used this dual interpretation of clauses as the basis of Prolog, which was implemented in the summer and autumn of 1972. The first Prolog program, also written in 1972 and implemented in Marseille, was a French question-answering system. The use of Prolog as a practical programming language was given great momentum by the development of a compiler by David Warren in Edinburgh in 1977. Experiments demonstrated that Edinburgh Prolog could compete with the processing speed of other symbolic programming languages such as Lisp. Edinburgh Prolog became the de facto standard and strongly influenced the definition of ISO standard Prolog.



Abductive logic programming is an extension of normal Logic Programming that allows some predicates, declared as abducible predicates, to be "open" or undefined. A clause in an abductive logic program has the form:
H :- B1, …, Bn, A1, …, An.
where H is an atomic formula that is not abducible, all the Bi are literals whose predicates are not abducible, and the Ai are atomic formulas whose predicates are abducible. The abducible predicates can be constrained by integrity constraints, which can have the form:
false :- B1, …, Bn.
where the Bi are arbitrary literals (defined or abducible, and atomic or negated). For example:

where the predicate normal is abducible.
Problem solving is achieved by deriving hypotheses expressed in terms of the abducible predicates as solutions of problems to be solved. These problems can be either observations that need to be explained (as in classical abductive reasoning) or goals to be solved (as in normal logic programming). For example, the hypothesis normal(mary) explains the observation canfly(mary). Moreover, the same hypothesis entails the only solution X = mary of the goal of finding something that can fly:

Abductive logic programming has been used for fault diagnosis, planning, natural language processing and machine learning. It has also been used to interpret Negation as Failure as a form of abductive reasoning.



Because mathematical logic has a long tradition of distinguishing between object language and metalanguage, logic programming also allows metalevel programming. The simplest metalogic program is the so-called "vanilla" meta-interpreter:

where true represents an empty conjunction, and clause(A,B) means there is an object-level clause of the form A :- B.
Metalogic programming allows object-level and metalevel representations to be combined, as in natural language. It can also be used to implement any logic that is specified by means of inference rules. Metalogic is used in logic programming to implement metaprograms, which manipulate other programs, databases, knowledge bases or axiomatic theories as data.




Constraint logic programming combines Horn clause logic programming with constraint solving. It extends Horn clauses by allowing some predicates, declared as constraint predicates, to occur as literals in the body of clauses. A constraint logic program is a set of clauses of the form:
H :- C1, …, Cn 
  
    
      
        ◊
      
    
    {\displaystyle \Diamond }
   B1, …, Bn.
where H and all the Bi are atomic formulas, and the Ci are constraints. Declaratively, such clauses are read as ordinary logical implications:
H if C1 and … and Cn and B1 and … and Bn.
However, whereas the predicates in the heads of clauses are defined by the constraint logic program, the predicates in the constraints are predefined by some domain-specific model-theoretic structure or theory.
Procedurally, subgoals whose predicates are defined by the program are solved by goal-reduction, as in ordinary logic programming, but constraints are checked for satisfiability by a domain-specific constraint-solver, which implements the semantics of the constraint predicates. An initial problem is solved by reducing it to a satisfiable conjunction of constraints.
The following constraint logic program represents a toy temporal database of john's history as a teacher:
teaches(john, hardware, T) :- 1990 ≤ T, T < 1999.
teaches(john, software, T) :- 1999 ≤ T, T < 2005.
teaches(john, logic, T) :- 2005 ≤ T, T ≤ 2012.
rank(john, instructor, T) :- 1990 ≤ T, T < 2010.
rank(john, professor, T) :- 2010 ≤ T, T < 2014.
Here ≤ and < are constraint predicates, with their usual intended semantics. The following goal clause queries the database to find out when john both taught logic and was a professor:
:- teaches(john, logic, T), rank(john, professor, T).
The solution is 2010 ≤ T, T ≤ 2012.
Constraint logic programming has been used to solve problems in such fields as civil engineering, mechanical engineering, digital circuit verification, automated timetabling, air traffic control, and finance. It is closely related to abductive logic programming.




Concurrent logic programming integrates concepts of logic programming with concurrent programming. Its development was given a big impetus in the 1980s by its choice for the systems programming language of the Japanese Fifth Generation Project (FGCS).
A concurrent logic program is a set of guarded Horn clauses of the form:

H :- G1, …, Gn | B1, …, Bn.

The conjunction G1, … , Gn is called the guard of the clause, and | is the commitment operator. Declaratively, guarded Horn clauses are read as ordinary logical implications:

H if G1 and … and Gn and B1 and … and Bn.

However, procedurally, when there are several clauses whose heads H match a given goal, then all of the clauses are executed in parallel, checking whether their guards G1, … , Gn hold. If the guards of more than one clause hold, then a committed choice is made to one of the clauses, and execution proceedes with the subgoals B1, …, Bn of the chosen clause. These subgoals can also be executed in parallel. Thus concurrent logic programming implements a form of "don't care nondeterminism", rather than "don't know nondeterminism".
For example, the following concurrent logic program defines a predicate shuffle(Left, Right, Merge) , which can be used to shuffle two lists Left and Right, combining them into a single list Merge that preserves the ordering of the two lists Left and Right:

Here, [] represents the empty list, and [Head | Tail] represents a list with first element Head followed by list Tail, as in Prolog. (Notice that the first occurrence of | in the second and third clauses is the list constructor, whereas the second occurrence of | is the commitment operator.) The program can be used, for example, to shuffle the lists [ace, queen, king] and [1, 4, 2] by invoking the goal clause:

The program will non-deterministically generate a single solution, for example Merge = [ace, queen, 1, king, 4, 2].
Arguably, concurrent logic programming is based on message passing and consequently is subject to the same indeterminacy as other concurrent message-passing systems, such as Actors (see Indeterminacy in concurrent computation). Carl Hewitt has argued that, concurrent logic programming is not based on logic in his sense that computational steps cannot be logically deduced. However, in concurrent logic programming, any result of a terminating computation is a logical consequence of the program, and any partial result of a partial computation is a logical consequence of the program and the residual goal (process network). Consequently, the indeterminacy of computations implies that not all logical consequences of the program can be deduced.




Concurrent constraint logic programming combines concurrent logic programming and constraint logic programming, using constraints to control concurrency. A clause can contain a guard, which is a set of constraints that may block the applicability of the clause. When the guards of several clauses are satisfied, concurrent constraint logic programming makes a committed choice to the use of only one.




Inductive logic programming is concerned with generalizing positive and negative examples in the context of background knowledge: machine learning of logic programs. Recent work in this area, combining logic programming, learning and probability, has given rise to the new field of statistical relational learning and probabilistic inductive logic programming.



Several researchers have extended logic programming with higher-order programming features derived from higher-order logic, such as predicate variables. Such languages include the Prolog extensions HiLog and λProlog.



Basing logic programming within linear logic has resulted in the design of logic programming languages that are considerably more expressive than those based on classical logic. Horn clause programs can only represent state change by the change in arguments to predicates. In linear logic programming, one can use the ambient linear logic to support state change. Some early designs of logic programming languages based on linear logic include LO [Andreoli & Pareschi, 1991], Lolli, ACL, and Forum [Miller, 1996]. Forum provides a goal-directed interpretation of all of linear logic.



F-logic extends logic programming with objects and the frame syntax. A number of systems are based on F-logic, including Flora-2, FLORID, and a highly scalable commercial system Ontobroker.
Logtalk extends the Prolog programming language with support for objects, protocols, and other OOP concepts. Highly portable, it supports most standard-complaint Prolog systems as backend compilers.



Transaction logic is an extension of logic programming with a logical theory of state-modifying updates. It has both a model-theoretic semantics and a procedural one. An implementation of a subset of Transaction logic is available in the Flora-2 system. Other prototypes are also available.



Boolean satisfiability problem
Constraint logic programming
Datalog
Fril
Functional programming
Fuzzy logic
Inductive logic programming
Logic in computer science (includes Formal methods)
Logic programming languages
Programming paradigm
R++
Reasoning system
Rule-based machine learning
Satisfiability






Baral, C.; Gelfond, M. (1994). "Logic programming and knowledge representation" (PDF). The Journal of Logic Programming. 19-20: 73–148. doi:10.1016/0743-1066(94)90025-6. 
Robert Kowalski. The Early Years of Logic Programming Kowalski, R. A. (1988). "The early years of logic programming" (PDF). Communications of the ACM. 31: 38–43. doi:10.1145/35043.35046. 
Lloyd, J. W. (1987). Foundations of Logic Programming. (2nd edition). Springer-Verlag. 



John McCarthy. Programs with common sense Symposium on Mechanization of Thought Processes. National Physical Laboratory. Teddington, England. 1958.
D. Miller, G. Nadathur, F. Pfenning, A. Scedrov. Uniform proofs as a foundation for logic programming, Annals of Pure and Applied Logic, vol. 51, pp 125–157, 1991.
Ehud Shapiro (Editor). Concurrent Prolog MIT Press. 1987.
James Slagle. Experiments with a Deductive Question-Answering Program CACM. December 1965.



Carl Hewitt. Procedural Embedding of Knowledge In Planner IJCAI 1971.
Carl Hewitt. The repeated demise of logic programming and why it will be reincarnated
Evgeny Dantsin, Thomas Eiter, Georg Gottlob, Andrei Voronkov: Complexity and expressive power of logic programming. ACM Comput. Surv. 33(3): 374-425 (2001)
Ulf Nilsson and Jan Maluszynski, Logic, Programming and Prolog



Logic Programming Virtual Library entry
Bibliographies on Logic Programming
Association for Logic Programming (ALP)
Theory and Practice of Logic Programming journal
Logic programming in C++ with Castor
Logic programming in Oz
Prolog Development Center
Racklog: Logic Programming in RacketSemantics (from Ancient Greek: σημαντικός sēmantikos, "significant") is primarily the linguistic, and also philosophical study of meaning—in language, programming languages, formal logics, and semiotics. It focuses on the relationship between signifiers—like words, phrases, signs, and symbols—and what they stand for, their denotation.
In international scientific vocabulary semantics is also called semasiology. The word semantics was first used by Michel Bréal, a French philologist. It denotes a range of ideas—from the popular to the highly technical. It is often used in ordinary language for denoting a problem of understanding that comes down to word selection or connotation. This problem of understanding has been the subject of many formal enquiries, over a long period of time, especially in the field of formal semantics. In linguistics, it is the study of the interpretation of signs or symbols used in agents or communities within particular circumstances and contexts. Within this view, sounds, facial expressions, body language, and proxemics have semantic (meaningful) content, and each comprises several branches of study. In written language, things like paragraph structure and punctuation bear semantic content; other forms of language bear other semantic content.
The formal study of semantics intersects with many other fields of inquiry, including lexicology, syntax, pragmatics, etymology and others. Independently, semantics is also a well-defined field in its own right, often with synthetic properties. In the philosophy of language, semantics and reference are closely connected. Further related fields include philology, communication, and semiotics. The formal study of semantics can therefore be manifold and complex.
Semantics contrasts with syntax, the study of the combinatorics of units of a language (without reference to their meaning), and pragmatics, the study of the relationships between the symbols of a language, their meaning, and the users of the language. Semantics as a field of study also has significant ties to various representational theories of meaning including truth theories of meaning, coherence theories of meaning, and correspondence theories of meaning. Each of these is related to the general philosophical study of reality and the representation of meaning. In 1960s psychosemantic studies became popular after Osgood's massive cross-cultural studies using his Semantic differential (SD) method that used thousands of nouns and adjective bipolar scales. A specific form of the SD, Projective Semantics method  uses only most common and neutral nouns that correspond to the 7 groups (factors) of adjective-scales most consistently found in cross-cultural studies (Evaluation, Potency, Activity as found by Osgood, and Reality, Organization, Complexity, Limitation as found in other studies). In this method, seven groups of bipolar adjective scales corresponded to seven types of nouns so the method was thought to have the object-scale symmetry (OSS) between the scales and nouns for evaluation using these scales. For example, the nouns corresponding to the listed 7 factors would be: Beauty, Power, Motion, Life, Work, Chaos. Beauty was expected to be assessed unequivocally as “very good” on adjectives of Evaluation-related scales, Life as “very real” on Reality-related scales, etc. However, deviations in this symmetric and very basic matrix might show underlying biases of two types: scales-related bias and objects-related bias. This OSS design meant to increase the sensitivity of the SD method to any semantic biases in responses of people within the same culture and educational background. 



In linguistics, semantics is the subfield that is devoted to the study of meaning, as inherent at the levels of words, phrases, sentences, and larger units of discourse (termed texts, or narratives). The study of semantics is also closely linked to the subjects of representation, reference and denotation. The basic study of semantics is oriented to the examination of the meaning of signs, and the study of relations between different linguistic units and compounds: homonymy, synonymy, antonymy, hypernymy, hyponymy, meronymy, metonymy, holonymy, paronyms. A key concern is how meaning attaches to larger chunks of text, possibly as a result of the composition from smaller units of meaning. Traditionally, semantics has included the study of sense and denotative reference, truth conditions, argument structure, thematic roles, discourse analysis, and the linkage of all of these to syntax.



In the late 1960s, Richard Montague proposed a system for defining semantic entries in the lexicon in terms of the lambda calculus. In these terms, the syntactic parse of the sentence John ate every bagel would consist of a subject (John) and a predicate (ate every bagel); Montague demonstrated that the meaning of the sentence altogether could be decomposed into the meanings of its parts and in relatively few rules of combination. The logical predicate thus obtained would be elaborated further, e.g. using truth theory models, which ultimately relate meanings to a set of Tarskiian universals, which may lie outside the logic. The notion of such meaning atoms or primitives is basic to the language of thought hypothesis from the 1970s.
Despite its elegance, Montague grammar was limited by the context-dependent variability in word sense, and led to several attempts at incorporating context, such as:
Situation semantics (1980s): truth-values are incomplete, they get assigned based on context
Generative lexicon (1990s): categories (types) are incomplete, and get assigned based on context



In Chomskyan linguistics there was no mechanism for the learning of semantic relations, and the nativist view considered all semantic notions as inborn. Thus, even novel concepts were proposed to have been dormant in some sense. This view was also thought unable to address many issues such as metaphor or associative meanings, and semantic change, where meanings within a linguistic community change over time, and qualia or subjective experience. Another issue not addressed by the nativist model was how perceptual cues are combined in thought, e.g. in mental rotation.
This view of semantics, as an innate finite meaning inherent in a lexical unit that can be composed to generate meanings for larger chunks of discourse, is now being fiercely debated in the emerging domain of cognitive linguistics and also in the non-Fodorian camp in philosophy of language. The challenge is motivated by:
factors internal to language, such as the problem of resolving indexical or anaphora (e.g. this x, him, last week). In these situations context serves as the input, but the interpreted utterance also modifies the context, so it is also the output. Thus, the interpretation is necessarily dynamic and the meaning of sentences is viewed as contexts changing potentials instead of propositions.
factors external to language, i.e. language is not a set of labels stuck on things, but "a toolbox, the importance of whose elements lie in the way they function rather than their attachments to things." This view reflects the position of the later Wittgenstein and his famous game example, and is related to the positions of Quine, Davidson, and others.
A concrete example of the latter phenomenon is semantic underspecification – meanings are not complete without some elements of context. To take an example of one word, red, its meaning in a phrase such as red book is similar to many other usages, and can be viewed as compositional. However, the colours implied in phrases such as red wine (very dark), and red hair (coppery), or red soil, or red skin are very different. Indeed, these colours by themselves would not be called red by native speakers. These instances are contrastive, so red wine is so called only in comparison with the other kind of wine (which also is not white for the same reasons). This view goes back to de Saussure:

Each of a set of synonyms like redouter ('to dread'), craindre ('to fear'), avoir peur ('to be afraid') has its particular value only because they stand in contrast with one another. No word has a value that can be identified independently of what else is in its vicinity.

and may go back to earlier Indian views on language, especially the Nyaya view of words as indicators and not carriers of meaning.
An attempt to defend a system based on propositional meaning for semantic underspecification can be found in the generative lexicon model of James Pustejovsky, who extends contextual operations (based on type shifting) into the lexicon. Thus meanings are generated "on the fly" (as you go), based on finite context.



Another set of concepts related to fuzziness in semantics is based on prototypes. The work of Eleanor Rosch in the 1970s led to a view that natural categories are not characterizable in terms of necessary and sufficient conditions, but are graded (fuzzy at their boundaries) and inconsistent as to the status of their constituent members. One may compare it with Jung's archetype, though the concept of archetype sticks to static concept. Some post-structuralists are against the fixed or static meaning of the words. Derrida, following Nietzsche, talked about slippages in fixed meanings.
Systems of categories are not objectively out there in the world but are rooted in people's experience. These categories evolve as learned concepts of the world – meaning is not an objective truth, but a subjective construct, learned from experience, and language arises out of the "grounding of our conceptual systems in shared embodiment and bodily experience". A corollary of this is that the conceptual categories (i.e. the lexicon) will not be identical for different cultures, or indeed, for every individual in the same culture. This leads to another debate (see the Sapir–Whorf hypothesis or Eskimo words for snow).







Originates from Montague's work (see above). A highly formalized theory of natural language semantics in which expressions are assigned denotations (meanings) such as individuals, truth values, or functions from one of these to another. The truth of a sentence, and more interestingly, its logical relation to other sentences, is then evaluated relative to a model.




Pioneered by the philosopher Donald Davidson, another formalized theory, which aims to associate each natural language sentence with a meta-language description of the conditions under which it is true, for example: 'Snow is white' is true if and only if snow is white. The challenge is to arrive at the truth conditions for any sentences from fixed meanings assigned to the individual words and fixed rules for how to combine them. In practice, truth-conditional semantics is similar to model-theoretic semantics; conceptually, however, they differ in that truth-conditional semantics seeks to connect language with statements about the real world (in the form of meta-language statements), rather than with abstract models.




This theory is an effort to explain properties of argument structure. The assumption behind this theory is that syntactic properties of phrases reflect the meanings of the words that head them. With this theory, linguists can better deal with the fact that subtle differences in word meaning correlate with other differences in the syntactic structure that the word appears in. The way this is gone about is by looking at the internal structure of words. These small parts that make up the internal structure of words are termed semantic primitives.




A linguistic theory that investigates word meaning. This theory understands that the meaning of a word is fully reflected by its context. Here, the meaning of a word is constituted by its contextual relations. Therefore, a distinction between degrees of participation as well as modes of participation are made. In order to accomplish this distinction any part of a sentence that bears a meaning and combines with the meanings of other constituents is labeled as a semantic constituent. Semantic constituents that cannot be broken down into more elementary constituents are labeled minimal semantic constituents.




Computational semantics is focused on the processing of linguistic meaning. In order to do this concrete algorithms and architectures are described. Within this framework the algorithms and architectures are also analyzed in terms of decidability, time/space complexity, data structures they require and communication protocols.




In computer science, the term semantics refers to the meaning of languages, as opposed to their form (syntax). According to Euzenat, semantics "provides the rules for interpreting the syntax which do not provide the meaning directly but constrains the possible interpretations of what is declared." In other words, semantics is about interpretation of an expression. Additionally, the term is applied to certain types of data structures specifically designed and used for representing information content.



The semantics of programming languages and other languages is an important issue and area of study in computer science. Like the syntax of a language, its semantics can be defined exactly.
For instance, the following statements use different syntaxes, but cause the same instructions to be executed:
Generally these operations would all perform an arithmetical addition of 'y' to 'x' and store the result in a variable called 'x'.
Various ways have been developed to describe the semantics of programming languages formally, building on mathematical logic:
Operational semantics: The meaning of a construct is specified by the computation it induces when it is executed on a machine. In particular, it is of interest how the effect of a computation is produced.
Denotational semantics: Meanings are modelled by mathematical objects that represent the effect of executing the constructs. Thus only the effect is of interest, not how it is obtained.
Axiomatic semantics: Specific properties of the effect of executing the constructs are expressed as assertions. Thus there may be aspects of the executions that are ignored.



Terms such as semantic network and semantic data model are used to describe particular types of data model characterized by the use of directed graphs in which the vertices denote concepts or entities in the world, and the arcs denote relationships between them.
The Semantic Web refers to the extension of the World Wide Web via embedding added semantic metadata, using semantic data modelling techniques such as Resource Description Framework (RDF) and Web Ontology Language (OWL).



In psychology, semantic memory is memory for meaning – in other words, the aspect of memory that preserves only the gist, the general significance, of remembered experience – while episodic memory is memory for the ephemeral details – the individual features, or the unique particulars of experience. The term 'episodic memory' was introduced by Tulving and Schacter in the context of 'declarative memory' which involved simple association of factual or objective information concerning its object. Word meaning is measured by the company they keep, i.e. the relationships among words themselves in a semantic network. The memories may be transferred intergenerationally or isolated in one generation due to a cultural disruption. Different generations may have different experiences at similar points in their own time-lines. This may then create a vertically heterogeneous semantic net for certain words in an otherwise homogeneous culture. In a network created by people analyzing their understanding of the word (such as Wordnet) the links and decomposition structures of the network are few in number and kind, and include part of, kind of, and similar links. In automated ontologies the links are computed vectors without explicit meaning. Various automated technologies are being developed to compute the meaning of words: latent semantic indexing and support vector machines as well as natural language processing, neural networks and predicate calculus techniques.
Ideasthesia is a psychological phenomenon in which activation of concepts evokes sensory experiences. For example, in synesthesia, activation of a concept of a letter (e.g., that of the letter A) evokes sensory-like experiences (e.g., of red color).






















semanticsarchive.net
Teaching page for A-level semantics
Chomsky, Noam; On Referring, Harvard University, 30 October 2007 (video)
Jackendoff, Ray; Conceptual Semantics, Harvard University, 13 November 2007 (video)
Semantics: an interview with Jerry Fodor (ReVEL, vol. 5, no. 8 (2007))Physics (from Ancient Greek: φυσική (ἐπιστήμη) phusikḗ (epistḗmē) "knowledge of nature", from φύσις phúsis "nature") is the natural science that involves the study of matter and its motion and behavior through space and time, along with related concepts such as energy and force. One of the most fundamental scientific disciplines, the main goal of physics is to understand how the universe behaves.
Physics is one of the oldest academic disciplines, perhaps the oldest through its inclusion of astronomy. Over the last two millennia, physics was a part of natural philosophy along with chemistry, biology, and certain branches of mathematics, but during the scientific revolution in the 17th century, the natural sciences emerged as unique research programs in their own right. Physics intersects with many interdisciplinary areas of research, such as biophysics and quantum chemistry, and the boundaries of physics are not rigidly defined. New ideas in physics often explain the fundamental mechanisms of other sciences while opening new avenues of research in areas such as mathematics and philosophy.
Physics also makes significant contributions through advances in new technologies that arise from theoretical breakthroughs. For example, advances in the understanding of electromagnetism or nuclear physics led directly to the development of new products that have dramatically transformed modern-day society, such as television, computers, domestic appliances, and nuclear weapons; advances in thermodynamics led to the development of industrialization, and advances in mechanics inspired the development of calculus.
The United Nations named 2005 the World Year of Physics.







Astronomy is the oldest of the natural sciences. The earliest civilizations dating back to beyond 3000 BCE, such as the Sumerians, ancient Egyptians, and the Indus Valley Civilization, all had a predictive knowledge and a basic understanding of the motions of the Sun, Moon, and stars. The stars and planets were often a target of worship, believed to represent their gods. While the explanations for these phenomena were often unscientific and lacking in evidence, these early observations laid the foundation for later astronomy.
According to Asger Aaboe, the origins of Western astronomy can be found in Mesopotamia, and all Western efforts in the exact sciences are descended from late Babylonian astronomy. Egyptian astronomers left monuments showing knowledge of the constellations and the motions of the celestial bodies, while Greek poet Homer wrote of various celestial objects in his Iliad and Odyssey; later Greek astronomers provided names, which are still used today, for most constellations visible from the northern hemisphere.




Natural philosophy has its origins in Greece during the Archaic period, (650 BCE – 480 BCE), when pre-Socratic philosophers like Thales rejected non-naturalistic explanations for natural phenomena and proclaimed that every event had a natural cause. They proposed ideas verified by reason and observation, and many of their hypotheses proved successful in experiment; for example, atomism was found to be correct approximately 2000 years after it was first proposed by Leucippus and his pupil Democritus.




Islamic scholarship had inherited Aristotelian physics from the Greeks and during the Islamic Golden Age developed it further, especially placing emphasis on observation and a priori reasoning, developing early forms of the scientific method.
The most notable innovations were in the field of optics and vision, which came from the works of many scientists like Ibn Sahl, Al-Kindi, Ibn al-Haytham, Al-Farisi and Avicenna. The most notable work was The Book of Optics (also known as Kitāb al-Manāẓir), written by Ibn Al-Haitham, in which he was not only the first to disprove the ancient Greek idea about vision, but also came up with a new theory. In the book, he was also the first to study the phenomenon of the pinhole camera and delved further into the way the eye itself works. Using dissections and the knowledge of previous scholars, he was able to begin to explain how light enters the eye, is focused, and is projected to the back of the eye: and built then the world's first camera obscura hundreds of years before the modern development of photography.

The seven-volume Book of Optics (Kitab al-Manathir) hugely influenced thinking across disciplines from the theory of visual perception to the nature of perspective in medieval art, in both the East and the West, for more than 600 years. Many later European scholars and fellow polymaths, from Robert Grosseteste and Leonardo da Vinci to René Descartes, Johannes Kepler and Isaac Newton, were in his debt. Indeed, the influence of Ibn al-Haytham's Optics ranks alongside that of Newton's work of the same title, published 700 years later.
The translation of The Book of Optics had a huge impact on Europe. From it, later European scholars were able to build the same devices as what Ibn al-Haytham did, and understand the way light works. From this, such important things as eyeglasses, magnifying glasses, telescopes, and cameras were developed.




Physics became a separate science when early modern Europeans used experimental and quantitative methods to discover what are now considered to be the laws of physics.
Major developments in this period include the replacement of the geocentric model of the solar system with the heliocentric Copernican model, the laws governing the motion of planetary bodies determined by Johannes Kepler between 1609 and 1619, pioneering work on telescopes and observational astronomy by Galileo Galilei in the 16th and 17th Centuries, and Isaac Newton's discovery and unification of the laws of motion and universal gravitation that would come to bear his name. Newton also developed calculus, the mathematical study of change, which provided new mathematical methods for solving physical problems.
The discovery of new laws in thermodynamics, chemistry, and electromagnetics resulted from greater research efforts during the Industrial Revolution as energy needs increased. The laws comprising classical physics remain very widely used for objects on everyday scales travelling at non-relativistic speeds, since they provide a very close approximation in such situations, and theories such as quantum mechanics and the theory of relativity simplify to their classical equivalents at such scales. However, inaccuracies in classical mechanics for very small objects and very high velocities led to the development of modern physics in the 20th century.




Modern physics began in the early 20th century with the work of Max Planck in quantum theory and Albert Einstein's theory of relativity. Both of these theories came about due to inaccuracies in classical mechanics in certain situations. Classical mechanics predicted a varying speed of light, which could not be resolved with the constant speed predicted by Maxwell's equations of electromagnetism; this discrepancy was corrected by Einstein's theory of special relativity, which replaced classical mechanics for fast-moving bodies and allowed for a constant speed of light. Black body radiation provided another problem for classical physics, which was corrected when Planck proposed that the excitation of material oscillators is possible only in discrete steps proportional to their frequency; this, along with the photoelectric effect and a complete theory predicting discrete energy levels of electron orbitals, led to the theory of quantum mechanics taking over from classical physics at very small scales.
Quantum mechanics would come to be pioneered by Werner Heisenberg, Erwin Schrödinger and Paul Dirac. From this early work, and work in related fields, the Standard Model of particle physics was derived. Following the discovery of a particle with properties consistent with the Higgs boson at CERN in 2012, all fundamental particles predicted by the standard model, and no others, appear to exist; however, physics beyond the Standard Model, with theories such as supersymmetry, is an active area of research. Areas of mathematics in general are important to this field, such as the study of probabilities and groups.




In many ways, physics stems from ancient Greek philosophy. From Thales' first attempt to characterise matter, to Democritus' deduction that matter ought to reduce to an invariant state, the Ptolemaic astronomy of a crystalline firmament, and Aristotle's book Physics (an early book on physics, which attempted to analyze and define motion from a philosophical point of view), various Greek philosophers advanced their own theories of nature. Physics was known as natural philosophy until the late 18th century.
By the 19th century, physics was realised as a discipline distinct from philosophy and the other sciences. Physics, as with the rest of science, relies on philosophy of science and its "scientific method" to advance our knowledge of the physical world. The scientific method employs a priori reasoning as well as a posteriori reasoning and the use of Bayesian inference to measure the validity of a given theory.
The development of physics has answered many questions of early philosophers, but has also raised new questions. Study of the philosophical issues surrounding physics, the philosophy of physics, involves issues such as the nature of space and time, determinism, and metaphysical outlooks such as empiricism, naturalism and realism.
Many physicists have written about the philosophical implications of their work, for instance Laplace, who championed causal determinism, and Erwin Schrödinger, who wrote on quantum mechanics. The mathematical physicist Roger Penrose has been called a Platonist by Stephen Hawking, a view Penrose discusses in his book, The Road to Reality. Hawking refers to himself as an "unashamed reductionist" and takes issue with Penrose's views.




Though physics deals with a wide variety of systems, certain theories are used by all physicists. Each of these theories were experimentally tested numerous times and found to be an adequate approximation of nature. For instance, the theory of classical mechanics accurately describes the motion of objects, provided they are much larger than atoms and moving at much less than the speed of light. These theories continue to be areas of active research today. Chaos theory, a remarkable aspect of classical mechanics was discovered in the 20th century, three centuries after the original formulation of classical mechanics by Isaac Newton (1642–1727).
These central theories are important tools for research into more specialised topics, and any physicist, regardless of their specialisation, is expected to be literate in them. These include classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, and special relativity.




Classical physics includes the traditional branches and topics that were recognised and well-developed before the beginning of the 20th century—classical mechanics, acoustics, optics, thermodynamics, and electromagnetism. Classical mechanics is concerned with bodies acted on by forces and bodies in motion and may be divided into statics (study of the forces on a body or bodies not subject to an acceleration), kinematics (study of motion without regard to its causes), and dynamics (study of motion and the forces that affect it); mechanics may also be divided into solid mechanics and fluid mechanics (known together as continuum mechanics), the latter include such branches as hydrostatics, hydrodynamics, aerodynamics, and pneumatics. Acoustics is the study of how sound is produced, controlled, transmitted and received. Important modern branches of acoustics include ultrasonics, the study of sound waves of very high frequency beyond the range of human hearing; bioacoustics, the physics of animal calls and hearing, and electroacoustics, the manipulation of audible sound waves using electronics.
Optics, the study of light, is concerned not only with visible light but also with infrared and ultraviolet radiation, which exhibit all of the phenomena of visible light except visibility, e.g., reflection, refraction, interference, diffraction, dispersion, and polarization of light. Heat is a form of energy, the internal energy possessed by the particles of which a substance is composed; thermodynamics deals with the relationships between heat and other forms of energy. Electricity and magnetism have been studied as a single branch of physics since the intimate connection between them was discovered in the early 19th century; an electric current gives rise to a magnetic field, and a changing magnetic field induces an electric current. Electrostatics deals with electric charges at rest, electrodynamics with moving charges, and magnetostatics with magnetic poles at rest.




Classical physics is generally concerned with matter and energy on the normal scale of observation, while much of modern physics is concerned with the behavior of matter and energy under extreme conditions or on a very large or very small scale. For example, atomic and nuclear physics studies matter on the smallest scale at which chemical elements can be identified. The physics of elementary particles is on an even smaller scale since it is concerned with the most basic units of matter; this branch of physics is also known as high-energy physics because of the extremely high energies necessary to produce many types of particles in particle accelerators. On this scale, ordinary, commonsense notions of space, time, matter, and energy are no longer valid.
The two chief theories of modern physics present a different picture of the concepts of space, time, and matter from that presented by classical physics. Classical mechanics approximates nature as continuous, while quantum theory is concerned with the discrete nature of many phenomena at the atomic and subatomic level and with the complementary aspects of particles and waves in the description of such phenomena. The theory of relativity is concerned with the description of phenomena that take place in a frame of reference that is in motion with respect to an observer; the special theory of relativity is concerned with relative uniform motion in a straight line and the general theory of relativity with accelerated motion and its connection with gravitation. Both quantum theory and the theory of relativity find applications in all areas of modern physics.




While physics aims to discover universal laws, its theories lie in explicit domains of applicability. Loosely speaking, the laws of classical physics accurately describe systems whose important length scales are greater than the atomic scale and whose motions are much slower than the speed of light. Outside of this domain, observations do not match predictions provided by classical mechanics. Albert Einstein contributed the framework of special relativity, which replaced notions of absolute time and space with spacetime and allowed an accurate description of systems whose components have speeds approaching the speed of light. Max Planck, Erwin Schrödinger, and others introduced quantum mechanics, a probabilistic notion of particles and interactions that allowed an accurate description of atomic and subatomic scales. Later, quantum field theory unified quantum mechanics and special relativity. General relativity allowed for a dynamical, curved spacetime, with which highly massive systems and the large-scale structure of the universe can be well-described. General relativity has not yet been unified with the other fundamental descriptions; several candidate theories of quantum gravity are being developed.






Mathematics provides a compact and exact language used to describe of the order in nature. This was noted and advocated by Pythagoras, Plato, Galileo, and Newton.
Physics uses mathematics to organise and formulate experimental results. From those results, precise or estimated solutions, quantitative results from which new predictions can be made and experimentally confirmed or negated. The results from physics experiments are numerical measurements. Technologies based on mathematics, like computation have made computational physics an active area of research.

Ontology is a prerequisite for physics, but not for mathematics. It means physics is ultimately concerned with descriptions of the real world, while mathematics is concerned with abstract patterns, even beyond the real world. Thus physics statements are synthetic, while mathematical statements are analytic. Mathematics contains hypotheses, while physics contains theories. Mathematics statements have to be only logically true, while predictions of physics statements must match observed and experimental data.
The distinction is clear-cut, but not always obvious. For example, mathematical physics is the application of mathematics in physics. Its methods are mathematical, but its subject is physical. The problems in this field start with a "mathematical model of a physical situation" (system) and a "mathematical description of a physical law" that will be applied to that system. Every mathematical statement used for solving has a hard-to-find physical meaning. The final mathematical solution has an easier-to-find meaning, because it is what the solver is looking for.
Physics is a branch of fundamental science, not practical science. Physics is also called "the fundamental science" because the subject of study of all branches of natural science like chemistry, astronomy, geology, and biology are constrained by laws of physics, similar to how chemistry is often called the central science because of its role in linking the physical sciences. For example, chemistry studies properties, structures, and reactions of matter (chemistry's focus on the atomic scale distinguishes it from physics). Structures are formed because particles exert electrical forces on each other, properties include physical characteristics of given substances, and reactions are bound by laws of physics, like conservation of energy, mass, and charge.
Physics is applied in industries like engineering and medicine.




Applied physics is a general term for physics research which is intended for a particular use. An applied physics curriculum usually contains a few classes in an applied discipline, like geology or electrical engineering. It usually differs from engineering in that an applied physicist may not be designing something in particular, but rather is using physics or conducting physics research with the aim of developing new technologies or solving a problem.
The approach is similar to that of applied mathematics. Applied physicists use physics in scientific research. For instance, people working on accelerator physics might seek to build better particle detectors for research in theoretical physics.
Physics is used heavily in engineering. For example, statics, a subfield of mechanics, is used in the building of bridges and other static structures. The understanding and use of acoustics results in sound control and better concert halls; similarly, the use of optics creates better optical devices. An understanding of physics makes for more realistic flight simulators, video games, and movies, and is often critical in forensic investigations.
With the standard consensus that the laws of physics are universal and do not change with time, physics can be used to study things that would ordinarily be mired in uncertainty. For example, in the study of the origin of the earth, one can reasonably model earth's mass, temperature, and rate of rotation, as a function of time allowing one to extrapolate forward or backward in time and so predict future or prior events. It also allows for simulations in engineering which drastically speed up the development of a new technology.
But there is also considerable interdisciplinarity in the physicist's methods, so many other important fields are influenced by physics (e.g., the fields of econophysics and sociophysics).






Physicists use the scientific method to test the validity of a physical theory. By using a methodical approach to compare the implications of a theory with the conclusions drawn from its related experiments and observations, physicists are better able to test the validity of a theory in a logical, unbiased, and repeatable way. To that end, experiments are performed and observations are made in order to determine the validity or invalidity of the theory.
A scientific law is a concise verbal or mathematical statement of a relation which expresses a fundamental principle of some theory, such as Newton's law of universal gravitation.




Theorists seek to develop mathematical models that both agree with existing experiments and successfully predict future experimental results, while experimentalists devise and perform experiments to test theoretical predictions and explore new phenomena. Although theory and experiment are developed separately, they are strongly dependent upon each other. Progress in physics frequently comes about when experimentalists make a discovery that existing theories cannot explain, or when new theories generate experimentally testable predictions, which inspire new experiments.
Physicists who work at the interplay of theory and experiment are called phenomenologists, who study complex phenomena observed in experiment and work to relate them to a fundamental theory.
Theoretical physics has historically taken inspiration from philosophy; electromagnetism was unified this way. Beyond the known universe, the field of theoretical physics also deals with hypothetical issues, such as parallel universes, a multiverse, and higher dimensions. Theorists invoke these ideas in hopes of solving particular problems with existing theories. They then explore the consequences of these ideas and work toward making testable predictions.
Experimental physics expands, and is expanded by, engineering and technology. Experimental physicists involved in basic research design and perform experiments with equipment such as particle accelerators and lasers, whereas those involved in applied research often work in industry developing technologies such as magnetic resonance imaging (MRI) and transistors. Feynman has noted that experimentalists may seek areas which are not well-explored by theorists.




Physics covers a wide range of phenomena, from elementary particles (such as quarks, neutrinos, and electrons) to the largest superclusters of galaxies. Included in these phenomena are the most basic objects composing all other things. Therefore, physics is sometimes called the "fundamental science". Physics aims to describe the various phenomena that occur in nature in terms of simpler phenomena. Thus, physics aims to both connect the things observable to humans to root causes, and then connect these causes together.
For example, the ancient Chinese observed that certain rocks (lodestone and magnetite) were attracted to one another by an invisible force. This effect was later called magnetism, which was first rigorously studied in the 17th century. But even before the Chinese discovered magnetism, the ancient Greeks knew of other objects such as amber, that when rubbed with fur would cause a similar invisible attraction between the two. This was also first studied rigorously in the 17th century and came to be called electricity. Thus, physics had come to understand two observations of nature in terms of some root cause (electricity and magnetism). However, further work in the 19th century revealed that these two forces were just two different aspects of one force—electromagnetism. This process of "unifying" forces continues today, and electromagnetism and the weak nuclear force are now considered to be two aspects of the electroweak interaction. Physics hopes to find an ultimate reason (Theory of Everything) for why nature is as it is (see section Current research below for more information).



Contemporary research in physics can be broadly divided into particle physics; condensed matter physics; atomic, molecular, and optical physics; astrophysics; and applied physics. Some physics departments also support physics education research and physics outreach.
Since the 20th century, the individual fields of physics have become increasingly specialised, and today most physicists work in a single field for their entire careers. "Universalists" such as Albert Einstein (1879–1955) and Lev Landau (1908–1968), who worked in multiple fields of physics, are now very rare.
The major fields of physics, along with their subfields and the theories and concepts they employ, are shown in the following table.




Particle physics is the study of the elementary constituents of matter and energy and the interactions between them. In addition, particle physicists design and develop the high energy accelerators, detectors, and computer programs necessary for this research. The field is also called "high-energy physics" because many elementary particles do not occur naturally but are created only during high-energy collisions of other particles.
Currently, the interactions of elementary particles and fields are described by the Standard Model. The model accounts for the 12 known particles of matter (quarks and leptons) that interact via the strong, weak, and electromagnetic fundamental forces. Dynamics are described in terms of matter particles exchanging gauge bosons (gluons, W and Z bosons, and photons, respectively). The Standard Model also predicts a particle known as the Higgs boson. In July 2012 CERN, the European laboratory for particle physics, announced the detection of a particle consistent with the Higgs boson, an integral part of a Higgs mechanism.
Nuclear physics is the field of physics that studies the constituents and interactions of atomic nuclei. The most commonly known applications of nuclear physics are nuclear power generation and nuclear weapons technology, but the research has provided application in many fields, including those in nuclear medicine and magnetic resonance imaging, ion implantation in materials engineering, and radiocarbon dating in geology and archaeology.




Atomic, molecular, and optical physics (AMO) is the study of matter–matter and light–matter interactions on the scale of single atoms and molecules. The three areas are grouped together because of their interrelationships, the similarity of methods used, and the commonality of their relevant energy scales. All three areas include both classical, semi-classical and quantum treatments; they can treat their subject from a microscopic view (in contrast to a macroscopic view).
Atomic physics studies the electron shells of atoms. Current research focuses on activities in quantum control, cooling and trapping of atoms and ions, low-temperature collision dynamics and the effects of electron correlation on structure and dynamics. Atomic physics is influenced by the nucleus (see, e.g., hyperfine splitting), but intra-nuclear phenomena such as fission and fusion are considered part of high-energy physics.
Molecular physics focuses on multi-atomic structures and their internal and external interactions with matter and light. Optical physics is distinct from optics in that it tends to focus not on the control of classical light fields by macroscopic objects but on the fundamental properties of optical fields and their interactions with matter in the microscopic realm.




Condensed matter physics is the field of physics that deals with the macroscopic physical properties of matter. In particular, it is concerned with the "condensed" phases that appear whenever the number of particles in a system is extremely large and the interactions between them are strong.
The most familiar examples of condensed phases are solids and liquids, which arise from the bonding by way of the electromagnetic force between atoms. More exotic condensed phases include the superfluid and the Bose–Einstein condensate found in certain atomic systems at very low temperature, the superconducting phase exhibited by conduction electrons in certain materials, and the ferromagnetic and antiferromagnetic phases of spins on atomic lattices.
Condensed matter physics is the largest field of contemporary physics. Historically, condensed matter physics grew out of solid-state physics, which is now considered one of its main subfields. The term condensed matter physics was apparently coined by Philip Anderson when he renamed his research group—previously solid-state theory—in 1967. In 1978, the Division of Solid State Physics of the American Physical Society was renamed as the Division of Condensed Matter Physics. Condensed matter physics has a large overlap with chemistry, materials science, nanotechnology and engineering.




Astrophysics and astronomy are the application of the theories and methods of physics to the study of stellar structure, stellar evolution, the origin of the Solar System, and related problems of cosmology. Because astrophysics is a broad subject, astrophysicists typically apply many disciplines of physics, including mechanics, electromagnetism, statistical mechanics, thermodynamics, quantum mechanics, relativity, nuclear and particle physics, and atomic and molecular physics.
The discovery by Karl Jansky in 1931 that radio signals were emitted by celestial bodies initiated the science of radio astronomy. Most recently, the frontiers of astronomy have been expanded by space exploration. Perturbations and interference from the earth's atmosphere make space-based observations necessary for infrared, ultraviolet, gamma-ray, and X-ray astronomy.
Physical cosmology is the study of the formation and evolution of the universe on its largest scales. Albert Einstein's theory of relativity plays a central role in all modern cosmological theories. In the early 20th century, Hubble's discovery that the universe is expanding, as shown by the Hubble diagram, prompted rival explanations known as the steady state universe and the Big Bang.
The Big Bang was confirmed by the success of Big Bang nucleosynthesis and the discovery of the cosmic microwave background in 1964. The Big Bang model rests on two theoretical pillars: Albert Einstein's general relativity and the cosmological principle. Cosmologists have recently established the ΛCDM model of the evolution of the universe, which includes cosmic inflation, dark energy, and dark matter.
Numerous possibilities and discoveries are anticipated to emerge from new data from the Fermi Gamma-ray Space Telescope over the upcoming decade and vastly revise or clarify existing models of the universe. In particular, the potential for a tremendous discovery surrounding dark matter is possible over the next several years. Fermi will search for evidence that dark matter is composed of weakly interacting massive particles, complementing similar experiments with the Large Hadron Collider and other underground detectors.
IBEX is already yielding new astrophysical discoveries: "No one knows what is creating the ENA (energetic neutral atoms) ribbon" along the termination shock of the solar wind, "but everyone agrees that it means the textbook picture of the heliosphere—in which the Solar System's enveloping pocket filled with the solar wind's charged particles is plowing through the onrushing 'galactic wind' of the interstellar medium in the shape of a comet—is wrong."




Research in physics is continually progressing on a large number of fronts.
In condensed matter physics, an important unsolved theoretical problem is that of high-temperature superconductivity. Many condensed matter experiments are aiming to fabricate workable spintronics and quantum computers.
In particle physics, the first pieces of experimental evidence for physics beyond the Standard Model have begun to appear. Foremost among these are indications that neutrinos have non-zero mass. These experimental results appear to have solved the long-standing solar neutrino problem, and the physics of massive neutrinos remains an area of active theoretical and experimental research. Large Hadron Collider had already found the Higgs Boson. Future research aims to prove or disprove the supersymmetry, which extends the Standard Model of particle physics. The research on dark matter and dark energy is also on the agenda.
Theoretical attempts to unify quantum mechanics and general relativity into a single theory of quantum gravity, a program ongoing for over half a century, have not yet been decisively resolved. The current leading candidates are M-theory, superstring theory and loop quantum gravity.
Many astronomical and cosmological phenomena have yet to be satisfactorily explained, including the existence of ultra-high energy cosmic rays, the baryon asymmetry, the acceleration of the universe and the anomalous rotation rates of galaxies.
Although much progress has been made in high-energy, quantum, and astronomical physics, many everyday phenomena involving complexity, chaos, or turbulence are still poorly understood. Complex problems that seem like they could be solved by a clever application of dynamics and mechanics remain unsolved; examples include the formation of sandpiles, nodes in trickling water, the shape of water droplets, mechanisms of surface tension catastrophes, and self-sorting in shaken heterogeneous collections.
These complex phenomena have received growing attention since the 1970s for several reasons, including the availability of modern mathematical methods and computers, which enabled complex systems to be modeled in new ways. Complex physics has become part of increasingly interdisciplinary research, as exemplified by the study of turbulence in aerodynamics and the observation of pattern formation in biological systems. In the 1932 Annual Review of Fluid Mechanics, Horace Lamb said:

I am an old man now, and when I die and go to heaven there are two matters on which I hope for enlightenment. One is quantum electrodynamics, and the other is the turbulent motion of fluids. And about the former I am rather optimistic.















Peter Woit (January 2017). Fake Physics,



General
Encyclopedia of Physics at Scholarpedia
de Haas, Paul, Historic Papers in Physics (20th Century) at the Wayback Machine (archived 26 August 2009)
PhysicsCentral – Web portal run by the American Physical Society
Physics.org – Web portal run by the Institute of Physics
The Skeptic's Guide to Physics
Usenet Physics FAQ – A FAQ compiled by sci.physics and other physics newsgroups
Website of the Nobel Prize in physics
World of Physics An online encyclopedic dictionary of physics
Nature: Physics
Physics announced 17 July 2008 by the American Physical Society
Physics/Publications at DMOZ
Physicsworld.com – News website from Institute of Physics Publishing
Physics Central – includes articles on astronomy, particle physics, and mathematics.
The Vega Science Trust – science videos, including physics
Video: Physics "Lightning" Tour with Justin Morgan
52-part video course: The Mechanical Universe...and Beyond Note: also available at 01 – Introduction at Google Videos
HyperPhysics website – HyperPhysics, a physics and astronomy mind-map from Georgia State University
Organizations
AIP.org – Website of the American Institute of Physics
APS.org – Website of the American Physical Society
IOP.org – Website of the Institute of Physics
PlanetPhysics.org
Royal Society – Although not exclusively a physics institution, it has a strong history of physics
SPS National – Website of the Society of Physics StudentsA particle accelerator is a machine that uses electromagnetic fields to propel charged particles to nearly light speed and to contain them in well-defined beams. Large accelerators are used in particle physics as colliders (e.g. the LHC at CERN, KEKB at KEK in Japan, RHIC at Brookhaven National Laboratory, and Tevatron at Fermilab), or as synchrotron light sources for the study of condensed matter physics. Smaller particle accelerators are used in a wide variety of applications, including particle therapy for oncological purposes, radioisotope production for medical diagnostics, ion implanters for manufacture of semiconductors, and accelerator mass spectrometers for measurements of rare isotopes such as radiocarbon. There are currently more than 30,000 accelerators in operation around the world.
There are two basic classes of accelerators: electrostatic and electrodynamic (or electromagnetic) accelerators.  Electrostatic accelerators use static electric fields to accelerate particles. The most common types are the Cockcroft–Walton generator and the Van de Graaff generator. A small-scale example of this class is the cathode ray tube in an ordinary old television set. The achievable kinetic energy for particles in these devices is determined by the accelerating voltage, which is limited by electrical breakdown. Electrodynamic or electromagnetic accelerators, on the other hand, use changing electromagnetic fields (either magnetic induction or oscillating radio frequency fields) to accelerate particles. Since in these types the particles can pass through the same accelerating field multiple times, the output energy is not limited by the strength of the accelerating field. This class, which was first developed in the 1920s, is the basis for most modern large-scale accelerators.
Rolf Widerøe, Gustav Ising, Leó Szilárd, Max Steenbeck, and Ernest Lawrence are considered pioneers of this field, conceiving and building the first operational linear particle accelerator, the betatron, and the cyclotron.
Because colliders can give evidence of the structure of the subatomic world, accelerators were commonly referred to as atom smashers in the 20th century. Despite the fact that most accelerators (but not ion facilities) actually propel subatomic particles, the term persists in popular usage when referring to particle accelerators in general.




Beams of high-energy particles are useful for both fundamental and applied research in the sciences, and also in many technical and industrial fields unrelated to fundamental research. It has been estimated that there are approximately 30,000 accelerators worldwide. Of these, only about 1% are research machines with energies above 1 GeV, while about 44% are for radiotherapy, 41% for ion implantation, 9% for industrial processing and research, and 4% for biomedical and other low-energy research. The bar graph shows the breakdown of the number of industrial accelerators according to their applications. The numbers are based on 2012 statistics available from various sources, including production and sales data published in presentations or market surveys, and data provided by a number of manufacturers.



For the most basic inquiries into the dynamics and structure of matter, space, and time, physicists seek the simplest kinds of interactions at the highest possible energies. These typically entail particle energies of many GeV, and the interactions of the simplest kinds of particles: leptons (e.g. electrons and positrons) and quarks for the matter, or photons and gluons for the field quanta. Since isolated quarks are experimentally unavailable due to color confinement, the simplest available experiments involve the interactions of, first, leptons with each other, and second, of leptons with nucleons, which are composed of quarks and gluons. To study the collisions of quarks with each other, scientists resort to collisions of nucleons, which at high energy may be usefully considered as essentially 2-body interactions of the quarks and gluons of which they are composed. Thus elementary particle physicists tend to use machines creating beams of electrons, positrons, protons, and antiprotons, interacting with each other or with the simplest nuclei (e.g., hydrogen or deuterium) at the highest possible energies, generally hundreds of GeV or more.
The largest and highest energy particle accelerator used for elementary particle physics is the Large Hadron Collider (LHC) at CERN, operating since 2009.



Nuclear physicists and cosmologists may use beams of bare atomic nuclei, stripped of electrons, to investigate the structure, interactions, and properties of the nuclei themselves, and of condensed matter at extremely high temperatures and densities, such as might have occurred in the first moments of the Big Bang. These investigations often involve collisions of heavy nuclei – of atoms like iron or gold – at energies of several GeV per nucleon. The largest such particle accelerator is the Relativistic Heavy Ion Collider (RHIC) at Brookhaven National Laboratory.
Particle accelerators can also produce proton beams, which can produce proton-rich medical or research isotopes as opposed to the neutron-rich ones made in fission reactors; however, recent work has shown how to make 99Mo, usually made in reactors, by accelerating isotopes of hydrogen, although this method still requires a reactor to produce tritium. An example of this type of machine is LANSCE at Los Alamos.



Besides being of fundamental interest, high energy electrons may be coaxed into emitting extremely bright and coherent beams of high energy photons via synchrotron radiation, which have numerous uses in the study of atomic structure, chemistry, condensed matter physics, biology, and technology. A large number of synchrotron light sources exist worldwide. Examples in the US are SSRL and LCLS at SLAC National Accelerator Laboratory, APS at Argonne National Laboratory, ALS at Lawrence Berkeley National Laboratory, and NSLS at Brookhaven National Laboratory. The ESRF in Grenoble, France has been used to extract detailed 3-dimensional images of insects trapped in amber. Thus there is a great demand for electron accelerators of moderate (GeV) energy and high intensity.



Everyday examples of particle accelerators are cathode ray tubes found in television sets and X-ray generators. These low-energy accelerators use a single pair of electrodes with a DC voltage of a few thousand volts between them. In an X-ray generator, the target itself is one of the electrodes. A low-energy particle accelerator called an ion implanter is used in the manufacture of integrated circuits.
At lower energies, beams of accelerated nuclei are also used in medicine as particle therapy, for the treatment of cancer.
DC accelerator types capable of accelerating particles to speeds sufficient to cause nuclear reactions are Cockcroft-Walton generators or voltage multipliers, which convert AC to high voltage DC, or Van de Graaff generators that use static electricity carried by belts.




Historically, the first accelerators used simple technology of a single static high voltage to accelerate charged particles. The charged particle was accelerated through an evacuated tube with an electrode at either end, with the static potential across it. Since the particle passed only once through the potential difference, the output energy was limited to the accelerating voltage of the machine. While this method is still extremely popular today, with the electrostatic accelerators greatly out-numbering any other type, they are more suited to lower energy studies owing to the practical voltage limit of about 1 MV for air insulated machines, or 30 MV when the accelerator is operated in a tank of pressurized gas with high dielectric strength, such as sulfur hexafluoride. In a tandem accelerator the potential is used twice to accelerate the particles, by reversing the charge of the particles while they are inside the terminal. This is possible with the acceleration of atomic nuclei by using anions (negatively charged ions), and then passing the beam through a thin foil to strip electrons off the anions inside the high voltage terminal, converting them to cations (positively charged ions), which are accelerated again as they leave the terminal.
The two main types of electrostatic accelerator are the Cockcroft-Walton accelerator, which uses a diode-capacitor voltage multiplier to produce high voltage, and the Van de Graaff accelerator, which uses a moving fabric belt to carry charge to the high voltage electrode. Although electrostatic accelerators accelerate particles along a straight line, the term linear accelerator is more often used for accelerators that employ oscillating rather than static electric fields.



Due to the high voltage ceiling imposed by electrical discharge, in order to accelerate particles to higher energies, techniques involving dynamic fields rather than static fields are used. Electrodynamic acceleration can arise from either of two mechanisms: non-resonant magnetic induction, or resonant circuits or cavities excited by oscillating RF fields.  Electrodynamic accelerators can be linear, with particles accelerating in a straight line, or circular, using magnetic fields to bend particles in a roughly circular orbit.



Magnetic induction accelerators accelerate particles by induction from an increasing magnetic field, as if the particles were the secondary winding in a transformer. The increasing magnetic field creates a circulating electric field which can be configured to accelerate the particles. Induction accelerators can be either linear or circular.




Linear induction accelerators utilize ferrite-loaded, non-resonant induction cavities. Each cavity can be thought of as two large washer-shaped disks connected by an outer cylindrical tube. Between the disks is a ferrite toroid. A voltage pulse applied between the two disks causes an increasing magnetic field which inductively couples power into the charged particle beam.
The linear induction accelerator was invented by Christofilos in the 1960s. Linear induction accelerators are capable of accelerating very high beam currents (>1000 A) in a single short pulse. They have been used to generate X-rays for flash radiography (e.g. DARHT at LANL), and have been considered as particle injectors for magnetic confinement fusion and as drivers for free electron lasers.




The Betatron is circular magnetic induction accelerator, invented by Donald Kerst in 1940 for accelerating electrons. The concept originates ultimately from Norwegian-German scientist Rolf Widerøe. These machines, like synchrotrons, use a donut-shaped ring magnet (see below) with a cyclically increasing B field, but accelerate the particles by induction from the increasing magnetic field, as if they were the secondary winding in a transformer, due to the changing magnetic flux through the orbit.
Achieving constant orbital radius while supplying the proper accelerating electric field requires that the magnetic flux linking the orbit be somewhat independent of the magnetic field on the orbit, bending the particles into a constant radius curve. These machines have in practice been limited by the large radiative losses suffered by the electrons moving at nearly the speed of light in a relatively small radius orbit.




In a linear particle accelerator (linac), particles are accelerated in a straight line with a target of interest at one end. They are often used to provide an initial low-energy kick to particles before they are injected into circular accelerators. The longest linac in the world is the Stanford Linear Accelerator, SLAC, which is 3 km (1.9 mi) long. SLAC is an electron-positron collider.
Linear high-energy accelerators use a linear array of plates (or drift tubes) to which an alternating high-energy field is applied. As the particles approach a plate they are accelerated towards it by an opposite polarity charge applied to the plate. As they pass through a hole in the plate, the polarity is switched so that the plate now repels them and they are now accelerated by it towards the next plate. Normally a stream of "bunches" of particles are accelerated, so a carefully controlled AC voltage is applied to each plate to continuously repeat this process for each bunch.
As the particles approach the speed of light the switching rate of the electric fields becomes so high that they operate at radio frequencies, and so microwave cavities are used in higher energy machines instead of simple plates.
Linear accelerators are also widely used in medicine, for radiotherapy and radiosurgery. Medical grade linacs accelerate electrons using a klystron and a complex bending magnet arrangement which produces a beam of 6-30 MeV energy. The electrons can be used directly or they can be collided with a target to produce a beam of X-rays. The reliability, flexibility and accuracy of the radiation beam produced has largely supplanted the older use of cobalt-60 therapy as a treatment tool.



In the circular accelerator, particles move in a circle until they reach sufficient energy. The particle track is typically bent into a circle using electromagnets. The advantage of circular accelerators over linear accelerators (linacs) is that the ring topology allows continuous acceleration, as the particle can transit indefinitely. Another advantage is that a circular accelerator is smaller than a linear accelerator of comparable power (i.e. a linac would have to be extremely long to have the equivalent power of a circular accelerator).
Depending on the energy and the particle being accelerated, circular accelerators suffer a disadvantage in that the particles emit synchrotron radiation. When any charged particle is accelerated, it emits electromagnetic radiation and secondary emissions. As a particle traveling in a circle is always accelerating towards the center of the circle, it continuously radiates towards the tangent of the circle. This radiation is called synchrotron light and depends highly on the mass of the accelerating particle. For this reason, many high energy electron accelerators are linacs. Certain accelerators (synchrotrons) are however built specially for producing synchrotron light (X-rays).
Since the special theory of relativity requires that matter always travels slower than the speed of light in a vacuum, in high-energy accelerators, as the energy increases the particle speed approaches the speed of light as a limit, but never attains it. Therefore, particle physicists do not generally think in terms of speed, but rather in terms of a particle's energy or momentum, usually measured in electron volts (eV). An important principle for circular accelerators, and particle beams in general, is that the curvature of the particle trajectory is proportional to the particle charge and to the magnetic field, but inversely proportional to the (typically relativistic) momentum.




The earliest operational circular accelerators were cyclotrons, invented in 1929 by Ernest O. Lawrence at the University of California, Berkeley. Cyclotrons have a single pair of hollow 'D'-shaped plates to accelerate the particles and a single large dipole magnet to bend their path into a circular orbit. It is a characteristic property of charged particles in a uniform and constant magnetic field B that they orbit with a constant period, at a frequency called the cyclotron frequency, so long as their speed is small compared to the speed of light c. This means that the accelerating D's of a cyclotron can be driven at a constant frequency by a radio frequency (RF) accelerating power source, as the beam spirals outwards continuously. The particles are injected in the centre of the magnet and are extracted at the outer edge at their maximum energy.
Cyclotrons reach an energy limit because of relativistic effects whereby the particles effectively become more massive, so that their cyclotron frequency drops out of synch with the accelerating RF. Therefore, simple cyclotrons can accelerate protons only to an energy of around 15 million electron volts (15 MeV, corresponding to a speed of roughly 10% of c), because the protons get out of phase with the driving electric field. If accelerated further, the beam would continue to spiral outward to a larger radius but the particles would no longer gain enough speed to complete the larger circle in step with the accelerating RF. To accommodate relativistic effects the magnetic field needs to be increased to higher radii like it is done in isochronous cyclotrons. An example of an isochronous cyclotron is the PSI Ring cyclotron in Switzerland, which provides protons at the energy of 590 MeV which corresponds to roughly 80% of the speed of light. The advantage of such a cyclotron is the maximum achievable extracted proton current which is currently 2.2 mA. The energy and current correspond to 1.3 MW beam power which is the highest of any accelerator currently existing.




A classic cyclotron can be modified to increase its energy limit. The historically first approach was the synchrocyclotron, which accelerates the particles in bunches. It uses a constant magnetic field 
  
    
      
        B
      
    
    {\displaystyle B}
  , but reduces the accelerating field's frequency so as to keep the particles in step as they spiral outward, matching their mass-dependent cyclotron resonance frequency. This approach suffers from low average beam intensity due to the bunching, and again from the need for a huge magnet of large radius and constant field over the larger orbit demanded by high energy.
The second approach to the problem of accelerating relativistic particles is the isochronous cyclotron. In such a structure, the accelerating field's frequency (and the cyclotron resonance frequency) is kept constant for all energies by shaping the magnet poles so to increase magnetic field with radius. Thus, all particles get accelerated in isochronous time intervals. Higher energy particles travel a shorter distance in each orbit than they would in a classical cyclotron, thus remaining in phase with the accelerating field. The advantage of the isochronous cyclotron is that it can deliver continuous beams of higher average intensity, which is useful for some applications. The main disadvantages are the size and cost of the large magnet needed, and the difficulty in achieving the high magnetic field values required at the outer edge of the structure.
Synchrocyclotrons have not been built since the isochronous cyclotron was developed.




To reach still higher energies, with relativistic mass approaching or exceeding the rest mass of the particles (for protons, billions of electron volts or GeV), it is necessary to use a synchrotron. This is an accelerator in which the particles are accelerated in a ring of constant radius. An immediate advantage over cyclotrons is that the magnetic field need only be present over the actual region of the particle orbits, which is much narrower than that of the ring. (The largest cyclotron built in the US had a 184-inch-diameter (4.7 m) magnet pole, whereas the diameter of synchrotrons such as the LEP and LHC is nearly 10 km. The aperture of the two beams of the LHC is of the order of a centimeter.)
However, since the particle momentum increases during acceleration, it is necessary to turn up the magnetic field B in proportion to maintain constant curvature of the orbit. In consequence, synchrotrons cannot accelerate particles continuously, as cyclotrons can, but must operate cyclically, supplying particles in bunches, which are delivered to a target or an external beam in beam "spills" typically every few seconds.
Since high energy synchrotrons do most of their work on particles that are already traveling at nearly the speed of light c, the time to complete one orbit of the ring is nearly constant, as is the frequency of the RF cavity resonators used to drive the acceleration.
In modern synchrotrons, the beam aperture is small and the magnetic field does not cover the entire area of the particle orbit as it does for a cyclotron, so several necessary functions can be separated. Instead of one huge magnet, one has a line of hundreds of bending magnets, enclosing (or enclosed by) vacuum connecting pipes. The design of synchrotrons was revolutionized in the early 1950s with the discovery of the strong focusing concept. The focusing of the beam is handled independently by specialized quadrupole magnets, while the acceleration itself is accomplished in separate RF sections, rather similar to short linear accelerators. Also, there is no necessity that cyclic machines be circular, but rather the beam pipe may have straight sections between magnets where beams may collide, be cooled, etc. This has developed into an entire separate subject, called "beam physics" or "beam optics".
More complex modern synchrotrons such as the Tevatron, LEP, and LHC may deliver the particle bunches into storage rings of magnets with a constant magnetic field, where they can continue to orbit for long periods for experimentation or further acceleration. The highest-energy machines such as the Tevatron and LHC are actually accelerator complexes, with a cascade of specialized elements in series, including linear accelerators for initial beam creation, one or more low energy synchrotrons to reach intermediate energy, storage rings where beams can be accumulated or "cooled" (reducing the magnet aperture required and permitting tighter focusing; see beam cooling), and a last large ring for final acceleration and experimentation.




Circular electron accelerators fell somewhat out of favor for particle physics around the time that SLAC's linear particle accelerator was constructed, because their synchrotron losses were considered economically prohibitive and because their beam intensity was lower than for the unpulsed linear machines. The Cornell Electron Synchrotron, built at low cost in the late 1970s, was the first in a series of high-energy circular electron accelerators built for fundamental particle physics, the last being LEP, built at CERN, which was used from 1989 until 2000.
A large number of electron synchrotrons have been built in the past two decades, as part of synchrotron light sources that emit ultraviolet light and X rays; see below.




For some applications, it is useful to store beams of high energy particles for some time (with modern high vacuum technology, up to many hours) without further acceleration. This is especially true for colliding beam accelerators, in which two beams moving in opposite directions are made to collide with each other, with a large gain in effective collision energy. Because relatively few collisions occur at each pass through the intersection point of the two beams, it is customary to first accelerate the beams to the desired energy, and then store them in storage rings, which are essentially synchrotron rings of magnets, with no significant RF power for acceleration.




Some circular accelerators have been built to deliberately generate radiation (called synchrotron light) as X-rays also called synchrotron radiation, for example the Diamond Light Source which has been built at the Rutherford Appleton Laboratory in England or the Advanced Photon Source at Argonne National Laboratory in Illinois, USA. High-energy X-rays are useful for X-ray spectroscopy of proteins or X-ray absorption fine structure (XAFS), for example.
Synchrotron radiation is more powerfully emitted by lighter particles, so these accelerators are invariably electron accelerators. Synchrotron radiation allows for better imaging as researched and developed at SLAC's SPEAR.




Fixed-Field Alternating Gradient accelerators (FFAG)s, in which a very strong radial field gradient, combined with strong focusing, allows the beam to be confined to a narrow ring, are an extension of the isochronous cyclotron idea that is lately under development. They use RF accelerating sections between the magnets, and so are isochronous for relativistic particles like electrons (which achieve essentially the speed of light at only a few MeV), but only over a limited energy range for protons and heavier particles at sub-relativistic energies. Like the isochronous cyclotrons, they achieve continuous beam operation, but without the need for a huge dipole bending magnet covering the entire radius of the orbits.




Ernest Lawrence's first cyclotron was a mere 4 inches (100 mm) in diameter. Later, in 1939, he built a machine with a 60-inch diameter pole face, and planned one with a 184-inch diameter in 1942, which was, however, taken over for World War II-related work connected with uranium isotope separation; after the war it continued in service for research and medicine over many years.
The first large proton synchrotron was the Cosmotron at Brookhaven National Laboratory, which accelerated protons to about 3 GeV (1953–1968). The Bevatron at Berkeley, completed in 1954, was specifically designed to accelerate protons to sufficient energy to create antiprotons, and verify the particle-antiparticle symmetry of nature, then only theorized. The Alternating Gradient Synchrotron (AGS) at Brookhaven (1960–) was the first large synchrotron with alternating gradient, "strong focusing" magnets, which greatly reduced the required aperture of the beam, and correspondingly the size and cost of the bending magnets. The Proton Synchrotron, built at CERN (1959–), was the first major European particle accelerator and generally similar to the AGS.
The Stanford Linear Accelerator, SLAC, became operational in 1966, accelerating electrons to 30 GeV in a 3 km long waveguide, buried in a tunnel and powered by hundreds of large klystrons. It is still the largest linear accelerator in existence, and has been upgraded with the addition of storage rings and an electron-positron collider facility. It is also an X-ray and UV synchrotron photon source.
The Fermilab Tevatron has a ring with a beam path of 4 miles (6.4 km). It has received several upgrades, and has functioned as a proton-antiproton collider until it was shut down due to budget cuts on September 30, 2011. The largest circular accelerator ever built was the LEP synchrotron at CERN with a circumference 26.6 kilometers, which was an electron/positron collider. It achieved an energy of 209 GeV before it was dismantled in 2000 so that the underground tunnel could be used for the Large Hadron Collider (LHC). The LHC is a proton collider, and currently the world's largest and highest-energy accelerator, achieving 6.5 TeV energy per beam (13 TeV in total).
The aborted Superconducting Super Collider (SSC) in Texas would have had a circumference of 87 km. Construction was started in 1991, but abandoned in 1993. Very large circular accelerators are invariably built in underground tunnels a few metres wide to minimize the disruption and cost of building such a structure on the surface, and to provide shielding against intense secondary radiations that occur, which are extremely penetrating at high energies.
Current accelerators such as the Spallation Neutron Source, incorporate superconducting cryomodules. The Relativistic Heavy Ion Collider, and Large Hadron Collider also make use of superconducting magnets and RF cavity resonators to accelerate particles.



The output of a particle accelerator can generally be directed towards multiple lines of experiments, one at a given time, by means of a deviating electromagnet. This makes it possible to operate multiple experiments without needing to move things around or shutting down the entire accelerator beam. Except for synchrotron radiation sources, the purpose of an accelerator is to generate high-energy particles for interaction with matter.
This is usually a fixed target, such as the phosphor coating on the back of the screen in the case of a television tube; a piece of uranium in an accelerator designed as a neutron source; or a tungsten target for an X-ray generator. In a linac, the target is simply fitted to the end of the accelerator. The particle track in a cyclotron is a spiral outwards from the centre of the circular machine, so the accelerated particles emerge from a fixed point as for a linear accelerator.
For synchrotrons, the situation is more complex. Particles are accelerated to the desired energy. Then, a fast acting dipole magnet is used to switch the particles out of the circular synchrotron tube and towards the target.
A variation commonly used for particle physics research is a collider, also called a storage ring collider. Two circular synchrotrons are built in close proximity – usually on top of each other and using the same magnets (which are then of more complicated design to accommodate both beam tubes). Bunches of particles travel in opposite directions around the two accelerators and collide at intersections between them. This can increase the energy enormously; whereas in a fixed-target experiment the energy available to produce new particles is proportional to the square root of the beam energy, in a collider the available energy is linear.




At present the highest energy accelerators are all circular colliders, but both hadron accelerators and electron accelerators are running into limits. Higher energy hadron and ion cyclic accelerators will require accelerator tunnels of larger physical size due to the increased beam rigidity.
For cyclic electron accelerators, a limit on practical bend radius is placed by synchrotron radiation losses and the next generation will probably be linear accelerators 10 times the current length. An example of such a next generation electron accelerator is the proposed 40 km long International Linear Collider.
It is believed that plasma wakefield acceleration in the form of electron-beam 'afterburners' and standalone laser pulsers might be able to provide dramatic increases in efficiency over RF accelerators within two to three decades. In plasma wakefield accelerators, the beam cavity is filled with a plasma (rather than vacuum). A short pulse of electrons or laser light either constitutes or immediately precedes the particles that are being accelerated. The pulse disrupts the plasma, causing the charged particles in the plasma to integrate into and move toward the rear of the bunch of particles that are being accelerated. This process transfers energy to the particle bunch, accelerating it further, and continues as long as the pulse is coherent.
Energy gradients as steep as 200 GeV/m have been achieved over millimeter-scale distances using laser pulsers and gradients approaching 1 GeV/m are being produced on the multi-centimeter-scale with electron-beam systems, in contrast to a limit of about 0.1 GeV/m for radio-frequency acceleration alone. Existing electron accelerators such as SLAC could use electron-beam afterburners to greatly increase the energy of their particle beams, at the cost of beam intensity. Electron systems in general can provide tightly collimated, reliable beams; laser systems may offer more power and compactness. Thus, plasma wakefield accelerators could be used – if technical issues can be resolved – to both increase the maximum energy of the largest accelerators and to bring high energies into university laboratories and medical centres.
Higher than 0.25 GeV/m gradients have been achieved by a dielectric laser accelerator, which may present another viable approach to building compact high-energy accelerators.




In the future, the possibility of black hole production at the highest energy accelerators may arise if certain predictions of superstring theory are accurate. This and other possibilities have led to public safety concerns that have been widely reported in connection with the LHC, which began operation in 2008. The various possible dangerous scenarios have been assessed as presenting "no conceivable danger" in the latest risk assessment produced by the LHC Safety Assessment Group. If black holes are produced, it is theoretically predicted that such small black holes should evaporate extremely quickly via Bekenstein-Hawking radiation, but which is as yet experimentally unconfirmed. If colliders can produce black holes, cosmic rays (and particularly ultra-high-energy cosmic rays, UHECRs) must have been producing them for eons, but they have yet to harm anybody. It has been argued that to conserve energy and momentum, any black holes created in a collision between an UHECR and local matter would necessarily be produced moving at relativistic speed with respect to the Earth, and should escape into space, as their accretion and growth rate should be very slow, while black holes produced in colliders (with components of equal mass) would have some chance of having a velocity less than Earth escape velocity, 11.2 km per sec, and would be liable to capture and subsequent growth. Yet even on such scenarios the collisions of UHECRs with white dwarfs and neutron stars would lead to their rapid destruction, but these bodies are observed to be common astronomical objects. Thus if stable micro black holes should be produced, they must grow far too slowly to cause any noticeable macroscopic effects within the natural lifetime of the solar system.



An accelerator operator controls the operation of a particle accelerator used in research experiments, reviews an experiment schedule to determine experiment parameters specified by an experimenter (physicist), adjust particle beam parameters such as aspect ratio, current intensity, and position on target, communicates with and assists accelerator maintenance personnel to ensure readiness of support systems, such as vacuum, magnet power supplies and controls, low conductivity water or LCW cooling, and radiofrequency power supplies and controls, and maintains a record of accelerator related events.




Accelerator physics
Atom smasher (disambiguation)
Dielectric wall accelerator
Nuclear transmutation
List of accelerators in particle physics
Rolf Widerøe
The idea of a particle accelerator has also been used in television shows such as The Flash.





Social media are computer-mediated technologies that allow the creating and sharing of information, ideas, career interests and other forms of expression via virtual communities and networks. The variety of stand-alone and built-in social media services currently available introduces challenges of definition. However, there are some common features.
Social media are interactive Web 2.0 Internet-based applications.
User-generated content, such as text posts or comments, digital photos or videos, and data generated through all online interactions, are the lifeblood of social media.
Users create service-specific profiles for the website or app that are designed and maintained by the social media organization.
Social media facilitate the development of online social networks by connecting a user's profile with those of other individuals and/or groups.
Social media use web-based technologies, desktop computers and mobile technologies (e.g., smartphones and tablet computers) to create highly interactive platforms through which individuals, communities and organizations can share, co-create, discuss, and modify user-generated content or pre-made content posted online. They introduce substantial and pervasive changes to communication between businesses, organizations, communities and individuals. Social media changes the way individuals and large organizations communicate. These changes are the focus of the emerging field of technoself studies. In America, a survey reported that 84 percent of adolescents in America have a Facebook account. Over 60% of 13 to 17-year-olds have at least one profile on social media, with many spending more than two hours a day on social networking sites. According to Nielsen, Internet users continue to spend more time on social media sites than on any other type of site. At the same time, the total time spent on social media sites in the U.S. across PCs as well as on mobile devices increased by 99 percent to 121 billion minutes in July 2012 compared to 66 billion minutes in July 2011. For content contributors, the benefits of participating in social media have gone beyond simply social sharing to building reputation and bringing in career opportunities and monetary income.
Social media differ from paper-based or traditional electronic media such as TV broadcasting in many ways, including quality, reach, frequency, usability, immediacy, and permanence. Social media operate in a dialogic transmission system (many sources to many receivers). This is in contrast to traditional media which operates under a monologic transmission model (one source to many receivers), such as a paper newspaper which is delivered to many subscribers. Some of the most popular social media websites are Facebook (and its associated Facebook Messenger), WhatsApp, Tumblr, Instagram, Twitter, Baidu Tieba, Pinterest, LinkedIn, Gab, Google+, YouTube, Viber, Snapchat, Weibo and WeChat. These social media websites have more than 100,000,000 registered users.
Observers have noted a range of positive and negative impacts from social media use. Social media can help to improve individuals' sense of connectedness with real and/or online communities and social media can be an effective communications (or marketing) tool for corporations, entrepreneurs, nonprofit organizations, including advocacy groups and political parties and governments. At the same time, concerns have been raised about possible links between heavy social media use and depression, and even the issues of cyberbullying, online harassment and "trolling". Currently, about half of young adults have been cyberbullied and of those, 20 percent said that they have been cyberbullied on a regular basis. Another survey was carried out among 7th grade students in America which is known as the Precaution Process Adoption Model. According to this study 69 percent of 7th grade students claim to have experienced cyberbullying and they also said that it is worse than face to face bullying.



The variety and evolving stand-alone and built-in social media services introduces a challenge of definition. The idea that social media are defined by their ability to bring people together has been seen as too broad a definition, as this would suggest that the telegraph and telephone were also social media – not the technologies scholars are intending to describe. The terminology is unclear, with some referring to social media as social networks.
A 2015 paper reviewed the prominent literature in the area and identified four commonalities unique to then-current social media services:
social media are Web 2.0 Internet-based applications,
user-generated content (UGC) is the lifeblood of the social media organism,
users create service-specific profiles for the site or app that are designed and maintained by the social media organization,
social media facilitate the development of online social networks by connecting a user's profile with those of other individuals and/or groups.
In 2016, Merriam-Webster defined social media as "Forms of electronic communication (such as Web sites) through which people create online communities to share information, ideas, personal messages, etc."

The term social media is usually used to describe social networking sites such as:
Facebook – an online social networking site that allows users to create their personal profiles, share photos and videos, and communicate with other users
Twitter – an internet service that allows users to post "tweets" for their followers to see updates in real-time
LinkedIn – a networking website for the business community that allows users to create professional profiles, post resumes, and communicate with other professionals and job-seekers.
Pinterest – an online community that allows users to display photos of items found on the web by "pinning" them and sharing ideas with others.
Snapchat – an app on mobile devices that allows users to send and share photos of themselves doing their daily activities.
Social media technologies take many different forms including blogs, business networks, enterprise social networks, forums, microblogs, photo sharing, products/services review, social bookmarking, social gaming, social networks, video sharing, and virtual worlds. The development of social media started off with simple platforms such as sixdegrees.com. Unlike instant messaging clients such as ICQ and AOL's AIM, or chat clients like IRC, iChat or Chat Television, sixdegrees.com was the first online business that was created for real people, using their real names. However, the first social networks were short-lived because their users lost interest. The Social Network Revolution has led to the rise of the networking sites. Research shows that the audience spends 22 percent of their time on social networking sites, thus proving how popular social media platforms have become. This increase is because of the smart phones that are now in the daily lives of most humans.






Some social media sites have greater potential for content that is posted there to spread virally over social networks. This is an analogy to the concept of a viral infectious disease in biology, some of which can spread rapidly from an infected person to another person. In a social media context, content or websites that are "viral" (or which "go viral") are those with a greater likelihood that users will reshare content posted (by another user) to their social network, leading to further sharing. In some cases, posts containing controversial content (e.g., Kim Kardashian's nude photos that "broke the Internet" and crashed servers) or fast-breaking news have been rapidly shared and re-shared by huge numbers of users. Many social media sites provide specific functionality to help users reshare content – for example, Twitter's retweet button, Pinterest's pin function, Facebook's share option or Tumblr's reblog function. Businesses have a particular interest in viral marketing tactics because such a campaign can achieve widespread advertising coverage (particularly if the "viral" reposting itself makes the news) for a fraction of the cost of a traditional marketing campaign (e.g., billboard ads, television commercials, magazine ads, etc.). Nonprofit organisations and activists may have similar interests in posting content online with the hopes that it goes viral. The social news website Slashdot sometimes has news stories that, once posted on its website, "go viral"; the Slashdot effect refers to this situation.




Mobile social media refers to the use of social media on mobile devices such as smartphones and tablet computers. This is a group of mobile marketing applications that allow the creation, exchange and circulation of user-generated content. Due to the fact that mobile social media run on mobile devices, they differ from traditional social media by incorporating new factors such as the current location of the user (location-sensitivity) or the time delay between sending and receiving messages (time-sensitivity). According to Andreas Kaplan, mobile social media applications can be differentiated among four types:
Space-timers (location and time sensitive): Exchange of messages with relevance mostly for one specific location at one specific point in time (e.g. Facebook Places; Foursquare)
Space-locators (only location sensitive): Exchange of messages, with relevance for one specific location, which are tagged to a certain place and read later by others (e.g. Yelp; Qype, Tumblr, Fishbrain)
Quick-timers (only time sensitive): Transfer of traditional social media applications to mobile devices to increase immediacy (e.g. posting Twitter messages or Facebook status updates)
Slow-timers (neither location, nor time sensitive): Transfer of traditional social media applications to mobile devices (e.g. watching a YouTube video or reading/editing a Wikipedia article)



Although social media accessed via desktop computers offer a variety of opportunities for companies in a wide range of business sectors, mobile social media, which users are accessing when they are "on the go" via tablet computer or smartphone can take advantage of the location- and time-sensitive awareness of users. Mobile social media tools can be used for marketing research, communication, sales promotions/discounts, and relationship development/loyalty programs.
Marketing research: Mobile social media applications offer data about offline consumer movements at a level of detail heretofore limited to online companies. Any firm can know the exact time at which a customer entered one of its outlets, as well as know the social media comments made during the visit.
Communication: Mobile social media communication takes two forms: company-to-consumer (in which a company may establish a connection to a consumer based on its location and provide reviews about locations nearby) and user-generated content. For example, McDonald's offered $5 and $10 gift-cards to 100 users randomly selected among those checking in at one of its restaurants. This promotion increased check-ins by 33% (from 2,146 to 2,865), resulted in over 50 articles and blog posts, and prompted several hundred thousand news feeds and Twitter messages.
Sales promotions and discounts: Although customers have had to use printed coupons in the past, mobile social media allows companies to tailor promotions to specific users at specific times. For example, when launching its California-Cancun service, Virgin America offered users who checked in through Loopt at one of three designated Border Grill taco trucks in San Francisco and Los Angeles between 11 a.m. and 3 p.m. on August 31, 2010, two tacos for $1 and two flights to Mexico for the price of one. This special promotion was only available to people who were at a certain location and at a certain time.
Relationship development and loyalty programs: In order to increase long-term relationships with customers, companies can develop loyalty programs that allow customers who check-in via social media regularly at a location to earn discounts or perks. For example, American Eagle Outfitters remunerates such customers with a tiered 10%, 15%, or 20% discount on their total purchase.
e-Commerce: Social media sites are increasingly implementing marketing-friendly strategies, creating platforms that are mutually beneficial for users, businesses, and the networks themselves in the popularity and accessibility of e-commerce, or online purchases. The user who posts her or his comments about a company's product or service benefits because they are able to share their views with their online friends and acquaintances. The company benefits because it obtains insight (positive or negative) about how their product or service is viewed by consumers. Mobile social media applications such as Amazon.com and Pinterest have started to influence an upward trend in the popularity and accessibility of e-commerce, or online purchases.
E-commerce businesses may refer to social media as consumer-generated media (CGM). A common thread running through all definitions of social media is a blending of technology and social interaction for the co-creation of value for the business or organization that is using it. People obtain valuable information, education, news, and other data from electronic and print media. Social media are distinct from industrial or traditional media such as newspapers, magazines, television, and film as they are comparatively inexpensive and accessible (at least once a person has already acquired Internet access and a computer). They enable anyone (even private individuals) to publish or access information. Industrial media generally require significant resources to publish information as in most cases the articles go through many revisions before being published. This process adds to the cost and the resulting market price. Originally social media was only used by individuals but now it is used by businesses, charities and also in government and politics.
One characteristic shared by both social and industrial media is the capability to reach small or large audiences; for example, either a blog post or a television show may reach no people or millions of people. Some of the properties that help describe the differences between social and industrial media are:
Quality: In industrial (traditional) publishing—mediated by a publisher—the typical range of quality is substantially narrower (skewing to the high quality side) than in niche, unmediated markets like user-generated social media posts. The main challenge posed by content in social media sites is the fact that the distribution of quality has high variance: from very high-quality items to low-quality, sometimes even abusive or inappropriate content.
Reach: Both industrial and social media technologies provide scale and are capable of reaching a global audience. Industrial media, however, typically use a centralized framework for organization, production, and dissemination, whereas social media are by their very nature more decentralized, less hierarchical, and distinguished by multiple points of production and utility.
Frequency: The number of times users access a type of media per day. Heavy social media users, such as young people, check their social media account numerous times throughout the day.
Accessibility: The means of production for industrial media are typically government and/or corporate (privately owned); social media tools are generally available to the public at little or no cost, or they are supported by advertising revenue. While social media tools are available to anyone with access to Internet and a computer or mobile device, due to the digital divide, the poorest segment of the population lacks access to the Internet and computer. Low-income people may have more access to traditional media (TV, radio, etc.), as an inexpensive TV and aerial or radio costs much less than an inexpensive computer or mobile device. Moreover, in many regions, TV or radio owners can tune into free over the air programming; computer or mobile device owners need Internet access to go on social media sites.
Usability: Industrial media production typically requires specialized skills and training. For example, in the 1970s, to record a pop song, an aspiring singer would have to rent time in an expensive professional recording studio and hire an audio engineer. Conversely, most social media activities, such as posting a video of oneself singing a song require only modest reinterpretation of existing skills (assuming a person understands Web 2.0 technologies); in theory, anyone with access to the Internet can operate the means of social media production, and post digital pictures, videos or text online.
Immediacy: The time lag between communications produced by industrial media can be long (days, weeks, or even months, by the time the content has been reviewed by various editors and fact checkers) compared to social media (which can be capable of virtually instantaneous responses). The immediacy of social media can be seen as a strength, in that it enables regular people to instantly communicate their opinions and information. At the same time, the immediacy of social media can also be seen as a weakness, as the lack of fact checking and editorial "gatekeepers" facilitates the circulation of hoaxes and fake news.
Permanence: Industrial media, once created, cannot be altered (e.g., once a magazine article or paper book is printed and distributed, changes cannot be made to that same article in that print run) whereas social media posts can be altered almost instantaneously, when the user decides to edit their post or due to comments from other readers.
Community media constitute a hybrid of industrial and social media. Though community-owned, some community radio, TV, and newspapers are run by professionals and some by amateurs. They use both social and industrial media frameworks. Social media have also been recognized for the way they have changed how public relations professionals conduct their jobs. They have provided an open arena where people are free to exchange ideas on companies, brands, and products. Doc Searls and David Wagner state that the "...best of the people in PR are not PR types at all. They understand that there aren't censors, they're the company's best conversationalists." Social media provides an environment where users and PR professionals can converse, and where PR professionals can promote their brand and improve their company's image by listening and responding to what the public is saying about their product.



Social media have a strong influence on business activities and business performance. There are four channels by which social media resources are transformed into business performance capabilities:
Social capital: represents the extent to which social media affects firms' and organizations' relationships with society and the degree to which the organizations' ue of social media increases corporate social performance capabilities.
Revealed preferences: represents the extent to which social media exposes customers' likings (e.g., "likes" and followers) and increases a firm's financial capabilities (e.g., stock price, revenue, profit), or for non-profits, increases their donations, volunteerism rate, etc.
Social marketing: represents the extent to which social marketing resources (e.g., online conversations, sharing links, online presence, sending text messages) are used to increase a firm's financial capabilities (e.g., sales, acquisition of new customers) or a non-profit's voluntary sector goals.
Social corporate networking: Social corporate networking refers to the informal ties and linkages of corporate/organizational staff with other people from their field or industry, clients, customers, and other members of the public, which were formed through social networks. Social corporate networking can increase operational performance capabilities in many ways, as it can enable sales staff to find new clients; marketing staff to learn about client/customer needs and demand; and management can learn about the public perceptions of their strategy or approach.
There are four tools or approaches that engage experts, customers, suppliers, and employees in the development of products and services using social media. Companies and other organizations can use these tools and approaches to improve their business capacity and performance.
Customer relationship management (CRM) is an approach to managing a company's interaction with current and potential future customers that tries to analyze data about customers' history with a company and to improve business relationships with customers, specifically focusing on customer retention and ultimately driving sales growth. One important aspect of the CRM approach is the systems of CRM that compile data from a range of different communication channels, including a company's website, telephone, email, live chat, marketing materials, and social media. Through the CRM approach and the systems used to facilitate CRM, businesses learn more about their target audiences and how to best cater to their needs. However, adopting the CRM approach may also occasionally lead to favoritism within an audience of consumers, resulting in dissatisfaction among customers and defeating the purpose of CRM.
Innovation can be defined simply as a "new idea, device, or method" or as the application of better solutions that meet new requirements, unarticulated needs, or existing market needs. This is accomplished through more-effective products, processes, services, technologies, or business models that are readily available to markets, governments and society. The term "innovation" can be defined as something original and more effective and, as a consequence, new, that "breaks into" the market or society.[3] It is related to, but not the same as, invention. Innovation is often manifested via the engineering process. Innovation is generally considered to be the result of a process that brings together various novel ideas in a way that they affect society. In industrial economics, innovations are created and found empirically from services to meet the growing consumer demand.
Training
Knowledge management



Companies are increasingly using social media monitoring tools to monitor, track, and analyze online conversations on the Web about their brand or products or about related topics of interest. This can be useful in public relations management and advertising campaign tracking, allowing the companies to measure return on investment for their social media ad spending, competitor-auditing, and for public engagement. Tools range from free, basic applications to subscription-based, more in-depth tools. Hootsuite is an example of a social media monitoring and tracking software that companies can use.
Social media tracking also enables companies to respond quickly to online posts that criticize their product or service. By responding quickly to critical online posts, and helping the user to resolve the concerns, this helps the company to lessen the negative effects that online complaints can have about company product or service sales. In the US, for example, if a customer criticizes a major hotel chain's cleanliness or service standards on a social media website, a company representative will usually quickly be alerted to this critical post, so that the company representative can go online and express concern for the sub-par service and offer the complaining person a coupon or discount on their next purchase, plus a promise to forward their concerns to the hotel manager so that the problem will not be repeated. This rapid response helps to show that the company cares about its customers.
The "honeycomb framework" defines how social media services focus on some or all of seven functional building blocks. These building blocks help explain the engagement needs of the social media audience. For instance, LinkedIn users are thought to care mostly about identity, reputation, and relationships, whereas YouTube's primary features are sharing, conversations, groups, and reputation. Many companies build their own social "containers" that attempt to link the seven functional building blocks around their brands. These are private communities that engage people around a more narrow theme, as in around a particular brand, vocation or hobby, rather than social media containers such as Google+, Facebook, and Twitter. PR departments face significant challenges in dealing with viral negative sentiment directed at organizations or individuals on social media platforms (dubbed "sentimentitis"), which may be a reaction to an announcement or event. In a 2011 article, Jan H. Kietzmann, Kristopher Hermkens, Ian P. McCarthy and Bruno S. Silvestre describe the honeycomb relationship as "present[ing] a framework that defines social media by using seven functional building blocks: identity, conversations, sharing, presence, relationships, reputation, and groups."
The elements of the honeycomb framework include:
Identity: This block represents the extent to which users reveal their identities in a social media setting. This can include disclosing information such as name, age, gender, profession, location, and also information that portrays users in certain ways.
Conversations: This block represents the extent to which users communicate with other users in a social media setting. Many social media sites are designed primarily to facilitate conversations among individuals and groups. These conversations happen for all sorts of reasons. People tweet, blog, make online comments and send messages to other users to meet new like-minded people, to ﬁnd a romantic partner, to build their self-esteem, or to be on the cutting edge of new ideas or trending topics. Yet others see social media as a way of making their message heard and positively impacting humanitarian causes, environmental problems, economic issues, or political debates.
Sharing: This block represents the extent to which users exchange, distribute, and receive content, ranging from a short text post to a link or a digital photo. The term 'social' implies that exchanges between people are crucial. In many cases, however, sociality is about the objects that mediate these ties between people—the reasons why they meet online and associate with each other.
Presence: This block represents the extent to which users can know if other users are accessible. It includes knowing where others are, in the virtual world and/or in the real world, and whether they are available. Some social media sites have icons that indicate when other users are online, such as Facebook.
Relationships: This block represents the extent to which users can be related or linked up to other users. Two or more users have some form of association that leads them to converse, share objects of sociality, send texts or messages, meet up, or simply just list each other as a friend or fan.
Reputation: This block represents the extent to which users can identify the standing of others, including themselves, in a social media setting. Reputation can have different meanings on social media platforms. In most cases, reputation is a matter of trust, but because information technologies are not yet good at determining such highly qualitative criteria, social media sites rely on 'mechanical Turks': tools that automatically aggregate user-generated information to determine trustworthiness. Reputation management is another aspect and use of social media.
Groups: This block represents the extent to which users can form communities and sub-communities of people with similar backgrounds, demographics or interests. The more 'social' a network becomes, the wider the group of friends, followers, and contacts can be developed. Some Facebook users develop a list of friends that includes people from all over the world.



Social media becomes effective through a process called "building social authority". One of the foundation concepts in social media has become that you cannot completely control your message through social media but rather you can simply begin to participate in the "conversation" expecting that you can achieve a significant influence in that conversation. However, this conversation participation must be cleverly executed because although people are resistant to marketing in general, they are even more resistant to direct or overt marketing through social media platforms. This may seem counterintuitive but it is the main reason building social authority with credibility is so important. A marketer can generally not expect people to be receptive to a marketing message in and of itself. In the Edelman Trust Barometer report in 2008, the majority (58%) of the respondents reported they most trusted company or product information coming from "people like me" inferred to be information from someone they trusted. In the 2010 Trust Report, the majority switched to 64% preferring their information from industry experts and academics. According to Inc. Technology's Brent Leary, "This loss of trust, and the accompanying turn towards experts and authorities, seems to be coinciding with the rise of social media and networks."




Social media "mining" is a type of data mining, a technique of analyzing data to detect patterns. Social media mining is a process of representing, analyzing, and extracting actionable patterns from data collected from people's activities on social media. Social media mining introduces basic concepts and principal algorithms suitable for investigating massive social media data; it discusses theories and methodologies from different disciplines such as computer science, data mining, machine learning, social network analysis, network science, sociology, ethnography, statistics, optimization, and mathematics. It encompasses the tools to formally represent, measure, model, and mine meaningful patterns from large-scale social media data. Detecting patterns in social media use by data mining is of particular interest to advertisers, major corporations and brands, governments and political parties, among others.




According to the article "The Emerging Role of Social Media in Political and Regime Change" by Rita Safranek, the Middle East and North Africa region has one of the most youthful populations in the world, with people under 25 making up between 35–45% of the population in each country. They make up the majority of social media users, including about 17 million Facebook users, 25,000 Twitter accounts and 40,000 active blogs, according to the Arab Advisors Group.



This is a list of the leading social networks based on number of active user accounts as of September 2016.
Facebook: 1,712,000,000 users
WhatsApp 1,000,000,000 users
Facebook Messenger: 1,000,000,000 users
QQ: 899,000,000 users
WeChat: 806,000,000 users
QZone: 652,000,000 users
Tumblr: 555,000,000 users
Instagram: 500,000,000 users
Twitter: 313,000,000 users
Baidu Tieba: 300,000,000 users
Skype: 300,000,000 users
Sina Weibo: 282,000,000 users
Viber: 249,000,000 users
Line: 218,000,000 users
Snapchat: 200,000,000 users



Just as television turned a nation of people who listened to media content into watchers of media content, the emergence of social media has created a nation of media content creators. According to 2011 Pew Research data, nearly 80% of American adults are online and nearly 60% of them use social networking sites. More Americans get their news via the Internet than from newspapers or radio, as well as three-fourths who say they get news from e-mail or social media sites updates, according to a report published by CNN. The survey suggests that Facebook and Twitter make news a more participatory experience than before as people share news articles and comment on other people's posts. According to CNN, in 2010 75% of people got their news forwarded through e-mail or social media posts, whereas 37% of people shared a news item via Facebook or Twitter. In the United States, 81% of people say they look online for news of the weather, first and foremost. National news at 73%, 52% for sports news, and 41% for entertainment or celebrity news. Based on this study, done for the Pew Center, two-thirds of the sample's online news users were younger than 50, and 30% were younger than 30. The survey involved tracking daily the habits of 2,259 adults 18 or older. Thirty-three percent of young adults get news from social networks. Thirty-four percent watched TV news and 13% read print or digital content. Nineteen percent of Americans got news from Facebook, Google+, or LinkedIn. Thirty-six percent of those who get news from social network got it yesterday from survey. More than 36% of Twitter users use accounts to follow news organizations or journalists. Nineteen percent of users say they got information from news organizations of journalists. TV remains most popular source of news, but audience is aging (only 34% of young people).
Of those younger than 25, 29% said they got no news yesterday either digitally or traditional news platforms. Only 5% under 30 said they follow news about political figures and events in DC. Only 14% of respondents could answer all four questions about which party controls the House, current unemployment rate, what nation Angela Merkel leads, and which presidential candidate favors taxing higher-income Americans. Facebook and Twitter now pathways to news, but are not replacements for traditional ones. Seventy percent get social media news from friends and family on Facebook.
Social media fosters communication. An Internet research company, PewResearch Center, claims that "more than half of internet users (52%) use two or more of the social media sites measured (Facebook, Twitter, Instagram, Pinterest) to communicate with their family or friends". For children, using social media sites can help promote creativity, interaction, and learning. It can also help them with homework and class work. Moreover, social media enable them to stay connected with their peers, and help them to interact with each other. Some can get involved with developing fundraising campaigns and political events. However, it can impact social skills due to the absence of face-to-face contact. Social media can affect mental health of teens. Teens who use Facebook frequently and especially who are susceptible may become more narcissistic, antisocial, and aggressive. Teens become strongly influenced by advertising, and it influences buying habits. Since the creation of Facebook in 2004, it has become a distraction and a way to waste time for many users. A head teacher in the United Kingdom commented in 2015 that social media caused more stress to teenage children than examinations, with constant interaction and monitoring by peers ending the past practice where what pupils did in the evening or at weekends was separate from the arguments and peer pressure at school.
In a 2014 study, high school students ages 18 and younger were examined in an effort to find their preference for receiving news. Based on interviews with 61 teenagers, conducted from December 2007 to February 2011, most of the teen participants reported reading print newspapers only "sometimes," with fewer than 10% reading them daily. The teenagers instead reported learning about current events from social media sites such as Facebook, MySpace, YouTube, and blogs. Another study showed that social media users read a set of news that is different from what newspaper editors feature in the print press. Using nanotechnology as an example, a study was conducted that studied tweets from Twitter and found that some 41% of the discourse about nanotechnology focused on its negative impacts, suggesting that a portion of the public may be concerned with how various forms of nanotechnology are used in the future. Although optimistic-sounding and neutral-sounding tweets were equally likely to express certainty or uncertainty, the pessimistic tweets were nearly twice as likely to appear certain of an outcome than uncertain. These results imply the possibility of a preconceived negative perception of many news articles associated with nanotechnology. Alternatively, these results could also imply that posts of a more pessimistic nature that are also written with an air of certainty are more likely to be shared or otherwise permeate groups on Twitter. Similar biases need to be considered when the utility of new media is addressed, as the potential for human opinion to over-emphasize any particular news story is greater despite the general improvement in addressed potential uncertainty and bias in news articles than in traditional media.
On October 2, 2013, the most common hashtag throughout the United States was "#governmentshutdown", as well as ones focusing on political parties, Obama, and healthcare. Most news sources have Twitter, and Facebook, pages, like CNN and the New York Times, providing links to their online articles, getting an increased readership. Additionally, several college news organizations and administrators have Twitter pages as a way to share news and connect to students. According to "Reuters Institute Digital News Report 2013", in the US, among those who use social media to find news, 47% of these people are under 45 years old, and 23% are above 45 years old. However social media as a main news gateway does not follow the same pattern across countries. For example, in this report, in Brazil, 60% of the respondents said social media was one of the five most important ways to find news online, 45% in Spain, 17% in the UK, 38% in Italy, 14% in France, 22% in Denmark, 30% in the U.S., and 12% in Japan. Moreover, there are differences among countries about commenting on news in social networks, 38% of the respondents in Brazil said they commented on news in social network in a week. These percentages are 21% in the U.S. and 10% in the UK. The authors argued that differences among countries may be due to culture difference rather than different levels of access to technical tools.



News media and television journalism have been instrumental in the shaping of American collective memory for much of the twentieth century. Indeed, since the United States' colonial era, news media has influenced collective memory and discourse about national development and trauma. In many ways, mainstream journalists have maintained an authoritative voice as the storytellers of the American past. Their documentary style narratives, detailed exposes, and their positions in the present make them prime sources for public memory. Specifically, news media journalists have shaped collective memory on nearly every major national event – from the deaths of social and political figures to the progression of political hopefuls. Journalists provide elaborate descriptions of commemorative events in U.S. history and contemporary popular cultural sensations. Many Americans learn the significance of historical events and political issues through news media, as they are presented on popular news stations. However, journalistic influence is growing less important, whereas social networking sites such as Facebook, YouTube and Twitter, provide a constant supply of alternative news sources for users.
As social networking becomes more popular among older and younger generations, sites such as Facebook and YouTube, gradually undermine the traditionally authoritative voices of news media. For example, American citizens contest media coverage of various social and political events as they see fit, inserting their voices into the narratives about America's past and present and shaping their own collective memories. An example of this is the public explosion of the Trayvon Martin shooting in Sanford, Florida. News media coverage of the incident was minimal until social media users made the story recognizable through their constant discussion of the case. Approximately one month after the fatal shooting of Trayvon Martin, its online coverage by everyday Americans garnered national attention from mainstream media journalists, in turn exemplifying media activism. In some ways, the spread of this tragic event through alternative news sources parallels that of the Emmitt Till – whose murder became a national story after it circulated African American and Communists newspapers. Social media was also influential in the widespread attention given to the revolutionary outbreaks in the Middle East and North Africa during 2011. However, there is some debate about the extent to which social media facilitated this kind of change. Another example of this shift is in the ongoing Kony 2012 campaign, which surfaced first on YouTube and later garnered a great amount of attention from mainstream news media journalists. These journalists now monitor social media sites to inform their reports on the movement. Lastly, in the past couple of presidential elections, the use of social media sites such as Facebook and Twitter were used to predict election results. U.S. President Barack Obama was more liked on Facebook than his opponent Mitt Romney and it was found by a study done by Oxford Institute Internet Experiment that more people liked to tweet about comments of President Obama rather than Romney.




Criticisms of social media range from criticisms of the ease of use of specific platforms and their capabilities, disparity of information available, issues with trustworthiness and reliability of information presented, the impact of social media use on an individual's concentration, ownership of media content, and the meaning of interactions created by social media. Although some social media platforms offer users the opportunity to cross-post simultaneously, some social network platforms have been criticized for poor interoperability between platforms, which leads to the creation of information silos, viz. isolated pockets of data contained in one social media platform. However, it is also argued that social media have positive effects such as allowing the democratization of the Internet while also allowing individuals to advertise themselves and form friendships. Others have noted that the term "social" cannot account for technological features of a platform alone, hence the level of sociability should be determined by the actual performances of its users. There has been a dramatic decrease in face-to-face interactions as more and more social media platforms have been introduced with the threat of cyber-bullying and online sexual predators being more prevalent. Social media may expose children to images of alcohol, tobacco, and sexual behaviors. In regards to cyber-bullying, it has been proven that individuals who have no experience with cyber-bullying often have a better well-being than individuals who have been bullied online.
Twitter is increasingly a target of heavy activity of marketers. Their actions, focused on gaining massive numbers of followers, include use of advanced scripts and manipulation techniques that distort the prime idea of social media by abusing human trustfulness. Twitter also promotes social connections among students. It can be used to enhance communication building and critical thinking. Domizi (2013) utilised Twitter in a graduate seminar requiring students to post weekly tweets to extend classroom discussions. Students reportedly used Twitter to connect with content and other students. Additionally, students found it "to be useful professionally and personally." British-American entrepreneur and author Andrew Keen criticizes social media in his book The Cult of the Amateur, writing, "Out of this anarchy, it suddenly became clear that what was governing the infinite monkeys now inputting away on the Internet was the law of digital Darwinism, the survival of the loudest and most opinionated. Under these rules, the only way to intellectually prevail is by infinite filibustering." This is also relative to the issue "justice" in the social network. For example, the phenomenon "Human flesh search engine" in Asia raised the discussion of "private-law" brought by social network platform. Comparative media professor José van Dijck contends in her book "The Culture of Connectivity" (2013) that to understand the full weight of social media, their technological dimensions should be connected to the social and the cultural. She critically describes six social media platforms. One of her findings is the way Facebook had been successful in framing the term 'sharing' in such a way that third party use of user data is neglected in favour of intra-user connectedness.




The digital divide is a measure of disparity in the level of access to technology between households, socioeconomic levels or other demographic categories. People who are homeless, living in poverty, elderly people and those living in rural or remote communities may have little or no access to computers and the Internet; in contrast, middle class and upper-class people in urban areas have very high rates of computer and Internet access. Other models argue that within a modern information society, some individuals produce Internet content while others only consume it, which could be a result of disparities in the education system where only some teachers integrate technology into the classroom and teach critical thinking. While social media has differences among age groups, a 2010 study in the United States found no racial divide. Some zero-rating programs offer subsidized data access to certain websites on low-cost plans. Critics say that this is an anti-competitive program that undermines net neutrality and creates a "walled garden" for platforms like Facebook Zero. A 2015 study found that 65% of Nigerians, 61% of Indonesians, and 58% of Indians agree with the statement that "Facebook is the Internet" compared with only 5% in the US.
Eric Ehrmann contends that social media in the form of public diplomacy create a patina of inclusiveness that covers traditional economic interests that are structured to ensure that wealth is pumped up to the top of the economic pyramid, perpetuating the digital divide and post Marxian class conflict. He also voices concern over the trend that finds social utilities operating in a quasi-libertarian global environment of oligopoly that requires users in economically challenged nations to spend high percentages of annual income to pay for devices and services to participate in the social media lifestyle. Neil Postman also contends that social media will increase an information disparity between "winners" – who are able to use the social media actively – and "losers" – who are not familiar with modern technologies or who do not have access to them. People with high social media skills may have better access to information about job opportunities, potential new friends, and social activities in their area, which may enable them to improve their standard of living and their quality of life.



Because large-scale collaborative co-creation is one of the main ways of forming information in the social network, the user generated content is sometimes viewed with skepticism; readers do not trust it as a reliable source of information. Aniket Kittur, Bongowon Suh, and Ed H. Chi took wikis under examination and indicated that, "One possibility is that distrust of wiki content is not due to the inherently mutable nature of the system but instead to the lack of available information for judging trustworthiness." To be more specific, the authors mention that reasons for distrusting collaborative systems with user-generated content, such as Wikipedia, include a lack of information regarding accuracy of contents, motives and expertise of editors, stability of content, coverage of topics and the absence of sources.
Social media is also an important source of news. According to 'Reuters Institute Digital News Report 2013', social media are one of the most important ways for people find news online (the others being traditional brands, search engines and news aggregators). The report suggested that in the United Kingdom, trust in news which comes from social media sources is low, compared to news from other sources (e.g. online news from traditional broadcaster or online news from national newspapers). People who aged at 24–35 trust social media most, whereas trust declined with the increase of age.
Rainie and Wellman have argued that media making now has become a participation work, which changes communication systems. The center of power is shifted from only the media (as the gatekeeper) to the peripheral area, which may include government, organizations, and out to the edge, the individual. These changes in communication systems raise empirical questions about trust to media effect. Prior empirical studies have shown that trust in information sources plays a major role in people's decision making. People's attitudes more easily change when they hear messages from trustworthy sources. In the Reuters report, 27% of respondents agree that they worry about the accuracy of a story on a blog. However, 40% of them believe the stories on blogs are more balanced than traditional papers because they are provided with a range of opinions. Recent research has shown that in the new social media communication environment, the civil or uncivil nature of comments will bias people's information processing even if the message is from a trustworthy source, which bring the practical and ethical question about the responsibility of communicator in the social media environment.



As media theorist Marshall McLuhan pointed out in the 1960s, media are not just passive channels of information or "dumb pipes". The media supply the stuff of thought, but they also shape the process of thought, as captured in his maxim "The medium is the message". For example, in the 1990s and 2000s, the increasing popularity of 24-hour all news channels such as CNN led to an increasing demand by news organizations for audience-grabbing headlines. As a result, even minor gaffes or misstatements by celebrities or public officials were made into leading news stories, to satisfy audience demand. Thus, in this example, the medium of 24-hour all-news channels started to shape the "message" that was sent on the media channel.



For Malcolm Gladwell, the role of social media, such as Twitter and Facebook, in revolutions and protests is overstated. On one hand, social media make it easier for individuals, and in this case activists, to express themselves. On the other hand, it is harder for that expression to have an impact. Gladwell distinguishes between social media activism and high risk activism, which brings real changes. Activism and especially high-risk activism involves strong-tie relationships, hierarchies, coordination, motivation, exposing oneself to high risks, making sacrifices. Gladwell discusses that social media are built around weak ties and he argues that "social networks are effective at increasing participation — by lessening the level of motivation that participation requires". According to him "Facebook activism succeeds not by motivating people to make a real sacrifice, but by motivating them to do the things that people do when they are not motivated enough to make a real sacrifice".
Furthermore, social media's role in democratizing media participation, which proponents herald as ushering in a new era of participatory democracy, with all users able to contribute news and comments, may fall short of the ideals. Social media has been championed as allowing anyone with an Internet connection to become a content creator and empowering the "active audience". But international survey data suggest online media audience members are largely passive consumers, while content creation is dominated by a small number of users who post comments and write new content. Others argue that the effect of social media will vary from one country to another, with domestic political structures playing a greater role than social media in determining how citizens express opinions about "current affairs stories involving the state". According to the "Reuters Institute Digital News Report 2013", the percent of online news users who blog about news issues ranges from 1–5%. Greater percentages use social media to comment on news, with participation ranging from 8% in Germany to 38% in Brazil. But online news users are most likely to just talk about online news with friends offline or use social media to share stories without creating content.



Evgeny Morozov, 2009–2010 Yahoo fellow at Georgetown University, contends that the information uploaded to Twitter may have little relevance to the rest of the people who do not use Twitter. In the article "Iran: Downside to the "Twitter Revolution"" in the magazine Dissent , he says:

"Twitter only adds to the noise: it's simply impossible to pack much context into its 140 characters. All other biases are present as well: in a country like Iran it's mostly pro-Western, technology-friendly and iPod-carrying young people who are the natural and most frequent users of Twitter. They are a tiny and, most important, extremely untypical segment of the Iranian population (the number of Twitter users in Iran — a country of more than seventy million people.)"

Even in the United States, the birth-country of Twitter, currently in 2015 the social network has 306 million accounts. Because there are likely to be many multi-account users, and the United States in 2012 had a population of 314.7 million, the adoption of Twitter is somewhat limited. Professor Matthew Auer of Bates College casts doubt on the conventional wisdom that social media are open and participatory. He also speculates on the emergence of "anti-social media" used as "instruments of pure control."



Social media content is generated through social media interactions done by the users through the site. There has always been a huge debate on the ownership of the content on social media platforms because it is generated by the users and hosted by the company. Added to this is the danger to security of information, which can be leaked to third parties with economic interests in the platform, or parasites who comb the data for their own databases. The author of Social Media Is Bullshit, Brandon Mendelson, claims that the "true" owners of content created on social media sites only benefits the large corporations who own those sites and rarely the users that created them.



Privacy rights advocates warn users on social media about the collection of their personal data. Some information is captured without the user's knowledge or consent through electronic tracking and third party applications. Data may also be collected for law enforcement and governmental purposes,[101] by social media intelligence using data mining techniques.[102] Data and information may also be collected for third party use. When information is shared on social media, that information is no longer private. There have been many cases in which young persons especially, share personal information, which can attract predators. It is very important to monitor what you share, and to be aware of who you could potentially be sharing that information with. Teens especially share significantly more information on the internet now than they have in the past. Teen are much more likely to share their personal information, such as email address, phone number, and school names. Studies suggest that teens are not aware of what they are posting and how much of that information can be accessed by third parties.
Other privacy concerns with employers and social media are when employers use social media as a tool to screen a prospective employee. This issue raises many ethical questions that some consider an employer's right and others consider discrimination. Except in the states of California, Maryland, and Illinois, there are no laws that prohibit employers from using social media profiles as a basis of whether or not someone should be hired.[105] Title VII also prohibits discrimination during any aspect of employment including hiring or firing, recruitment, or testing.[106] Social media has been integrating into the workplace and this has led to conflicts within employees and employers.[107] Particularly, Facebook has been seen as a popular platform for employers to investigate in order to learn more about potential employees. This conflict first started in Maryland when an employer requested and received an employee's Facebook username and password. State lawmakers first introduced legislation in 2012 to prohibit employers from requesting passwords to personal social accounts in order to get a job or to keep a job. This led to Canada, Germany, the U.S. Congress and 11 U.S. states to pass or propose legislation that prevents employers' access to private social accounts of employees.[108]
It is not only an issue in the workplace, but an issue in schools as well. There have been situations where students have been forced to give up their social media passwords to school administrators. There are inadequate laws to protect a student's social media privacy, and organizations such as the ACLU are pushing for more privacy protection, as it is an invasion. They urge students who are pressured to give up their account information to tell the administrators to contact a parent and/or lawyer before they take the matter any further. Although they are students, they still have the right to keep their password-protected information private.
Many Western European countries have already implemented laws that restrict the regulation of social media in the workplace. States including Arkansas, California, Colorado, Illinois, Maryland, Michigan, Nevada, New Jersey, New Mexico, Utah, Washington, and Wisconsin have passed legislation that protects potential employees and current employees from employers that demand them to give forth their username or password for a social media account.[109] Laws that forbid employers from disciplining an employee based on activity off the job on social media sites have also been put into act in states including California, Colorado, Connecticut, North Dakota, and New York. Several states have similar laws that protect students in colleges and universities from having to grant access to their social media accounts. Eight states have passed the law that prohibits post secondary institutions from demanding social media login information from any prospective or current students and privacy legislation has been introduced or is pending in at least 36 states as of July 2013.[110] As of May 2014, legislation has been introduced and is in the process of pending in at least 28 states and has been enacted in Maine and Wisconsin.[111] In addition, the National Labor Relations Board has been devoting a lot of their attention to attacking employer policies regarding social media that can discipline employees who seek to speak and post freely on social media sites.
There are arguments that "privacy is dead" and that with social media growing more and more, some heavy social media users appear to have become quite unconcerned with privacy. Others argue, however, that people are still very concerned about their privacy, but are being ignored by the companies running these social networks, who can sometimes make a profit off of sharing someone's personal information. There is also a disconnect between social media user's words and their actions. Studies suggest that surveys show that people want to keep their lives private, but their actions on social media suggest otherwise. Another factor is ignorance of how accessible social media posts are. Some social media users who have been criticized for inappropriate comments stated that they did not realize that anyone outside their circle of friends would read their post; in fact, on some social media sites, unless a user selects higher privacy settings, their content is shared with a wide audience.



Data suggest that participants use social media to fulfill perceived social needs, but are typically disappointed.  Lonely individuals are drawn to the Internet for emotional support. This could interfere with "real life socializing" by reducing face-to-face relationships. Some of these views are summed up in an Atlantic article by Stephen Marche entitled Is Facebook Making Us Lonely?, in which the author argues that social media provides more breadth, but not the depth of relationships that humans require and that users begin to find it difficult to distinguish between the meaningful relationships which we foster in the real world, and the numerous casual relationships that are formed through social media. Sherry Turkle explores similar issues in her book Alone Together as she discusses how people confuse social media usage with authentic communication. She posits that people tend to act differently online and are less afraid to hurt each other's feelings. Some online behaviors can cause stress and anxiety, due to the permanence of online posts, the fear of being hacked, or of colleges and employers exploring social media pages. Turkle also speculates that people are beginning to prefer texting to face-to-face communication, which can contribute to feelings of loneliness. Some researchers have also found that only exchanges that involved direct communication and reciprocation of messages to each other increased feelings of connectedness. However, passively using social media without sending or receiving messages to individuals does not make people feel less lonely unless they were lonely to begin with.
A study published in the Public Library of Science in 2013 revealed that the perception of Facebook being an important resource for social connection was diminished by the number of people found to have developed low self-esteem, and the more they used the network the lower their level of self-esteem. A current controversial topic is whether or not social media addiction should be explicitly categorized as a psychological ailment. Extended use of social media has led to increased Internet addiction, cyberbullying, sexting, sleep deprivation, and the decline of face-to-face interaction. Several clinics in the UK classify social media addiction is a certifiable medical condition with one psychiatric consultant claiming that he treats as many as one hundred cases a year. Lori Ann Wagner, a psychotherapist, argues that humans communicate best face to face with their five senses involved. In addition, a study on social media done by PhD's Hsuan-Ting Kim and Yonghwan Kim, suggests that social networking sites have begun to raise concern because of the expectations people seek to fulfill from these sites and the amount of time users are willing to invest.
However, there are also positive effects as there are negative ones. Social media is a great way to make sure that people know that one is in a relationship or not, advertising in their about section. This can reduce the complications and confusions that may have been a problem when social media was not so popular. It is a great way of sharing big dates that may have occurred in ones life, such as a pregnancy announcement or engagement. Not all aspects about social media negatively effect interpersonal relationships. Some aspects encourage the relationships and build them to be stronger.




As social media usage has become increasingly widespread, social media has to a large extent come to be subjected to commercialization by marketing companies and advertising agencies. Christofer Laurell, a digital marketing researcher, suggested that the social media landscape currently consists of three types of places becacuse of this development: consumer-dominated places, professionally dominated places and places undergoing commercialization. As social media becomes commercialized, this process have been shown to create novel forms of value networks stretching between consumer and producer in which a combination of personal, private and commercial contents are created. The commercial development of social media has been criticized as the actions of consumers in these settings has become increasingly value-creating, for example when consumers contribute to the marketing and branding of specific products by posting positive reviews. As such, value-creating activities also increase the value of a specific product, which could, according to the marketing professors Bernad Cova and Daniele Dalli, lead to what they refer to as "double exploitation". Companies are getting consumers to create content for the companies' websites for which the consumers are not paid.



There are several negative effects to social media which receive criticism, for example regarding privacy issues, information overload and Internet fraud. Social media can also have negative social effects on users. Angry or emotional conversations can lead to real-world interactions outside of the Internet, which can get users into dangerous situations. Some users have experienced threats of violence online and have feared these threats manifesting themselves offline. Studies also show that social media have negative effects on peoples' self-esteem and self-worth. The authors of "Who Compares and Despairs? The Effect of Social Comparison Orientation on Social Media Use and its Outcomes" found that people with a higher social comparison orientation appear to use social media more heavily than people with low social comparison orientation. This finding was consistent with other studies that found people with high social comparison orientation make more social comparisons once on social media. People compare their own lives to the lives of their friends through their friends' posts. People are motivated to portray themselves in a way that is appropriate to the situation and serves their best interest. Often the things posted online are the positive aspects of people's lives, making other people question why their own lives are not as exciting or fulfilling. This can lead to depression and other self-esteem issues.
Three researchers at Blanquerna University, Spain, examined how adolescents interact with social media and specifically Facebook.They suggest that interactions on the website encourage representing oneself in the traditional gender constructs, which helps maintain gender stereotypes. The authors noted that girls generally show more emotion in their posts and more frequently change their profile pictures, which according to some psychologists can lead to self-objectification. On the other hand, the researchers found that boys prefer to portray themselves as strong, independent, and powerful. For example, men often post pictures of objects and not themselves, and rarely change their profile pictures; using the pages more for entertainment and pragmatic reasons. In contrast girls generally post more images that include themselves, friends and things they have emotional ties to, which the researchers attributed that to the higher emotional intelligence of girls at a younger age. The authors sampled over 632 girls and boys from the ages of 12–16 from Spain in an effort to confirm their beliefs. The researchers concluded that masculinity is more commonly associated with a positive psychological well-being, while femininity displays less psychological well-being. Furthermore, the researchers discovered that people tend not to completely conform to either stereotype, and encompass desirable parts of both. Users of Facebook generally use their profile to reflect that they are a "normal" person. Social media was found to uphold gender stereotypes both feminine and masculine. The researchers also noted that the traditional stereotypes are often upheld by boys more so than girls. The authors described how neither stereotype was entirely positive, but most people viewed masculine values as more positive.
Terri H. Chan, the author of "Facebook and its Effects on Users' Empathic Social Skills and Life Satisfaction: A Double Edged Sword Effect", claims that the more time people spend on Facebook, the less satisfied they feel about their life. Self-presentational theory explains that people will consciously manage their self-image or identity related information in social contexts. According to Gina Chen, the author of Losing Face on Social Media: Threats to Positive Face Lead to an Indirect Effect on Retaliatory Aggression Through Negative Affect, when people are not accepted or are criticized online they feel emotional pain. This may lead to some form of online retaliation such as online bullying. Trudy Hui Hui Chua and Leanne Chang's article, "Follow Me and Like My Beautiful Selfies: Singapore Teenage Girls' Engagement in Self-Presentation and Peer Comparison on Social Media" states that teenage girls manipulate their self-presentation on social media to achieve a sense of beauty that is projected by their peers. These authors also discovered that teenage girls compare themselves to their peers on social media and present themselves in certain ways in effort to earn regard and acceptance, which can actually lead to problems with self-confidence and self-satisfaction.
According to writer Christine Rosen in "Virtual Friendship, and the New Narcissism," many social media sites encourage status-seeking. According to Rosen, the practice and definition of "friendship" changes in virtuality. Friendship "in these virtual spaces is thoroughly different from real-world friendship. In its traditional sense, friendship is a relationship which, broadly speaking, involves the sharing of mutual interests, reciprocity, trust, and the revelation of intimate details over time and within specific social (and cultural) contexts. Because friendship depends on mutual revelations that are concealed from the rest of the world, it can only flourish within the boundaries of privacy; the idea of public friendship is an oxymoron." Rosen also cites Brigham Young University researchers who "recently surveyed 184 users of social networking sites and found that heavy users 'feel less socially involved with the community around them.'" Critic Nicholas G. Carr in "Is Google Making Us Stupid?" questions how technology affects cognition and memory. "The kind of deep reading that a sequence of printed pages promotes is valuable not just for the knowledge we acquire from the author's words but for the intellectual vibrations those words set off within our own minds. In the quiet spaces opened up by the sustained, undistracted reading of a book, or by any other act of contemplation, for that matter, we make our own associations, draw our own inferences and analogies, foster our own ideas... If we lose those quiet spaces, or fill them up with "content," we will sacrifice something important not only in our selves but in our culture."
Bo Han, a social media researcher at Texas A&M University-Commerce, finds that users are likely to experience the "social media burnout" issue. Ambivalence, emotional exhaustion, and depersonalization are usually the main symptoms if a user experiences social media burnout. Ambivalence refers to a user's confusion about the benefits she can get from using a social media site. Emotional exhaustion refers to the stress a user has when using a social media site. Depersonalization refers to the emotional detachment from a social media site a user experiences. The three burnout factors can all negatively influence the user's social media continuance. This study provides an instrument to measure the burnout a user can experience, when her social media "friends" are generating an overwhelming amount of useless information (e.g., "what I had for dinner", "where I am now").



In the book Networked – The New Social Operating System by Lee Rainie and Barry Wellman, the two authors reflect on mainly positive effects of social media and other Internet-based social networks. According to the authors, social media are used to document memories, learn about and explore things, advertise oneself and form friendships. For instance, they claim that the communication through Internet based services can be done more privately than in real life. Furthermore, Rainie and Wellman discuss that everybody has the possibility to become a content creator. Content creation provides networked individuals opportunities to reach wider audiences. Moreover, it can positively affect their social standing and gain political support. This can lead to influence on issues that are important for someone. As a concrete example of the positive effects of social media, the authors use the Tunisian revolution in 2011, where people used Facebook to gather meetings, protest actions, etc. Rainie and Wellman (Ibid) also discuss that content creation is a voluntary and participatory act. What is important is that networked individuals create, edit, and manage content in collaboration with other networked individuals. This way they contribute in expanding knowledge. Wikis are examples of collaborative content creation.
A survey conducted (in 2011), by Pew Internet Research, discussed in Lee Rainie and Barry Wellman's Networked – The New Social Operating System, illustrates that 'networked individuals' are engaged to a further extent regarding numbers of content creation activities and that the 'networked individuals' are increasing over a larger age span. These are some of the content creation activities that networked individuals take part in:
writing material, such as text or online comments, on a social networking site such as Facebook: 65% of Internet users do this
sharing digital photos: 55%
contributing rankings and reviews of products or services: 37%
creating "tags" of content, such as tagging songs by genre: 33%
posting comments on third-party websites or blogs: 26%
taking online material and remixing it into a new creation: 15% of Internet users do this with photos, video, audio, or text
creating or working on a blog: 14%
Another survey conducted (in 2015) by Pew Internet Research shows that the Internet users among American adults who uses at least one social networking site has increased from 10% to 76% since 2005. Pew Internet Research illustrates furthermore that it nowadays is no real gender difference among Americans when it comes to social media usage. Women were even more active on social media a couple of years ago, however today's numbers point at women: 68%, and men: 62%. Social media have been used to assist in searches for missing persons. When 21-year-old University of Cincinnati student Brogan Dulle disappeared in May 2014 from near his apartment in the Clifton neighborhood of Cincinnati, Ohio, his friends and family used social media to organize and fund a search effort. The disappearance made international news when their efforts went viral on Facebook, Twitter, GoFundMe, and The Huffington Post during the week-long search. Dulle's body was eventually found in a building next door to his apartment.




Use of social media by young people has caused significant problems for some applicants who are active on social media when they try to enter the job market. A survey of 17,000 young people in six countries in 2013 found that 1 in 10 people aged 16 to 34 have been rejected for a job because of online comments they made on social media websites. A 2014 survey of recruiters found that 93% of them check candidates' social media postings. Moreover, professor Stijn Baert of Ghent University conducted a field experiment in which fictitious job candidates applied for real job vacancies in Belgium. They were identical except in one respect: their Facebook profile photos. It was found that candidates with the most wholesome photos were a lot more likely to receive invitations for job interviews than those with the more controversial photos. In addition, Facebook profile photos had a greater impact on hiring decisions when candidates were highly educated. These cases have created some privacy implications as to whether or not companies should have the right to look at employee's Facebook profiles. In March 2012, Facebook decided they might take legal action against employers for gaining access to employee's profiles through their passwords. According to Facebook Chief Privacy Officer for policy, Erin Egan, the company has worked hard to give its users the tools to control who sees their information. He also said users shouldn't be forced to share private information and communications just to get a job. According to the network's Statement of Rights and Responsibilities, sharing or soliciting a password is a violation of Facebook policy. Employees may still give their password information out to get a job, but according to Erin Egan, Facebook will continue to do their part to protect the privacy and security of their users.




Before social media, admissions officials in the United States used SAT and other standardized test scores, extra-curricular activities, letters of recommendation, and high school report cards to determine whether to accept or deny an applicant. In the 2010s, while colleges and universities still use these traditional methods to evaluate applicants, these institutions are increasingly accessing applicants' social media profiles to learn about their character and activities. According to Kaplan, Inc, a corporation that provides higher education preparation, in 2012 27% of admissions officers used Google to learn more about an applicant, with 26% checking Facebook. Students whose social media pages include offensive jokes or photos, racist or homophobic comments, photos depicting the applicant engaging in illegal drug use or drunkenness, and so on, may be screened out from admission processes.




People are increasingly getting political news and information from social media platforms. A 2014 study showed that 62% of web users turn to Facebook to find political news. This social phenomenon allows for political information, true or not, to spread quickly and easily among peer networks. Furthermore, social media sites are now encouraging political involvement by uniting like-minded people, reminding users to vote in elections, and analyzing users' political affiliation data to find cultural similarities and differences. Social media can help taint the reputation of political figures fairly quickly with information that may or may not be true. Information spreads like wildfire and before a politician can even get an opportunity to address the information, either to confirm, deny, or explain, the public has already formed an opinion about the politician based on that information. However, when conducted on purpose, the spread of information on social media for political means can help campaigns immensely. The Barack Obama presidential campaign, 2008, is considered to be one of the most successful in terms of social media. On the other hand, negative word-of-mouth in social media concerning a political figure can be very unfortunate for a politician and can cost the politician his/her career if the information is very damaging. For example, Anthony Weiner's misuse of the social media platform Twitter to send inappropriate messages eventually led to his resignation from U.S. Congress.
Open forums online have led to some negative effects in the political sphere. Some politicians have made the mistake of using open forums to try and reach a broader audience and thus more potential voters. What they forgot to account for was that the forums would be open to everyone, including those in opposition. Having no control over the comments being posted, negative included, has been damaging for some with unfortunate oversight. Additionally, a constraint of social media as a tool for public political discourse is that if oppressive governments recognize the ability social media has to cause change, they shut it down. During the peak of the Egyptian Revolution of 2011, the Internet and social media played a huge role in facilitating information. At that time, Hosni Mubarak was the president of Egypt and head the regime for almost 30 years. Mubarak was so threatened by the immense power that the Internet and social media gave the people that the government successfully shut down the Internet, using the Ramses Exchange, for a period of time in February 2011.
Social media as an open forum gives a voice to those who have previously not had the ability to be heard. In 2015, some countries are still becoming equipped with Internet accessibility and other technologies. Social media is giving everyone a voice to speak out against government regimes. In 2014, the rural areas in Paraguay were only just receiving access to social media, such as Facebook. In congruence with the users worldwide, teens and young adults in Paraguay are drawn to Facebook and others types of social media as a means to self-express. Social media is becoming a main conduit for social mobilization and government critiques because, "the government can't control what we say on the Internet."
Younger generations are becoming more involved in politics due to the increase of political news posted on various types of social media. Due to the heavier use of social media among younger generations, they are exposed to politics more frequently, and in a way that is integrated into their online social lives. While informing younger generations of political news is important, there are many biases within the realms of social media. It can be difficult for outsiders to truly understand the conditions of dissent when they are removed from direct involvement. Social media can create a false sense of understanding among people who are not directly involved in the issue. An example of social media creating misconceptions can be seen during the Arab Spring protests. Today's generation rely heavily on social media to understand what is happening in the world, and consequently people are exposed to both true and false information. For example, Americans have several misconceptions surrounding the events of the Arab Springs movement. Social media can be used to create political change, both major and minor. For example, in 2011 Egyptians used Facebook, Twitter, and YouTube as a means to communicate and organize demonstrations and rallies to overthrow President Hosni Mubarak. Statistics show that during this time the rate of Tweets from Egypt increased from 2,300 to 230,000 per day and the top 23 protest videos had approximately 5.5 million views.



People around the world are taking advantage of social media as one of their key components of communication. According to King, 67 percent of US citizens ages 12 and up use social media of some type. With the expansion of social media networks there are many positive and negative alternatives. As the use of Twitter increases, its influence impacts users as well. The potential role of Twitter as a means of both service feedback and a space in which mental health can be openly discussed and considered from a variety of perspectives. The study conducted shows a positive outlook for using Twitter to discuss health issues with a patient and a professional, in this case alcohol. On the other hand, there can be negatives that arise from the use of social media. If a clinician prescribes abstinence from alcohol but then posts pictures on social media of one's own drunken exploits, the clinician's credibility is potentially lost in the eyes of the patient. In these two studies, both negative and positive outcomes were examined. Although social media can be beneficial, it is important to understand the negative consequences as well.



As the world is becoming increasingly connected via the power of the Internet, political movements, including militant groups, have begun to see social media as a major organizing and recruiting tool. Islamic State of Iraq and the Levant, also known as ISIS, has used social media to promote their cause. ISIS produces an online magazine named the Islamic State Report to recruit more fighters. ISIS produces online materials in a number of languages and uses recruiters to contact potential recruitees over the Internet.
In Canada, two girls from Montreal left their country to join ISIS in Syria after exploring ISIS on social media and eventually being recruited. On Twitter, there is an app called the Dawn of Glad Tidings that users can download and keep up to date on news about ISIS. Hundreds of users around the world have signed up for the app which once downloaded will post tweets and hash-tags to your account that are in support of ISIS. As ISIS marched on the northern region of Iraq, tweets in support of their efforts reached a high of 40,000 a day. ISIS support online is a factor in the radicalization of youth. Mass media has yet to adopt the view that social media plays a vital link in the radicalization of people. When tweets supportive of ISIS make their way onto Twitter, they result in 72 re-tweets to the original, which further spreads the message of ISIS. These tweets have made their way to the account known as active hashtags, which further helps broadcast ISIS' message as the account sends out to its followers the most popular hashtags of the day. Other militant groups such as al-Qaeda and the Taliban are increasingly using social media to raise funds, recruit and radicalize persons, and it has become increasingly effective.




There has been rapid growth in the number of US patent applications that cover new technologies related to social media, and the number of them that are published has been growing rapidly over the past five years. There are now over 2000 published patent applications. As many as 7000 applications may be currently on file including those that haven't been published yet. Only slightly over 100 of these applications have issued as patents, however, largely due to the multi-year backlog in examination of business method patents, patents which outline and claim new methods of doing business.



Having social media in the classroom has been a controversial topic in the 2010s. Many parents and educators have been fearful of the repercussions of having social media in the classroom. There are concerns that social media tools can be misused for cyberbullying or sharing inappropriate content. As result, cell phones have been banned from some classrooms, and some schools have blocked many popular social media websites. However, despite apprehensions, students in industrialized countries are (or will be) active social media users. As a result, many schools have realized that they need to loosen restrictions, teach digital citizenship skills, and even incorporate these tools into classrooms. The Peel District School Board (PDSB) in Ontario is one of many school boards that has begun to accept the use of social media in the classroom. In 2013, the PDSB introduced a "Bring Your Own Device" (BYOD) policy and have unblocked many social media sites. Fewkes and McCabe (2012) have researched about the benefits of using Facebook in the classroom. Some schools permit students to use smartphones or tablet computers in class, as long as the students are using these devices for academic purposes, such as doing research.




In early 2013, Steve Joordens, a professor at the University of Toronto, encouraged the 1,900 students enrolled in his introductory psychology course to add content to Wikipedia pages featuring content that related to the course. Like other educators, Joordens argued that the assignment would not only strengthen the site's psychology-related content, but also provide an opportunity for students to engage in critical reflection about the negotiations involved in collaborative knowledge production. However, Wikipedia's all-volunteer editorial staff complained that the students' contributions resulted in an overwhelming number of additions to the site, and that some of the contributions were inaccurate.



Using Facebook in class allows for both an asynchronous and synchronous, open speech via a familiar and regularly accessed medium, and supports the integration of multimodal content such as student-created photographs and video and URLs to other texts, in a platform that many students are already familiar with. Further, it allows students to ask more minor questions that they might not otherwise feel motivated to visit a professor in person during office hours to ask. It also allows students to manage their own privacy settings, and often work with the privacy settings they have already established as registered users. Facebook is one alternative means for shyer students to be able to voice their thoughts in and outside of the classroom. It allows students to collect their thoughts and articulate them in writing before committing to their expression. Further, the level of informality typical to Facebook can also aid students in self-expression and encourage more frequent student-and-instructor and student-and-student communication. At the same time, Towner and Munoz note that this informality may actually drive many educators and students away from using Facebook for educational purposes.
From a course management perspective, Facebook may be less efficient as a replacement for more conventional course management systems, both because of its limitations with regards to uploading assignments and due to some students' (and educators') resistance to its use in education. Specifically, there are features of student-to-student collaboration that may be conducted more efficiently on dedicated course management systems, such as the organization of posts in a nested and linked format. That said, a number of studies suggest that students post to discussion forums more frequently and are generally more active discussants on Facebook posts versus conventional course management systems like WebCT or Blackboard (Chu and Meulemans, 2008; Salaway, et al., 2008; Schroeder and Greenbowe, 2009).
Further, familiarity and comfortability with Facebook is often divided by socio-economic class, with students whose parents obtained a college degree, or at least having attended college for some span of time, being more likely to already be active users. Instructors ought to seriously consider and respect these hesitancies, and refrain from "forcing" Facebook on their students for academic purposes. Instructors also ought to consider that rendering Facebook optional, but continuing to provide content through it to students who elect to use it, places an unfair burden on hesitant students, who then are forced to choose between using a technology they are uncomfortable with and participating fully in the course. A related limitation, particularly at the level of K-12 schooling, is the distrust (and in some cases, outright prohibition) of the use of Facebook in formal classroom settings in many educational jurisdictions. However, this hesitancy towards Facebook use is continually diminishing in the United States, as the Pew Internet & American Life Project's annual report for 2012 shows that the likelihood of a person to be a registered Facebook user only fluctuates by 13 percent between different levels of educational attainment, 9 percent between urban, suburban, and rural users, only 5 percent between different household income brackets. The largest gap occurs between age brackets, with 86 percent of 18- to 29-year-olds reported as registered users as opposed to only 35 percent of 65-and-up-year-old users.




Twitter can be used to enhance communication building and critical thinking. Domizi (2013) utilized Twitter in a graduate seminar requiring students to post weekly tweets to extend classroom discussions. Students reportedly used Twitter to connect with content and other students. Additionally, students found it "to be useful professionally and personally". Junco, Heibergert, and Loken (2011) completed a study of 132 students to examine the link between social media and student engagement and social media and grades. They divided the students into two groups, one used Twitter and the other did not. Twitter was used to discuss material, organize study groups, post class announcements, and connect with classmates. Junco and his colleagues (2011) found that the students in the Twitter group had higher GPAs and greater engagement scores than the control group.
Gao, Luo, and Zhang (2012) reviewed literature about Twitter published between 2008 and 2011. They concluded that Twitter allowed students to participate with each other in class (by creating an informal "back channel"), and extend discussion outside of class time. They also reported that students used Twitter to get up-to-date news and connect with professionals in their field. Students reported that microblogging encouraged students to "participate at a higher level". Because the posts cannot exceed 140 characters, students were required to express ideas, reflect, and focus on important concepts in a concise manner. Some students found this very beneficial. Other students did not like the character limit. Also, some students found microblogging to be overwhelming (information overload). The research indicated that many students did not actually participate in the discussions, "they just lurked" online and watched the other participants.



A popular component and feature of Twitter is retweeting. Twitter allows other people to keep up with important events, stay connected with their peers, and can contribute in various ways throughout social media. When certain posts become popular, they start to get tweeted over and over again, becoming viral. Ellen DeGeneres is a prime example of this. She was a host during the 86th Academy Awards, when she took the opportunity to take a selfie with about twelve other celebrities that joined in on the highlight of the night, including Jennifer Lawrence, Brad Pitt and Julia Roberts. This picture went viral within forty minutes and was retweeted 1.8 million times within the first hour. This was an astonishing record for Twitter and the use of selfies, which other celebrities have tried to recreate. Retweeting is beneficial strategy, which notifies individuals on Twitter about popular trends, posts, and events.




YouTube is a frequently used social media tool in the classroom (also the second most visited website in the world). Students can watch videos, answer questions, and discuss content. Additionally, students can create videos to share with others. Sherer and Shea (2011) claimed that YouTube increased participation, personalization (customization), and productivity. YouTube also improved students' digital skills and provided opportunity for peer learning and problem solving Eick et al. (2012) found that videos kept students' attention, generated interest in the subject, and clarified course content. Additionally, the students reported that the videos helped them recall information and visualize real world applications of course concepts.



LinkedIn is a professional social network that enables employers and job-seeking workers to connect. It was created by Reid Hoffman in 2002 and was launched on May 2003. LinkedIn is now the world's largest professional social network with over 300 million members in over 200 countries. The mission of LinkedIn is to "connect the world's professionals to make them more productive and successful." A lot of people describe LinkedIn as a "professional Facebook", but it is important to remember that LinkedIn is not Facebook. Users tend to avoid informal nicknames and any inappropriate pictures of their private lives in their profile. Instead, they use a standard headshot as a profile picture and keep the content and information as professional and career-focused as possible. Most LinkedIn users put their CV online. Some also provide a list of the courses they have taken in college or university. Users can also post articles that they have written or published, which enables prospective employers to see their written work.
There are over 39 million students and recent college graduates on LinkedIn, becoming the fastest-growing demographic on the site. There are many ways that LinkedIn can be used in the classroom. First and foremost, using LinkedIn in the classroom encourages students to have a professional online social presence and can help them become comfortable in searching for a job or internship. "The key to making LinkedIn a great social learning tool is to encourage learners to build credibility through their profiles, so that experts and professionals won't think twice about connecting with them and share knowledge." Dedicating class time solely for the purpose of setting up LinkedIn accounts and showing students how to navigate it and build their profile will set them up for success in the future. Next, professors can create assignments that involve using LinkedIn as a research tool. The search tool in LinkedIn gives students the opportunity to seek out organizations they are interested in and allow them to learn more.
Giving students the class time to work on their LinkedIn profile allows them to network with each other, and stresses the importance of networking. Finally, professors can design activities that revolve around resume building and interviews. A person's LinkedIn and resume are what employers look at first, and they need to know how to make a strong first impression. It's important to learn how to construct a strong resume as soon as possible, as well as learn strong interviewing skills. Not only is the information and skills learned in the classroom important, but it is also important to know how to apply the information and skills to their LinkedIn profile so they can get a job in their field of study. These skills can be gained while incorporating LinkedIn into the classroom.






In 2013, the United Kingdom Advertising Standards Authority (ASA) began to advise celebrities and sports stars to make it clear if they had been paid to tweet about a product or service by using the hashtag #spon or #ad within tweets containing endorsements. In July 2013, Wayne Rooney was accused of misleading followers by not including either of these tags in a tweet promoting Nike. The tweet read:"The pitches change. The killer instinct doesn't. Own the turf, anywhere. @NikeFootball #myground." The tweet was investigated by the ASA but no charges were pressed. The ASA stated that "We considered the reference to Nike Football was prominent and clearly linked the tweet with the Nike brand." When asked about whether the number of complaints regarding misleading social advertising had increased, the ASA stated that the number of complaints had risen marginally since 2011 but that complaints were "very low" in the "grand scheme."




Social media often features in political struggles to control public perception and online activity. In some countries, Internet police or secret police monitor or control citizens' use of social media. For example, in 2013 some social media was banned in Turkey after the Taksim Gezi Park protests. Both Twitter and YouTube were temporarily suspended in the country by a court's decision. A new law, passed by Turkish Parliament, has granted immunity to Telecommunications Directorate (TİB) personnel. The TİB was also given the authority to block access to specific websites without the need for a court order. Yet TİB's 2014 blocking of Twitter was ruled by the constitutional court to violate free speech. More recently, in the 2014 Thai coup d'état, the public was explicitly instructed not to 'share' or 'like' dissenting views on social media or face prison. In July that same year, in response to Wikileaks' release of a secret suppression order made by the Victorian Supreme Court, media lawyers were quoted in the Australian media to the effect that "anyone who tweets a link to the Wikileaks report, posts it on Facebook, or shares it in any way online could also face charges".



Social media has affected the way youth communicate, by introducing new forms of language. Abbreviations have been introduced to cut down on the time it takes to respond online. The commonly known "LOL" has become globally recognized as the abbreviation for "laugh out loud" thanks to social media. Online linguistics has changed the way youth communicate and will continue to do so in the future, as each year new catchphrases and neologisms such as "YOLO", which stands for "you only live once", and "BAE", which stands for "before anyone else" arise and start "trending" around the world.
Other trends that influence the way youth communicate is through hashtags. With the introduction of social media platforms such as Twitter, Facebook and Instagram, the hashtag was created to easily organize and search for information. As hashtags such as #tbt ("throwback Thursday") become a part of online communication, it influenced the way in which youth share and communicate in their daily lives. Because of these changes in linguistics and communication etiquette, researchers of media semiotics have found that this has altered youth's communications habits and more.
Social media also alters the way we understand each other. Social media has allowed for mass cultural exchange and intercultural communication. For example, people from different regions or even different countries can discuss current issues on Facebook. As different cultures have different value systems, cultural themes, grammar, and worldviews, they also communicate differently. The emergence of social media platforms collided different cultures and their communication methods together, forcing them to realign in order to communicate with ease with other cultures. As different cultures continue to connect through social media platforms, thinking patterns, expression styles and cultural content that influence cultural values are chipped away.









Benkler, Yochai (2006). The Wealth of Networks. New Haven: Yale University Press. ISBN 0-300-11056-1. OCLC 61881089. 
Gentle, Anne (2012). Conversation and Community: The Social Web for Documentation (2nd ed.). Laguna Hills, CA: XML Press. ISBN 978-1-937434-10-6. OCLC 794490599. 
Johnson, Steven Berlin (2005). Everything Bad Is Good for You. New York: Riverhead Books. ISBN 1-57322-307-7. OCLC 57514882. 
Jue, Arthur L., Jackie Alcalde Marr, Mary Ellen Kassotakis (2010). Social media at work : how networking tools propel organizational performance (1st ed.). San Francisco, CA: Jossey-Bass. ISBN 978-0470405437. 
Lardi, Kamales; Fuchs, Rainer (2013). Social Media Strategy – A step-by-step guide to building your social business (1st ed.). Zurich: vdf. ISBN 978-3-7281-3557-5. 
Li, Charlene; Bernoff, Josh (2008). Groundswell: Winning in a World Transformed by Social Technologies. Boston: Harvard Business Press. ISBN 978-1-4221-2500-7. OCLC 423555651. 
McHale, Robert; Garulay, Eric (2012). Navigating Social Media Legal Risks: Safeguarding Your Business. Que. ISBN 978-0-789-74953-6. 
Piskorski, Mikołaj Jan (2014). A Social Strategy: How We Profit from Social Media. Princeton, NJ: Princeton University Press. ISBN 978-0-691-15339-1. 
Powell, Guy R.; Groves, Steven W.; Dimos, Jerry (2011). ROI of Social Media: How to improve the return on your social marketing investment. New York: John Wiley & Sons. ISBN 978-0-470-82741-3. OCLC 0470827416. 
Rheingold, Howard (2002). Smart mobs: The next social revolution (1st printing ed.). Cambridge, MA: Perseus Pub. p. 288. ISBN 978-0-7382-0608-0. 
Scoble, Robert; Israel, Shel (2006). Naked Conversations: How Blogs are Changing the Way Businesses Talk with Customers. Hoboken, N.J: John Wiley. ISBN 0-471-74719-X. OCLC 61757953. 
Shirky, Clay (2008). Here Comes Everybody. New York: Penguin Press. ISBN 978-1-59420-153-0. OCLC 458788924. 
Siegel, Alyssa (September 7, 2015). "How Social Media Affects Our Relationships". Psychology Tomorrow. 
Surowiecki, James (2004). The Wisdom of Crowds. New York: Anchor Books. ISBN 0-385-72170-6. OCLC 156770258. 
Tapscott, Don; Williams, Anthony D. (2006). Wikinomics. New York: Portfolio. ISBN 1-59184-138-0. OCLC 318389282. 
Watts, Duncan J. (2003). Six degrees: The science of a connected age. London: Vintage. p. 368. ISBN 978-0-09-944496-1. 
Tedesco, Laura Anne. The Metropolitan Museum of Art. 200-2013. 12 02 2014
Agozzino, Alisa. "Building A Personal Relationship Through Social Media: A Study Of Millennial Students' Brand Engagement." Ohio Communication Journal 50. (2012): 181–204. Communication Abstracts. Web. 3 Dec. 2013.
Schoen, Harald, et al. "The Power Of Prediction With Social Media." Internet Research 23.5 (2013): 528–543. Communication Abstracts. Web. 3 Dec. 2013.
Mateus, Samuel (2012). "Social Networks Scopophilic dimension – social belonging through spectatorship". 
Schrape, Jan-Felix (2016). "Social Media, Mass Media and the ›Public Sphere‹. Differentiation, Complementarity and Co-existence" (PDF). Stuttgart: Research Contributions to Organizational Sociology and Innovation Studies 2016-01. ISSN 2191-4990.Journalism is the production and distribution of reports on the interaction of events, facts, ideas, and people that are the "news of the day" and that informs society to at least some degree. The word applies to the occupation (professional or not), the methods of gathering information, and the organizing literary styles. Journalistic media include: print, television, radio, Internet, and, in the past, newsreels.
Concepts of the appropriate role for journalism varies between countries. In some nations, the news media is controlled by a government intervention, and is not a fully independent body. In others, the news media is independent from the government but the profit motive is in tension with constitutional protections of freedom of the press. Access to freely available information gathered by independent and competing journalistic enterprises with transparent editorial standards can enable citizens to effectively participate in the political process. In the United States, journalism is protected by the freedom of the press clause in the First Amendment.
The role and status of journalism, along with that of the mass media, has undergone changes over the last two decades with the advent of digital technology and publication of news on the Internet. This has created a shift in the consumption of print media channels, as people increasingly consume news through e-readers, smartphones, and other electronic devices, challenging news organizations to fully monetize their digital wing, as well as improvise on the context in which they publish news in print. Notably, in the American media landscape, newsrooms have reduced their staff and coverage as traditional media channels, such as television, grapple with declining audiences. For instance, between 2007 and 2012, CNN edited its story packages into nearly half of their original time length.
This compactness in coverage has been linked to broad audience attrition, as a large majority of respondents in recent studies show changing preferences in news consumption. The digital era has also ushered in a new kind of journalism in which ordinary citizens play a greater role in the process of news making, with the rise of citizen journalism being possible through the Internet. Using video camera equipped smartphones, active citizens are now enabled to record footage of news events and upload them onto channels like YouTube, which is often discovered and used by mainstream news media outlets. Meanwhile, easy access to news from a variety of online sources, like blogs and other social media, has resulted in readers being able to pick from a wider choice of official and unofficial sources, instead of only from traditional media organizations. Journalism is nonfiction.




Journalistic conventions vary by country. In the United States, journalism is produced by media organizations or by individuals. Bloggers are often, but not always, journalists. The Federal Trade Commission requires that bloggers who receive free promotional gifts, then write about products, must disclose that they received the products for free. This is to eliminate conflicts of interest and protect consumers.
Fake news is news that is not truthful or is produced by unreliable media organizations. Fake news is easily spread on social media. Readers can determine fake news by evaluating whether the news has been published by a credible news organization. In the US, a credible news organization is an incorporated entity; has an editorial board; and has a clear division between editorial and advertising departments. Credible news organizations, or their employees, belong to one or more professional organizations such as the American Society of News Editors, the Society of Professional Journalists, Investigative Reporters & Editors, or the Online News Association. All of these organizations have codes of ethics that members abide by. Many news organizations have their own codes of ethics that guide journalists' professional publications. The New York Times code of standards and ethics is considered particularly rigorous.
When they write stories, journalists are concerned with issues of objectivity and bias. Some types of stories are intended to represent the author's own opinion; other types of stories are intended to be more neutral or balanced. In a physical newspaper, information is organized into sections and it is easy to see which stories are supposed to be opinion and which are supposed to be neutral. Online, many of these distinctions break down. Readers should pay careful attention to headings and other design elements to ensure that they understand the journalist's intent. Opinion pieces generally are written by regular columnists or appear in a section titled "Op-ed." Feature stories, breaking news, and hard news stories are generally not opinion pieces.
Many debates center on whether journalists are "supposed" to be "objective" or "neutral." The idea of "journalistic objectivity" is considered out of date. Journalists are people who produce news out of and as part of a particular social context. They are guided by professional codes of ethics and do their best to represent all legitimate points of view.




There are several different forms of journalism, all with diverse audiences. Journalism is said to serve the role of a "fourth estate", acting as a watchdog on the workings of the government. A single publication (such as a newspaper) contains many forms of journalism, each of which may be presented in different formats. Each section of a newspaper, magazine, or website may cater to different audiences.

Some forms include:

Advocacy journalism – writing to advocate particular viewpoints or influence the opinions of the audience.
Broadcast journalism – written or spoken journalism for radio or television.
Citizen journalism -- participatory journalism.
Data journalism -- the practice of finding stories in numbers, and using numbers to tell stories. Data journalists may use data to support their reporting. They may also report about uses and misuses of data. The US news organization ProPublica is known as a pioneer of data journalism.
Drone journalism – use of drones to capture journalistic footage.
Gonzo journalism – first championed by Hunter S. Thompson, gonzo journalism is a "highly personal style of reporting".
Interactive journalism: a type of online journalism that is presented on the web
Investigative journalism: in-depth reporting that uncovers social problems. Often leads to major social problems being resolved.
Photojournalism: the practice of telling true stories through images
Sensor journalism: the use of sensors to support journalistic inquiry.
Tabloid journalism – writing that is light-hearted and entertaining. Considered less legitimate than mainstream journalism.
Yellow journalism (or sensationalism) – writing which emphasizes exaggerated claims or rumours.

The recent rise of social media has resulted in arguments to reconsider journalism as a process rather than attributing it to particular news products. From this perspective, journalism is participatory, a process distributed among multiple authors and involving journalists as well as the socially mediating public.




Johann Carolus's Relation aller Fürnemmen und gedenckwürdigen Historien, published in 1605 in Strassburg, is often recognized as the first newspaper. The first successful English daily, the Daily Courant, was published from 1702 to 1735. The reform of the Diário Carioca newspaper in the 1950s is usually referred to as the birth of modern journalism in Brazil.




 In the 1920s, as modern journalism was just taking form, writer Walter Lippmann and American philosopher John Dewey debated over the role of journalism in a democracy. Their differing philosophies still characterize a debate about the role of journalism in society and the nation-state.
To Lippmann, the journalist fulfilled the role of mediator, or translator, between the general public and policy-making elites. Lippmann reasoned that the public could not assess modern society's growingly complex flurry of facts; therefore, it needed an intermediary to filter its news. Journalists served as this intermediary, recording the information exchanged among elites, distilling it, and passing it on for public consumption. The public would affect the decisions of the elite with its vote; in the meantime, the elite would keep the business of power running. Effectively, Lippmann's philosophy had the public at the bottom of the power chain, inheriting its information from the elite.

Lippmann's elitism had consequences that he came to deplore. An apostle of historicism and scientism, Lippmann did not merely hold that democratic government was a problematic exercise, but regarded all political communities, of whatever stripe, as needing guidance from a transcendent partisanship for accurate information and dispassionate judgment. In "Liberty and the News" (1919) and "Public Opinion" (1921) Lippmann expressed the hope that liberty could be redefined to take account of the scientific and historical perspective and that public opinion could be managed by a system of intelligence in and out of government. Thus the liberty of the journalist was to be dedicated to gathering verifiable facts while commentators like himself would place the news in the broader perspective. Lippmann deplored the influence of powerful newspaper publishers and preferred the judgments of the "patient and fearless men of science". In so doing, he denigrated not only the opinion of the majority but also the opinion of those who had influence or power as well. In a republican form of government, the representatives are chosen by the people and share with them adherence to the fundamental principles and political institutions of the polity. Lippmann's quarrel was with those very principles and institutions, for they are the product of the pre-scientific and pre-historical viewpoint and what for him was a groundless natural-rights political philosophy.
But Lippmann turned against what he called the "collectivism" of the Progressive movement he encouraged with its de-emphasis on the foundations of American politics and government and ultimately wrote a work, "The Public Philosophy" (1955), which came very close to a return to the principles of the American founders.
Dewey, on the other hand, believed not only that the public was capable of understanding the issues created or responded to by the elite, but also that it was in the public forum that decisions should be made after discussion and debate. When issues were thoroughly vetted, then the best ideas would bubble to the surface. Dewey believed journalists should do more than simply pass on information. He believed they should weigh the consequences of the policies being enacted. Over time, his idea has been implemented in various degrees, and is more commonly known as "community journalism".

This concept of community journalism is at the centre of new developments in journalism. In this new paradigm, journalists are able to engage citizens and the experts and elites in the proposition and generation of content. While there is an assumption of equality, Dewey still celebrated expertise. Dewey believed the shared knowledge of many to be far superior to a single individual's knowledge. Experts and scholars are welcome in Dewey's framework, but there is not the hierarchical structure present in Lippmann's understanding of journalism and society. According to Dewey, conversation, debate, and dialogue lie at the heart of a democracy.
While Lippmann's journalistic philosophy might be more acceptable to government leaders, Dewey's approach is a more encompassing description of how many journalists see their role in society, and, in turn, how much of society expects journalists to function. Americans, for example, may criticize some of the excesses committed by journalists, but they tend to expect journalists to serve as watchdogs on government, businesses and actors, enabling people to make informed decisions on the issues of the time.



Bill Kovach and Tom Rosenstiel propose several guidelines for journalists in their book The Elements of Journalism. Because journalism's first loyalty is to the citizenry, journalists are obliged to tell the truth and must serve as an independent monitor of powerful individuals and institutions within society. The essence of journalism is to provide citizens with reliable information through the discipline of verification.




While various existing codes have some differences, most share common elements including the principles of — truthfulness, accuracy, objectivity, impartiality, fairness and public accountability — as these apply to the acquisition of newsworthy information and its subsequent dissemination to the public.
Some journalistic Codes of Ethics, notably the European ones, also include a concern with discriminatory references in news based on race, religion, sexual orientation, and physical or mental disabilities. The Parliamentary Assembly of the Council of Europe approved in 1993 Resolution 1003 on the Ethics of Journalism which recommends journalists to respect the presumption of innocence, in particular in cases that are still sub judice.
In the UK, all newspapers are bound by the Code of Practice of the Independent Press Standards Organisation.This includes points like respecting people's privacy and ensuring accuracy. However, the Media Standards Trust has criticized the PCC, claiming it needs to be radically changed to secure public trust of newspapers.
This is in stark contrast to the media climate prior to the 20th century, where the media market was dominated by smaller newspapers and pamphleteers who usually had an overt and often radical agenda, with no presumption of balance or objectivity.
Because of the pressure on journalists to report news promptly and before their competitors, factual errors occur more frequently than in writing produced and edited under less time pressure. Thus a typical issue of a major daily newspaper may contain several corrections of articles published the previous day. Perhaps the most famous journalistic mistake caused by time pressure was the Dewey Defeats Truman edition of the Chicago Daily Tribune, based on early election returns that failed to anticipate the actual result of the 1948 US presidential election.



Such a code of conduct can, in the real world, be difficult to uphold consistently. Reporting and editing do not occur in a vacuum but always reflect the political context in which journalists, no less than other citizens, operate.
A news organization's budget inevitably reflects decision-making about what news to cover, for what audience, and in what depth. When budgets are cut, editors may sacrifice reporters in distant news bureaus, reduce the number of staff assigned to low-income areas, or wipe entire communities from the publication's zone of interest.
Publishers, owners and other corporate executives, especially advertising sales executives, could try to use their powers over journalists to influence how news is reported and published. For this reason, journalists traditionally relied on top management to create and maintain a "firewall" between the news and other departments in a news organization to prevent undue influence on the news department.
Although some analysts point to the inherent difficulty of maintaining objectivity, and others practically deny that it is possible, still others point to the requirements of a free press in a democratic society governed by public opinion and a republican government under a limited constitution. According to this latter view, direct or implicit criticism of the government, political parties, corporations, unions, schools and colleges and even churches is both inevitable and desirable, and cannot be done well without clarity regarding fundamental political principles. Hence, objectivity consists both in truthful, accurate reporting and well-reasoned and thoughtful commentary, based upon a firm commitment to a free society's principles of equality, liberty and government by consent.




Governments have widely varying policies and practices towards journalists, which control what they can research and write, and what press organizations can publish. Some governments guarantee the freedom of the press; while other nations severely restrict what journalists can research or publish.
Journalists in many nations have some privileges that members of the general public do not, including better access to public events, crime scenes and press conferences, and to extended interviews with public officials, celebrities and others in the public eye.
Journalists who elect to cover conflicts, whether wars between nations or insurgencies within nations, often give up any expectation of protection by government, if not giving up their rights to protection by government. Journalists who are captured or detained during a conflict are expected to be treated as civilians and to be released to their national government. Many governments around the world target journalists for intimidation, harassment, and violence because of the nature of their work.




Journalists' interaction with sources sometimes involves confidentiality, an extension of freedom of the press giving journalists a legal protection to keep the identity of a confidential informant private even when demanded by police or prosecutors; withholding their sources can land journalists in contempt of court, or in jail.
In the United States, there is no right to protect sources in a federal court. However, federal courts will refuse to force journalists to reveal their sources, unless the information the court seeks is highly relevant to the case and there's no other way to get it. State courts provide varying degrees of such protection. Journalists who refuse to testify even when ordered to can be found in contempt of court and fined or jailed. On the journalistic side of keeping sources confidential, there is also a risk to the journalist's credibility because there can be no actual confirmation of whether the information is valid. As such it is highly discouraged for journalists to have confidential sources.






American Journalism Review
Columbia Journalism Review
Health News Review
Ryerson Review of Journalism
Online Journalism Review









Harcup, Tony (2009), Journalism: Principles and Practice, Thousand Oaks, California: Sage Publications, ISBN 978-1847872500, OCLC 280437077 



Quick, Amanda C. ed. World Press Encyclopedia: A Survey of Press Systems Worldwide (2nd ed. 2 vol 2002); 2500 pp; highly detailed coverage of every country large and small.
de Beer Arnold S. and John C. Merrill, eds. Global Journalism: Topical Issues and Media Systems (5th ed. 2008)
Shoemaker, Pamela J. and Akiba A. Cohen, eds. News Around the World: Content, Practitioners, and the Public (2nd ed. 2005)
Sloan, W. David and Lisa Mullikin Parcell, eds. (2002). American Journalism: History, Principles, Practices. McFarland.  CS1 maint: Multiple names: authors list (link)
Sterling, Christopher H. (ed.), Encyclopedia of journalism, Thousand Oaks, California: SAGE, 2009, 6 vols.




Journalism at DMOZA lawyer is a person who practices law, as an advocate, barrister, attorney, counselor or solicitor or chartered legal executive. Working as a lawyer involves the practical application of abstract legal theories and knowledge to solve specific individualized problems, or to advance the interests of those who hire lawyers to perform legal services.
The role of the lawyer varies greatly across legal jurisdictions, and so it can be treated here in only the most general terms.



In practice, legal jurisdictions exercise their right to determine who is recognized as being a lawyer. As a result, the meaning of the term "lawyer" may vary from place to place.
In Australia, the word "lawyer" is used to refer to both barristers and solicitors (whether in private practice or practicing as corporate in-house counsel).
In Canada, the word "lawyer" only refers to individuals who have been called to the bar or, in Quebec, have qualified as civil law notaries. Common law lawyers in Canada are formally and properly called "barristers and solicitors", but should not be referred to as "attorneys", since that term has a different meaning in Canadian usage. However, in Quebec, civil law advocates (or avocats in French) often call themselves "attorney" and sometimes "barrister and solicitor" in English.
In England and Wales, "lawyer" is used to refer to persons who provide reserved and unreserved legal activities and includes practitioners such as barristers, attorneys, solicitors, registered foreign lawyers, patent attorneys, trade mark attorneys, licensed conveyancers, public notaries, commissioners for oaths, immigration advisers and claims management services. The Legal Services Act 2007 defines the "legal activities" that may only be performed by a person who is entitled to do so pursuant to the Act. 'Lawyer' is not a protected title.
In South Asia, the term "lawyer" is often colloquially used, but the official term is "advocate" as prescribed under the Advocates Act, 1961.
In Scotland, the word "lawyer" refers to a more specific group of legally trained people. It specifically includes advocates and solicitors. In a generic sense, it may also include judges and law-trained support staff.
In the United States, the term generally refers to attorneys who may practice law. It is never used to refer to patent agents or paralegals. In fact, there are regulatory restrictions on non-lawyers like paralegals practicing law.
Other nations tend to have comparable terms for the analogous concept.



In most countries, particularly civil law countries, there has been a tradition of giving many legal tasks to a variety of civil law notaries, clerks, and scriveners. These countries do not have "lawyers" in the American sense, insofar as that term refers to a single type of general-purpose legal services provider; rather, their legal professions consist of a large number of different kinds of law-trained persons, known as jurists, some of whom are advocates who are licensed to practice in the courts. It is difficult to formulate accurate generalizations that cover all the countries with multiple legal professions, because each country has traditionally had its own peculiar method of dividing up legal work among all its different types of legal professionals.
Notably, England, the mother of the common law jurisdictions, emerged from the Dark Ages with similar complexity in its legal professions, but then evolved by the 19th century to a single dichotomy between barristers and solicitors. An equivalent dichotomy developed between advocates and procurators in some civil law countries; these two types did not always monopolize the practice of law, in that they coexisted with civil law notaries.
Several countries that originally had two or more legal professions have since fused or united their professions into a single type of lawyer. Most countries in this category are common law countries, though France, a civil law country, merged its jurists in 1990 and 1991 in response to Anglo-American competition. In countries with fused professions, a lawyer is usually permitted to carry out all or nearly all the responsibilities listed below.




Arguing a client's case before a judge or jury in a court of law is the traditional province of the barrister in England, and of advocates in some civil law jurisdictions. However, the boundary between barristers and solicitors has evolved. In England today, the barrister monopoly covers only appellate courts, and barristers must compete directly with solicitors in many trial courts. In countries like the United States, that have fused legal professions, there are trial lawyers who specialize in trying cases in court, but trial lawyers do not have a de jure monopoly like barristers. In some countries, litigants have the option of arguing pro se, or on their own behalf. It is common for litigants to appear unrepresented before certain courts like small claims courts; indeed, many such courts do not allow lawyers to speak for their clients, in an effort to save money for all participants in a small case. In other countries, like Venezuela or Portugal, no one may appear before a judge unless represented by a lawyer. The advantage of the latter regime is that lawyers are familiar with the court's customs and procedures, and make the legal system more efficient for all involved. Unrepresented parties often damage their own credibility or slow the court down as a result of their inexperience....



Often, lawyers brief a court in writing on the issues in a case before the issues can be orally argued. They may have to perform extensive research into relevant facts and law while drafting legal papers and preparing for oral argument.
In England, the usual division of labor is that a solicitor will obtain the facts of the case from the client and then brief a barrister (usually in writing). The barrister then researches and drafts the necessary court pleadings (which will be filed and served by the solicitor) and orally argues the case.
In Spain, the procurator merely signs and presents the papers to the court, but it is the advocate who drafts the papers and argues the case.
In some countries, like Japan, a scrivener or clerk may fill out court forms and draft simple papers for lay persons who cannot afford or do not need attorneys, and advise them on how to manage and argue their own cases.



In most developed countries, the legislature has granted original jurisdiction over highly technical matters to executive branch administrative agencies which oversee such things. As a result, some lawyers have become specialists in administrative law. In a few countries, there is a special category of jurists with a monopoly over this form of advocacy; for example, France formerly had conseils juridiques (who were merged into the main legal profession in 1991). In other countries, like the United States, lawyers have been effectively barred by statute from certain types of administrative hearings in order to preserve their informality.



An important aspect of a lawyer's job is developing and managing relationships with clients (or the client's employees, if the lawyer works in-house for a government or corporation). The client-lawyer relationship often begins with an intake interview where the lawyer gets to know the client personally, discovers the facts of the client's case, clarifies what the client wants to accomplish, shapes the client's expectations as to what actually can be accomplished, begins to develop various claims or defenses, and explains her or his fees to the client.
In England, only solicitors were traditionally in direct contact with the client. The solicitor retained a barrister if one was necessary and acted as an intermediary between the barrister and the client. In most cases barristers were obliged, under what is known as the "cab rank rule", to accept instructions for a case in an area in which they held themselves out as practicing, at a court at which they normally appeared and at their usual rates.




Legal advice is the application of abstract principles of law to the concrete facts of the client's case in order to advise the client about what they should do next. In many countries, only a properly licensed lawyer may provide legal advice to clients for good consideration, even if no lawsuit is contemplated or is in progress. Therefore, even conveyancers and corporate in-house counsel must first get a license to practice, though they may actually spend very little of their careers in court. Failure to obey such a rule is the crime of unauthorized practice of law.
In other countries, jurists who hold law degrees are allowed to provide legal advice to individuals or to corporations, and it is irrelevant if they lack a license and cannot appear in court. Some countries go further; in England and Wales, there is no general prohibition on the giving of legal advice. Sometimes civil law notaries are allowed to give legal advice, as in Belgium. In many countries, non-jurist accountants may provide what is technically legal advice in tax and accounting matters.



In virtually all countries, patents, trademarks, industrial designs and other forms of intellectual property must be formally registered with a government agency in order to receive maximum protection under the law. The division of such work among lawyers, licensed non-lawyer jurists/agents, and ordinary clerks or scriveners varies greatly from one country to the next.



In some countries, the negotiating and drafting of contracts is considered to be similar to the provision of legal advice, so that it is subject to the licensing requirement explained above. In others, jurists or notaries may negotiate or draft contracts.
Lawyers in some civil law countries traditionally deprecated "transactional law" or "business law" as beneath them. French law firms developed transactional departments only in the 1990s when they started to lose business to international firms based in the United States and the United Kingdom (where solicitors have always done transactional work).



Conveyancing is the drafting of the documents necessary for the transfer of real property, such as deeds and mortgages. In some jurisdictions, all real estate transactions must be carried out by a lawyer (or a solicitor where that distinction still exists). Such a monopoly is quite valuable from the lawyer's point of view; historically, conveyancing accounted for about half of English solicitors' income (though this has since changed), and a 1978 study showed that conveyancing "accounts for as much as 80 percent of solicitor-client contact in New South Wales." In most common law jurisdictions outside of the United States, this monopoly arose from an 1804 law that was introduced by William Pitt the Younger as a quid pro quo for the raising of fees on the certification of legal professionals such as barristers, solicitors, attorneys and notaries.
In others, the use of a lawyer is optional and banks, title companies, or realtors may be used instead. In some civil law jurisdictions, real estate transactions are handled by civil law notaries. In England and Wales a special class of legal professional–the licensed conveyancer–is also allowed to carry out conveyancing services for reward.



In many countries, only lawyers have the legal authority to draft wills, trusts, and any other documents that ensure the efficient disposition of a person's property after death. In some civil law countries this responsibility is handled by civil law notaries.
In the United States, the estates of the deceased must generally be administered by a court through probate. American lawyers have a profitable monopoly on dispensing advice about probate law (which has been heavily criticized).



In many civil law countries, prosecutors are trained and employed as part of the judiciary; they are law-trained jurists, but may not necessarily be lawyers in the sense that the word is used in the common law world. In common law countries, prosecutors are usually lawyers holding regular licenses who simply happen to work for the government office that files criminal charges against suspects. Criminal defense lawyers specialize in the defense of those charged with any crimes.




The educational prerequisites for becoming a lawyer vary greatly from country to country. In some countries, law is taught by a faculty of law, which is a department of a university's general undergraduate college. Law students in those countries pursue a Master or Bachelor of Laws degree. In some countries it is common or even required for students to earn another bachelor's degree at the same time. Nor is the LL.B the sole obstacle; it is often followed by a series of advanced examinations, apprenticeships, and additional coursework at special government institutes.
In other countries, particularly the UK and USA, law is primarily taught at law schools. In America, the American Bar Association decides which law schools to approve and thereby which ones are deemed most respectable. In England and Wales, the Bar Professional Training Course (BPTC) must be taken to have the right to work and be named as a barrister. In the United States and countries following the American model, (such as Canada with the exception of the province of Quebec) law schools are graduate/professional schools where a bachelor's degree is a prerequisite for admission. Most law schools are part of universities but a few are independent institutions. Law schools in the United States (and a few in Canada, where an LL.B or LL.M degree is much more common, and elsewhere) award graduating students a J.D. (Juris Doctor/Doctor of Jurisprudence) (as opposed to the Bachelor of Laws) as the practitioner's law degree. Many schools also offer post-doctoral law degrees such as the LL.M (Legum Magister/Master of Laws), or the S.J.D. (Scientiae Juridicae Doctor/Doctor of Juridical Science) for students interested in advancing their research knowledge and credentials in a specific area of law.
The methods and quality of legal education vary widely. Some countries require extensive clinical training in the form of apprenticeships or special clinical courses. Others, like Venezuela, do not. A few countries prefer to teach through assigned readings of judicial opinions (the casebook method) followed by intense in-class cross-examination by the professor (the Socratic method). Many others have only lectures on highly abstract legal doctrines, which forces young lawyers to figure out how to actually think and write like a lawyer at their first apprenticeship (or job). Depending upon the country, a typical class size could range from five students in a seminar to five hundred in a giant lecture room. In the United States, law schools maintain small class sizes, and as such, grant admissions on a more limited and competitive basis.
Some countries, particularly industrialized ones, have a traditional preference for full-time law programs, while in developing countries, students often work full- or part-time to pay the tuition and fees of their part-time law programs.
Law schools in developing countries share several common problems, such as an over reliance on practicing judges and lawyers who treat teaching as a part-time hobby (and a concomitant scarcity of full-time law professors); incompetent faculty with questionable credentials; and textbooks that lag behind the current state of the law by two or three decades.




Some jurisdictions grant a "diploma privilege" to certain institutions, so that merely earning a degree or credential from those institutions is the primary qualification for practicing law. Mexico allows anyone with a law degree to practice law. However, in a large number of countries, a law student must pass a bar examination (or a series of such examinations) before receiving a license to practice. In a handful of U.S. states, one may become an attorney (a so-called country lawyer) by simply "reading law" and passing the bar examination, without having to attend law school first (although very few people actually become lawyers that way).
Some countries require a formal apprenticeship with an experienced practitioner, while others do not. For example, a few jurisdictions still allow an apprenticeship in place of any kind of formal legal education (though the number of persons who actually become lawyers that way is increasingly rare).




The career structure of lawyers varies widely from one country to the next.



In most common law countries, especially those with fused professions, lawyers have many options over the course of their careers. Besides private practice, they can become a prosecutor, government counsel, corporate in-house counsel, administrative law judge, judge, arbitrator, or law professor. There are also many non-legal jobs for which legal training is good preparation, such as politician, corporate executive, government administrator, investment banker, entrepreneur, or journalist. In developing countries like India, a large majority of law students never actually practice, but simply use their law degree as a foundation for careers in other fields.
In most civil law countries, lawyers generally structure their legal education around their chosen specialty; the boundaries between different types of lawyers are carefully defined and hard to cross. After one earns a law degree, career mobility may be severely constrained. For example, unlike their American counterparts, it is difficult for German judges to leave the bench and become advocates in private practice. Another interesting example is France, where for much of the 20th century, all judiciary officials were graduates of an elite professional school for judges. Although the French judiciary has begun experimenting with the Anglo-American model of appointing judges from accomplished advocates, the few advocates who have actually joined the bench this way are looked down upon by their colleagues who have taken the traditional route to judicial office.
In a few civil law countries, such as Sweden, the legal profession is not rigorously bifurcated and everyone within it can easily change roles and arenas.



In many countries, lawyers are general practitioners who will take almost any kind of case that walks in the door. In others, there has been a tendency since the start of the 20th century for lawyers to specialize early in their careers. In countries where specialization is prevalent, many lawyers specialize in representing one side in one particular area of the law; thus, it is common in the United States to hear of plaintiffs' personal injury attorneys.




Lawyers in private practice generally work in specialized businesses known as law firms, with the exception of English barristers. The vast majority of law firms worldwide are small businesses that range in size from 1 to 10 lawyers. The United States, with its large number of firms with more than 50 lawyers, is an exception. The United Kingdom and Australia are also exceptions, as the UK, Australia and the U.S. are now home to several firms with more than 1,000 lawyers after a wave of mergers in the late 1990s.
Notably, barristers in England, Wales, Northern Ireland and some states in Australia do not work in "law firms". Those who offer their services to members of the general public—as opposed to those working "in-house" — are required to be self-employed. Most work in groupings known as "sets" or "chambers", where some administrative and marketing costs are shared. An important effect of this different organizational structure is that there is no conflict of interest where barristers in the same chambers work for opposing sides in a case, and in some specialised chambers this is commonplace.






In some jurisdictions, either the judiciary or the Ministry of Justice directly supervises the admission, licensing, and regulation of lawyers.
Other jurisdictions, by statute, tradition, or court order, have granted such powers to a professional association which all lawyers must belong to. In the U.S., such associations are known as mandatory, integrated, or unified bar associations. In the Commonwealth of Nations, similar organizations are known as Inns of Court, bar councils or law societies. In civil law countries, comparable organizations are known as Orders of Advocates, Chambers of Advocates, Colleges of Advocates, Faculties of Advocates, or similar names. Generally, a nonmember caught practicing law may be liable for the crime of unauthorized practice of law.
In common law countries with divided legal professions, barristers traditionally belong to the bar council (or an Inn of Court) and solicitors belong to the law society. In the English-speaking world, the largest mandatory professional association of lawyers is the State Bar of California, with 230,000 members.
Some countries admit and regulate lawyers at the national level, so that a lawyer, once licensed, can argue cases in any court in the land. This is common in small countries like New Zealand, Japan, Portugal and Belgium. Others, especially those with federal governments, tend to regulate lawyers at the state or provincial level; this is the case in the United States, Canada, Australia, and Switzerland, to name a few. Brazil is the most well-known federal government that regulates lawyers at the national level.
Some countries, like Italy, regulate lawyers at the regional level, and a few, like Belgium, even regulate them at the local level (that is, they are licensed and regulated by the local equivalent of bar associations but can advocate in courts nationwide). In Germany, lawyers are admitted to regional bars and may appear for clients before all courts nationwide with the exception of the Federal Court of Justice of Germany (Bundesgerichtshof or BGH); oddly, securing admission to the BGH's bar limits a lawyer's practice solely to the supreme federal courts and the Federal Constitutional Court of Germany.
Generally, geographic limitations can be troublesome for a lawyer who discovers that his client's cause requires him to litigate in a court beyond the normal geographic scope of his license. Although most courts have special pro hac vice rules for such occasions, the lawyer will still have to deal with a different set of professional responsibility rules, as well as the possibility of other differences in substantive and procedural law.
Some countries grant licenses to non-resident lawyers, who may then appear regularly on behalf of foreign clients. Others require all lawyers to live in the jurisdiction or to even hold national citizenship as a prerequisite for receiving a license to practice. But the trend in industrialized countries since the 1970s has been to abolish citizenship and residency restrictions. For example, the Supreme Court of Canada struck down a citizenship requirement on equality rights grounds in 1989, and similarly, American citizenship and residency requirements were struck down as unconstitutional by the U.S. Supreme Court in 1973 and 1985, respectively. The European Court of Justice made similar decisions in 1974 and 1977 striking down citizenship restrictions in Belgium and France.



A key difference among countries is whether lawyers should be regulated solely by an independent judiciary and its subordinate institutions (a self-regulating legal profession), or whether lawyers should be subject to supervision by the Ministry of Justice in the executive branch.
In most civil law countries, the government has traditionally exercised tight control over the legal profession in order to ensure a steady supply of loyal judges and bureaucrats. That is, lawyers were expected first and foremost to serve the state, and the availability of counsel for private litigants was an afterthought. Even in civil law countries like Norway which have partially self-regulating professions, the Ministry of Justice is the sole issuer of licenses, and makes its own independent re-evaluation of a lawyer's fitness to practice after a lawyer has been expelled from the Advocates' Association. Brazil is an unusual exception in that its national Order of Advocates has become a fully self-regulating institution (with direct control over licensing) and has successfully resisted government attempts to place it under the control of the Ministry of Labor.
Of all the civil law countries, Communist countries historically went the farthest towards total state control, with all Communist lawyers forced to practice in collectives by the mid-1950s. China is a prime example: technically, the People's Republic of China did not have lawyers, and instead had only poorly trained, state-employed "legal workers," prior to the enactment of a comprehensive reform package in 1996 by the Standing Committee of the National People's Congress.
In contrast, common law lawyers have traditionally regulated themselves through institutions where the influence of non-lawyers, if any, was weak and indirect (despite nominal state control). Such institutions have been traditionally dominated by private practitioners who opposed strong state control of the profession on the grounds that it would endanger the ability of lawyers to zealously and competently advocate their clients' causes in the adversarial system of justice.
However, the concept of the self-regulating profession has been criticized as a sham which serves to legitimize the professional monopoly while protecting the profession from public scrutiny. Disciplinary mechanisms have been astonishingly ineffective, and penalties have been light or nonexistent.



Lawyers are always free to form voluntary associations of their own, apart from any licensing or mandatory membership that may be required by the laws of their jurisdiction. Like their mandatory counterparts, such organizations may exist at all geographic levels. In American English, such associations are known as voluntary bar associations. The largest voluntary professional association of lawyers in the English-speaking world is the American Bar Association.
In some countries, like France and Italy, lawyers have also formed trade unions.




Hostility towards the legal profession is a widespread phenomenon. The legal profession was abolished in Prussia in 1780 and in France in 1789, though both countries eventually realized that their judicial systems could not function efficiently without lawyers. Complaints about too many lawyers were common in both England and the United States in the 1840s, Germany in the 1910s, and in Australia, Canada, the United States, and Scotland in the 1980s.
Public distrust of lawyers reached record heights in the United States after the Watergate scandal. In the aftermath of Watergate, legal self-help books became popular among those who wished to solve their legal problems without having to deal with lawyers. Lawyer jokes (already a perennial favorite) also soared in popularity in English-speaking North America as a result of Watergate. In 1989, American legal self-help publisher Nolo Press published a 171-page compilation of negative anecdotes about lawyers from throughout human history.
In Adventures in Law and Justice (2003), legal researcher Bryan Horrigan dedicated a chapter to "Myths, Fictions, and Realities" about law and illustrated the perennial criticism of lawyers as "amoral [...] guns for hire" with a quote from Ambrose Bierce's satirical The Devil's Dictionary (1911) that summarized the noun as: "LAWYER, n. One skilled in circumvention of the law."
More generally, in Legal Ethics: A Comparative Study (2004), law professor Geoffrey C. Hazard, Jr. with Angelo Dondi briefly examined the "regulations attempting to suppress lawyer misconduct" and noted that their similarity around the world was paralleled by a "remarkable consistency" in certain "persistent grievances" about lawyers that transcends both time and locale, from the Bible to medieval England to dynastic China. The authors then generalized these common complaints about lawyers as being classified into five "general categories" as follows:

abuse of litigation in various ways, including using dilatory tactics and false evidence and making frivolous arguments to the courts;
preparation of false documentation, such as false deeds, contracts, or wills;
deceiving clients and other persons and misappropriating property;
procrastination in dealings with clients; and
charging excessive fees.

Some studies have shown that suicide rates among lawyers may be as much as six times higher than the average population, and commentators suggest that the low opinion the public has of lawyers, combined with their own high ideals of justice, which in practice they may see denied, increase the depression rates of those in this profession. Additionally, lawyers are twice as likely to suffer from addiction to alcohol and other drugs.




In the United States, lawyers typically earn between $100,000 and $200,000 per year, although earnings vary by age and experience, practice setting, sex, and race. Solo practitioners typically earn less than lawyers in corporate law firms but more than those working for state or local government.
Lawyers are paid for their work in a variety of ways. In private practice, they may work for an hourly fee according to a billable hour structure, a contingency fee (usually in cases involving personal injury), or a lump sum payment if the matter is straightforward. Normally, most lawyers negotiate a written fee agreement up front and may require a non-refundable retainer in advance. Recent studies suggest that when lawyers charge a fixed-fee rather than billing by the hour, they work less hard on behalf of clients and client get worse outcomes. In many countries there are fee-shifting arrangements by which the loser must pay the winner's fees and costs; the United States is the major exception, although in turn, its legislators have carved out many exceptions to the so-called "American Rule" of no fee shifting.
Lawyers working directly on the payroll of governments, nonprofits, and corporations usually earn a regular annual salary. In many countries, with the notable exception of Germany, lawyers can also volunteer their labor in the service of worthy causes through an arrangement called pro bono (short for pro bono publico, "for the common good"). Traditionally such work was performed on behalf of the poor, but in some countries it has now expanded to many other causes such as the environment.
In some countries, there are legal aid lawyers who specialize in providing legal services to the indigent. France and Spain even have formal fee structures by which lawyers are compensated by the government for legal aid cases on a per-case basis. A similar system, though not as extensive or generous, operates in Australia, Canada, and South Africa.
In other countries, legal aid specialists are practically nonexistent. This may be because non-lawyers are allowed to provide such services; in both Italy and Belgium, trade unions and political parties provide what can be characterized as legal aid services. Some legal aid in Belgium is also provided by young lawyer apprentices subsidized by local bar associations (known as the pro deo system), as well as consumer protection nonprofit organizations and Public Assistance Agencies subsidized by local governments. In Germany, mandatory fee structures have enabled widespread implementation of affordable legal expense insurance.






The earliest people who could be described as "lawyers" were probably the orators of ancient Athens (see History of Athens). However, Athenian orators faced serious structural obstacles. First, there was a rule that individuals were supposed to plead their own cases, which was soon bypassed by the increasing tendency of individuals to ask a "friend" for assistance. However, around the middle of the fourth century, the Athenians disposed of the perfunctory request for a friend. Second, a more serious obstacle, which the Athenian orators never completely overcame, was the rule that no one could take a fee to plead the cause of another. This law was widely disregarded in practice, but was never abolished, which meant that orators could never present themselves as legal professionals or experts. They had to uphold the legal fiction that they were merely an ordinary citizen generously helping out a friend for free, and thus they could never organize into a real profession—with professional associations and titles and all the other pomp and circumstance—like their modern counterparts. Therefore, if one narrows the definition to those men who could practice the legal profession openly and legally, then the first lawyers would have to be the orators of ancient Rome.



A law enacted in 204 BC barred Roman advocates from taking fees, but the law was widely ignored. The ban on fees was abolished by Emperor Claudius, who legalized advocacy as a profession and allowed the Roman advocates to become the first lawyers who could practice openly—but he also imposed a fee ceiling of 10,000 sesterces. This was apparently not much money; the Satires of Juvenal complained that there was no money in working as an advocate.
Like their Greek contemporaries, early Roman advocates were trained in rhetoric, not law, and the judges before whom they argued were also not law-trained. But very early on, unlike Athens, Rome developed a class of specialists who were learned in the law, known as jurisconsults (iuris consulti). Jurisconsults were wealthy amateurs who dabbled in law as an intellectual hobby; they did not make their primary living from it. They gave legal opinions (responsa) on legal issues to all comers (a practice known as publice respondere). Roman judges and governors would routinely consult with an advisory panel of jurisconsults before rendering a decision, and advocates and ordinary people also went to jurisconsults for legal opinions. Thus, the Romans were the first to have a class of people who spent their days thinking about legal problems, and this is why their law became so "precise, detailed, and technical."

During the Roman Republic and the early Roman Empire, jurisconsults and advocates were unregulated, since the former were amateurs and the latter were technically illegal. Any citizen could call himself an advocate or a legal expert, though whether people believed him would depend upon his personal reputation. This changed once Claudius legalized the legal profession. By the start of the Byzantine Empire, the legal profession had become well-established, heavily regulated, and highly stratified. The centralization and bureaucratization of the profession was apparently gradual at first, but accelerated during the reign of Emperor Hadrian. At the same time, the jurisconsults went into decline during the imperial period.
In the words of Fritz Schulz, "by the fourth century things had changed in the eastern Empire: advocates now were really lawyers." For example, by the fourth century, advocates had to be enrolled on the bar of a court to argue before it, they could only be attached to one court at a time, and there were restrictions (which came and went depending upon who was emperor) on how many advocates could be enrolled at a particular court. By the 380s, advocates were studying law in addition to rhetoric (thus reducing the need for a separate class of jurisconsults); in 460, Emperor Leo imposed a requirement that new advocates seeking admission had to produce testimonials from their teachers; and by the sixth century, a regular course of legal study lasting about four years was required for admission. Claudius's fee ceiling lasted all the way into the Byzantine period, though by then it was measured at 100 solidi. Of course, it was widely evaded, either through demands for maintenance and expenses or a sub rosa barter transaction. The latter was cause for disbarment.
The notaries (tabelliones) appeared in the late Roman Empire. Like their modern-day descendants, the civil law notaries, they were responsible for drafting wills, conveyances, and contracts. They were ubiquitous and most villages had one. In Roman times, notaries were widely considered to be inferior to advocates and jury consults.




After the fall of the Western Roman Empire and the onset of the Early Middle Ages, the legal profession of Western Europe collapsed. As James Brundage has explained: "[by 1140], no one in Western Europe could properly be described as a professional lawyer or a professional canonist in anything like the modern sense of the term 'professional.' " However, from 1150 onward, a small but increasing number of men became experts in canon law but only in furtherance of other occupational goals, such as serving the Roman Catholic Church as priests. From 1190 to 1230, however, there was a crucial shift in which some men began to practice canon law as a lifelong profession in itself.
The legal profession's return was marked by the renewed efforts of church and state to regulate it. In 1231 two French councils mandated that lawyers had to swear an oath of admission before practicing before the bishop's courts in their regions, and a similar oath was promulgated by the papal legate in London in 1237. During the same decade, the emperor of the Holy Roman Empire Frederick II, the king of the Kingdom of Sicily, imposed a similar oath in his civil courts. By 1250 the nucleus of a new legal profession had clearly formed. The new trend towards professionalization culminated in a controversial proposal at the Second Council of Lyon in 1275 that all ecclesiastical courts should require an oath of admission. Although not adopted by the council, it was highly influential in many such courts throughout Europe. The civil courts in England also joined the trend towards professionalization; in 1275 a statute was enacted that prescribed punishment for professional lawyers guilty of deceit, and in 1280 the mayor's court of the city of London promulgated regulations concerning admission procedures, including the administering of an oath. And in 1345, the French crown promulgated a royal ordinance which set forth 24 rules governing advocates, of which 12 were integrated into the oath to be taken by them.
The French medieval oaths were widely influential and of enduring importance; for example, they directly influenced the structure of the advocates' oath adopted by the Canton of Geneva in 1816. In turn, the 1816 Geneva oath served as the inspiration for the attorney's oath drafted by David Dudley Field as Section 511 of the proposed New York Code of Civil Procedure of 1848, which was the first attempt in the United States at a comprehensive statement of a lawyer's professional duties.




Generally speaking, the modern practice is for lawyers to avoid use of any title, although formal practice varies across the world.
Historically lawyers in most European countries were addressed with the title of doctor, and countries outside of Europe have generally followed the practice of the European country which had policy influence through colonization. The first university degrees, starting with the law school of the University of Bologna (or glossators) in the 11th century, were all law degrees and doctorates. Degrees in other fields did not start until the 13th century, but the doctor continued to be the only degree offered at many of the old universities until the 20th century. Therefore, in many of the southern European countries, including Portugal and Italy, lawyers have traditionally been addressed as “doctor,” a practice, which was transferred to many countries in South America and Macau. The term "doctor" has since fallen into disuse, although it is still a legal title in Italy and in use in many countries outside of Europe.
In French- (France, Quebec, Belgium, Luxembourg) and Dutch-speaking countries (Netherlands, Belgium), legal professionals are addressed as Maître ..., abbreviated to Me ... (in French) or Meester ..., abbreviated to mr. ... (in Dutch).
The title of doctor has never been used to address lawyers in England or other common law countries (with the exception of the United States). This is because until 1846 lawyers in England were not required to have a university degree and were trained by other attorneys by apprenticeship or in the Inns of Court. Since law degrees started to become a requirement for lawyers in England, the degree awarded has been the undergraduate LL.B. In South Africa holders of a law degree who have completed a year of pupillage and have been admitted to the bar may use the title "Advocate", abbreviated to "Adv" in written correspondence. Likewise, Italian law graduates who have qualified for the bar use the title "Avvocato", abbreviated in "Avv."
Even though most lawyers in the United States do not use any titles, the law degree in that country is the Juris Doctor, a professional doctorate degree, and some J.D. holders in the United States use the title of "Doctor" in professional and academic situations.
In countries where holders of the first law degree traditionally use the title of doctor (e.g. Peru, Brazil, Macau, Portugal, Argentina), J.D. holders who are attorneys will often use the title of doctor as well. It is common for English-language male lawyers to use the honorific suffix "Esq." (for "Esquire"). In the United States the style is also used by female lawyers.
In many Asian countries, holders of the Juris Doctor degree are also called "博士" (doctor).
In the Philippines and Filipino communities overseas, lawyers who are either Filipino or naturalized-citizen expatriates at work there, especially those who also profess other jobs at the same time, are addressed and introduced as Attorney, rather than Sir/Madam in speech or Mr./Mrs./Ms. (G./Gng./Bb. in Filipino) before surnames. That word is used either in itself or before the given name or surname.








A newspaper is a serial publication containing news about current events, other informative articles (listed below) about politics, sports, arts, and so on, and advertising. A newspaper is usually, but not exclusively, printed on relatively inexpensive, low-grade paper such as newsprint. The journalism organizations that publish newspapers are themselves often metonymically called newspapers. As of 2017, most newspapers are now published online as well as in print. The online versions are called online newspapers or news websites. Newspapers are typically published daily or weekly. News magazines are also weekly, but they have a magazine format. General-interest newspapers typically publish news articles and feature articles on national and international news as well as local news. The news includes political events and personalities, business and finance, crime, weather, and natural disasters; health and medicine, science, and computers and technology; sports; and entertainment, society, food and cooking, clothing and home fashion, and the arts.
Typically the paper is divided into sections for each of those major groupings (labeled A, B, C, and so on, with pagination prefixes yielding page numbers A1-A20, B1-B20, C1-C20, and so on). Most traditional papers also feature an editorial page containing editorials written by an editor (or by the paper's editorial board) and expressing an opinion on a public issue, opinion articles called "op-eds" written by guest writers (which are typically in the same section as the editorial), and columns that express the personal opinions of columnists, usually offering analysis and synthesis that attempts to translate the raw data of the news into information telling the reader "what it all means" and persuading them to concur. Papers also include articles which have no byline; these articles are written by staff writers.
A wide variety of material has been published in newspapers. Besides the aforementioned news, information and opinions, they include weather forecasts; criticism and reviews of the arts (including literature, film, television, theater, fine arts, and architecture) and of local services such as restaurants; obituaries, birth notices and graduation announcements; entertainment features such as crosswords, horoscopes, editorial cartoons, gag cartoons, and comic strips; advice columns, food, and other columns; and radio and television listings (program schedules). As of 2017, newspapers may also provide information about new movies and TV shows available on streaming video services like Netflix. Newspapers have classified ad sections where people and businesses can buy small advertisements to sell goods or services; as of 2017, the huge increase in Internet websites for selling goods, such as Craigslist and eBay has led to significantly less classified ad sales for newspapers.
Most newspapers are businesses, and they pay their expenses with a mixture of subscription revenue, newsstand sales, and advertising revenue (other businesses or individuals pay to place advertisements in the pages, including display ads, classified ads, and their online equivalents). Some newspapers are government-run or at least government-funded; their reliance on advertising revenue and on profitability is less critical to their survival. The editorial independence of a newspaper is thus always subject to the interests of someone, whether owners, advertisers, or a government. Some newspapers with high editorial independence, high journalism quality, and large circulation are viewed as newspapers of record.
Many newspapers, besides employing journalists on their own payrolls, also subscribe to news agencies (wire services) (such as the Associated Press, Reuters, or Agence France-Presse), which employ journalists to find, assemble, and report the news, then sell the content to the various newspapers. This is a way to avoid duplicating the expense of reporting from around the world. Circa 2005, there were approximately 6,580 daily newspaper titles in the world selling 395 million print copies a day (in the U.S., 1,450 titles selling 55 million copies). The late 2000s–early 2010s global recession, combined with the rapid growth of free web-based alternatives, has helped cause a decline in advertising and circulation, as many papers had to retrench operations to stanch the losses. Worldwide annual revenue approached $100 billion in 2005-7, then plunged during the worldwide financial crisis of 2008-9. Revenue in 2016 fell to only $53 billion, hurting every major publisher as their efforts to gain online income fell far short of the goal.
The decline in advertising revenues affected both the print and online media as well as all other mediums; print advertising was once lucrative but has greatly declined, and the prices of online advertising are often lower than those of their print precursors. Besides remodeling advertising, the internet (especially the web) has also challenged the business models of the print-only era by crowdsourcing both publishing in general (sharing information with others) and, more specifically, journalism (the work of finding, assembling, and reporting the news). In addition, the rise of news aggregators, which bundle linked articles from many online newspapers and other sources, influences the flow of web traffic. Increasing paywalling of online newspapers may be counteracting those effects. The oldest newspaper still published is the Gazzetta di Mantova, which was established in Mantua in 1664.



Newspapers typically meet four criteria:
Public accessibility: Its contents are reasonably accessible to the public, traditionally by the paper being sold or distributed at newsstands, shops and libraries, and, since the 1990s, made available over the Internet with online newspaper websites. While online newspapers have increased access to newspapers by people with Internet access, people without Internet or computer access (e.g., homeless people, impoverished people and people living in remote or rural regions may not be able to access the Internet, and thus will not be able to read online news). Literacy is also a factor which prevents people who cannot read from being able to benefit from reading newspapers (paper or online).
Periodicity: They are published at regular intervals, typically daily or weekly. This ensures that newspapers can provide information on newly-emerging news stories or events.
Currency: Its information is as up to date as its publication schedule allows. The degree of up-to-date-ness of a print newspaper is limited by the need of time to print and distribute the newspaper. In major cities, there may be a morning edition and a later edition of the same day's paper, so that the later edition can incorporate breaking news that has occurred since the morning edition was printed. Online newspapers can be updated as frequently as new information becomes available, even a number of times per day, which means that online editions can be very up-to-date.
Universality: Newspapers covers a range of topics, from political and business news to updates on science and technology, arts, culture, and entertainment.






In Ancient Rome, Acta Diurna, or government announcement bulletins, were produced. They were carved in metal or stone and posted in public places. In China, early government-produced news-sheets, called Dibao, circulated among court officials during the late Han dynasty (second and third centuries AD). Between 713 and 734, the Kaiyuan Za Bao ("Bulletin of the Court") of the Chinese Tang Dynasty published government news; it was handwritten on silk and read by government officials. In 1582, there was the first reference to privately published newssheets in Beijing, during the late Ming Dynasty.
In Early modern Europe the increased cross-border interaction created a rising need for information which was met by concise handwritten news-sheets, called avvisi. In 1556, the government of Venice first published the monthly Notizie scritte, which cost one gazetta, a small coin. These avvisi were handwritten newsletters and used to convey political, military, and economic news quickly and efficiently to Italian cities (1500–1700)—sharing some characteristics of newspapers though usually not considered true newspapers. However, none of these publications fully met the classical criteria for proper newspapers, as they were typically not intended for the general public and restricted to a certain range of topics.







The emergence of the new media in the 17th century has to be seen in close connection with the spread of the printing press from which the publishing press derives its name. The German-language Relation aller Fürnemmen und gedenckwürdigen Historien, printed from 1605 onwards by Johann Carolus in Strasbourg, is often recognized as the first newspaper. At the time, Strasbourg was a free imperial city in the Holy Roman Empire of the German Nation; the first newspaper of modern Germany was the Avisa, published in 1609 in Wolfenbüttel.
The Dutch Courante uyt Italien, Duytslandt, &c. ('Courant from Italy, Germany, etc.') of 1618 was the first to appear in folio- rather than quarto-size. Amsterdam, a center of world trade, quickly became home to newspapers in many languages, often before they were published in their own country. The first English-language newspaper, Corrant out of Italy, Germany, etc., was published in Amsterdam in 1620. A year and a half later, Corante, or weekely newes from Italy, Germany, Hungary, Poland, Bohemia, France and the Low Countreys. was published in England by an "N.B." (generally thought to be either Nathaniel Butter or Nicholas Bourne) and Thomas Archer. The first newspaper in France was published in 1631, La Gazette (originally published as Gazette de France). The first newspaper in Portugal, A Gazeta da Restauração, was published in 1641 in Lisbon. The first Spanish newspaper, Gaceta de Madrid, was published in 1661.
Post- och Inrikes Tidningar (founded as Ordinari Post Tijdender) was first published in Sweden in 1645, and is the oldest newspaper still in existence, though it now publishes solely online. Opregte Haarlemsche Courant from Haarlem, first published in 1656, is the oldest paper still printed. It was forced to merge with the newspaper Haarlems Dagblad in 1942 when Germany occupied the Netherlands. Since then the Haarlems Dagblad has appeared with the subtitle Oprechte Haerlemse Courant 1656. Merkuriusz Polski Ordynaryjny was published in Kraków, Poland in 1661. The first successful English daily, The Daily Courant, was published from 1702 to 1735.




In Boston in 1690, Benjamin Harris published Publick Occurrences Both Forreign and Domestick. This is considered the first newspaper in the American colonies even though only one edition was published before the paper was suppressed by the government. In 1704, the governor allowed The Boston News-Letter to be published and it became the first continuously published newspaper in the colonies. Soon after, weekly papers began publishing in New York and Philadelphia. These early newspapers followed the British format and were usually four pages long. They mostly carried news from Britain and content depended on the editor's interests. In 1783, the Pennsylvania Evening Post became the first American daily.
In 1752, John Bushell published the Halifax Gazette, which claims to be "Canada's first newspaper." However, its official descendant, the Royal Gazette, is a government publication for legal notices and proclamations rather than a proper newspaper; In 1764, the Quebec Gazette was first printed 21 June 1764 and remains the oldest continuously published newspaper in North America as the Quebec Chronicle-Telegraph. It is currently published as an English-language weekly from its offices at 1040 Belvédère, suite 218, Quebec City, Quebec, Canada. In 1808, the Gazeta do Rio de Janeiro had his first edition, printed in devices brought from England, publishing news favourable for the government of the United Kingdom of Portugal, Brazil and the Algarves since it was produced by the official press service of the Portuguese crown.
In 1821, after the ending of private newspaper circulation ban, appears the first non-imperial printed publication, Diário do Rio de Janeirothough there it was already the Correio Braziliense, published by Hipólito José da Costa at the same time of the Gazeta, but from London and with a strong political and critical ideas, aiming to show the administration faults (see Portuguese Wikipedia ). The first newspaper in Peru was El Peruano, established in October 1825 and still published today, but with several name changes.




During the Tang Dynasty in China (618–906), the Kai Yuan Za Bao published the government news; it was block printed onto paper. It was the earliest newspaper to be published The first recorded attempt to found a modern-day newspaper in South Asia was by William Bolts, a Dutchman in the employ of the British East India Company in September 1768 in Calcutta. However, before he could begin his newspaper, he was deported back to Europe. A few years later, the first newsprint from this region - Hicky's Bengal Gazette- was published by an Irishman James Augustus Hicky. He used it as a means to criticize the British rule through journalism.




The history of Middle Eastern newspapers goes back to the 19th century. Many editors were not only journalists but also writers, philosophers and politicians. With unofficial journals, these intellectuals encouraged public discourse on politics in the Ottoman and Persian Empires. Literary works of all genres were serialized and published in the press as well.
The first newspapers in the Ottoman Empire were owned by foreigners living there who wanted to make propaganda about the Western world. The earliest was printed in 1795 by the Palais de France in Pera. Indigenous Middle Eastern journalism started in 1828, when Muhammad Ali, Khedive of Egypt, ordered the local establishment of the gazette Vekayi-i Misriye (Egyptian Affairs). It was first paper written in Ottoman Turkish and Arabic on opposite pages, and later in Arabic only, under the title "al-Waqa'i`a al-Masriya".
The first non-official Turkish newspaper, Ceride-i Havadis (Register of Events), was published by an Englishman, William Churchill, in 1840. The first private newspaper to be published by Turkish journalists, Tercüman-ı Ahvâl (Interpreter of Events), was founded by İbrahim Şinasi and Agah Efendi and issued in 1860. The first newspaper in Iran, Kaghaz-e Akhbar (The Newspaper), was created for the government by Mirza Saleh Shirazi in 1837. The first journals in the Arabian Peninsula appeared in Hijaz, once it had become independent of Ottoman rule, towards the end of World War I.One of the earliest women to sign her articles in the Arab press was the female medical practitioner Galila Tamarhan, who contributed articles to a medical magazine called "Ya`asub al-Tib" (Leader in Medicine) in the 1860s.



By the early 19th century, many cities in Europe, as well as North and South America, published newspaper-type publications though not all of them developed in the same way; content was vastly shaped by regional and cultural preferences. Advances in printing technology related to the Industrial Revolution enabled newspapers to become an even more widely circulated means of communication, as new printing technologies made printing less expensive and more efficient. In 1814, The Times (London) acquired a printing press capable of making 1,100 impressions per hour. Soon, this press was adapted to print on both sides of a page at once. This innovation made newspapers cheaper and thus available to a larger part of the population.
In 1830, the first inexpensive "penny press" newspaper came to the market: Lynde M. Walter's Boston Transcript. Penny press papers cost about one sixth the price of other newspapers and appealed to a wider audience, including less educated and lower-income people. In France, Émile de Girardin started "La Presse" in 1836, introducing cheap, advertising-supported dailies to France. In 1848, August Zang, an Austrian who knew Girardin in Paris, returned to Vienna to introduce the same methods with "Die Presse" (which was named for and frankly copied Girardin's publication).




While most newspapers are aimed at a broad spectrum of readers, usually geographically defined, some focus on groups of readers defined more by their interests than their location: for example, there are daily and weekly business newspapers (e.g., The Wall Street Journal) and sports newspapers. More specialist still are some weekly newspapers, usually free and distributed within limited regional areas; these may serve communities as specific as certain immigrant populations, the local gay community or indie rock enthusiasts within a city or region.






A daily newspaper is printed every day, sometimes with the exception of Sundays and occasionally Saturdays, (and some major holidays) and often of some national holidays. Saturday and, where they exist, Sunday editions of daily newspapers tend to be larger, include more specialized sections (e.g., on arts, films, entertainment) and advertising inserts, and cost more. Typically, the majority of these newspapers' staff members work Monday to Friday, so the Sunday and Monday editions largely depend on content done in advance or content that is syndicated. Most daily newspapers are sold in the morning. Afternoon or evening papers, once common but now scarce, are aimed more at commuters and office workers. In practice (though this may vary according to country), a morning newspaper is available in early editions from before midnight on the night before its cover date, further editions being printed and distributed during the night. The later editions can include breaking news which was first revealed that day, after the morning edition was already printed. Previews of tomorrow's newspapers are often a feature of late night news programs, such as Newsnight in the United Kingdom. In 1650, the first daily newspaper appeared, Einkommende Zeitung, published by Timotheus Ritzsch in Leipzig, Germany.
In the UK, unlike most other countries, "daily" newspapers do not publish on Sundays. In the past there were independent Sunday newspapers; nowadays the same publisher often produces a Sunday newspaper, distinct in many ways from the daily, usually with a related name; e.g., The Times and The Sunday Times are distinct newspapers owned by the same company, and an article published in the latter would never be credited to The Times.




Weekly newspapers are published once a week, and tend to be smaller than daily papers. Some newspapers are published two or three times a week and are known as biweekly publications. Some publications are published, for example, fortnightly (or bimonthly in American parlance).






A local newspaper serves a region such as a city, or part of a large city. Almost every market has one or two newspapers that dominate the area. Large metropolitan newspapers often have large distribution networks, and can be found outside their normal area, sometimes widely, sometimes from fewer sources.




Most nations have at least one newspaper that circulates throughout the whole country: a national newspaper. Some national newspapers, such as The Financial Times and The Wall Street Journal, are specialised (in these examples, on financial matters). There are many national newspapers in the UK, but only a few in the United States and Canada. In Canada, The Globe and Mail is sold throughout the country. In the United States, in addition to national newspapers as such, The New York Times is available throughout the country.

There is also a small group of newspapers which may be characterized as international newspapers. Some, such as The International Herald Tribune, have always had that focus, while others are repackaged national newspapers or "international editions" of national or large metropolitan newspapers. In some cases, articles that might not interest the wider range of readers are omitted from international editions; in others, of interest to expatriates, significant national news is retained. As English became the international language of business and technology, many newspapers formerly published only in non-English languages have also developed English-language editions. In places as varied as Jerusalem and Mumbai, newspapers are printed for a local and international English-speaking public, and for tourists. The advent of the Internet has also allowed non-English-language newspapers to put out a scaled-down English version to give their newspaper a global outreach.
Similarly, in many countries with a large foreign-language-speaking population or many tourists, newspapers in languages other than the national language are both published locally and imported. For example, newspapers and magazines from many countries, and locally published newspapers in many languages, are readily to be found on news-stands in central London. In the US state of Florida, so many tourists from the French-speaking Canadian province of Quebec visit for long stays during the winter ("snowbirds") that some newsstands and stores sell French-language newspapers such as Le Droit.



General newspapers cover all topics, with different emphasis. While at least mentioning all topics, some might have good coverage of international events of importance; others might concentrate more on national or local entertainment or sports. Specialised newspapers might concentrate more specifically on, for example, financial matters. There are publications covering exclusively sports, or certain sports, horse-racing, theatre, and so on, although they may no longer be called newspapers.






For centuries newspapers were printed on paper and supplied physically to readers either by local distribution, or in some cases by mail, for example for British expatriates living in India or Hong Kong who subscribed to British newspapers. Newspapers can be delivered to subscribers homes and/or businesses by a paper's own delivery people, sent via the mail, sold at newsstands, grocery stores and convenience stores, and delivered to libraries and bookstores. Newspaper organizations need a large distribution system to deliver their papers to these different distributors, which typically involves delivery trucks and delivery people. In recent years, newspapers and other media have adapted to the changing technology environment by starting to offer online editions to cater to the needs of the public. In the future, the trend towards more electronic delivery of the news will continue with more emphasis on the Internet, social media and other electronic delivery methods. However, while the method of delivery is changing, the newspaper and the industry still has a niche in the world.




As of 2007, virtually all major printed newspapers have online editions distributed over the Internet which, depending on the country may be regulated by journalism organizations such as the Press Complaints Commission in the UK. But as some publishers find their print-based models increasingly unsustainable, Web-based "newspapers" have also started to appear, such as the Southport Reporter in the UK and the Seattle Post-Intelligencer, which stopped publishing in print after 149 years in March 2009 and became an online only paper.
A new trend in newspaper publishing is the introduction of personalization through on-demand printing technologies or with online news aggregator websites like Google news. Customized newspapers allow the reader to create their individual newspaper through the selection of individual pages from multiple publications. This "Best of" approach allows to revive the print-based model and opens up a new distribution channel to increase coverage beneath the usual boundaries of distribution. Customized newspapers online have been offered by MyYahoo, I-Google, CRAYON, ICurrent.com, Kibboko.com, Twitter.times and many others. With these online newspapers, the reader can select how much of each section (politics, sports, arts, etc.) they wish to see in their news.
The newspaper has been a part of our daily life for several centuries. They have been a way for the public to be informed of important events that are occurring around the world. Newspapers have undergone dramatic changes over the course of history. Some of the earliest newspapers date back to Ancient Rome where important announcements were carved in stone tablets and placed in highly populated areas where citizens could be informed of the announcements.




In the United States, the overall manager or chief executive of the newspaper is the publisher. In small newspapers, the owner of the publication (or the largest shareholder in the corporation that owns the publication) is usually the publisher. Although he or she rarely or perhaps never writes stories, the publisher is legally responsible for the contents of the entire newspaper and also runs the business, including hiring editors, reporters, and other staff members. This title is less common outside the U.S. The equivalent position in the film industry and television news shows is the executive producer. Most newspapers have four main departments devoted to publishing the newspaper itself—editorial, production/printing, circulation, and advertising, although they are frequently referred to by a variety of other names—as well as the non-newspaper-specific departments also found in other businesses of comparable size, such as accounting, marketing, human resources, and IT.
Throughout the English-speaking world, the person who selects the content for the newspaper is usually referred to as the editor. Variations on this title such as editor-in-chief, executive editor, and so on are common. For small newspapers, a single editor may be responsible for all content areas. At large newspapers, the most senior editor is in overall charge of the publication, while less senior editors may each focus on one subject area, such as local news or sports. These divisions are called news bureaus or "desks", and each is supervised by a designated editor. Most newspaper editors copy edit the stories for their part of the newspaper, but they may share their workload with proofreaders and fact checkers.

Reporters are journalists who primarily report facts that they have gathered and those who write longer, less news-oriented articles may be called feature writers. Photographers and graphic artists provide images and illustrations to support articles. Journalists often specialize in a subject area, called a beat, such as sports, religion, or science. Columnists are journalists who write regular articles recounting their personal opinions and experiences. Printers and press operators physically print the newspaper. Printing is outsourced by many newspapers, partly because of the cost of an offset web press (the most common kind of press used to print newspapers) and also because a small newspaper's print run might require less than an hour of operation, meaning that if the newspaper had its own press it would sit idle most of the time. If the newspaper offers information online, webmasters and web designers may be employed to upload stories to the newspaper's website.
The staff of the circulation department liaise with retailers who sell the newspaper; sell subscriptions; and supervise distribution of the printed newspapers through the mail, by newspaper carriers, at retailers, and through vending machines. Free newspapers do not sell subscriptions, but they still have a circulation department responsible for distributing the newspapers. Sales staff in the advertising department not only sell ad space to clients such as local businesses, but also help clients design and plan their advertising campaigns. Other members of the advertising department may include graphic designers, who design ads according to the customers' specifications and the department's policies. In an advertising-free newspaper, there is no advertising department.




Newspapers often refine distribution of ads and news through zoning and editioning. Zoning occurs when advertising and editorial content change to reflect the location to which the product is delivered. The editorial content often may change merely to reflect changes in advertising—the quantity and layout of which affects the space available for editorial—or may contain region-specific news. In rare instances, the advertising may not change from one zone to another, but there will be different region-specific editorial content. As the content can vary widely, zoned editions are often produced in parallel. Editioning occurs in the main sections as news is updated throughout the night. The advertising is usually the same in each edition (with the exception of zoned regionals, in which it is often the 'B' section of local news that undergoes advertising changes). As each edition represents the latest news available for the next press run, these editions are produced linearly, with one completed edition being copied and updated for the next edition. The previous edition is always copied to maintain a Newspaper of Record and to fall back on if a quick correction is needed for the press. For example, both The New York Times and The Wall Street Journal offer a regional edition, printed through a local contractor, and featuring locale specific content. The Journal's global advertising rate card provides a good example of editioning.
See also Los Angeles Times suburban sections.




Most modern newspapers are in one of three sizes:
Broadsheets: 600 mm × 380 mm (23½ × 15 inches), generally associated with more intellectual newspapers, although a trend towards "compact" newspapers is changing this. Examples include The Daily Telegraph in the United Kingdom.
Tabloids: half the size of broadsheets at 380 mm × 300 mm (15 × 11¾ inches), and often perceived as sensationalist in contrast to broadsheets. Examples include The Sun, The National Enquirer, The Star Magazine, New York Post, the Chicago Sun-Times, The Princely State, The Globe.
"Microdaily" is infrequently used to refer to a tabloid-sized free daily newspaper that offers lower ad rates than its broadsheet competitors. The content of a microdaily can range from intense local news coverage to a combination of local and national stories.

Berliner or Midi: 470 mm × 315 mm (18½ × 12¼ inches) used by European papers such as Le Monde in France, La Stampa in Italy, El País in Spain and, since 2005, The Guardian in the United Kingdom.
Newspapers are usually printed on cheap, off-white paper known as newsprint. Since the 1980s, the newspaper industry has largely moved away from lower-quality letterpress printing to higher-quality, four-color process, offset printing. In addition, desktop computers, word processing software, graphics software, digital cameras and digital prepress and typesetting technologies have revolutionized the newspaper production process. These technologies have enabled newspapers to publish color photographs and graphics, as well as innovative layouts and better design. To help their titles stand out on newsstands, some newspapers are printed on coloured newsprint. For example, the Financial Times is printed on a distinctive salmon pink paper, and Sheffield's weekly sports publication derives its name, the Green 'Un, from the traditional colour of its paper. The Italian sports newspaper La Gazzetta dello Sport is also printed on pink paper while L'Équipe (formerly L'Auto) is printed on yellow paper. Both the latter promoted major cycling races and their newsprint colours were reflected in the colours of the jerseys used to denote the race leader; for example the leader in the Giro d'Italia wears a pink jersey.




The number of copies distributed, either on an average day or on particular days (typically Sunday), is called the newspaper's circulation and is one of the principal factors used to set advertising rates. Circulation is not necessarily the same as copies sold, since some copies or newspapers are distributed without cost. Readership figures may be higher than circulation figures because many copies are read by more than one person, although this is offset by the number of copies distributed but not read (especially for those distributed free). In the United States, the Alliance for Audited Media maintains historical and current data on average circulation of daily and weekly newspapers and other periodicals.
According to the Guinness Book of Records, the daily circulation of the Soviet newspaper Trud exceeded 21,500,000 in 1990, while the Soviet weekly Argumenty i Fakty boasted a circulation of 33,500,000 in 1991. According to United Nations data from 1995 Japan has three daily papers—the Yomiuri Shimbun, Asahi Shimbun, and Mainichi Shimbun—with circulations well above 5.5 million. Germany's Bild, with a circulation of 3.8 million, was the only other paper in that category. In the United Kingdom, The Sun is the top seller, with around 3.24 million copies distributed daily. In the U.S., The Wall Street Journal has a daily circulation of approximately 2.02 million, making it the most widely distributed paper in the country.
While paid readership of print newspapers has been steadily declining in the developed OECD nations, it has been rising in the chief developing nations (Brazil, India, Indonesia, China and South Africa), whose paid daily circulation exceeded those of the developed nations for the first time in 2008. In India, The Times of India is the largest-circulation English newspaper, with 3.14 million copies daily. According to the 2009 Indian Readership Survey, the Dainik Jagran is the most-read, local-language (Hindi) newspaper, with 55.7 million readers. According to Tom Standage of The Economist, India currently has daily newspaper circulation of 110 million copies.

A common measure of a newspaper's health is market penetration, expressed as a percentage of households that receive a copy of the newspaper against the total number of households in the paper's market area. In the 1920s, on a national basis in the U.S., daily newspapers achieved market penetration of 123 percent (meaning the average U.S. household received 1.23 newspapers). As other media began to compete with newspapers, and as printing became easier and less expensive giving rise to a greater diversity of publications, market penetration began to decline. It wasn't until the early 1970s, however, that market penetration dipped below 100 percent. By 2000, it was 53 percent and still falling. Many paid-for newspapers offer a variety of subscription plans. For example, someone might want only a Sunday paper, or perhaps only Sunday and Saturday, or maybe only a workweek subscription, or perhaps a daily subscription. Most newspapers provide some or all of their content on the Internet, either at no cost or for a fee. In some cases, free access is available only for a matter of days or weeks, or for a certain number of viewed articles, after which readers must register and provide personal data. In other cases, free archives are provided.




A newspaper typically generates 70–80% of its revenue from advertising, and the remainder from sales and subscriptions. The portion of the newspaper that is not advertising is called editorial content, editorial matter, or simply editorial, although the last term is also used to refer specifically to those articles in which the newspaper and its guest writers express their opinions. (This distinction, however, developed over time – early publishers like Girardin (France) and Zang (Austria) did not always distinguish paid items from editorial content.). The business model of having advertising subsidize the cost of printing and distributing newspapers (and, it is always hoped, the making of a profit) rather than having subscribers cover the full cost was first done, it seems, in 1833 by The Sun, a daily paper that was published in New York City. Rather than charging 6 cents per copy, the price of a typical New York daily at the time, they charged 1-cent, and depended on advertising to make up the difference.

Newspapers in countries with easy access to the web have been hurt by the decline of many traditional advertisers. Department stores and supermarkets could be relied upon in the past to buy pages of newspaper advertisements, but due to industry consolidation are much less likely to do so now. Additionally, newspapers are seeing traditional advertisers shift to new media platforms. The classified category is shifting to sites including Craigslist, employment websites, and auto sites. National advertisers are shifting to many types of digital content including websites, rich media platforms, and mobile.
In recent years, the advertorial emerged. Advertorials are most commonly recognized as an opposite-editorial which third parties pay a fee to have included in the paper. Advertorials commonly advertise new products or techniques, such as a new design for golf equipment, a new form of laser surgery, or weight-loss drugs. The tone is usually closer to that of a press release than of an objective news story. Such articles are often clearly distinguished from editorial content through either the design and layout of the page or with a label declaring the article as an advertisement. However, there has been growing concern over the blurring of the line between editorial and advertorial content.




Since newspapers began as a journal (record of current events), the profession involved in the making of newspapers began to be called journalism. In the yellow journalism era of the 19th century, many newspapers in the United States relied on sensational stories that were meant to anger or excite the public, rather than to inform. The restrained style of reporting that relies on fact checking and accuracy regained popularity around World War II. Criticism of journalism is varied and sometimes vehement. Credibility is questioned because of anonymous sources; errors in facts, spelling, and grammar; real or perceived bias; and scandals involving plagiarism and fabrication.
In the past, newspapers have often been owned by so-called press barons, and were used for gaining a political voice. After 1920 most major newspapers became parts of chains run by large media corporations such as Gannett, The McClatchy Company, Hearst Corporation, Cox Enterprises, Landmark Media Enterprises LLC, Morris Communications, The Tribune Company, Hollinger International, News Corporation, Swift Communications, etc. Newspapers have, in the modern world, played an important role in the exercise of freedom of expression. Whistle-blowers, and those who "leak" stories of corruption in political circles often choose to inform newspapers before other mediums of communication, relying on the perceived willingness of newspaper editors to expose the secrets and lies of those who would rather cover them. However, there have been many circumstances of the political autonomy of newspapers being curtailed. Recent research has examined the effects of a newspaper's closing on the reelection of incumbents, voter turnout, and campaign spending.
Opinions of other writers and readers are expressed in the op-ed ("opposite the editorial page") and letters to the editors sections of the paper. Some ways newspapers have tried to improve their credibility are: appointing ombudsmen, developing ethics policies and training, using more stringent corrections policies, communicating their processes and rationale with readers, and asking sources to review articles after publication.




By the late 1990s, the availability of news via 24-hour television channels and then the availability of online journalism posed an ongoing challenge to the business model of most newspapers in developed countries. Paid circulation has declined, while advertising revenue—which makes up the bulk of most newspapers' income—has been shifting from print to the new media (social media websites and news websites), resulting in a general decline in print newspapers' revenues and profits. Many newspapers around the world launched online editions in the 2000s, in an attempt to follow or stay ahead of their audience. One of the big challenges is that a number of online news websites, such as Google news, are free to access. Some online news sites are free, and rely on online advertising; other online news sites have a paywall and require paid subscription for access. However, in the non-developed countries, cheaper printing and distribution, increased literacy, the growing middle class and other factors have more than compensated for the emergence of electronic media and newspapers continue to grow.
On 10 April 1995, The American Reporter became the first daily Internet-based newspaper, with its own paid reporters around the world and all-original content. The editor-in-chief and founder is Joe Shea. The site is owned by 400 journalists. The future of newspapers in countries with high levels of Internet access has been widely debated as the industry has faced down soaring newsprint prices, slumping ad sales, the loss of much classified advertising to Craiglist, eBay and other websites, and precipitous drops in circulation. In the late 1990s the number of newspapers slated for closure, bankruptcy or severe cutbacks has risen—especially in the United States, where the industry has shed a fifth of its journalists since 2001. Revenue has plunged while competition from internet media has squeezed older print publishers.
The debate has become more urgent lately, as the 2008-2009 recession shaved newspapers' profits, and as once-explosive growth in newspaper web revenues has leveled off, forestalling what the industry hoped would become an important source of revenue. At issue is whether the newspaper industry faces a cyclical trough (or dip), or whether new technology has rendered print newspapers obsolete, at least in their traditional paper format. As of 2017, an increasing percentage of Millennials (young adults) get their news from social media websites such as Facebook. In the 2010s, many traditional newspapers have begun offering "digital editions", which can be accessed via desktop computer, laptops, and mobile devices such as tablet computers and smartphones. Online newspapers may offer new advertising opportunities to newspaper companies, as online advertising enables much more precise targeting of ads; with an online newspaper, for example, different readers, such as Baby boomers and Millennials can be sent different advertisements.



List of newspaper comic strips
Lists of newspapers






Willings Press Guide (134th ed. 3 vol. 2010), comprehensive guide to world press. Vol 1 UK, Vol 2 Europe and Vol 3 World. ISBN 1-906035-17-2
Editor and Publisher International Year Book (90th ed. 2009), comprehensive guide to American newspapers
Kevin G. Barnhurst, and John Nerone. The Form of News, A History (2001) excerpt and text search
Conley, David, and Stephen Lamble. The Daily Miracle: An Introduction to Journalism (3rd ed. 2006), 518pp; global viewpoint
Harrower, Tim. The Newspaper Designer's Handbook (6th ed. 2007) excerpt and text search
Jones, Alex. Losing the News: The Future of the News That Feeds Democracy (2009)
Sousa, Jorge Pedro Sousa (Coord.); Maria do Carmo Castelo Branco; Mário Pinto; Sandra Tuna; Gabriel Silva; Eduardo Zilles Borba; Mônica Delicato; Carlos Duarte; Nair Silva; Patrícia Teixeira. A Gazeta "da Restauração": Primeiro Periódico Português. Uma análise do discurso VOL. II — Reproduções(2011) ISBN 978-989-654-061-6
Walravens, Hartmut, ed. Newspapers in Central And Eastern Europe (2004) 251pp
Williams, Kevin. Read All About It!: A History of the British Newspaper (2009) excerpt and text search



 "Newspaper". The New Student's Reference Work. 1914. 
NewsTornado – Worldwide Newspaper Circulation Map
Print Culture at A History of Central Florida Podcast
Chart – Real and Fake News (2016)/Vanessa Otero (basis) (Mark Frauenfelder)
Chart – Real and Fake News (2014) (2016)/Pew Research Center



Wikipedia List
Newspapercat – University of Florida Historical Digital Newspaper Catalog Collection
Historical newspapers from 1700s–Present : Newspapers.com
Historical newspaper database, from NewspaperARCHIVE.com
More than 8m pages of Historic European newspapers Free
Chronicling America: Historic American Newspapers from National Digital Newspaper Program.
Tairiku Nippō – A Japanese-Canadian newspaper published between 1907 and 1941, and now digitized by the UBC Library Digital Collections
All Daily Newspapers – All Daily Newspapers from around the world.
Plusnoticias – South American journals.
diarios de argentina – South America Newspapers.A programming language is a formal computer language designed to communicate instructions to a machine, particularly a computer. Programming languages can be used to create programs to control the behavior of a machine or to express algorithms.
The earliest known programmable machine preceded the invention of the digital computer and is the automatic flute player described in the 9th century by the brothers Musa in Baghdad, "during the Islamic Golden Age". From the early 1800s, "programs" were used to direct the behavior of machines such as Jacquard looms and player pianos. Thousands of different programming languages have been created, mainly in the computer field, and many more still are being created every year. Many programming languages require computation to be specified in an imperative form (i.e., as a sequence of operations to perform) while other languages use other forms of program specification such as the declarative form (i.e. the desired result is specified, not how to achieve it).
The description of a programming language is usually split into the two components of syntax (form) and semantics (meaning). Some languages are defined by a specification document (for example, the C programming language is specified by an ISO Standard) while other languages (such as Perl) have a dominant implementation that is treated as a reference. Some languages have both, with the basic language defined by a standard and extensions taken from the dominant implementation being common.



A programming language is a notation for writing programs, which are specifications of a computation or algorithm. Some, but not all, authors restrict the term "programming language" to those languages that can express all possible algorithms. Traits often considered important for what constitutes a programming language include:
Function and target
A computer programming language is a language used to write computer programs, which involve a computer performing some kind of computation or algorithm and possibly control external devices such as printers, disk drives, robots, and so on. For example, PostScript programs are frequently created by another program to control a computer printer or display. More generally, a programming language may describe computation on some, possibly abstract, machine. It is generally accepted that a complete specification for a programming language includes a description, possibly idealized, of a machine or processor for that language. In most practical contexts, a programming language involves a computer; consequently, programming languages are usually defined and studied this way. Programming languages differ from natural languages in that natural languages are only used for interaction between people, while programming languages also allow humans to communicate instructions to machines.
Abstractions
Programming languages usually contain abstractions for defining and manipulating data structures or controlling the flow of execution. The practical necessity that a programming language support adequate abstractions is expressed by the abstraction principle; this principle is sometimes formulated as a recommendation to the programmer to make proper use of such abstractions.
Expressive power
The theory of computation classifies languages by the computations they are capable of expressing. All Turing complete languages can implement the same set of algorithms. ANSI/ISO SQL-92 and Charity are examples of languages that are not Turing complete, yet often called programming languages.
Markup languages like XML, HTML, or troff, which define structured data, are not usually considered programming languages. Programming languages may, however, share the syntax with markup languages if a computational semantics is defined. XSLT, for example, is a Turing complete XML dialect. Moreover, LaTeX, which is mostly used for structuring documents, also contains a Turing complete subset.
The term computer language is sometimes used interchangeably with programming language. However, the usage of both terms varies among authors, including the exact scope of each. One usage describes programming languages as a subset of computer languages. In this vein, languages used in computing that have a different goal than expressing computer programs are generically designated computer languages. For instance, markup languages are sometimes referred to as computer languages to emphasize that they are not meant to be used for programming.
Another usage regards programming languages as theoretical constructs for programming abstract machines, and computer languages as the subset thereof that runs on physical computers, which have finite hardware resources. John C. Reynolds emphasizes that formal specification languages are just as much programming languages as are the languages intended for execution. He also argues that textual and even graphical input formats that affect the behavior of a computer are programming languages, despite the fact they are commonly not Turing-complete, and remarks that ignorance of programming language concepts is the reason for many flaws in input formats.






The earliest computers were often programmed without the help of a programming language, by writing programs in absolute machine language. The programs, in decimal or binary form, were read in from punched cards or magnetic tape or toggled in on switches on the front panel of the computer. Absolute machine languages were later termed first-generation programming languages (1GL).
The next step was development of so-called second-generation programming languages (2GL) or assembly languages, which were still closely tied to the instruction set architecture of the specific computer. These served to make the program much more human-readable and relieved the programmer of tedious and error-prone address calculations.
The first high-level programming languages, or third-generation programming languages (3GL), were written in the 1950s. An early high-level programming language to be designed for a computer was Plankalkül, developed for the German Z3 by Konrad Zuse between 1943 and 1945. However, it was not implemented until 1998 and 2000.
John Mauchly's Short Code, proposed in 1949, was one of the first high-level languages ever developed for an electronic computer. Unlike machine code, Short Code statements represented mathematical expressions in understandable form. However, the program had to be translated into machine code every time it ran, making the process much slower than running the equivalent machine code.

At the University of Manchester, Alick Glennie developed Autocode in the early 1950s. A programming language, it used a compiler to automatically convert the language into machine code. The first code and compiler was developed in 1952 for the Mark 1 computer at the University of Manchester and is considered to be the first compiled high-level programming language.
The second autocode was developed for the Mark 1 by R. A. Brooker in 1954 and was called the "Mark 1 Autocode". Brooker also developed an autocode for the Ferranti Mercury in the 1950s in conjunction with the University of Manchester. The version for the EDSAC 2 was devised by D. F. Hartley of University of Cambridge Mathematical Laboratory in 1961. Known as EDSAC 2 Autocode, it was a straight development from Mercury Autocode adapted for local circumstances and was noted for its object code optimisation and source-language diagnostics which were advanced for the time. A contemporary but separate thread of development, Atlas Autocode was developed for the University of Manchester Atlas 1 machine.
In 1954, FORTRAN was invented at IBM by John Backus. It was the first widely used high-level general purpose programming language to have a functional implementation, as opposed to just a design on paper. It is still popular language for high-performance computing and is used for programs that benchmark and rank the world's fastest supercomputers.
Another early programming language was devised by Grace Hopper in the US, called FLOW-MATIC. It was developed for the UNIVAC I at Remington Rand during the period from 1955 until 1959. Hopper found that business data processing customers were uncomfortable with mathematical notation, and in early 1955, she and her team wrote a specification for an English programming language and implemented a prototype. The FLOW-MATIC compiler became publicly available in early 1958 and was substantially complete in 1959. Flow-Matic was a major influence in the design of COBOL, since only it and its direct descendant AIMACO were in actual use at the time.



The increased use of high-level languages introduced a requirement for low-level programming languages or system programming languages. These languages, to varying degrees, provide facilities between assembly languages and high-level languages and can be used to perform tasks which require direct access to hardware facilities but still provide higher-level control structures and error-checking.
The period from the 1960s to the late 1970s brought the development of the major language paradigms now in use:
APL introduced array programming and influenced functional programming.
ALGOL refined both structured procedural programming and the discipline of language specification; the "Revised Report on the Algorithmic Language ALGOL 60" became a model for how later language specifications were written.
Lisp, implemented in 1958, was the first dynamically typed functional programming language
In the 1960s, Simula was the first language designed to support object-oriented programming; in the mid-1970s, Smalltalk followed with the first "purely" object-oriented language.
C was developed between 1969 and 1973 as a system programming language for the Unix operating system and remains popular.
Prolog, designed in 1972, was the first logic programming language.
In 1978, ML built a polymorphic type system on top of Lisp, pioneering statically typed functional programming languages.
Each of these languages spawned descendants, and most modern programming languages count at least one of them in their ancestry.
The 1960s and 1970s also saw considerable debate over the merits of structured programming, and whether programming languages should be designed to support it. Edsger Dijkstra, in a famous 1968 letter published in the Communications of the ACM, argued that GOTO statements should be eliminated from all "higher level" programming languages.




The 1980s were years of relative consolidation. C++ combined object-oriented and systems programming. The United States government standardized Ada, a systems programming language derived from Pascal and intended for use by defense contractors. In Japan and elsewhere, vast sums were spent investigating so-called "fifth generation" languages that incorporated logic programming constructs. The functional languages community moved to standardize ML and Lisp. Rather than inventing new paradigms, all of these movements elaborated upon the ideas invented in the previous decades.
One important trend in language design for programming large-scale systems during the 1980s was an increased focus on the use of modules or large-scale organizational units of code. Modula-2, Ada, and ML all developed notable module systems in the 1980s, which were often wedded to generic programming constructs.
The rapid growth of the Internet in the mid-1990s created opportunities for new languages. Perl, originally a Unix scripting tool first released in 1987, became common in dynamic websites. Java came to be used for server-side programming, and bytecode virtual machines became popular again in commercial settings with their promise of "Write once, run anywhere" (UCSD Pascal had been popular for a time in the early 1980s). These developments were not fundamentally novel, rather they were refinements of many existing languages and paradigms (although their syntax was often based on the C family of programming languages).
Programming language evolution continues, in both industry and research. Current directions include security and reliability verification, new kinds of modularity (mixins, delegates, aspects), and database integration such as Microsoft's LINQ.
Fourth-generation programming languages (4GL) are a computer programming languages which aim to provide a higher level of abstraction of the internal computer hardware details than 3GLs. Fifth generation programming languages (5GL) are programming languages based on solving problems using constraints given to the program, rather than using an algorithm written by a programmer.



All programming languages have some primitive building blocks for the description of data and the processes or transformations applied to them (like the addition of two numbers or the selection of an item from a collection). These primitives are defined by syntactic and semantic rules which describe their structure and meaning respectively.




A programming language's surface form is known as its syntax. Most programming languages are purely textual; they use sequences of text including words, numbers, and punctuation, much like written natural languages. On the other hand, there are some programming languages which are more graphical in nature, using visual relationships between symbols to specify a program.
The syntax of a language describes the possible combinations of symbols that form a syntactically correct program. The meaning given to a combination of symbols is handled by semantics (either formal or hard-coded in a reference implementation). Since most languages are textual, this article discusses textual syntax.
Programming language syntax is usually defined using a combination of regular expressions (for lexical structure) and Backus–Naur form (for grammatical structure). Below is a simple grammar, based on Lisp:

This grammar specifies the following:
an expression is either an atom or a list;
an atom is either a number or a symbol;
a number is an unbroken sequence of one or more decimal digits, optionally preceded by a plus or minus sign;
a symbol is a letter followed by zero or more of any characters (excluding whitespace); and
a list is a matched pair of parentheses, with zero or more expressions inside it.
The following are examples of well-formed token sequences in this grammar: 12345, () and (a b c232 (1)).
Not all syntactically correct programs are semantically correct. Many syntactically correct programs are nonetheless ill-formed, per the language's rules; and may (depending on the language specification and the soundness of the implementation) result in an error on translation or execution. In some cases, such programs may exhibit undefined behavior. Even when a program is well-defined within a language, it may still have a meaning that is not intended by the person who wrote it.
Using natural language as an example, it may not be possible to assign a meaning to a grammatically correct sentence or the sentence may be false:
"Colorless green ideas sleep furiously." is grammatically well-formed but has no generally accepted meaning.
"John is a married bachelor." is grammatically well-formed but expresses a meaning that cannot be true.
The following C language fragment is syntactically correct, but performs operations that are not semantically defined (the operation *p >> 4 has no meaning for a value having a complex type and p->im is not defined because the value of p is the null pointer):

If the type declaration on the first line were omitted, the program would trigger an error on compilation, as the variable "p" would not be defined. But the program would still be syntactically correct since type declarations provide only semantic information.
The grammar needed to specify a programming language can be classified by its position in the Chomsky hierarchy. The syntax of most programming languages can be specified using a Type-2 grammar, i.e., they are context-free grammars. Some languages, including Perl and Lisp, contain constructs that allow execution during the parsing phase. Languages that have constructs that allow the programmer to alter the behavior of the parser make syntax analysis an undecidable problem, and generally blur the distinction between parsing and execution. In contrast to Lisp's macro system and Perl's BEGIN blocks, which may contain general computations, C macros are merely string replacements and do not require code execution.



The term semantics refers to the meaning of languages, as opposed to their form (syntax).



The static semantics defines restrictions on the structure of valid texts that are hard or impossible to express in standard syntactic formalisms. For compiled languages, static semantics essentially include those semantic rules that can be checked at compile time. Examples include checking that every identifier is declared before it is used (in languages that require such declarations) or that the labels on the arms of a case statement are distinct. Many important restrictions of this type, like checking that identifiers are used in the appropriate context (e.g. not adding an integer to a function name), or that subroutine calls have the appropriate number and type of arguments, can be enforced by defining them as rules in a logic called a type system. Other forms of static analyses like data flow analysis may also be part of static semantics. Newer programming languages like Java and C# have definite assignment analysis, a form of data flow analysis, as part of their static semantics.




Once data has been specified, the machine must be instructed to perform operations on the data. For example, the semantics may define the strategy by which expressions are evaluated to values, or the manner in which control structures conditionally execute statements. The dynamic semantics (also known as execution semantics) of a language defines how and when the various constructs of a language should produce a program behavior. There are many ways of defining execution semantics. Natural language is often used to specify the execution semantics of languages commonly used in practice. A significant amount of academic research went into formal semantics of programming languages, which allow execution semantics to be specified in a formal manner. Results from this field of research have seen limited application to programming language design and implementation outside academia.




A type system defines how a programming language classifies values and expressions into types, how it can manipulate those types and how they interact. The goal of a type system is to verify and usually enforce a certain level of correctness in programs written in that language by detecting certain incorrect operations. Any decidable type system involves a trade-off: while it rejects many incorrect programs, it can also prohibit some correct, albeit unusual programs. In order to bypass this downside, a number of languages have type loopholes, usually unchecked casts that may be used by the programmer to explicitly allow a normally disallowed operation between different types. In most typed languages, the type system is used only to type check programs, but a number of languages, usually functional ones, infer types, relieving the programmer from the need to write type annotations. The formal design and study of type systems is known as type theory.



A language is typed if the specification of every operation defines types of data to which the operation is applicable, with the implication that it is not applicable to other types. For example, the data represented by "this text between the quotes" is a string, and in many programming languages dividing a number by a string has no meaning and will be rejected by the compilers. The invalid operation may be detected when the program is compiled ("static" type checking) and will be rejected by the compiler with a compilation error message, or it may be detected when the program is run ("dynamic" type checking), resulting in a run-time exception. Many languages allow a function called an exception handler to be written to handle this exception and, for example, always return "-1" as the result.
A special case of typed languages are the single-type languages. These are often scripting or markup languages, such as REXX or SGML, and have only one data type—most commonly character strings which are used for both symbolic and numeric data.
In contrast, an untyped language, such as most assembly languages, allows any operation to be performed on any data, which are generally considered to be sequences of bits of various lengths. High-level languages which are untyped include BCPL, Tcl, and some varieties of Forth.
In practice, while few languages are considered typed from the point of view of type theory (verifying or rejecting all operations), most modern languages offer a degree of typing. Many production languages provide means to bypass or subvert the type system, trading type-safety for finer control over the program's execution (see casting).



In static typing, all expressions have their types determined prior to when the program is executed, typically at compile-time. For example, 1 and (2+2) are integer expressions; they cannot be passed to a function that expects a string, or stored in a variable that is defined to hold dates.
Statically typed languages can be either manifestly typed or type-inferred. In the first case, the programmer must explicitly write types at certain textual positions (for example, at variable declarations). In the second case, the compiler infers the types of expressions and declarations based on context. Most mainstream statically typed languages, such as C++, C# and Java, are manifestly typed. Complete type inference has traditionally been associated with less mainstream languages, such as Haskell and ML. However, many manifestly typed languages support partial type inference; for example, Java and C# both infer types in certain limited cases. Additionally, some programming languages allow for some types to be automatically converted to other types; for example, an int can be used where the program expects a float.
Dynamic typing, also called latent typing, determines the type-safety of operations at run time; in other words, types are associated with run-time values rather than textual expressions. As with type-inferred languages, dynamically typed languages do not require the programmer to write explicit type annotations on expressions. Among other things, this may permit a single variable to refer to values of different types at different points in the program execution. However, type errors cannot be automatically detected until a piece of code is actually executed, potentially making debugging more difficult. Lisp, Smalltalk, Perl, Python, JavaScript, and Ruby are dynamically typed.



Weak typing allows a value of one type to be treated as another, for example treating a string as a number. This can occasionally be useful, but it can also allow some kinds of program faults to go undetected at compile time and even at run time.
Strong typing prevents the above. An attempt to perform an operation on the wrong type of value raises an error. Strongly typed languages are often termed type-safe or safe.
An alternative definition for "weakly typed" refers to languages, such as Perl and JavaScript, which permit a large number of implicit type conversions. In JavaScript, for example, the expression 2 * x implicitly converts x to a number, and this conversion succeeds even if x is null, undefined, an Array, or a string of letters. Such implicit conversions are often useful, but they can mask programming errors. Strong and static are now generally considered orthogonal concepts, but usage in the literature differs. Some use the term strongly typed to mean strongly, statically typed, or, even more confusingly, to mean simply statically typed. Thus C has been called both strongly typed and weakly, statically typed.
It may seem odd to some professional programmers that C could be "weakly, statically typed". However, notice that the use of the generic pointer, the void* pointer, does allow for casting of pointers to other pointers without needing to do an explicit cast. This is extremely similar to somehow casting an array of bytes to any kind of datatype in C without using an explicit cast, such as (int) or (char).




Most programming languages have an associated core library (sometimes known as the 'standard library', especially if it is included as part of the published language standard), which is conventionally made available by all implementations of the language. Core libraries typically include definitions for commonly used algorithms, data structures, and mechanisms for input and output.
The line between a language and its core library differs from language to language. In some cases, the language designers may treat the library as a separate entity from the language. However, a language's core library is often treated as part of the language by its users, and some language specifications even require that this library be made available in all implementations. Indeed, some languages are designed so that the meanings of certain syntactic constructs cannot even be described without referring to the core library. For example, in Java, a string literal is defined as an instance of the java.lang.String class; similarly, in Smalltalk, an anonymous function expression (a "block") constructs an instance of the library's BlockContext class. Conversely, Scheme contains multiple coherent subsets that suffice to construct the rest of the language as library macros, and so the language designers do not even bother to say which portions of the language must be implemented as language constructs, and which must be implemented as parts of a library.



Programming languages share properties with natural languages related to their purpose as vehicles for communication, having a syntactic form separate from its semantics, and showing language families of related languages branching one from another. But as artificial constructs, they also differ in fundamental ways from languages that have evolved through usage. A significant difference is that a programming language can be fully described and studied in its entirety, since it has a precise and finite definition. By contrast, natural languages have changing meanings given by their users in different communities. While constructed languages are also artificial languages designed from the ground up with a specific purpose, they lack the precise and complete semantic definition that a programming language has.
Many programming languages have been designed from scratch, altered to meet new needs, and combined with other languages. Many have eventually fallen into disuse. Although there have been attempts to design one "universal" programming language that serves all purposes, all of them have failed to be generally accepted as filling this role. The need for diverse programming languages arises from the diversity of contexts in which languages are used:
Programs range from tiny scripts written by individual hobbyists to huge systems written by hundreds of programmers.
Programmers range in expertise from novices who need simplicity above all else, to experts who may be comfortable with considerable complexity.
Programs must balance speed, size, and simplicity on systems ranging from microcontrollers to supercomputers.
Programs may be written once and not change for generations, or they may undergo continual modification.
Programmers may simply differ in their tastes: they may be accustomed to discussing problems and expressing them in a particular language.
One common trend in the development of programming languages has been to add more ability to solve problems using a higher level of abstraction. The earliest programming languages were tied very closely to the underlying hardware of the computer. As new programming languages have developed, features have been added that let programmers express ideas that are more remote from simple translation into underlying hardware instructions. Because programmers are less tied to the complexity of the computer, their programs can do more computing with less effort from the programmer. This lets them write more functionality per time unit.
 Natural language programming has been proposed as a way to eliminate the need for a specialized language for programming. However, this goal remains distant and its benefits are open to debate. Edsger W. Dijkstra took the position that the use of a formal language is essential to prevent the introduction of meaningless constructs, and dismissed natural language programming as "foolish". Alan Perlis was similarly dismissive of the idea. Hybrid approaches have been taken in Structured English and SQL.
A language's designers and users must construct a number of artifacts that govern and enable the practice of programming. The most important of these artifacts are the language specification and implementation.




The specification of a programming language is an artifact that the language users and the implementors can use to agree upon whether a piece of source code is a valid program in that language, and if so what its behavior shall be.
A programming language specification can take several forms, including the following:
An explicit definition of the syntax, static semantics, and execution semantics of the language. While syntax is commonly specified using a formal grammar, semantic definitions may be written in natural language (e.g., as in the C language), or a formal semantics (e.g., as in Standard ML and Scheme specifications).
A description of the behavior of a translator for the language (e.g., the C++ and Fortran specifications). The syntax and semantics of the language have to be inferred from this description, which may be written in natural or a formal language.
A reference or model implementation, sometimes written in the language being specified (e.g., Prolog or ANSI REXX). The syntax and semantics of the language are explicit in the behavior of the reference implementation.




An implementation of a programming language provides a way to write programs in that language and execute them on one or more configurations of hardware and software. There are, broadly, two approaches to programming language implementation: compilation and interpretation. It is generally possible to implement a language using either technique.
The output of a compiler may be executed by hardware or a program called an interpreter. In some implementations that make use of the interpreter approach there is no distinct boundary between compiling and interpreting. For instance, some implementations of BASIC compile and then execute the source a line at a time.
Programs that are executed directly on the hardware usually run several orders of magnitude faster than those that are interpreted in software.
One technique for improving the performance of interpreted programs is just-in-time compilation. Here the virtual machine, just before execution, translates the blocks of bytecode which are going to be used to machine code, for direct execution on the hardware.



Although most of the most commonly used programming languages have fully open specifications and implementations, many programming languages exist only as proprietary programming languages with the implementation available only from a single vendor, which may claim that such a proprietary language is their intellectual property. Proprietary programming languages are commonly domain specific languages or internal scripting languages for a single product; some proprietary languages are used only internally within a vendor, while others are available to external users.
Some programming languages exist on the border between proprietary and open; for example, Oracle Corporation asserts proprietary rights to some aspects of the Java programming language, and Microsoft's C# programming language, which has open implementations of most parts of the system, also has Common Language Runtime (CLR) as a closed environment.
Many proprietary languages are widely used, in spite of their proprietary nature; examples include MATLAB and VBScript. Some languages may make the transition from closed to open; for example, Erlang was originally an Ericsson's internal programming language.



Thousands of different programming languages have been created, mainly in the computing field. Software is commonly built with 5 programming languages or more.
Programming languages differ from most other forms of human expression in that they require a greater degree of precision and completeness. When using a natural language to communicate with other people, human authors and speakers can be ambiguous and make small errors, and still expect their intent to be understood. However, figuratively speaking, computers "do exactly what they are told to do", and cannot "understand" what code the programmer intended to write. The combination of the language definition, a program, and the program's inputs must fully specify the external behavior that occurs when the program is executed, within the domain of control of that program. On the other hand, ideas about an algorithm can be communicated to humans without the precision required for execution by using pseudocode, which interleaves natural language with code written in a programming language.
A programming language provides a structured mechanism for defining pieces of data, and the operations or transformations that may be carried out automatically on that data. A programmer uses the abstractions present in the language to represent the concepts involved in a computation. These concepts are represented as a collection of the simplest elements available (called primitives). Programming is the process by which programmers combine these primitives to compose new programs, or adapt existing ones to new uses or a changing environment.
Programs for a computer might be executed in a batch process without human interaction, or a user might type commands in an interactive session of an interpreter. In this case the "commands" are simply programs, whose execution is chained together. When a language can run its commands through an interpreter (such as a Unix shell or other command-line interface), without compiling, it is called a scripting language.




It is difficult to determine which programming languages are most widely used, and what usage means varies by context. One language may occupy the greater number of programmer hours, a different one have more lines of code, and a third may consume the most CPU time. Some languages are very popular for particular kinds of applications. For example, COBOL is still strong in the corporate data center, often on large mainframes; Fortran in scientific and engineering applications; Ada in aerospace, transportation, military, real-time and embedded applications; and C in embedded applications and operating systems. Other languages are regularly used to write many different kinds of applications.
Various methods of measuring language popularity, each subject to a different bias over what is measured, have been proposed:
counting the number of job advertisements that mention the language
the number of books sold that teach or describe the language
estimates of the number of existing lines of code written in the language –  which may underestimate languages not often found in public searches
counts of language references (i.e., to the name of the language) found using a web search engine.
Combining and averaging information from various internet sites, langpop.com claims that in 2013 the ten most popular programming languages are (in descending order by overall popularity): C, Java, PHP, JavaScript, C++, Python, Shell, Ruby, Objective-C and C#.




There is no overarching classification scheme for programming languages. A given programming language does not usually have a single ancestor language. Languages commonly arise by combining the elements of several predecessor languages with new ideas in circulation at the time. Ideas that originate in one language will diffuse throughout a family of related languages, and then leap suddenly across familial gaps to appear in an entirely different family.
The task is further complicated by the fact that languages can be classified along multiple axes. For example, Java is both an object-oriented language (because it encourages object-oriented organization) and a concurrent language (because it contains built-in constructs for running multiple threads in parallel). Python is an object-oriented scripting language.
In broad strokes, programming languages divide into programming paradigms and a classification by intended domain of use, with general-purpose programming languages distinguished from domain-specific programming languages. Traditionally, programming languages have been regarded as describing computation in terms of imperative sentences, i.e. issuing commands. These are generally called imperative programming languages. A great deal of research in programming languages has been aimed at blurring the distinction between a program as a set of instructions and a program as an assertion about the desired answer, which is the main feature of declarative programming. More refined paradigms include procedural programming, object-oriented programming, functional programming, and logic programming; some languages are hybrids of paradigms or multi-paradigmatic. An assembly language is not so much a paradigm as a direct model of an underlying machine architecture. By purpose, programming languages might be considered general purpose, system programming languages, scripting languages, domain-specific languages, or concurrent/distributed languages (or a combination of these). Some general purpose languages were designed largely with educational goals.
A programming language may also be classified by factors unrelated to programming paradigm. For instance, most programming languages use English language keywords, while a minority do not. Other languages may be classified as being deliberately esoteric or not.








In software engineering, performance testing is in general, a testing practice performed to determine how a system performs in terms of responsiveness and stability under a particular workload. It can also serve to investigate, measure, validate or verify other quality attributes of the system, such as scalability, reliability and resource usage.
Performance testing, a subset of performance engineering, is a computer science practice which strives to build performance standards into the implementation, design and architecture of a system.






Load testing is the simplest form of performance testing. A load test is usually conducted to understand the behaviour of the system under a specific expected load. This load can be the expected concurrent number of users on the application performing a specific number of transactions within the set duration. This test will give out the response times of all the important business critical transactions. The database, application server, etc. are also monitored during the test, this will assist in identifying bottlenecks in the application software and the hardware that the software is installed on.



Stress testing is normally used to understand the upper limits of capacity within the system. This kind of test is done to determine the system's robustness in terms of extreme load and helps application administrators to determine if the system will perform sufficiently if the current load goes well above the expected maximum.



Soak testing, also known as endurance testing, is usually done to determine if the system can sustain the continuous expected load. During soak tests, memory utilization is monitored to detect potential leaks. Also important, but often overlooked is performance degradation, i.e. to ensure that the throughput and/or response times after some long period of sustained activity are as good as or better than at the beginning of the test. It essentially involves applying a significant load to a system for an extended, significant period of time. The goal is to discover how the system behaves under sustained use.



Spike testing is done by suddenly increasing or decreasing the load generated by a very large number of users, and observing the behaviour of the system. The goal is to determine whether performance will suffer, the system will fail, or it will be able to handle dramatic changes in load.



Rather than testing for performance from a load perspective, tests are created to determine the effects of configuration changes to the system's components on the system's performance and behaviour. A common example would be experimenting with different methods of load-balancing.



Isolation testing is not unique to performance testing but involves repeating a test execution that resulted in a system problem. Such testing can often isolate and confirm the fault domain.



Performance testing can serve different purposes:
It can demonstrate that the system meets performance criteria.
It can compare two systems to find which performs better.
It can measure which parts of the system or workload cause the system to perform badly.
Many performance tests are undertaken without setting sufficiently realistic, goal-oriented performance goals. The first question from a business perspective should always be, "why are we performance-testing?". These considerations are part of the business case of the testing. Performance goals will differ depending on the system's technology and purpose, but should always include some of the following:



If a system identifies end-users by some form of log-in procedure then a concurrency goal is highly desirable. By definition this is the largest number of concurrent system users that the system is expected to support at any given moment. The work-flow of a scripted transaction may impact true concurrency especially if the iterative part contains the log-in and log-out activity.
If the system has no concept of end-users, then performance goal is likely to be based on a maximum throughput or transaction rate. A common example would be casual browsing of a web site such as Wikipedia.



This refers to the time taken for one system node to respond to the request of another. A simple example would be a HTTP 'GET' request from browser client to web server. In terms of response time this is what all load testing tools actually measure. It may be relevant to set server response time goals between all nodes of the system.



Load-testing tools have difficulty measuring render-response time, since they generally have no concept of what happens within a node apart from recognizing a period of time where there is no activity 'on the wire'. To measure render response time, it is generally necessary to include functional test scripts as part of the performance test scenario. Many load testing tools do not offer this feature.



It is critical to detail performance specifications (requirements) and document them in any performance test plan. Ideally, this is done during the requirements development phase of any system development project, prior to any design effort. See Performance Engineering for more details.
However, performance testing is frequently not performed against a specification; e.g., no one will have expressed what the maximum acceptable response time for a given population of users should be. Performance testing is frequently used as part of the process of performance profile tuning. The idea is to identify the “weakest link” – there is inevitably a part of the system which, if it is made to respond faster, will result in the overall system running faster. It is sometimes a difficult task to identify which part of the system represents this critical path, and some test tools include (or can have add-ons that provide) instrumentation that runs on the server (agents) and reports transaction times, database access times, network overhead, and other server monitors, which can be analyzed together with the raw performance statistics. Without such instrumentation one might have to have someone crouched over Windows Task Manager at the server to see how much CPU load the performance tests are generating (assuming a Windows system is under test).
Performance testing can be performed across the web, and even done in different parts of the country, since it is known that the response times of the internet itself vary regionally. It can also be done in-house, although routers would then need to be configured to introduce the lag that would typically occur on public networks. Loads should be introduced to the system from realistic points. For example, if 50% of a system's user base will be accessing the system via a 56K modem connection and the other half over a T1, then the load injectors (computers that simulate real users) should either inject load over the same mix of connections (ideal) or simulate the network latency of such connections, following the same user profile.
It is always helpful to have a statement of the likely peak number of users that might be expected to use the system at peak times. If there can also be a statement of what constitutes the maximum allowable 95 percentile response time, then an injector configuration could be used to test whether the proposed system met that specification.



Performance specifications should ask the following questions, at a minimum:
In detail, what is the performance test scope? What subsystems, interfaces, components, etc. are in and out of scope for this test?
For the user interfaces (UIs) involved, how many concurrent users are expected for each (specify peak vs. nominal)?
What does the target system (hardware) look like (specify all server and network appliance configurations)?
What is the Application Workload Mix of each system component? (for example: 20% log-in, 40% search, 30% item select, 10% checkout).
What is the System Workload Mix? [Multiple workloads may be simulated in a single performance test] (for example: 30% Workload A, 20% Workload B, 50% Workload C).
What are the time requirements for any/all back-end batch processes (specify peak vs. nominal)?



A stable build of the system which must resemble the production environment as closely as is possible.
To ensure consistent results, the performance testing environment should be isolated from other environments, such as user acceptance testing (UAT) or development. As a best practice it is always advisable to have a separate performance testing environment resembling the production environment as much as possible.



In performance testing, it is often crucial for the test conditions to be similar to the expected actual use. However, in practice this is hard to arrange and not wholly possible, since production systems are subjected to unpredictable workloads. Test workloads may mimic occurrences in the production environment as far as possible, but only in the simplest systems can one exactly replicate this workload variability.
Loosely-coupled architectural implementations (e.g.: SOA) have created additional complexities with performance testing. To truly replicate production-like states, enterprise services or assets that share a common infrastructure or platform require coordinated performance testing, with all consumers creating production-like transaction volumes and load on shared infrastructures or platforms. Because this activity is so complex and costly in money and time, some organizations now use tools to monitor and simulate production-like conditions (also referred as "noise") in their performance testing environments (PTE) to understand capacity and resource requirements and verify / validate quality attributes.



It is critical to the cost performance of a new system that performance test efforts begin at the inception of the development project and extend through to deployment. The later a performance defect is detected, the higher the cost of remediation. This is true in the case of functional testing, but even more so with performance testing, due to the end-to-end nature of its scope. It is crucial for a performance test team to be involved as early as possible, because it is time-consuming to acquire and prepare the testing environment and other key performance requisites.



In the diagnostic case, software engineers use tools such as profilers to measure what parts of a device or software contribute most to the poor performance, or to establish throughput levels (and thresholds) for maintained acceptable response time.



Performance testing technology employs one or more PCs or Unix servers to act as injectors, each emulating the presence of numbers of users and each running an automated sequence of interactions (recorded as a script, or as a series of scripts to emulate different types of user interaction) with the host whose performance is being tested. Usually, a separate PC acts as a test conductor, coordinating and gathering metrics from each of the injectors and collating performance data for reporting purposes. The usual sequence is to ramp up the load: to start with a few virtual users and increase the number over time to a predetermined maximum. The test result shows how the performance varies with the load, given as number of users vs. response time. Various tools are available to perform such tests. Tools in this category usually execute a suite of tests which emulate real users against the system. Sometimes the results can reveal oddities, e.g., that while the average response time might be acceptable, there are outliers of a few key transactions that take considerably longer to complete – something that might be caused by inefficient database queries, pictures, etc.
Performance testing can be combined with stress testing, in order to see what happens when an acceptable load is exceeded. Does the system crash? How long does it take to recover if a large load is reduced? Does its failure cause collateral damage?
Analytical Performance Modeling is a method to model the behaviour of a system in a spreadsheet. The model is fed with measurements of transaction resource demands (CPU, disk I/O, LAN, WAN), weighted by the transaction-mix (business transactions per hour). The weighted transaction resource demands are added up to obtain the hourly resource demands and divided by the hourly resource capacity to obtain the resource loads. Using the responsetime formula (R=S/(1-U), R=responsetime, S=servicetime, U=load), responsetimes can be calculated and calibrated with the results of the performance tests. Analytical performance modeling allows evaluation of design options and system sizing based on actual or anticipated business use. It is therefore much faster and cheaper than performance testing, though it requires thorough understanding of the hardware platforms.



Tasks to perform such a test would include:
Decide whether to use internal or external resources to perform the tests, depending on inhouse expertise (or lack of it)
Gather or elicit performance requirements (specifications) from users and/or business analysts
Develop a high-level plan (or project charter), including requirements, resources, timelines and milestones
Develop a detailed performance test plan (including detailed scenarios and test cases, workloads, environment info, etc.)
Choose test tool(s)
Specify test data needed and charter effort (often overlooked, but vital to carrying out a valid performance test)
Develop proof-of-concept scripts for each application/component under test, using chosen test tools and strategies
Develop detailed performance test project plan, including all dependencies and associated timelines
Install and configure injectors/controller
Configure the test environment (ideally identical hardware to the production platform), router configuration, quiet network (we don’t want results upset by other users), deployment of server instrumentation, database test sets developed, etc.
Dry run the tests - before actually executing the load test with predefined users, a dry run is carried out in order to check the correctness of the script
Execute tests – probably repeatedly (iteratively) in order to see whether any unaccounted-for factor might affect the results
Analyze the results - either pass/fail, or investigation of critical path and recommendation of corrective action






According to the Microsoft Developer Network the Performance Testing Methodology consists of the following activities:
Identify the Test Environment. Identify the physical test environment and the production environment as well as the tools and resources available to the test team. The physical environment includes hardware, software, and network configurations. Having a thorough understanding of the entire test environment at the outset enables more efficient test design and planning and helps you identify testing challenges early in the project. In some situations, this process must be revisited periodically throughout the project’s life cycle.
Identify Performance Acceptance Criteria. Identify the response time, throughput, and resource-use goals and constraints. In general, response time is a user concern, throughput is a business concern, and resource use is a system concern. Additionally, identify project success criteria that may not be captured by those goals and constraints; for example, using performance tests to evaluate which combination of configuration settings will result in the most desirable performance characteristics.
Plan and Design Tests. Identify key scenarios, determine variability among representative users and how to simulate that variability, define test data, and establish metrics to be collected. Consolidate this information into one or more models of system usage to be implemented, executed, and analyzed.
Configure the Test Environment. Prepare the test environment, tools, and resources necessary to execute each strategy, as features and components become available for test. Ensure that the test environment is instrumented for resource monitoring as necessary.
Implement the Test Design. Develop the performance tests in accordance with the test design.
Execute the Test. Run and monitor your tests. Validate the tests, test data, and results collection. Execute validated tests for analysis while monitoring the test and the test environment.
Analyze Results, Tune, and Retest. Analyse, Consolidate and share results data. Make a tuning change and retest. Compare the results of both tests. Each improvement made will return smaller improvement than the previous improvement. When do you stop? When you reach a CPU bottleneck, the choices then are either improve the code or add more CPU.




Stress testing (software)
Benchmark (computing)
Web server benchmarking
Application Response Measurement



The Art of Application Performance Testing - O'Reilly ISBN 978-0-596-52066-3 (Book)
Performance Testing Guidance for Web Applications (MSDN)
Performance Testing Guidance for Web Applications (Book)
Performance Testing Guidance for Web Applications (PDF)
Performance Testing Guidance (Online KB)
Enterprise IT Performance Testing (Online KB)
Performance Testing Videos (MSDN)
Open Source Performance Testing tools
"User Experience, not Metrics" and "Beyond Performance Testing"
"Performance Testing Traps / Pitfalls"A Federal Perkins Loan, or Perkins Loan, is a need-based student loan offered by the U.S. Department of Education to assist American college students in funding their post-secondary education. The program is named after Carl D. Perkins, a former member of the U.S. House of Representatives from Kentucky.
Perkins Loans carry a fixed interest rate of 5% for the duration of the ten-year repayment period. The Perkins Loan Program has a nine-month grace period, so that borrowers begin repayment in the tenth month upon graduating, falling below half-time status, or withdrawing from their college or university. Because the Perkins Loan is subsidized by the government, interest does not begin to accrue until the borrower begins to repay the loan. As of the 2009-2010 academic year, the loan limits for undergraduates are $5,500 per year with a lifetime maximum loan of $27,500. For graduate students, the limit is $8,000 per year with a lifetime limit of $60,000 (including undergraduate loans).
Perkins Loans are eligible for Federal Loan Cancellation for individuals choosing to work in a number of different public service occupations including early childhood education, elementary and secondary school teaching, speech therapy, nursing, law enforcement, librarian, public defense attorney, fire fighting and certain active duty military postings. Depending on the field of employment, further restrictions on the setting of employment may apply. For example, forgiveness for teachers may be restricted to designated low-income schools or specific teacher shortage areas such as math, science, and bilingual education and forgiveness for nurses requires employment at a non-profit medical facility. A percentage of the loan is cancelled for each year spent teaching full-time(as long as the loan remains in good standing). This cancellation also applies to Peace Corps Volunteers. Cancellation typically occurs on a graduating scale: 15% for year 1, 15% for year 2, 20% for year 3, 20% for year 4, 30% for year 5. These percentages are based on the original debt amount. Thus after 3 years of service, one would have 50% of their original debt cancelled.
The Federal Perkins Loan program is set to expire on September 30, 2017, this is an extension to an earlier program expiration of September 30, 2015. The extension was made possible by the General Education Provisions Act (GEPA), however this act also prohibits any further extensions of the Perkins Loan Program. 



Stafford Loan
Student loans in the United States



U.S. Dept. of Education: Federal Perkins Loans
U.S. Dept. of Education: Federal Perkins Loan Teacher Cancellation
U.S. Office of Management and Budget: Federal Perkins Loans Program Assessment


The arts represent an outlet of expression that is usually influenced by culture and which in turn helps to change culture. As such, the arts are a physical manifestation of the internal creative impulse. Major constituents of the arts include literature – including poetry, novels and short stories, and epics; performing arts – among them music, dance, and theatre; culinary arts such as baking, chocolatiering, and winemaking; media arts like photography and cinematography, and visual arts – including drawing, painting, ceramics, and sculpting. Some art forms combine a visual element with performance (e.g. film) and the written word (e.g. comics).
From prehistoric cave paintings to modern day films, art serves as a vessel for storytelling and conveying humankind's relationship with its environment.



In its most basic abstract definition, art is a documented expression of a sentient being through or on an accessible medium so that anyone can view, hear or experience it. The act itself of producing an expression can also be referred to as a certain art, or as art in general.
If this solidified expression, or the act of producing it, is "good" or has value depends on those who access and rate it and this public rating is dependent on various subjective factors.
Merriam-Webster defines "the arts" as "painting, sculpture, music, theater, literature, etc., considered as a group of activities done by people with skill and imagination." Similarly, the United States Congress, in the National Foundation on the Arts and Humanities Act, defined "the arts" as follows:

The term 'the arts' includes, but is not limited to, music (instrumental and vocal), dance, drama, folk art, creative writing, architecture and allied fields, painting, sculpture, photography, graphic and craft arts, industrial design, costume and fashion design, motion pictures, television, radio, film, video, tape and sound recording, the arts related to the presentation, performance, execution, and exhibition of such major art forms, all those traditional arts practiced by the diverse peoples of this country. (sic) and the study and application of the arts to the human environment.




In Ancient Greece, all art and craft was referred to by the same word, Techne. Thus, there was no distinction between the arts. Ancient Greek art brought the veneration of the animal form and the development of equivalent skills to show musculature, poise, beauty, and anatomically correct proportions. Ancient Roman art depicted gods as idealized humans, shown with characteristic distinguishing features (i.e. Zeus' thunderbolt).
In Byzantine and Gothic art of the Middle Ages, the dominance of the church insisted on the expression of biblical and not material truths.
Eastern art has generally worked in a style akin to Western medieval art, namely a concentration on surface patterning and local colour (meaning the plain colour of an object, such as basic red for a red robe, rather than the modulations of that colour brought about by light, shade and reflection). A characteristic of this style is that the local colour is often defined by an outline (a contemporary equivalent is the cartoon). This is evident in, for example, the art of India, Tibet and Japan.
Religious Islamic art forbids iconography, and expresses religious ideas through geometry instead.




In the Middle Ages, the Artes Liberales (liberal arts) were taught in universities as part of the Trivium, an introductory curriculum involving grammar, rhetoric, and logic, and of the Quadrivium, a curriculum involving the "mathematical arts" of arithmetic, geometry, music, and astronomy. The Artes Mechanicae (consisting of vestiaria – tailoring and weaving; agricultura – agriculture; architectura – architecture and masonry; militia and venatoria – warfare, hunting, military education, and the martial arts; mercatura – trade; coquinaria – cooking; and metallaria – blacksmithing and metallurgy) were practised and developed in guild environments. The modern distinction between "artistic" and "non-artistic" skills did not develop until the Renaissance.
In modern academia, the arts are usually grouped with or as a subset of the humanities. Some subjects in the humanities are history, linguistics, literature, theology, philosophy, and/or logic.
The arts have also been classified as seven: Literature, painting, sculpture, and music comprise the main four arts, of which the other three are derivative; drama is literature with acting, dance is music expressed through motion, and song is music with literature and voice.







Drawing is a means of making an image, using any of a wide variety of tools and techniques. It generally involves making marks on a surface by applying pressure from a tool, or moving a tool across a surface. Common tools are graphite pencils, pen and ink, inked brushes, wax colour pencils, crayons, charcoals, pastels, and markers. Digital tools which can simulate the effects of these are also used. The main techniques used in drawing are line drawing, hatching, crosshatching, random hatching, scribbling, stippling, and blending. An artist who excels in drawing is referred to as a drafter, draftswoman, or draughtsman.
Drawing can be used to create art such as illustrations, comics, and animation.




Painting, taken literally, is the practice of applying pigment suspended in a vehicle (or medium) and a binding agent (a glue) to a surface (support) such as paper, canvas, wood panel or a wall. However, when used in an artistic sense, it means the use of this activity in combination with drawing, composition and other aesthetic considerations in order to manifest the expressive and conceptual intention of the practitioner. Painting is also used to express spiritual motifs and ideas; sites of this kind of painting range from artwork depicting mythological figures on pottery to The Sistine Chapel to the human body itself.
Colour is the essence of painting as sound is of music. Colour is highly subjective, but has observable psychological effects, although these can differ from one culture to the next. Black is associated with mourning in the West, but elsewhere white may be. Some painters, theoreticians, writers and scientists, including Goethe, Kandinsky, and Newton, have written their own colour theory. Moreover, the use of language is only an abstraction for a colour equivalent. The word "red," for example, can cover a wide range of variations on the pure red of the spectrum. There is not a formalized register of different colours in the way that there is agreement on different notes in music, such as C or C#, although the Pantone system is widely used in the printing and design industry for this purpose.
Modern artists have extended the practice of painting considerably to include, for example, collage. This began with Cubism, and is not painting in the strict sense. Some modern painters incorporate different materials such as sand, cement, straw, wood or strands of hair for their texture. Examples of this are the works of Elito Circa, Jean Dubuffet or Anselm Kiefer.
Modern and contemporary art has moved away from the historic value of craft in favour of concept; this has led some to say that painting, as a serious art form, is dead, although this has not deterred the majority of artists from continuing to practise it either as whole or part of their work. Indigenouism is also considered as Modern and contemporary Art in early 20th Century.




Ceramic art is art made from ceramic materials (including clay), which may take forms such as pottery, tile, figurines, sculpture, and tableware. While some ceramic products are considered fine art, some are considered to be decorative, industrial, or applied art objects. Ceramics may also be considered artefacts in archaeology.Ceramic art can be made by one person or by a group of people. In a pottery or ceramic factory, a group of people design, manufacture, and decorate the pottery. Products from a pottery are sometimes referred to as "art pottery." In a one-person pottery studio, ceramists or potters produce studio pottery. In modern ceramic engineering usage, "ceramics" is the art and science of making objects from inorganic, non-metallic materials by the action of heat. It excludes glass and mosaic made from glass tesserae.




Photography as an art form refers to photographs that are created in accordance with the creative vision of the photographer. Art photography stands in contrast to photojournalism, which provides a visual account for news events, and commercial photography, the primary focus of which is to advertise products or services.




Architecture is the art and science of designing buildings and structures. The word architecture comes from the Greek arkhitekton, "master builder, director of works," from αρχι- (arkhi) "chief" + τεκτων (tekton) "builder, carpenter".
A wider definition would include the design of the built environment, from the macrolevel of town planning, urban design, and landscape architecture to the microlevel of creating furniture. Architectural design usually must address both feasibility and cost for the builder, as well as function and aesthetics for the user.

In modern usage, architecture is the art and discipline of creating, or inferring an implied or apparent plan of, a complex object or system. The term can be used to connote the implied architecture of abstract things such as music or mathematics, the apparent architecture of natural things, such as geological formations or the structure of biological cells, or explicitly planned architectures of human-made things such as software, computers, enterprises, and databases, in addition to buildings. In every usage, an architecture may be seen as a subjective mapping from a human perspective (that of the user in the case of abstract or physical artefacts) to the elements or components of some kind of structure or system, which preserves the relationships among the elements or components.
Planned architecture manipulates space, volume, texture, light, shadow, or abstract elements in order to achieve pleasing aesthetics. This distinguishes it from applied science or engineering, which usually concentrate more on the functional and feasibility aspects of the design of constructions or structures.
In the field of building architecture, the skills demanded of an architect range from the more complex, such as for a hospital or a stadium, to the apparently simpler, such as planning residential houses. Many architectural works may be seen also as cultural and political symbols, and/or works of art. The role of the architect, though changing, has been central to the successful (and sometimes less than successful) design and implementation of pleasingly built environments in which people live.




Sculpture is the branch of the visual arts that operates in three dimensions. It is one of the plastic arts. Durable sculptural processes originally used carving (the removal of material) and modelling (the addition of material, as clay), in stone, metal, ceramics, wood and other materials; but since modernism, shifts in sculptural process led to an almost complete freedom of materials and process. A wide variety of materials may be worked by removal such as carving, assembled by welding or modelling, or moulded, or cast.




Conceptual art is art in which the concept(s) or idea(s) involved in the work takes precedence over traditional aesthetic and material concerns. The inception of the term in the 1960s referred to a strict and focused practice of idea-based art that often defied traditional visual criteria associated with the visual arts in its presentation as text. Through its association with the Young British Artists and the Turner Prize during the 1990s, its popular usage, particularly in the UK, developed as a synonym for all contemporary art that does not practise the traditional skills of painting and sculpture.




Literature is literally "acquaintance with letters" as in the first sense given in the Oxford English Dictionary. The noun "literature" comes from the Latin word littera meaning "an individual written character (letter)." The term has generally come to identify a collection of writings, which in Western culture are mainly prose (both fiction and non-fiction), drama and poetry. In much, if not all of the world, the artistic linguistic expression can be oral as well, and include such genres as epic, legend, myth, ballad, other forms of oral poetry, and as folktale.
Comics, the combination of drawings or other visual arts with narrating literature, are often called the "ninth art" (le neuvième art) in Francophone scholarship.




Performing arts comprise dance, music, theatre, opera, mime, and other art forms in which a human performance is the principal product. Performing arts are distinguished by this performance element in contrast with disciplines such as visual and literary arts where the product is an object that does not require a performance to be observed and experienced. Each discipline in the performing arts is temporal in nature, meaning the product is performed over a period of time. Products are broadly categorized as being either repeatable (for example, by script or score) or improvised for each performance.
Artists who participate in these arts in front of an audience are called performers, including actors, magicians, comedians, dancers, musicians, and singers. Performing arts are also supported by the services of other artists or essential workers, such as songwriting and stagecraft. Performers often adapt their appearance with tools such as costume and stage makeup.




Music is an art form whose medium is sound. Common elements of music are pitch (which governs melody and harmony), rhythm (and its associated concepts tempo, metre, and articulation), dynamics, and the sonic qualities of timbre and texture. The creation, performance, significance, and even the definition of music vary according to culture and social context. Music ranges from strictly organized compositions (and their reproduction in performance) through improvisational music to aleatoric pieces. Music can be divided into genres and subgenres, although the dividing lines and relationships between music genres are often subtle, sometimes open to individual interpretation, and occasionally controversial. Within "the arts," music may be classified as a performing art, a fine art, and auditory art.




Theatre or theater (from Greek theatron (θέατρον); from theasthai, "behold") is the branch of the performing arts concerned with acting out stories in front of an audience using combinations of speech, gesture, music, dance, sound and spectacle – indeed, any one or more elements of the other performing arts. In addition to the standard narrative dialogue style, theatre takes such forms as opera, ballet, mime, kabuki, classical Indian dance, Chinese opera and mummers' plays.




Dance (from Old French dancier, of unknown origin) generally refers to human movement either used as a form of expression or presented in a social, spiritual or performance setting. Dance is also used to describe methods of non-verbal communication (see body language) between humans or animals (bee dance, mating dance), motion in inanimate objects (the leaves danced in the wind), and certain musical forms or genres.
Choreography is the art of making dances, and the person who does this is called a choreographer. People danced to relieve stress.
Definitions of what constitutes dance are dependent on social, cultural, aesthetic, artistic and moral constraints and range from functional movement (such as Folk dance) to codified, virtuoso techniques such as ballet. In sports, gymnastics, figure skating and synchronized swimming are dance disciplines while Martial arts "kata" are often compared to dances.




Areas exist in which artistic works incorporate multiple artistic fields, such as film, opera and performance art. While opera is often categorized in the performing arts of music, the word itself is Italian for "works," because opera combines several artistic disciplines in a singular artistic experience. In a typical traditional opera, the entire work utilizes the following: the sets (visual arts), costumes (fashion), acting (dramatic performing arts), the libretto, or the words/story (literature), and singers and an orchestra (music). The composer Richard Wagner recognized the fusion of so many disciplines into a single work of opera, exemplified by his cycle Der Ring des Nibelungen ("The Ring of the Nibelung"). He did not use the term opera for his works, but instead Gesamtkunstwerk ("synthesis of the arts"), sometimes referred to as "Music Drama" in English, emphasizing the literary and theatrical components which were as important as the music. Classical ballet is another form which emerged in the 19th century in which orchestral music is combined with dance.
Other works in the late 19th, 20th and 21st centuries have fused other disciplines in unique and creative ways, such as performance art. Performance art is a performance over time which combines any number of instruments, objects, and art within a predefined or less well-defined structure, some of which can be improvised. Performance art may be scripted, unscripted, random or carefully organized; even audience participation may occur.John Cage is regarded by many as a performance artist rather than a composer, although he preferred the latter term. He did not compose for traditional ensembles. Cage's composition Living Room Music composed in 1940 is a "quartet" for unspecified instruments, really non-melodic objects, which can be found in a living room of a typical house, hence the title.




A debate exists in the fine arts and video game cultures over whether video games can be counted as an art form. Game designer Hideo Kojima professes that video games are a type of service, not an art form, because they are meant to entertain and attempt to entertain as many people as possible, rather than being a single artistic voice (despite Kojima himself being considered a gaming auteur, and the mixed opinions his games typically receive). However, he acknowledged that since video games are made up of artistic elements (for example, the visuals), game designers could be considered museum curators - not creating artistic pieces, but arranging them in a way that displays their artistry and sells tickets.
In May 2011, the National Endowment of the Arts included video games in its redefinition of what is considered a "work of art" when applying of a grant. In 2012, the Smithsonian American Art Museum presented an exhibit, The Art of the Video Game. Reviews of the exhibit were mixed, including questioning whether video games belong in an art museum.




Gastronomy is the study of the relationship between culture and food. It is often thought erroneously that the term gastronomy refers exclusively to the art of cooking (see culinary art), but this is only a small part of this discipline; it cannot always be said that a cook is also a gourmet. Gastronomy studies various cultural components with food as its central axis. Thus, it is related to the Fine Arts and Social Sciences, and even to the Natural Sciences in terms of human nutritious activity and digestive function.



Architecture criticism
Visual art criticism
Dance criticism
Film criticism
Music criticism
Television criticism
Theatre criticism




Culinary art
Fine art
Martial arts
Art in odd places
Arts education






"The Art of Video Games". SI.edu. Smithsonian American Art Museum. Retrieved 7 March 2015. 
Barron, Christina (29 April 2012). "Museum exhibit asks: Is it art if you push 'start'?". The Washington Post. Retrieved 12 February 2013. 
"Conceptual art". Tate Glossary. Retrieved 7 March 2015. 
Feynman, Richard (1985). QED: The Strange Theory of Light and Matter. Princeton University Press. ISBN 0691024170. 
"FY 2012 Arts in Media Guidelines". Endow.gov. National Endowment for the Arts. Archived from the original on 13 February 2012. Retrieved 7 March 2015. 
Gibson, Ellie (24 January 2006). "Games aren't art, says Kojima". Eurogamer. Gamer Network. Retrieved 7 March 2015. 
Kennicott, Philip (18 March 2012). "The Art of Video Games". The Washington Post. Retrieved 12 February 2013. 
Levinson, Jerrold. "Performing Arts". The Oxford Companion to Philosophy. Oxford University Press. doi:10.1093/acref/9780199264797.001.0001. ISBN 9780199264797. Retrieved 7 March 2015. 




This article displayed as a mindmap, at wikimindmap.com
Cowan, Tyler (2008). "Arts". In David R. Henderson (ed.). Concise Encyclopedia of Economics (2nd ed.). Indianapolis: Library of Economics and Liberty. ISBN 978-0865976658. OCLC 237794267.  – A look at how general economic principles govern the arts.Social history, often called the new social history, is a field of history that looks at the lived experience of the past. In its "golden age" it was a major growth field in the 1960s and 1970s among scholars, and still is well represented in history departments in Britain, Canada, France, Germany, and the United States. In the two decades from 1975 to 1995, the proportion of professors of history in American universities identifying with social history rose from 31% to 41%, while the proportion of political historians fell from 40% to 30%. In the history departments of British and Irish universities in 2014, of the 3410 faculty members reporting, 878 (26%) identified themselves with social history while political history came next with 841 (25%).



The older social history (before 1960) included numerous topics that were not part of the mainstream historiography of political, military, diplomatic and constitutional history. It was a hodgepodge without a central theme, and it often included political movements, like Populism, that were "social" in the sense of being outside the elite system. People's history was sometimes so Marxist that non-Marxists were alienated by it. Social history was contrasted with political history, intellectual history and the history of great men. English historian G. M. Trevelyan saw it as the bridging point between economic and political history, reflecting that, "Without social history, economic history is barren and political history unintelligible." While the field has often been viewed negatively as history with the politics left out, it has also been defended as "history with the people put back in."



The "new social history" exploded on the scene in the 1960s, quickly becoming one of the dominant styles of historiography in the U.S., Britain and Canada. The French version, promulgated by the Annales School, was very well organized and dominated French historiography, and influenced much of Europe and Latin America. Jürgen Kocka finds two meanings to "social history." At the simplest level, it was the subdivision of historiography that focused on social structures and processes. In this regard it stood in contrast to political or economic history. The second meaning was broader, and the Germans called it "Gesellschaftsgeschichte." It is the history of an entire society from a social-historical viewpoint.
In Germany the "Gesellschaftsgeschichte" movement introduced a vast range of topics, as Kocka, a leader of the Bielefeld School recalls:
In the 1960s and 1970s, "social history" caught the imagination of a young generation of historians. It became a central concept -- and a rallying point -- of historiographic revisionism. It meant many things at the same time. It gave priority to the study of particular kinds of phenomena, such as classes and movements, urbanization and industrialization, family and education, work and leisure, mobility, inequality, conflicts and revolutions. It stressed structures and processes over actors and events. It emphasized analytical approaches close to the social sciences rather than by the traditional methods of historical hermeneutics. Frequently social historians sympathized with the causes (as they saw them) of the little people, of the underdog, of popular movements, or of the working class. Social history was both demanded and rejected as a vigorous revisionist alternative to the more established ways of historiography, in which the reconstruction of politics and ideas, the history of events and hermeneutic methods traditionally dominated.
Americanist Paul E. Johnson recalls the heady early promise of the movement in the late 1960s:
The New Social History reached UCLA at about that time, and I was trained as a quantitative social science historian. I learned that "literary" evidence and the kinds of history that could be written from it were inherently elitist and untrustworthy. Our cousins, the Annalistes, talked of ignoring heroes and events and reconstructing the more constitutive and enduring "background" of history. Such history could be made only with quantifiable sources. The result would be a "History from the Bottom Up" that ultimately engulfed traditional history and, somehow, helped to make a Better World. Much of this was acted out with mad-scientist bravado. One well-known quantifier said that anyone who did not know statistics at least through multiple regression should not hold a job in a history department. My own advisor told us that he wanted history to become "a predictive social science." I never went that far. I was drawn to the new social history by its democratic inclusiveness as much as by its system and precision. I wanted to write the history of ordinary people—to historicize them, put them into the social structures and long-term trends that shaped their lives, and at the same time resurrect what they said and did. In the late 1960s, quantitative social history looked like the best way to do that.
The Social Science History Association was formed in 1976 to bring together scholars from numerous disciplines interested in social history. It is still active and publishes Social Science History quarterly. The field is also the specialty of the Journal of Social History, edited since 1967 by Peter Stearns It covers such topics as gender relations; race in American history; the history of personal relationships; consumerism; sexuality; the social history of politics; crime and punishment, and history of the senses. Most of the major historical journals have coverage as well.
However, after 1990 social history was increasingly challenged by cultural history, which emphasizes language and the importance of beliefs and assumptions and their causal role in group behavior.







The study of the lives of ordinary people was revolutionized in the 1960s by the introduction of sophisticated quantitative and demographic methods, often using individual data from the census and from local registers of births, marriages, deaths and taxes, as well as theoretical models from sociology such as social mobility. H-DEMOG is a daily email discussion group that covers the field broadly.
Historical demography is the study of population history and demographic processes, usually using census or similar statistical data. It became an important specialty inside social history, with strong connections with the larger field of demography, as in the study of the Demographic Transition.



Black history or African-American history studies African Americans and Africans in American history. The Association for the Study of African American Life and History was founded by Carter G. Woodson in 1915 and has 2500 members and publishes the Journal of African American History, formerly the Journal of Negro History. Since 1926 it has sponsored Black History Month every February.



Ethnic history is especially important in the U.S. and Canada, where major encyclopedias helped define the field. It covers the history of ethnic groups (usually not including blacks or Native Americans). Typical approaches include critical ethnic studies; comparative ethnic studies; critical race studies; Asian-American, and Latino/a or Chicano/a studies. In recent years Chicano/Chicana studies has become important as the Hispanic population has become the largest minority in the U.S.
The Immigration and Ethnic History Society was formed in 1976 and publishes a journal for libraries and its 829 members.
The American Conference for Irish Studies, founded in 1960, has 1,700 members and has occasional publications but no journal.
The American Italian Historical Association was founded in 1966 and has 400 members; it does not publish a journal 
The American Jewish Historical Society is the oldest ethnic society, founded in 1892; it has 3,300 members and publishes American Jewish History
The Polish American Historical Association was founded in 1942, and publishes a newsletter and Polish American Studies, an interdisciplinary, refereed scholarly journal twice each year.
H-ETHNIC is a daily discussion list founded in 1993 with 1400 members; it covers topics of ethnicity and migration globally.




Labor history, deals with labor unions and the social history of workers. See for example Labor history of the United States The Study Group on International Labor and Working-Class History was established: 1971 and has a membership of 1000. It publishes International Labor and Working-Class History. H-LABOR is a daily email-based discussion group formed in 1993 that reaches over a thousand scholars and advanced students. the Labor and Working-Class History Association formed in 1988 and publishes Labor: Studies in Working-Class History of the Americas.
Kirk (2010) surveys labour historiography in Britain since the formation of the Society for the Study of Labour History in 1960. He reports that labour history has been mostly pragmatic, eclectic and empirical; it has played an important role in historiographical debates, such as those revolving around history from below, institutionalism versus the social history of labour, class, populism, gender, language, postmodernism and the turn to politics. Kirk rejects suggestions that the field is declining, and stresses its innovation, modification and renewal. Kirk also detects a move into conservative insularity and academicism. He recommends a more extensive and critical engagement with the kinds of comparative, transnational and global concerns increasingly popular among labour historians elsewhere, and calls for a revival of public and political interest in the topics. Meanwhile, Navickas, (2011) examines recent scholarship including the histories of collective action, environment and human ecology, and gender issues, with a focus on work by James Epstein, Malcolm Chase, and Peter Jones.



Women's history exploded into prominence in the 1970s, and is now well represented in every geographical topic; increasingly it includes gender history. Social history uses the approach of women's history to understand the experiences of ordinary women, as opposed to "Great Women," in the past. Feminist women's historians have critiqued early studies of social history for being too focused on the male experience.



Gender history focuses on the categories, discourses and experiences of femininity and masculinity as they develop over time. Gender history gained prominence after it was conceptualized by Joan W. Scott in her article "Gender: A Useful Category of Historical Analysis." Many social historians use Scott's concept of "perceived differences" to study how gender relations in the past have unfolded and continue to unfold. In keeping with the cultural turn, many social historians are also gender historians who study how discourses interact with everyday experiences.




The History of the family emerged as a separate field in the 1970s, with close ties to anthropology and sociology. The trend was especially pronounced in the U.S. and Canada. It emphasizes on demographic patterns, and public policy. It is quite separate from Genealogy, though often drawing on the same primary sources such as censuses and family records. An influential pioneering study was Women, Work, and Family (1978), by Louise A. Tilly and Joan W. Scott. It broke new ground with their broad interpretive framework and emphasis on the variable factors shaping women's place in the family and economy in France and England. It considered the interaction of production and reproduction in analysis of women's wage labor and thus helped to bring together labor and family history. Much work has been done on the dichotomy in women's lives between the private sphere and the public. For a recent worldwide overview covering 7000 years see Maynes and Waltner (2012). For comprehensive coverage of the American case, see Marilyn Coleman and Lawrence Ganong, eds. The Social History of the American Family: An Encyclopedia (4 vol, 2014).
The history of childhood is a growing subfield.




For much of the 20th century, the dominant American historiography, as exemplified by Ellwood Patterson Cubberley (1868-1941) at Stanford, emphasized the rise of American education as a powerful force for literacy, democracy, and equal opportunity, and a firm basis for higher education and advanced research institutions. It was a story of enlightenment and modernization triumphing over ignorance, cost-cutting, and narrow traditionalism whereby parents tried to block their children's intellectual access to the wider world. Teachers dedicated to the public interest, reformers with a wide vision, and public support from the civic-minded community were the heroes. The textbooks help inspire students to become public schools teachers and thereby fulfill their own civic mission.
The crisis came in the 1960s, when a new generation of New Left scholars and students rejected the traditional celebratory accounts, and identified the educational system as the villain for many of America's weaknesses, failures, and crimes. Michael Katz (1939-2014) states they:
tried to explain the origins of the Vietnam War; the persistence of racism and segregation; the distribution Of power among gender and classes; intractable poverty and the decay of cities; and the failure of social institutions and policies designed to deal with mental illness, crime, delinquency, and education.
The old guard fought back and bitter historiographical contests, with the younger students and scholars largely promoting the proposition that schools were not the solution To America's ills, they were In part the cause of Americans problems. The fierce battles of the 1960s died out by the 1990s, but enrollment in education history courses and never recovered.
By the 1980s, compromise had been worked out, with all sides focusing on the heavily bureaucratic nature of the American public schooling.
In recent years most histories of education deal with institutions or focus on the ideas histories of major reformers, but a new social history has recently emerged, focused on who were the students in terms of social background and social mobility. In the U.S. attention has often focused on minority and ethnic students. In Britain, Raftery et al. (2007) looks at the historiography on social change and education in Ireland, Scotland, and Wales, with particular reference to 19th-century schooling. They developed distinctive systems of schooling in the 19th century that reflected not only their relationship to England but also significant contemporaneous economic and social change. This article seeks to create a basis for comparative work by identifying research that has treated this period, offering brief analytical commentaries on some key works, discussing developments in educational historiography, and pointing to lacunae in research.
Historians have recently looked at the relationship between schooling and urban growth by studying educational institutions as agents in class formation, relating urban schooling to changes in the shape of cities, linking urbanization with social reform movements, and examining the material conditions affecting child life and the relationship between schools and other agencies that socialize the young.
The most economics-minded historians have sought to relate education to changes in the quality of labor, productivity and economic growth, and rates of return on investment in education. A major recent exemplar is Claudia Goldin and Lawrence F. Katz, The Race between Education and Technology (2009), on the social and economic history of 20th-century American schooling.




The "new urban history" emerged in the 1950s in Britain and in the 1960s in the U.S. It looked at the "city as process" and, often using quantitative methods, to learn more about the inarticulate masses in the cities, as opposed to the mayors and elites. A major early study was Stephan Thernstrom's Poverty and Progress: Social Mobility in a Nineteenth Century City (1964), which used census records to study Newburyport, Massachusetts, 1850-1880. A seminal, landmark book, it sparked interest in the 1960s and 1970s in quantitative methods, census sources, "bottom-up" history, and the measurement of upward social mobility by different ethnic groups. Other exemplars of the new urban history included Kathleen Conzen, Immigrant Milwaukee, 1836-1860 (1976); Alan Dawley, Class and Community: The Industrial Revolution in Lynn (1975; 2nd ed. 2000); Michael B. Katz, The People of Hamilton, Canada West (1976); Eric H. Monkkonen, The Dangerous Class: Crime and Poverty in Columbus Ohio 1860-1865 (1975); and Michael P. Weber, Social Change in an Industrial Town: Patterns of Progress in Warren, Pennsylvania, From Civil War to World War I. (1976).
Representative comparative studies include Leonardo Benevolo, The European City (1993); Christopher R. Friedrichs, The Early Modern City, 1450-1750 (1995), and James L. McClain, John M. Merriman, and Ugawa Kaoru. eds. Edo and Paris (1994) (Edo was the old name for Tokyo).
There were no overarching social history theories that emerged developed to explain urban development. Inspiration from urban geography and sociology, as well as a concern with workers (as opposed to labor union leaders), families, ethnic groups, racial segregation, and women's roles have proven useful. Historians now view the contending groups within the city as "agents" who shape the direction of urbanization. The subfield has flourished in Australia—where most people live in cities.




Agricultural History handles the economic and technological dimensions, while Rural history handles the social dimension. Burchardt (2007) evaluates the state of modern English rural history and identifies an "orthodox" school, focused on the economic history of agriculture. This historiography has made impressive progress in quantifying and explaining the output and productivity achievements of English farming since the "agricultural revolution." The celebratory style of the orthodox school was challenged by a dissident tradition emphasizing the social costs of agricultural progress, notably enclosure, which forced poor tenant farmers off the land. Recently, a new school, associated with the journal Rural History, has broken away from this narrative of agricultural change, elaborating a wider social history. The work of Alun Howkins has been pivotal in the recent historiography, in relation to these three traditions. Howkins, like his precursors, is constrained by an increasingly anachronistic equation of the countryside with agriculture. Geographers and sociologists have developed a concept of a "post-productivist" countryside, dominated by consumption and representation that may have something to offer historians, in conjunction with the well-established historiography of the "rural idyll." Most rural history has focused on the American South—overwhelmingly rural until the 1950s—but there is a "new rural history" of the North as well. Instead of becoming agrarian capitalists, farmers held onto preindustrial capitalist values emphasizing family and community. Rural areas maintained population stability; kinship ties determined rural immigrant settlement and community structures; and the defeminization of farm work encouraged the rural version of the "women's sphere." These findings strongly contrast with those in the old frontier history as well as those found in the new urban history.




The historiography of religion focuses mostly on theology and church organization and development. Recently the study of the social history or religious behavior and belief has become important.







Social history has dominated French historiography since the 1920s, thanks to the central role of the Annales School. Its journal Annales focuses attention on the synthesizing of historical patterns identified from social, economic, and cultural history, statistics, medical reports, family studies, and even psychoanalysis.




Social history developed within West German historiography during the 1950s-60s as the successor to the national history discredited by National Socialism. The German brand of "history of society" - Gesellschaftsgeschichte - has been known from its beginning in the 1960s for its application of sociological and political modernization theories to German history. Modernization theory was presented by Hans-Ulrich Wehler (1931-2014) and his Bielefeld School as the way to transform "traditional" German history, that is, national political history, centered on a few "great men," into an integrated and comparative history of German society encompassing societal structures outside politics. Wehler drew upon the modernization theory of Max Weber, with concepts also from Karl Marx, Otto Hintze, Gustav Schmoller, Werner Sombart and Thorstein Veblen.
In the 1970s and early 1980s German historians of society, led by Wehler and Jürgen Kocka at the "Bielefeld school" gained dominance in Germany by applying both modernization theories and social science methods. From the 1980s, however, they were increasingly criticized by proponents of the "cultural turn" for not incorporating culture in the history of society, for reducing politics to society, and for reducing individuals to structures. Historians of society inverted the traditional positions they criticized (on the model of Marx's inversion of Hegel). As a result, the problems pertaining to the positions criticized were not resolved but only turned on their heads. The traditional focus on individuals was inverted into a modern focus on structures, the traditional focus on culture was inverted into a modern focus on structures, and traditional emphatic understanding was inverted into modern causal explanation.



Before World War II, political history was in decline and an effort was made to introduce social history in the style of the French Annales School. After the war only Marxist interpretations were allowed. With the end of Communism in Hungary in 1989. Marxist historiography collapsed and social history came into its own, especially the study of the demographic patterns of the early modern period. Research priorities have shifted toward urban history and the conditions of everyday life.



When Communism ended in 1991, large parts of the Soviet archives were opened. The historians' data base leapt from a limited range of sources to a vast array of records created by modern bureaucracies. Social history flourished. The old Marxist historiography collapsed overnight.




Social history had a "golden age" in Canada in the 1970s, and continues to flourish among scholars. Its strengths include demography, women, labour, and urban studies.



While the study of elites and political institutions has produced a vast body of scholarship, the impact after 1960 of social historians has shifted emphasis onto the politics of ordinary people—especially voters and collective movements. Political historians responded with the "new political history," which has shifted attention to political cultures. Some scholars have recently applied a cultural approach to political history. Some political historians complain that social historians are likely to put too much stress on the dimensions of class, gender and race, reflecting a leftist political agenda that assumes outsiders in politics are more interesting than the actual decision makers.
Social history, with its leftist political origins, initially sought to link state power to everyday experience in the 1960s. Yet by the 1970s, social historians increasingly excluded analyses of state power from its focus. Social historians have recently engaged with political history through studies of the relationships between state formation, power and everyday life with the theoretical tools of cultural hegemony and governmentality.



List of history journals
Annales School
History of sociology
Living history and open-air museums



Marc Bloch (1886–1944). Medieval, Annales School
Asa Briggs, Baron Briggs, British
Martin Broszat (1926–1989), Germany
Merle Curti (1897-1997) American
Natalie Zemon Davis, (b. 1928) France
Herbert Gutman (1928-1985), American black and labor history
Eugene D. Genovese (1930-2012), American slavery
Oscar Handlin (1915-2011), American ethnic
Emmanuel Le Roy Ladurie (b. 1929), leader of Annales School, France
Ram Sharan Sharma (1919-2011), India
Stephan Thernstrom (b. 1934), ethnic American; social mobility
Charles Tilly (1929 – 2008), European; theory
Louise A. Tilly (born 1930, Europe; women and family
Eric Hobsbawm (1917-2012), labor history, social movements and resistances
E. P. Thompson (1924–1993), British labour
Hans-Ulrich Wehler (1931-2014), 19th-century Germany, Bielefeld School



Adas, Michael. "Social History and the Revolution in African and Asian Historiography," Journal of Social History 19 (1985): 335-378.
Anderson, Michael. Approaches to the History of the Western Family 1500-1914 (1995) 104pp excerpt and text search
Cabrera, Miguel A. Postsocial History: An Introduction. (2004). 163 pp.
Cayton, Mary Kupiec, Elliott J. Gorn, and Peter W. Williams, eds. Encyclopedia of American Social History (3 vol 1993) 2653pp; long articles pages by leading scholars; see v I: Part II, Methods and Contexts, pp 235–434
Cross, Michael S. "Social History," Canadian Encyclopedia (2008) online
Cross, Michael S. and Kealey, Gregory S., eds. Readings in Canadian Social History (5 vol 1984). 243 pp.
Dewald, Jonathan. Lost Worlds: The Emergence of French Social History, 1815-1970. (2006). 241 pp.
Eley, Geoff. A Crooked Line: From Cultural History to the History of Society. (2005). 301 pp.
Fairburn, Miles. Social History: Problems, Strategies and Methods. (1999). 325 pp.
Fass, Paula, ed. Encyclopedia of Children and Childhood: In History and Society, (3 vols. 2003).
Fletcher, Roger. "Recent Developments in West German Historiography: the Bielefeld School and its Critics." German Studies Review 1984 7(3): 451-480. ISSN 0149-7952 Fulltext: in Jstor
Hareven, Tamara K. "The History of the Family and the Complexity of Social Change," American Historical Review, Feb 1991, Vol. 96 Issue 1, pp 95–124 in JSTOR
Henretta, James. "Social History as Lived and Written," American Historical Review 84 (1979): 1293-1323 in JSTOR
Kanner, Barbara. Women in English Social History, 1800-1914: A Guide to Research (2 vol 1988-1990). 871 pp.
Lloyd, Christopher. Explanation in Social History. (1986). 375 pp.
Lorenz, Chris. "'Won't You Tell Me, Where Have All the Good Times Gone'? On the Advantages and Disadvantages of Modernization Theory for History." Rethinking History 2006 10(2): 171-200. ISSN 1364-2529 Fulltext: Ebsco
Mintz, Steven. Huck's Raft: A History of American Childhood (2006). excerpt and text search
Mintz, Steven and Susan Kellogg. Domestic Revolutions: A Social History Of American Family Life (1989) excerpt and text search
Mosley, Stephen. "Common Ground: Integrating Social and Environmental History," Journal of Social History, Volume 39, Number 3, Spring 2006, pp. 915–933, relations with Environmental History, in Project MUSE
Palmer, Bryan D., and Todd McCallum, "Working-Class History" Canadian Encyclopedia (2008)
Pomeranz, Kenneth. "Social History and World History: from Daily Life to Patterns of Change." Journal of World History 2007 18(1): 69-98. ISSN 1045-6007 Fulltext: in History Cooperative and Project Muse
Stearns, Peter N. "Social History Today ... And Tomorrow," Journal of Social History 10 (1976): 129-155.
Stearns, Peter N. "Social History Present and Future." Journal of Social History. Volume: 37. Issue: 1. (2003). pp 9+. online edition
Stearns, Peter, ed. Encyclopedia of Social History (1994) 856 pp.
Stearns, Peter, ed. Encyclopedia of European Social History from 1350 to 2000 (5 vol 2000), 209 essays by leading scholars in 3000 pp.
Sutherland, Neil. "Childhood, History of," Canadian Encyclopedia (2008)
Hobsbawm, Eric. The Age of Revolution: Europe 1789-1848.
Skocpol, Theda, and Daniel Chirot, eds. Vision and method in historical sociology (1984).
Thompson, E. P. The Essential E. P. Thompson. (2001). 512 pp. highly influential British historian of the working class
Thompson, F. M. L., ed. The Cambridge Social History of Britain, 1750-1950." Vol. 1: Regions and Communities. Vol. 2: People and Their Environment; Vol. 3: Social Agencies and Institutions. (1990). 492 pp.
Tilly, Charles. "The Old New Social History and the New Old Social History," Review 7 (3), Winter 1984: 363-406 (online)
Tilly, Charles. Big Structures, Large Processes, Huge Comparisons (1984).
Timmins, Geoffrey. "The Future of Learning and Teaching in Social History: the Research Approach and Employability." Journal of Social History 2006 39(3): 829-842. ISSN 0022-4529 Fulltext: History Cooperative and Project Muse
Wilson, Adrian, ed. Rethinking Social History: English Society, 1570-1920 and Its Interpretation. (1993). 342 pp.
Zunz, Olivier, ed. Reliving the Past: The Worlds of Social History, (1985) online edition



Binder, Frederick M. and David M. Reimers, eds. The Way We Lived: Essays and Documents in American Social History. (2000). 313 pp.






International Institute of Social History
American Social History Project
Social History Society (UK)
Amsab-Institute of Social History (Belgium)
Victorian-era social history
Society for the social history of medicine
History online
International Association of Labour History Institutions
American Social History Online (19th and 20th digital resources)
StoryCorps: National Social History Project Records Ordinary People Telling Their Stories - video by Democracy Now!Astronomy is a natural science that studies celestial objects and phenomena. It applies mathematics, physics, and chemistry, in an effort to explain the origin of those objects and phenomena and their evolution. Objects of interest include planets, moons, stars, galaxies, and comets; while the phenomena include supernovae explosions, gamma ray bursts, and cosmic microwave background radiation. More generally, all astronomical phenomena that originate outside Earth's atmosphere are within the purview of astronomy. A related but distinct subject, physical cosmology, is concerned with the study of the Universe as a whole.
Astronomy is the oldest of the natural sciences. The early civilizations in recorded history, such as the Babylonians, Greeks, Indians, Egyptians, Nubians, Iranians, Chinese, and Maya performed methodical observations of the night sky. Historically, astronomy has included disciplines as diverse as astrometry, celestial navigation, observational astronomy and the making of calendars, but professional astronomy is now often considered to be synonymous with astrophysics.
During the 20th century, the field of professional astronomy split into observational and theoretical branches. Observational astronomy is focused on acquiring data from observations of astronomical objects, which is then analyzed using basic principles of physics. Theoretical astronomy is oriented toward the development of computer or analytical models to describe astronomical objects and phenomena. The two fields complement each other, with theoretical astronomy seeking to explain the observational results and observations being used to confirm theoretical results.
Astronomy is one of the few sciences where amateurs can still play an active role, especially in the discovery and observation of transient phenomena. Amateur astronomers have made and contributed to many important astronomical discoveries, such as finding new comets.




Astronomy (from the Greek ἀστρονομία from ἄστρον astron, "star" and -νομία -nomia from νόμος nomos, "law" or "culture") means "law of the stars" (or "culture of the stars" depending on the translation). Astronomy should not be confused with astrology, the belief system which claims that human affairs are correlated with the positions of celestial objects. Although the two fields share a common origin, they are now entirely distinct.



Generally, either the term "astronomy" or "astrophysics" may be used to refer to this subject. Based on strict dictionary definitions, "astronomy" refers to "the study of objects and matter outside the Earth's atmosphere and of their physical and chemical properties" and "astrophysics" refers to the branch of astronomy dealing with "the behavior, physical properties, and dynamic processes of celestial objects and phenomena". In some cases, as in the introduction of the introductory textbook The Physical Universe by Frank Shu, "astronomy" may be used to describe the qualitative study of the subject, whereas "astrophysics" is used to describe the physics-oriented version of the subject. However, since most modern astronomical research deals with subjects related to physics, modern astronomy could actually be called astrophysics. Few fields, such as astrometry, are purely astronomy rather than also astrophysics. Various departments in which scientists carry out research on this subject may use "astronomy" and "astrophysics," partly depending on whether the department is historically affiliated with a physics department, and many professional astronomers have physics rather than astronomy degrees. One of the leading scientific journals in the field is the European journal named Astronomy and Astrophysics. The leading American journals are The Astrophysical Journal and The Astronomical Journal.




In early times, astronomy only comprised the observation and predictions of the motions of objects visible to the naked eye. In some locations, early cultures assembled massive artifacts that possibly had some astronomical purpose. In addition to their ceremonial uses, these observatories could be employed to determine the seasons, an important factor in knowing when to plant crops, as well as in understanding the length of the year.
Before tools such as the telescope were invented, early study of the stars was conducted using the naked eye. As civilizations developed, most notably in Mesopotamia, Greece, Persia, India, China, Egypt, and Central America, astronomical observatories were assembled, and ideas on the nature of the Universe began to be explored. Most of early astronomy actually consisted of mapping the positions of the stars and planets, a science now referred to as astrometry. From these observations, early ideas about the motions of the planets were formed, and the nature of the Sun, Moon and the Earth in the Universe were explored philosophically. The Earth was believed to be the center of the Universe with the Sun, the Moon and the stars rotating around it. This is known as the geocentric model of the Universe, or the Ptolemaic system, named after Ptolemy.
A particularly important early development was the beginning of mathematical and scientific astronomy, which began among the Babylonians, who laid the foundations for the later astronomical traditions that developed in many other civilizations. The Babylonians discovered that lunar eclipses recurred in a repeating cycle known as a saros.

Following the Babylonians, significant advances in astronomy were made in ancient Greece and the Hellenistic world. Greek astronomy is characterized from the start by seeking a rational, physical explanation for celestial phenomena. In the 3rd century BC, Aristarchus of Samos estimated the size and distance of the Moon and Sun, and was the first to propose a heliocentric model of the solar system. In the 2nd century BC, Hipparchus discovered precession, calculated the size and distance of the Moon and invented the earliest known astronomical devices such as the astrolabe. Hipparchus also created a comprehensive catalog of 1020 stars, and most of the constellations of the northern hemisphere derive from Greek astronomy. The Antikythera mechanism (c. 150–80 BC) was an early analog computer designed to calculate the location of the Sun, Moon, and planets for a given date. Technological artifacts of similar complexity did not reappear until the 14th century, when mechanical astronomical clocks appeared in Europe.
During the Middle Ages, astronomy was mostly stagnant in medieval Europe, at least until the 13th century. However, astronomy flourished in the Islamic world and other parts of the world. This led to the emergence of the first astronomical observatories in the Muslim world by the early 9th century. In 964, the Andromeda Galaxy, the largest galaxy in the Local Group, was discovered by the Persian astronomer Azophi and first described in his Book of Fixed Stars. The SN 1006 supernova, the brightest apparent magnitude stellar event in recorded history, was observed by the Egyptian Arabic astronomer Ali ibn Ridwan and the Chinese astronomers in 1006. Some of the prominent Islamic (mostly Persian and Arab) astronomers who made significant contributions to the science include Al-Battani, Thebit, Azophi, Albumasar, Biruni, Arzachel, Al-Birjandi, and the astronomers of the Maragheh and Samarkand observatories. Astronomers during that time introduced many Arabic names now used for individual stars. It is also believed that the ruins at Great Zimbabwe and Timbuktu may have housed an astronomical observatory. Europeans had previously believed that there had been no astronomical observation in pre-colonial Middle Ages sub-Saharan Africa but modern discoveries show otherwise.
The Roman Catholic Church gave more financial and social support to the study of astronomy for over six centuries, from the recovery of ancient learning during the late Middle Ages into the Enlightenment, than any other, and, probably, all other, institutions. Among the Church's motives was finding the date for Easter.




During the Renaissance, Nicolaus Copernicus proposed a heliocentric model of the solar system. His work was defended, expanded upon, and corrected by Galileo Galilei and Johannes Kepler. Galileo used telescopes to enhance his observations.
Kepler was the first to devise a system that described correctly the details of the motion of the planets with the Sun at the center. However, Kepler did not succeed in formulating a theory behind the laws he wrote down. It was left to Newton's invention of celestial dynamics and his law of gravitation to finally explain the motions of the planets. Newton also developed the reflecting telescope.
The English astronomer John Flamsteed catalogued over 3000 stars. Further discoveries paralleled the improvements in the size and quality of the telescope. More extensive star catalogues were produced by Lacaille. The astronomer William Herschel made a detailed catalog of nebulosity and clusters, and in 1781 discovered the planet Uranus, the first new planet found. The distance to a star was first announced in 1838 when the parallax of 61 Cygni was measured by Friedrich Bessel.
During the 18–19th centuries, the study of the three body problem by Euler, Clairaut, and D'Alembert led to more accurate predictions about the motions of the Moon and planets. This work was further refined by Lagrange and Laplace, allowing the masses of the planets and moons to be estimated from their perturbations.
Significant advances in astronomy came about with the introduction of new technology, including the spectroscope and photography. Fraunhofer discovered about 600 bands in the spectrum of the Sun in 1814–15, which, in 1859, Kirchhoff ascribed to the presence of different elements. Stars were proven to be similar to the Earth's own Sun, but with a wide range of temperatures, masses, and sizes.
The existence of the Earth's galaxy, the Milky Way, as a separate group of stars, was only proved in the 20th century, along with the existence of "external" galaxies. The observed recession of those galaxies led to the discovery of the expansion of the Universe. Theoretical astronomy led to speculations on the existence of objects such as black holes and neutron stars, which have be used to explain such observed phenomena such as quasars, pulsars, blazars, and radio galaxies. Physical cosmology made huge advances during the 20th century, with the model of the Big Bang, which is heavily supported by evidence provided by cosmic microwave background radiation, Hubble's law, and the cosmological abundances of elements. Space telescopes have enabled measurements in parts of the electromagnetic spectrum normally blocked or blurred by the atmosphere. Recently, in February 2016, it was revealed that the LIGO project had detected evidence of gravitational waves, in September 2015.




Our main source of information about celestial bodies and other objects is visible light more generally electromagnetic radiation. Observational astronomy may be divided according to the observed region of the electromagnetic spectrum. Some parts of the spectrum can be observed from the Earth's surface, while other parts are only observable from either high altitudes or outside the Earth's atmosphere. Specific information on these subfields is given below.




Radio astronomy uses radiation outside the visible range with wavelengths greater than approximately one millimeter. Radio astronomy is different from most other forms of observational astronomy in that the observed radio waves can be treated as waves rather than as discrete photons. Hence, it is relatively easier to measure both the amplitude and phase of radio waves, whereas this is not as easily done at shorter wavelengths.
Although some radio waves are emitted directly by astronomical objects, a product of thermal emission, most of the radio emission that is observed is the result of synchrotron radiation, which is produced when electrons orbit magnetic fields. Additionally, a number of spectral lines produced by interstellar gas, notably the hydrogen spectral line at 21 cm, are observable at radio wavelengths.
A wide variety of objects are observable at radio wavelengths, including supernovae, interstellar gas, pulsars, and active galactic nuclei.




Infrared astronomy is founded on the detection and analysis of infrared radiation, wavelengths longer than red light and outside the range of our vision. The infrared spectrum is useful for studying objects that are too cold to radiate visible light, such as planets, circumstellar disks or nebulae whose light is blocked by dust. The longer wavelengths of infrared can penetrate clouds of dust that block visible light, allowing the observation of young stars embedded in molecular clouds and the cores of galaxies. Observations from the Wide-field Infrared Survey Explorer (WISE) have been particularly effective at unveiling numerous Galactic protostars and their host star clusters. With the exception of infrared wavelengths close to visible light, such radiation is heavily absorbed by the atmosphere, or masked, as the atmosphere itself produces significant infrared emission. Consequently, infrared observatories have to be located in high, dry places on Earth or in space. Some molecules radiate strongly in the infrared. This allows the study of the chemistry of space; more specifically it can detect water in comets.




Historically, optical astronomy, also called visible light astronomy, is the oldest form of astronomy. Images of observations were originally drawn by hand. In the late 19th century and most of the 20th century, images were made using photographic equipment. Modern images are made using digital detectors, particularly using charge-coupled devices (CCDs) and recorded on modern medium. Although visible light itself extends from approximately 4000 Å to 7000 Å (400 nm to 700 nm), that same equipment can be used to observe some near-ultraviolet and near-infrared radiation.




Ultraviolet astronomy employs ultraviolet wavelengths between approximately 100 and 3200 Å (10 to 320 nm). Light at those wavelengths are absorbed by the Earth's atmosphere, requiring observations at these wavelengths to be performed from the upper atmosphere or from space. Ultraviolet astronomy is best suited to the study of thermal radiation and spectral emission lines from hot blue stars (OB stars) that are very bright in this wave band. This includes the blue stars in other galaxies, which have been the targets of several ultraviolet surveys. Other objects commonly observed in ultraviolet light include planetary nebulae, supernova remnants, and active galactic nuclei. However, as ultraviolet light is easily absorbed by interstellar dust, an adjustment of ultraviolet measurements is necessary.




X-ray astronomy uses X-ray wavelengths. Typically, X-ray radiation is produced by synchrotron emission (the result of electrons orbiting magnetic field lines), thermal emission from thin gases above 107 (10 million) kelvins, and thermal emission from thick gases above 107 Kelvin. Since X-rays are absorbed by the Earth's atmosphere, all X-ray observations must be performed from high-altitude balloons, rockets, or X-ray astronomy satellites. Notable X-ray sources include X-ray binaries, pulsars, supernova remnants, elliptical galaxies, clusters of galaxies, and active galactic nuclei.




Gamma ray astronomy observes astronomical objects at the shortest wavelengths of the electromagnetic spectrum. Gamma rays may be observed directly by satellites such as the Compton Gamma Ray Observatory or by specialized telescopes called atmospheric Cherenkov telescopes. The Cherenkov telescopes do not detect the gamma rays directly but instead detect the flashes of visible light produced when gamma rays are absorbed by the Earth's atmosphere.
Most gamma-ray emitting sources are actually gamma-ray bursts, objects which only produce gamma radiation for a few milliseconds to thousands of seconds before fading away. Only 10% of gamma-ray sources are non-transient sources. These steady gamma-ray emitters include pulsars, neutron stars, and black hole candidates such as active galactic nuclei.



In addition to electromagnetic radiation, a few other events originating from great distances may be observed from the Earth.
In neutrino astronomy, astronomers use heavily shielded underground facilities such as SAGE, GALLEX, and Kamioka II/III for the detection of neutrinos. The vast majority of the neutrinos streaming through the Earth originate from the Sun, but 24 neutrinos were also detected from supernova 1987A. Cosmic rays, which consist of very high energy particles (atomic nuclei) that can decay or be absorbed when they enter the Earth's atmosphere, result in a cascade of secondary particles which can be detected by current observatories. Some future neutrino detectors may also be sensitive to the particles produced when cosmic rays hit the Earth's atmosphere.
Gravitational-wave astronomy is an emerging field of astronomy that employs gravitational-wave detectors to collect observational data about distant massive objects. A few observatories have been constructed, such as the Laser Interferometer Gravitational Observatory LIGO. LIGO made its first detection on 14 September 2015, observing gravitational waves from a binary black hole. A second gravitational wave was detected on 26 December 2015 and additional observations should continue but gravitational waves require extremely sensitive instruments.
The combination of observations made using electromagnetic radiation, neutrinos or gravitational waves and other complementary information, is known as multi-messenger astronomy.




One of the oldest fields in astronomy, and in all of science, is the measurement of the positions of celestial objects. Historically, accurate knowledge of the positions of the Sun, Moon, planets and stars has been essential in celestial navigation (the use of celestial objects to guide navigation) and in the making of calendars.
Careful measurement of the positions of the planets has led to a solid understanding of gravitational perturbations, and an ability to determine past and future positions of the planets with great accuracy, a field known as celestial mechanics. More recently the tracking of near-Earth objects will allow for predictions of close encounters or potential collisions of the Earth with those objects.
The measurement of stellar parallax of nearby stars provides a fundamental baseline in the cosmic distance ladder that is used to measure the scale of the Universe. Parallax measurements of nearby stars provide an absolute baseline for the properties of more distant stars, as their properties can be compared. Measurements of the radial velocity and proper motion motion of stars allows astronomers to plot the movement of these systems through the Milky Way galaxy. Astrometric results are the basis used to calculate the distribution of speculated dark matter in the galaxy.
During the 1990s, the measurement of the stellar wobble of nearby stars was used to detect large extrasolar planets orbiting those stars.




Theoretical astronomers use several tools including analytical models and computational numerical simulations; each has its particular advantages. Analytical models of a process are generally better for giving broader insight into the heart of what is going on. Numerical models reveal the existence of phenomena and effects otherwise unobserved.
Theorists in astronomy endeavor to create theoretical models and from the results predict observational consequences of those models. The observation of a phenomenon predicted by a model allows astronomers to select between several alternate or conflicting models as the one best able to describe the phenomena.
Theorists also try to generate or modify models to take into account new data. In the case of an inconsistency between the data and model's results, the general tendency is to try to make minimal modifications to the model so that it produces results that fit the data. In some cases, a large amount of inconsistent data over time may lead to total abandonment of a model.
Phenomena modeled by theoretical astronomers include: stellar dynamics and evolution; galaxy formation; large-scale distribution of matter in the Universe; origin of cosmic rays; general relativity and physical cosmology, including string cosmology and astroparticle physics. Astrophysical relativity serves as a tool to gauge the properties of large scale structures for which gravitation plays a significant role in physical phenomena investigated and as the basis for black hole (astro)physics and the study of gravitational waves.
Some widely accepted and studied theories and models in astronomy, now included in the Lambda-CDM model are the Big Bang, Cosmic inflation, dark matter, and fundamental theories of physics.
A few examples of this process:
Dark matter and dark energy are the current leading topics in astronomy, as their discovery and controversy originated during the study of the galaxies.







At a distance of about eight light-minutes, the most frequently studied star is the Sun, a typical main-sequence dwarf star of stellar class G2 V, and about 4.6 billion years (Gyr) old. The Sun is not considered a variable star, but it does undergo periodic changes in activity known as the sunspot cycle. This is an 11-year oscillation in sunspot number. Sunspots are regions of lower-than- average temperatures that are associated with intense magnetic activity.
The Sun has steadily increased in luminosity by 40% since it first became a main-sequence star. The Sun has also undergone periodic changes in luminosity that can have a significant impact on the Earth. The Maunder minimum, for example, is believed to have caused the Little Ice Age phenomenon during the Middle Ages.
The visible outer surface of the Sun is called the photosphere. Above this layer is a thin region known as the chromosphere. This is surrounded by a transition region of rapidly increasing temperatures, and finally by the super-heated corona.
At the center of the Sun is the core region, a volume of sufficient temperature and pressure for nuclear fusion to occur. Above the core is the radiation zone, where the plasma conveys the energy flux by means of radiation. Above that is the convection zone where the gas material transports energy primarily through physical displacement of the gas known as convection. It is believed that the movement of mass within the convection zone creates the magnetic activity that generates sunspots.
A solar wind of plasma particles constantly streams outward from the Sun until, at the outermost limit of the Solar System, it reaches the heliopause. As the solar wind passes the Earth, it interacts with the Earth's magnetic field (magnetosphere) and deflects the solar wind, but traps some creating the Van Allen radiation belts that envelop the Earth . The aurora are created when solar wind particles are guided by the magnetic flux lines into the Earth's polar regions where the lines the descend into the atmosphere.




Planetary science is the study of the assemblage of planets, moons, dwarf planets, comets, asteroids, and other bodies orbiting the Sun, as well as extrasolar planets. The Solar System has been relatively well-studied, initially through telescopes and then later by spacecraft. This has provided a good overall understanding of the formation and evolution of this planetary system, although many new discoveries are still being made.

The Solar System is subdivided into the inner planets, the asteroid belt, and the outer planets. The inner terrestrial planets consist of Mercury, Venus, Earth, and Mars. The outer gas giant planets are Jupiter, Saturn, Uranus, and Neptune. Beyond Neptune lies the Kuiper Belt, and finally the Oort Cloud, which may extend as far as a light-year.
The planets were formed 4.6 billion years ago in the protoplanetary disk that surrounded the early Sun. Through a process that included gravitational attraction, collision, and accretion, the disk formed clumps of matter that, with time, became protoplanets. The radiation pressure of the solar wind then expelled most of the unaccreted matter, and only those planets with sufficient mass retained their gaseous atmosphere. The planets continued to sweep up, or eject, the remaining matter during a period of intense bombardment, evidenced by the many impact craters on the Moon. During this period, some of the protoplanets may have collided and one such collision may have formed the Moon.
Once a planet reaches sufficient mass, the materials of different densities segregate within, during planetary differentiation. This process can form a stony or metallic core, surrounded by a mantle and an outer crust. The core may include solid and liquid regions, and some planetary cores generate their own magnetic field, which can protect their atmospheres from solar wind stripping.
A planet or moon's interior heat is produced from the collisions that created the body, by the decay of radioactive materials (e.g. uranium, thorium, and 26Al), or tidal heating caused by interactions with other bodies. Some planets and moons accumulate enough heat to drive geologic processes such as volcanism and tectonics. Those that accumulate or retain an atmosphere can also undergo surface erosion from wind or water. Smaller bodies, without tidal heating, cool more quickly; and their geological activity ceases with the exception of impact cratering.




The study of stars and stellar evolution is fundamental to our understanding of the Universe. The astrophysics of stars has been determined through observation and theoretical understanding; and from computer simulations of the interior. Star formation occurs in dense regions of dust and gas, known as giant molecular clouds. When destabilized, cloud fragments can collapse under the influence of gravity, to form a protostar. A sufficiently dense, and hot, core region will trigger nuclear fusion, thus creating a main-sequence star.
Almost all elements heavier than hydrogen and helium were created inside the cores of stars.
The characteristics of the resulting star depend primarily upon its starting mass. The more massive the star, the greater its luminosity, and the more rapidly it fuses its hydrogen fuel into helium in its core. Over time, this hydrogen fuel is completely converted into helium, and the star begins to evolve. The fusion of helium requires a higher core temperature. A star with a high enough core temperature will push its outer layers outward while increasing its core density. The resulting red giant formed by the expanding outer layers enjoys a brief life span, before the helium fuel in the core is in turn consumed. Very massive stars can also undergo a series of evolutionary phases, as they fuse increasingly heavier elements.
The final fate of the star depends on its mass, with stars of mass greater than about eight times the Sun becoming core collapse supernovae; while smaller stars blow off their outer layers and leave behind the inert core in the form of a white dwarf. The ejection of the outer layers forms a planetary nebulae. The remnant of a supernova is a dense neutron star, or, if the stellar mass was at least three times that of the Sun, a black hole. Closely orbiting binary stars can follow more complex evolutionary paths, such as mass transfer onto a white dwarf companion that can potentially cause a supernova. Planetary nebulae and supernovae distribute the "metals" produced in the star by fusion to the interstellar medium; without them, all new stars (and their planetary systems) would be formed from hydrogen and helium alone.




Our solar system orbits within the Milky Way, a barred spiral galaxy that is a prominent member of the Local Group of galaxies. It is a rotating mass of gas, dust, stars and other objects, held together by mutual gravitational attraction. As the Earth is located within the dusty outer arms, there are large portions of the Milky Way that are obscured from view.
In the center of the Milky Way is the core, a bar-shaped bulge with what is believed to be a supermassive black hole at its center. This is surrounded by four primary arms that spiral from the core. This is a region of active star formation that contains many younger, population I stars. The disk is surrounded by a spheroid halo of older, population II stars, as well as relatively dense concentrations of stars known as globular clusters.
Between the stars lies the interstellar medium, a region of sparse matter. In the densest regions, molecular clouds of molecular hydrogen and other elements create star-forming regions. These begin as a compact pre-stellar core or dark nebulae, which concentrate and collapse (in volumes determined by the Jeans length) to form compact protostars.
As the more massive stars appear, they transform the cloud into an H II region (ionized atomic hydrogen) of glowing gas and plasma. The stellar wind and supernova explosions from these stars eventually cause the cloud to disperse, often leaving behind one or more young open clusters of stars. These clusters gradually disperse, and the stars join the population of the Milky Way.
Kinematic studies of matter in the Milky Way and other galaxies have demonstrated that there is more mass than can be accounted for by visible matter. A dark matter halo appears to dominate the mass, although the nature of this dark matter remains undetermined.




The study of objects outside our galaxy is a branch of astronomy concerned with the formation and evolution of Galaxies, their morphology (description) and classification, the observation of active galaxies, and at a larger scale, the groups and clusters of galaxies. Finally, the latter is important for the understanding of the large-scale structure of the cosmos.
Most galaxies are organized into distinct shapes that allow for classification schemes. They are commonly divided into spiral, elliptical and Irregular galaxies.
As the name suggests, an elliptical galaxy has the cross-sectional shape of an ellipse. The stars move along random orbits with no preferred direction. These galaxies contain little or no interstellar dust, few star-forming regions, and generally older stars. Elliptical galaxies are more commonly found at the core of galactic clusters, and may have been formed through mergers of large galaxies.
A spiral galaxy is organized into a flat, rotating disk, usually with a prominent bulge or bar at the center, and trailing bright arms that spiral outward. The arms are dusty regions of star formation within which massive young stars produce a blue tint. Spiral galaxies are typically surrounded by a halo of older stars. Both the Milky Way and one of our nearest galaxy neighbors, the Andromeda Galaxy, are spiral galaxies.
Irregular galaxies are chaotic in appearance, and are neither spiral nor elliptical. About a quarter of all galaxies are irregular, and the peculiar shapes of such galaxies may be the result of gravitational interaction.
An active galaxy is a formation that emits a significant amount of its energy from a source other than its stars, dust and gas. It is powered by a compact region at the core, thought to be a super-massive black hole that is emitting radiation from in-falling material.
A radio galaxy is an active galaxy that is very luminous in the radio portion of the spectrum, and is emitting immense plumes or lobes of gas. Active galaxies that emit shorter frequency, high-energy radiation include Seyfert galaxies, Quasars, and Blazars. Quasars are believed to be the most consistently luminous objects in the known universe.
The large-scale structure of the cosmos is represented by groups and clusters of galaxies. This structure is organized into a hierarchy of groupings, with the largest being the superclusters. The collective matter is formed into filaments and walls, leaving large voids between.




Cosmology (from the Greek κόσμος (kosmos) "world, universe" and λόγος (logos) "word, study" or literally "logic") could be considered the study of the Universe as a whole.

Observations of the large-scale structure of the Universe, a branch known as physical cosmology, have provided a deep understanding of the formation and evolution of the cosmos. Fundamental to modern cosmology is the well-accepted theory of the big bang, wherein our Universe began at a single point in time, and thereafter expanded over the course of 13.8 billion years to its present condition. The concept of the big bang can be traced back to the discovery of the microwave background radiation in 1965.
In the course of this expansion, the Universe underwent several evolutionary stages. In the very early moments, it is theorized that the Universe experienced a very rapid cosmic inflation, which homogenized the starting conditions. Thereafter, nucleosynthesis produced the elemental abundance of the early Universe. (See also nucleocosmochronology.)
When the first neutral atoms formed from a sea of primordial ions, space became transparent to radiation, releasing the energy viewed today as the microwave background radiation. The expanding Universe then underwent a Dark Age due to the lack of stellar energy sources.
A hierarchical structure of matter began to form from minute variations in the mass density of space. Matter accumulated in the densest regions, forming clouds of gas and the earliest stars, the Population III stars. These massive stars triggered the reionization process and are believed to have created many of the heavy elements in the early Universe, which, through nuclear decay, create lighter elements, allowing the cycle of nucleosynthesis to continue longer.
Gravitational aggregations clustered into filaments, leaving voids in the gaps. Gradually, organizations of gas and dust merged to form the first primitive galaxies. Over time, these pulled in more matter, and were often organized into groups and clusters of galaxies, then into larger-scale superclusters.
Fundamental to the structure of the Universe is the existence of dark matter and dark energy. These are now thought to be its dominant components, forming 96% of the mass of the Universe. For this reason, much effort is expended in trying to understand the physics of these components.



Astronomy and astrophysics have developed significant interdisciplinary links with other major scientific fields. Archaeoastronomy is the study of ancient or traditional astronomies in their cultural context, utilizing archaeological and anthropological evidence. Astrobiology is the study of the advent and evolution of biological systems in the Universe, with particular emphasis on the possibility of non-terrestrial life. Astrostatistics is the application of statistics to astrophysics to the analysis of vast amount of observational astrophysical data.
The study of chemicals found in space, including their formation, interaction and destruction, is called astrochemistry. These substances are usually found in molecular clouds, although they may also appear in low temperature stars, brown dwarfs and planets. Cosmochemistry is the study of the chemicals found within the Solar System, including the origins of the elements and variations in the isotope ratios. Both of these fields represent an overlap of the disciplines of astronomy and chemistry. As "forensic astronomy", finally, methods from astronomy have been used to solve problems of law and history.




Astronomy is one of the sciences to which amateurs can contribute the most.
Collectively, amateur astronomers observe a variety of celestial objects and phenomena sometimes with equipment that they build themselves. Common targets of amateur astronomers include the Moon, planets, stars, comets, meteor showers, and a variety of deep-sky objects such as star clusters, galaxies, and nebulae. Astronomy clubs are located throughout the world and many have programs to help their members set up and complete observational programs including those to observe all the objects in the Messier (110 objects) or Herschel 400 catalogues of points of interest in the night sky. One branch of amateur astronomy, amateur astrophotography, involves the taking of photos of the night sky. Many amateurs like to specialize in the observation of particular objects, types of objects, or types of events which interest them.
Most amateurs work at visible wavelengths, but a small minority experiment with wavelengths outside the visible spectrum. This includes the use of infrared filters on conventional telescopes, and also the use of radio telescopes. The pioneer of amateur radio astronomy was Karl Jansky, who started observing the sky at radio wavelengths in the 1930s. A number of amateur astronomers use either homemade telescopes or use radio telescopes which were originally built for astronomy research but which are now available to amateurs (e.g. the One-Mile Telescope).
Amateur astronomers continue to make scientific contributions to the field of astronomy and it is one of the few scientific disciplines where amateurs can still make significant contributions. Amateurs can make occultation measurements that are used to refine the orbits of minor planets. They can also discover comets, and perform regular observations of variable stars. Improvements in digital technology have allowed amateurs to make impressive advances in the field of astrophotography.




Although the scientific discipline of astronomy has made tremendous strides in understanding the nature of the Universe and its contents, there remain some important unanswered questions. Answers to these may require the construction of new ground- and space-based instruments, and possibly new developments in theoretical and experimental physics.
What is the origin of the stellar mass spectrum? That is, why do astronomers observe the same distribution of stellar masses – the initial mass function – apparently regardless of the initial conditions? A deeper understanding of the formation of stars and planets is needed.
Is there other life in the Universe? Especially, is there other intelligent life? If so, what is the explanation for the Fermi paradox? The existence of life elsewhere has important scientific and philosophical implications. Is the Solar System normal or atypical?
What is the nature of dark matter and dark energy? These dominate the evolution and fate of the cosmos, yet their true nature remains unknown. What will be the ultimate fate of the universe?
How did the first galaxies form? How did supermassive black holes form?
What is creating the ultra-high-energy cosmic rays?
Why is the abundance of lithium in the cosmos four times lower than predicted by the standard Big Bang model?
What really happens beyond the event horizon?









Forbes, George (1909). History of Astronomy. London: Plain Label Books. ISBN 1-60303-159-6.  Available at Project Gutenberg,Google books
Harpaz, Amos (1994). Stellar Evolution. A K Peters, Ltd. ISBN 978-1-56881-012-6. 
Unsöld, A.; Baschek, B. (2001). The New Cosmos: An Introduction to Astronomy and Astrophysics. Springer. ISBN 3-540-67877-8. 




NASA/IPAC Extragalactic Database (NED) (NED-Distances)
International Year of Astronomy 2009 IYA2009 Main website
Cosmic Journey: A History of Scientific Cosmology from the American Institute of Physics
Southern Hemisphere Astronomy
Celestia Motherlode Educational site for Astronomical journeys through space
Kroto, Harry, Astrophysical Chemistry Lecture Series.
Core books and Core journals in Astronomy, from the Smithsonian/NASA Astrophysics Data System
A Journey with Fred Hoyle by Wickramasinghe, Chandra.
Astronomy books from the History of Science Collection at Linda Hall LibraryIn physics, energy is a property of objects which can be transferred to other objects or converted into different forms but never created or destroyed. The amount of energy constitutes a fundamental limitation on the capacity of a system to perform work, or to provide heat. The SI unit of energy is the joule, which is the energy transferred to an object by the mechanical work of moving it a distance of 1 metre against a force of 1 newton.
Common energy forms include the kinetic energy of a moving object, the potential energy stored by an object's position in a force field (gravitational, electric or magnetic), the elastic energy stored by stretching solid objects, the chemical energy released when a fuel burns, the radiant energy carried by light, and the thermal energy due to an object's temperature. All of the many forms of energy are convertible to other kinds of energy. In Newtonian physics, there is a universal law of conservation of energy which says that energy can be neither created nor be destroyed; however, it can change from one form to another.
For "closed systems" with no external source or sink of energy, the first law of thermodynamics states that a system's energy is constant unless energy is transferred in or out by mechanical work or heat, and that no energy is lost in transfer. This means that it is impossible to create or destroy energy. While heat can always be fully converted into work in a reversible isothermal expansion of an ideal gas, for cyclic processes of practical interest in heat engines the second law of thermodynamics states that the system doing work always loses some energy as waste heat. This creates a limit to the amount of heat energy that can do work in a cyclic process, a limit called the available energy. Mechanical and other forms of energy can be transformed in the other direction into thermal energy without such limitations. The total energy of a system can be calculated by adding up all forms of energy in the system.
Examples of energy transformation include generating electric energy from heat energy via a steam turbine, or lifting an object against gravity using electrical energy driving a crane motor. Lifting against gravity performs mechanical work on the object and stores gravitational potential energy in the object. If the object falls to the ground, gravity does mechanical work on the object which transforms the potential energy in the gravitational field to the kinetic energy released as heat on impact with the ground. Our Sun transforms nuclear potential energy to other forms of energy; its total mass does not decrease due to that in itself (since it still contains the same total energy even if in different forms), but its mass does decrease when the energy escapes out to its surroundings, largely as radiant energy.
Mass and energy are closely related. Due to mass–energy equivalence, any object that has mass when stationary in a frame of reference (called rest mass) also has an equivalent amount of energy whose form is called rest energy in that frame, and any additional energy acquired by the object above that rest energy will increase an object's mass. For example, with a sensitive enough scale, one could measure an increase in mass after heating an object.
Because energy exists in many interconvertible forms, and yet can't be created or destroyed, its measurement may be equivalently "defined" and quantified via its transfer or conversions into various forms that may be found to be convenient or pedagogic or to facilitate accurate measurement; for example by energy transfer in the form of work (as measured via forces and acceleration) or heat (as measured via temperature changes of materials) or into particular forms such as kinetic (as measured via mass and speed) or by its equivalent mass.
Living organisms require available energy to stay alive, such as the energy humans get from food. Civilisation gets the energy it needs from energy resources such as fossil fuels, nuclear fuel, or renewable energy. The processes of Earth's climate and ecosystem are driven by the radiant energy Earth receives from the sun and the geothermal energy contained within the earth.
In biology, energy can be thought of as what's needed to keep entropy low.




The total energy of a system can be subdivided and classified in various ways. For example, classical mechanics distinguishes between kinetic energy, which is determined by an object's movement through space, and potential energy, which is a function of the position of an object within a field. It may also be convenient to distinguish gravitational energy, thermal energy, several types of nuclear energy (which utilize potentials from the nuclear force and the weak force), electric energy (from the electric field), and magnetic energy (from the magnetic field), among others. Many of these classifications overlap; for instance, thermal energy usually consists partly of kinetic and partly of potential energy.
Some types of energy are a varying mix of both potential and kinetic energy. An example is mechanical energy which is the sum of (usually macroscopic) kinetic and potential energy in a system. Elastic energy in materials is also dependent upon electrical potential energy (among atoms and molecules), as is chemical energy, which is stored and released from a reservoir of electrical potential energy between electrons, and the molecules or atomic nuclei that attract them..The list is also not necessarily complete. Whenever physical scientists discover that a certain phenomenon appears to violate the law of energy conservation, new forms are typically added that account for the discrepancy.
Heat and work are special cases in that they are not properties of systems, but are instead properties of processes that transfer energy. In general we cannot measure how much heat or work are present in an object, but rather only how much energy is transferred among objects in certain ways during the occurrence of a given process. Heat and work are measured as positive or negative depending on which side of the transfer we view them from.
Potential energies are often measured as positive or negative depending on whether they are greater or less than the energy of a specified base state or configuration such as two interacting bodies being infinitely far apart. Wave energies (such as radiant or sound energy), kinetic energy, and rest energy are each greater than or equal to zero because they are measured in comparison to a base state of zero energy: "no wave", "no motion", and "no inertia", respectively.
The distinctions between different kinds of energy is not always clear-cut. As Richard Feynman points out:

These notions of potential and kinetic energy depend on a notion of length scale. For example, one can speak of macroscopic potential and kinetic energy, which do not include thermal potential and kinetic energy. Also what is called chemical potential energy is a macroscopic notion, and closer examination shows that it is really the sum of the potential and kinetic energy on the atomic and subatomic scale. Similar remarks apply to nuclear "potential" energy and most other forms of energy. This dependence on length scale is non-problematic if the various length scales are decoupled, as is often the case ... but confusion can arise when different length scales are coupled, for instance when friction converts macroscopic work into microscopic thermal energy.

Some examples of different kinds of energy:




The word energy derives from the Ancient Greek: ἐνέργεια energeia "activity, operation", which possibly appears for the first time in the work of Aristotle in the 4th century BC. In contrast to the modern definition, energeia was a qualitative philosophical concept, broad enough to include ideas such as happiness and pleasure.
In the late 17th century, Gottfried Leibniz proposed the idea of the Latin: vis viva, or living force, which defined as the product of the mass of an object and its velocity squared; he believed that total vis viva was conserved. To account for slowing due to friction, Leibniz theorized that thermal energy consisted of the random motion of the constituent parts of matter, a view shared by Isaac Newton, although it would be more than a century until this was generally accepted. The modern analog of this property, kinetic energy, differs from vis viva only by a factor of two.
In 1807, Thomas Young was possibly the first to use the term "energy" instead of vis viva, in its modern sense. Gustave-Gaspard Coriolis described "kinetic energy" in 1829 in its modern sense, and in 1853, William Rankine coined the term "potential energy". The law of conservation of energy was also first postulated in the early 19th century, and applies to any isolated system. It was argued for some years whether heat was a physical substance, dubbed the caloric, or merely a physical quantity, such as momentum. In 1845 James Prescott Joule discovered the link between mechanical work and the generation of heat.
These developments led to the theory of conservation of energy, formalized largely by William Thomson (Lord Kelvin) as the field of thermodynamics. Thermodynamics aided the rapid development of explanations of chemical processes by Rudolf Clausius, Josiah Willard Gibbs, and Walther Nernst. It also led to a mathematical formulation of the concept of entropy by Clausius and to the introduction of laws of radiant energy by Jožef Stefan. According to Noether's theorem, the conservation of energy is a consequence of the fact that the laws of physics do not change over time. Thus, since 1918, theorists have understood that the law of conservation of energy is the direct mathematical consequence of the translational symmetry of the quantity conjugate to energy, namely time.




In 1843 James Prescott Joule independently discovered the mechanical equivalent in a series of experiments. The most famous of them used the "Joule apparatus": a descending weight, attached to a string, caused rotation of a paddle immersed in water, practically insulated from heat transfer. It showed that the gravitational potential energy lost by the weight in descending was equal to the internal energy gained by the water through friction with the paddle.
In the International System of Units (SI), the unit of energy is the joule, named after James Prescott Joule. It is a derived unit. It is equal to the energy expended (or work done) in applying a force of one newton through a distance of one metre. However energy is also expressed in many other units not part of the SI, such as ergs, calories, British Thermal Units, kilowatt-hours and kilocalories, which require a conversion factor when expressed in SI units.
The SI unit of energy rate (energy per unit time) is the watt, which is a joule per second. Thus, one joule is one watt-second, and 3600 joules equal one watt-hour. The CGS energy unit is the erg and the imperial and US customary unit is the foot pound. Other energy units such as the electronvolt, food calorie or thermodynamic kcal (based on the temperature change of water in a heating process), and BTU are used in specific areas of science and commerce.







In classical mechanics, energy is a conceptually and mathematically useful property, as it is a conserved quantity. Several formulations of mechanics have been developed using energy as a core concept.
Work, a form of energy, is force times distance.

  
    
      
        W
        =
        
          ∫
          
            C
          
        
        
          F
        
        ⋅
        
          d
        
        
          s
        
      
    
    {\displaystyle W=\int _{C}\mathbf {F} \cdot \mathrm {d} \mathbf {s} }
  
This says that the work (
  
    
      
        W
      
    
    {\displaystyle W}
  ) is equal to the line integral of the force F along a path C; for details see the mechanical work article. Work and thus energy is frame dependent. For example, consider a ball being hit by a bat. In the center-of-mass reference frame, the bat does no work on the ball. But, in the reference frame of the person swinging the bat, considerable work is done on the ball.
The total energy of a system is sometimes called the Hamiltonian, after William Rowan Hamilton. The classical equations of motion can be written in terms of the Hamiltonian, even for highly complex or abstract systems. These classical equations have remarkably direct analogs in nonrelativistic quantum mechanics.
Another energy-related concept is called the Lagrangian, after Joseph-Louis Lagrange. This formalism is as fundamental as the Hamiltonian, and both can be used to derive the equations of motion or be derived from them. It was invented in the context of classical mechanics, but is generally useful in modern physics. The Lagrangian is defined as the kinetic energy minus the potential energy. Usually, the Lagrange formalism is mathematically more convenient than the Hamiltonian for non-conservative systems (such as systems with friction).
Noether's theorem (1918) states that any differentiable symmetry of the action of a physical system has a corresponding conservation law. Noether's theorem has become a fundamental tool of modern theoretical physics and the calculus of variations. A generalisation of the seminal formulations on constants of motion in Lagrangian and Hamiltonian mechanics (1788 and 1833, respectively), it does not apply to systems that cannot be modeled with a Lagrangian; for example, dissipative systems with continuous symmetries need not have a corresponding conservation law.



In the context of chemistry, energy is an attribute of a substance as a consequence of its atomic, molecular or aggregate structure. Since a chemical transformation is accompanied by a change in one or more of these kinds of structure, it is invariably accompanied by an increase or decrease of energy of the substances involved. Some energy is transferred between the surroundings and the reactants of the reaction in the form of heat or light; thus the products of a reaction may have more or less energy than the reactants. A reaction is said to be exergonic if the final state is lower on the energy scale than the initial state; in the case of endergonic reactions the situation is the reverse. Chemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy. The speed of a chemical reaction (at given temperature T) is related to the activation energy E, by the Boltzmann's population factor e−E/kT – that is the probability of molecule to have energy greater than or equal to E at the given temperature T. This exponential dependence of a reaction rate on temperature is known as the Arrhenius equation.The activation energy necessary for a chemical reaction can be in the form of thermal energy.




In biology, energy is an attribute of all biological systems from the biosphere to the smallest living organism. Within an organism it is responsible for growth and development of a biological cell or an organelle of a biological organism. Energy is thus often said to be stored by cells in the structures of molecules of substances such as carbohydrates (including sugars), lipids, and proteins, which release energy when reacted with oxygen in respiration. In human terms, the human equivalent (H-e) (Human energy conversion) indicates, for a given amount of energy expenditure, the relative quantity of energy needed for human metabolism, assuming an average human energy expenditure of 12,500 kJ per day and a basal metabolic rate of 80 watts. For example, if our bodies run (on average) at 80 watts, then a light bulb running at 100 watts is running at 1.25 human equivalents (100 ÷ 80) i.e. 1.25 H-e. For a difficult task of only a few seconds' duration, a person can put out thousands of watts, many times the 746 watts in one official horsepower. For tasks lasting a few minutes, a fit human can generate perhaps 1,000 watts. For an activity that must be sustained for an hour, output drops to around 300; for an activity kept up all day, 150 watts is about the maximum. The human equivalent assists understanding of energy flows in physical and biological systems by expressing energy units in human terms: it provides a "feel" for the use of a given amount of energy.
Sunlight is also captured by plants as chemical potential energy in photosynthesis, when carbon dioxide and water (two low-energy compounds) are converted into the high-energy compounds carbohydrates, lipids, and proteins. Plants also release oxygen during photosynthesis, which is utilized by living organisms as an electron acceptor, to release the energy of carbohydrates, lipids, and proteins. Release of the energy stored during photosynthesis as heat or light may be triggered suddenly by a spark, in a forest fire, or it may be made available more slowly for animal or human metabolism, when these molecules are ingested, and catabolism is triggered by enzyme action.
Any living organism relies on an external source of energy—radiation from the Sun in the case of green plants, chemical energy in some form in the case of animals—to be able to grow and reproduce. The daily 1500–2000 Calories (6–8 MJ) recommended for a human adult are taken as a combination of oxygen and food molecules, the latter mostly carbohydrates and fats, of which glucose (C6H12O6) and stearin (C57H110O6) are convenient examples. The food molecules are oxidised to carbon dioxide and water in the mitochondria

C6H12O6 + 6O2 → 6CO2 + 6H2O
C57H110O6 + 81.5O2 → 57CO2 + 55H2O

and some of the energy is used to convert ADP into ATP.

ADP + HPO42− → ATP + H2O

The rest of the chemical energy in O2 and the carbohydrate or fat is converted into heat: the ATP is used as a sort of "energy currency", and some of the chemical energy it contains is used for other metabolism when ATP reacts with OH groups and eventually splits into ADP and phosphate (at each stage of a metabolic pathway, some chemical energy is converted into heat). Only a tiny fraction of the original chemical energy is used for work:
gain in kinetic energy of a sprinter during a 100 m race: 4 kJ
gain in gravitational potential energy of a 150 kg weight lifted through 2 metres: 3 kJ
Daily food intake of a normal adult: 6–8 MJ
It would appear that living organisms are remarkably inefficient (in the physical sense) in their use of the energy they receive (chemical energy or radiation), and it is true that most real machines manage higher efficiencies. In growing organisms the energy that is converted to heat serves a vital purpose, as it allows the organism tissue to be highly ordered with regard to the molecules it is built from. The second law of thermodynamics states that energy (and matter) tends to become more evenly spread out across the universe: to concentrate energy (or matter) in one specific place, it is necessary to spread out a greater amount of energy (as heat) across the remainder of the universe ("the surroundings"). Simpler organisms can achieve higher energy efficiencies than more complex ones, but the complex organisms can occupy ecological niches that are not available to their simpler brethren. The conversion of a portion of the chemical energy to heat at each step in a metabolic pathway is the physical reason behind the pyramid of biomass observed in ecology: to take just the first step in the food chain, of the estimated 124.7 Pg/a of carbon that is fixed by photosynthesis, 64.3 Pg/a (52%) are used for the metabolism of green plants, i.e. reconverted into carbon dioxide and heat.



In geology, continental drift, mountain ranges, volcanoes, and earthquakes are phenomena that can be explained in terms of energy transformations in the Earth's interior, while meteorological phenomena like wind, rain, hail, snow, lightning, tornadoes and hurricanes are all a result of energy transformations brought about by solar energy on the atmosphere of the planet Earth.
Sunlight may be stored as gravitational potential energy after it strikes the Earth, as (for example) water evaporates from oceans and is deposited upon mountains (where, after being released at a hydroelectric dam, it can be used to drive turbines or generators to produce electricity). Sunlight also drives many weather phenomena, save those generated by volcanic events. An example of a solar-mediated weather event is a hurricane, which occurs when large unstable areas of warm ocean, heated over months, give up some of their thermal energy suddenly to power a few days of violent air movement.
In a slower process, radioactive decay of atoms in the core of the Earth releases heat. This thermal energy drives plate tectonics and may lift mountains, via orogenesis. This slow lifting represents a kind of gravitational potential energy storage of the thermal energy, which may be later released to active kinetic energy in landslides, after a triggering event. Earthquakes also release stored elastic potential energy in rocks, a store that has been produced ultimately from the same radioactive heat sources. Thus, according to present understanding, familiar events such as landslides and earthquakes release energy that has been stored as potential energy in the Earth's gravitational field or elastic strain (mechanical potential energy) in rocks. Prior to this, they represent release of energy that has been stored in heavy atoms since the collapse of long-destroyed supernova stars created these atoms.



In cosmology and astronomy the phenomena of stars, nova, supernova, quasars and gamma-ray bursts are the universe's highest-output energy transformations of matter. All stellar phenomena (including solar activity) are driven by various kinds of energy transformations. Energy in such transformations is either from gravitational collapse of matter (usually molecular hydrogen) into various classes of astronomical objects (stars, black holes, etc.), or from nuclear fusion (of lighter elements, primarily hydrogen). The nuclear fusion of hydrogen in the Sun also releases another store of potential energy which was created at the time of the Big Bang. At that time, according to theory, space expanded and the universe cooled too rapidly for hydrogen to completely fuse into heavier elements. This meant that hydrogen represents a store of potential energy that can be released by fusion. Such a fusion process is triggered by heat and pressure generated from gravitational collapse of hydrogen clouds when they produce stars, and some of the fusion energy is then transformed into sunlight.




In quantum mechanics, energy is defined in terms of the energy operator as a time derivative of the wave function. The Schrödinger equation equates the energy operator to the full energy of a particle or a system. Its results can be considered as a definition of measurement of energy in quantum mechanics. The Schrödinger equation describes the space- and time-dependence of a slowly changing (non-relativistic) wave function of quantum systems. The solution of this equation for a bound system is discrete (a set of permitted states, each characterized by an energy level) which results in the concept of quanta. In the solution of the Schrödinger equation for any oscillator (vibrator) and for electromagnetic waves in a vacuum, the resulting energy states are related to the frequency by Planck's relation: 
  
    
      
        E
        =
        h
        ν
      
    
    {\displaystyle E=h\nu }
   (where 
  
    
      
        h
      
    
    {\displaystyle h}
   is Planck's constant and 
  
    
      
        ν
      
    
    {\displaystyle \nu }
   the frequency). In the case of an electromagnetic wave these energy states are called quanta of light or photons.



When calculating kinetic energy (work to accelerate a mass from zero speed to some finite speed) relativistically – using Lorentz transformations instead of Newtonian mechanics – Einstein discovered an unexpected by-product of these calculations to be an energy term which does not vanish at zero speed. He called it rest mass energy: energy which every mass must possess even when being at rest. The amount of energy is directly proportional to the mass of body:

  
    
      
        E
        =
        m
        
          c
          
            2
          
        
      
    
    {\displaystyle E=mc^{2}}
  ,
where
m is the mass,
c is the speed of light in vacuum,
E is the rest mass energy.
For example, consider electron–positron annihilation, in which the rest mass of individual particles is destroyed, but the inertia equivalent of the system of the two particles (its invariant mass) remains (since all energy is associated with mass), and this inertia and invariant mass is carried off by photons which individually are massless, but as a system retain their mass. This is a reversible process – the inverse process is called pair creation – in which the rest mass of particles is created from energy of two (or more) annihilating photons. In this system the matter (electrons and positrons) is destroyed and changed to non-matter energy (the photons). However, the total system mass and energy do not change during this interaction.
In general relativity, the stress–energy tensor serves as the source term for the gravitational field, in rough analogy to the way mass serves as the source term in the non-relativistic Newtonian approximation.
It is not uncommon to hear that energy is "equivalent" to mass. It would be more accurate to state that every energy has an inertia and gravity equivalent, and because mass is a form of energy, then mass too has inertia and gravity associated with it.
In classical physics, energy is a scalar quantity, the canonical conjugate to time. In special relativity energy is also a scalar (although not a Lorentz scalar but a time component of the energy–momentum 4-vector). In other words, energy is invariant with respect to rotations of space, but not invariant with respect to rotations of space-time (= boosts).




Energy may be transformed between different forms at various efficiencies. Items that transform between these forms are called transducers. Examples of transducers include a battery, from chemical energy to electric energy; a dam: gravitational potential energy to kinetic energy of moving water (and the blades of a turbine) and ultimately to electric energy through an electric generator; or a heat engine, from heat to work.
There are strict limits to how efficiently heat can be converted into work in a cyclic process, e.g. in a heat engine, as described by Carnot's theorem and the second law of thermodynamics. However, some energy transformations can be quite efficient. The direction of transformations in energy (what kind of energy is transformed to what other kind) is often determined by entropy (equal energy spread among all available degrees of freedom) considerations. In practice all energy transformations are permitted on a small scale, but certain larger transformations are not permitted because it is statistically unlikely that energy or matter will randomly move into more concentrated forms or smaller spaces.
Energy transformations in the universe over time are characterized by various kinds of potential energy that has been available since the Big Bang later being "released" (transformed to more active types of energy such as kinetic or radiant energy) when a triggering mechanism is available. Familiar examples of such processes include nuclear decay, in which energy is released that was originally "stored" in heavy isotopes (such as uranium and thorium), by nucleosynthesis, a process ultimately using the gravitational potential energy released from the gravitational collapse of supernovae, to store energy in the creation of these heavy elements before they were incorporated into the solar system and the Earth. This energy is triggered and released in nuclear fission bombs or in civil nuclear power generation. Similarly, in the case of a chemical explosion, chemical potential energy is transformed to kinetic energy and thermal energy in a very short time. Yet another example is that of a pendulum. At its highest points the kinetic energy is zero and the gravitational potential energy is at maximum. At its lowest point the kinetic energy is at maximum and is equal to the decrease of potential energy. If one (unrealistically) assumes that there is no friction or other losses, the conversion of energy between these processes would be perfect, and the pendulum would continue swinging forever.
Energy is also transferred from potential energy (
  
    
      
        
          E
          
            p
          
        
      
    
    {\displaystyle E_{p}}
  ) to kinetic energy (
  
    
      
        
          E
          
            k
          
        
      
    
    {\displaystyle E_{k}}
  ) and then back to potential energy constantly. This is referred to as conservation of energy. In this closed system, energy cannot be created or destroyed; therefore, the initial energy and the final energy will be equal to each other. This can be demonstrated by the following:

The equation can then be simplified further since 
  
    
      
        
          E
          
            p
          
        
        =
        m
        g
        h
      
    
    {\displaystyle E_{p}=mgh}
   (mass times acceleration due to gravity times the height) and 
  
    
      
        
          E
          
            k
          
        
        =
        
          
            1
            2
          
        
        m
        
          v
          
            2
          
        
      
    
    {\displaystyle E_{k}={\frac {1}{2}}mv^{2}}
   (half mass times velocity squared). Then the total amount of energy can be found by adding 
  
    
      
        
          E
          
            p
          
        
        +
        
          E
          
            k
          
        
        =
        
          E
          
            t
            o
            t
            a
            l
          
        
      
    
    {\displaystyle E_{p}+E_{k}=E_{total}}
  .



Energy gives rise to weight when it is trapped in a system with zero momentum, where it can be weighed. It is also equivalent to mass, and this mass is always associated with it. Mass is also equivalent to a certain amount of energy, and likewise always appears associated with it, as described in mass-energy equivalence. The formula E = mc², derived by Albert Einstein (1905) quantifies the relationship between rest-mass and rest-energy within the concept of special relativity. In different theoretical frameworks, similar formulas were derived by J. J. Thomson (1881), Henri Poincaré (1900), Friedrich Hasenöhrl (1904) and others (see Mass-energy equivalence#History for further information).
Matter may be converted to energy (and vice versa), but mass cannot ever be destroyed; rather, mass/energy equivalence remains a constant for both the matter and the energy, during any process when they are converted into each other. However, since 
  
    
      
        
          c
          
            2
          
        
      
    
    {\displaystyle c^{2}}
   is extremely large relative to ordinary human scales, the conversion of ordinary amount of matter (for example, 1 kg) to other forms of energy (such as heat, light, and other radiation) can liberate tremendous amounts of energy (~
  
    
      
        9
        ×
        
          10
          
            16
          
        
      
    
    {\displaystyle 9\times 10^{16}}
   joules = 21 megatons of TNT), as can be seen in nuclear reactors and nuclear weapons. Conversely, the mass equivalent of a unit of energy is minuscule, which is why a loss of energy (loss of mass) from most systems is difficult to measure by weight, unless the energy loss is very large. Examples of energy transformation into matter (i.e., kinetic energy into particles with rest mass) are found in high-energy nuclear physics.



Thermodynamics divides energy transformation into two kinds: reversible processes and irreversible processes. An irreversible process is one in which energy is dissipated (spread) into empty energy states available in a volume, from which it cannot be recovered into more concentrated forms (fewer quantum states), without degradation of even more energy. A reversible process is one in which this sort of dissipation does not happen. For example, conversion of energy from one type of potential field to another, is reversible, as in the pendulum system described above. In processes where heat is generated, quantum states of lower energy, present as possible excitations in fields between atoms, act as a reservoir for part of the energy, from which it cannot be recovered, in order to be converted with 100% efficiency into other forms of energy. In this case, the energy must partly stay as heat, and cannot be completely recovered as usable energy, except at the price of an increase in some other kind of heat-like increase in disorder in quantum states, in the universe (such as an expansion of matter, or a randomisation in a crystal).
As the universe evolves in time, more and more of its energy becomes trapped in irreversible states (i.e., as heat or other kinds of increases in disorder). This has been referred to as the inevitable thermodynamic heat death of the universe. In this heat death the energy of the universe does not change, but the fraction of energy which is available to do work through a heat engine, or be transformed to other usable forms of energy (through the use of generators attached to heat engines), grows less and less.




According to conservation of energy, energy can neither be created (produced) nor destroyed by itself. It can only be transformed. The total inflow of energy into a system must equal the total outflow of energy from the system, plus the change in the energy contained within the system. Energy is subject to a strict global conservation law; that is, whenever one measures (or calculates) the total energy of a system of particles whose interactions do not depend explicitly on time, it is found that the total energy of the system always remains constant.
Richard Feynman said during a 1961 lecture:

There is a fact, or if you wish, a law, governing all natural phenomena that are known to date. There is no known exception to this law—it is exact so far as we know. The law is called the conservation of energy. It states that there is a certain quantity, which we call energy, that does not change in manifold changes which nature undergoes. That is a most abstract idea, because it is a mathematical principle; it says that there is a numerical quantity which does not change when something happens. It is not a description of a mechanism, or anything concrete; it is just a strange fact that we can calculate some number and when we finish watching nature go through her tricks and calculate the number again, it is the same.

Most kinds of energy (with gravitational energy being a notable exception) are subject to strict local conservation laws as well. In this case, energy can only be exchanged between adjacent regions of space, and all observers agree as to the volumetric density of energy in any given space. There is also a global law of conservation of energy, stating that the total energy of the universe cannot change; this is a corollary of the local law, but not vice versa.
This law is a fundamental principle of physics. As shown rigorously by Noether's theorem, the conservation of energy is a mathematical consequence of translational symmetry of time, a property of most phenomena below the cosmic scale that makes them independent of their locations on the time coordinate. Put differently, yesterday, today, and tomorrow are physically indistinguishable. This is because energy is the quantity which is canonical conjugate to time. This mathematical entanglement of energy and time also results in the uncertainty principle - it is impossible to define the exact amount of energy during any definite time interval. The uncertainty principle should not be confused with energy conservation - rather it provides mathematical limits to which energy can in principle be defined and measured.
Each of the basic forces of nature is associated with a different type of potential energy, and all types of potential energy (like all other types of energy) appears as system mass, whenever present. For example, a compressed spring will be slightly more massive than before it was compressed. Likewise, whenever energy is transferred between systems by any mechanism, an associated mass is transferred with it.
In quantum mechanics energy is expressed using the Hamiltonian operator. On any time scales, the uncertainty in the energy is by

  
    
      
        Δ
        E
        Δ
        t
        ≥
        
          
            ℏ
            2
          
        
      
    
    {\displaystyle \Delta E\Delta t\geq {\frac {\hbar }{2}}}
  
which is similar in form to the Heisenberg Uncertainty Principle (but not really mathematically equivalent thereto, since H and t are not dynamically conjugate variables, neither in classical nor in quantum mechanics).
In particle physics, this inequality permits a qualitative understanding of virtual particles which carry momentum, exchange by which and with real particles, is responsible for the creation of all known fundamental forces (more accurately known as fundamental interactions). Virtual photons (which are simply lowest quantum mechanical energy state of photons) are also responsible for electrostatic interaction between electric charges (which results in Coulomb law), for spontaneous radiative decay of exited atomic and nuclear states, for the Casimir force, for van der Waals bond forces and some other observable phenomena.






Energy transfer can be considered for the special case of systems which are closed to transfers of matter. The portion of the energy which is transferred by conservative forces over a distance is measured as the work the source system does on the receiving system. The portion of the energy which does not do work during the transfer is called heat. Energy can be transferred between systems in a variety of ways. Examples include the transmission of electromagnetic energy via photons, physical collisions which transfer kinetic energy, and the conductive transfer of thermal energy.
Energy is strictly conserved and is also locally conserved wherever it can be defined. In thermodynamics, for closed systems, the process of energy transfer is described by the first law:

where 
  
    
      
        E
      
    
    {\displaystyle E}
   is the amount of energy transferred, 
  
    
      
        W
      
    
    {\displaystyle W}
    represents the work done on the system, and 
  
    
      
        Q
      
    
    {\displaystyle Q}
   represents the heat flow into the system. As a simplification, the heat term, 
  
    
      
        Q
      
    
    {\displaystyle Q}
  , is sometimes ignored, especially when the thermal efficiency of the transfer is high.

This simplified equation is the one used to define the joule, for example.



Beyond the constraints of closed systems, open systems can gain or lose energy in association with matter transfer (both of these process are illustrated by fueling an auto, a system which gains in energy thereby, without addition of either work or heat). Denoting this energy by 
  
    
      
        E
      
    
    {\displaystyle E}
  , one may write






Internal energy is the sum of all microscopic forms of energy of a system. It is the energy needed to create the system. It is related to the potential energy, e.g., molecular structure, crystal structure, and other geometric aspects, as well as the motion of the particles, in form of kinetic energy. Thermodynamics is chiefly concerned with changes in internal energy and not its absolute value, which is impossible to determine with thermodynamics alone.



The first law of thermodynamics asserts that energy (but not necessarily thermodynamic free energy) is always conserved and that heat flow is a form of energy transfer. For homogeneous systems, with a well-defined temperature and pressure, a commonly used corollary of the first law is that, for a system subject only to pressure forces and heat transfer (e.g., a cylinder-full of gas) without chemical changes, the differential change in the internal energy of the system (with a gain in energy signified by a positive quantity) is given as

  
    
      
        
          d
        
        E
        =
        T
        
          d
        
        S
        −
        P
        
          d
        
        V
        
      
    
    {\displaystyle \mathrm {d} E=T\mathrm {d} S-P\mathrm {d} V\,}
  ,
where the first term on the right is the heat transferred into the system, expressed in terms of temperature T and entropy S (in which entropy increases and the change dS is positive when the system is heated), and the last term on the right hand side is identified as work done on the system, where pressure is P and volume V (the negative sign results since compression of the system requires work to be done on it and so the volume change, dV, is negative when work is done on the system).
This equation is highly specific, ignoring all chemical, electrical, nuclear, and gravitational forces, effects such as advection of any form of energy other than heat and pV-work. The general formulation of the first law (i.e., conservation of energy) is valid even in situations in which the system is not homogeneous. For these cases the change in internal energy of a closed system is expressed in a general form by

  
    
      
        
          d
        
        E
        =
        δ
        Q
        +
        δ
        W
      
    
    {\displaystyle \mathrm {d} E=\delta Q+\delta W}
  
where 
  
    
      
        δ
        Q
      
    
    {\displaystyle \delta Q}
   is the heat supplied to the system and 
  
    
      
        δ
        W
      
    
    {\displaystyle \delta W}
   is the work applied to the system.



The energy of a mechanical harmonic oscillator (a mass on a spring) is alternatively kinetic and potential. At two points in the oscillation cycle it is entirely kinetic, and alternatively at two other points it is entirely potential. Over the whole cycle, or over many cycles, net energy is thus equally split between kinetic and potential. This is called equipartition principle; total energy of a system with many degrees of freedom is equally split among all available degrees of freedom.
This principle is vitally important to understanding the behaviour of a quantity closely related to energy, called entropy. Entropy is a measure of evenness of a distribution of energy between parts of a system. When an isolated system is given more degrees of freedom (i.e., given new available energy states that are the same as existing states), then total energy spreads over all available degrees equally without distinction between "new" and "old" degrees. This mathematical result is called the second law of thermodynamics.




Combustion
Index of energy articles
Index of wave articles
Orders of magnitude (energy)
Transfer energy













Energy at DMOZ
Differences between Heat and Thermal energy - BioCabA robot is a machine—especially one programmable by a computer—capable of carrying out a complex series of actions automatically. Robots can be guided by an external control device or the control may be embedded within. Robots may be constructed to take on human form but most robots are machines designed to perform a task with no regard to how they look.
Robots can be autonomous or semi-autonomous and range from humanoids such as Honda's Advanced Step in Innovative Mobility (ASIMO) and TOSY's TOSY Ping Pong Playing Robot (TOPIO) to industrial robots, medical operating robots, patent assist robots, dog therapy robots, collectively programmed swarm robots, UAV drones such as General Atomics MQ-1 Predator, and even microscopic nano robots. By mimicking a lifelike appearance or automating movements, a robot may convey a sense of intelligence or thought of its own.
The branch of technology that deals with the design, construction, operation, and application of robots, as well as computer systems for their control, sensory feedback, and information processing is robotics. These technologies deal with automated machines that can take the place of humans in dangerous environments or manufacturing processes, or resemble humans in appearance, behavior, and/or cognition. Many of today's robots are inspired by nature contributing to the field of bio-inspired robotics. These robots have also created a newer branch of robotics: soft robotics.
From the time of ancient civilization there have been many accounts of user-configurable automated devices and even automata resembling animals and humans, designed primarily as entertainment. As mechanical techniques developed through the Industrial age, there appeared more practical applications such as automated machines, remote-control and wireless remote-control.
The word 'robot' was first used to denote a fictional humanoid in a 1920 play R.U.R. by the Czech writer, Karel Čapek but it was Karel's brother Josef Čapek who was the word's true inventor. Electronics evolved into the driving force of development with the advent of the first electronic autonomous robots created by William Grey Walter in Bristol, England in 1948. The first digital and programmable robot was invented by George Devol in 1954 and was named the Unimate. It was sold to General Motors in 1961 where it was used to lift pieces of hot metal from die casting machines at the Inland Fisher Guide Plant in the West Trenton section of Ewing Township, New Jersey.
Robots have replaced humans in performing repetitive and dangerous tasks which humans prefer not to do, or are unable to do because of size limitations, or which take place in extreme environments such as outer space or the bottom of the sea.
There are concerns about the increasing use of robots and their role in society. Robots are blamed for rising unemployment as they replace workers in increasing numbers of functions. The use of robots in military combat raises ethical concerns. The possibilities of robot autonomy and potential repercussions have been addressed in fiction and may be a realistic concern in the future.




The word robot can refer to both physical robots and virtual software agents, but the latter are usually referred to as bots. There is no consensus on which machines qualify as robots but there is general agreement among experts, and the public, that robots tend to possess some or all of the following abilities and functions: accept electronic programming, process data or physical perceptions electronically, operate autonomously to some degree, move around, operate physical parts of itself or physical processes, sense and manipulate their environment, and exhibit intelligent behavior — especially behavior which mimics humans or other animals. Closely related to the concept of a robot is the field of Synthetic Biology, which studies entities whose nature is more comparable to beings than to machines.




The idea of automata originates in the mythologies of many cultures around the world. Engineers and inventors from ancient civilizations, including Ancient China, Ancient Greece, and Ptolemaic Egypt, attempted to build self-operating machines, some resembling animals and humans. Early descriptions of automata include the artificial doves of Archytas, the artificial birds of Mozi and Lu Ban, a "speaking" automaton by Hero of Alexandria, a washstand automaton by Philo of Byzantium, and a human automaton described in the Lie Zi.



Many ancient mythologies, and most modern religions include artificial people, such as the mechanical servants built by the Greek god Hephaestus (Vulcan to the Romans), the clay golems of Jewish legend and clay giants of Norse legend, and Galatea, the mythical statue of Pygmalion that came to life. Since circa 400 BC, myths of Crete include Talos, a man of bronze who guarded the Cretan island of Europa from pirates.

In ancient Greece, the Greek engineer Ctesibius (c. 270 BC) "applied a knowledge of pneumatics and hydraulics to produce the first organ and water clocks with moving figures." In the 4th century BC, the Greek mathematician Archytas of Tarentum postulated a mechanical steam-operated bird he called "The Pigeon". Hero of Alexandria (10–70 AD), a Greek mathematician and inventor, created numerous user-configurable automated devices, and described machines powered by air pressure, steam and water.

The 11th century Lokapannatti tells of how the Buddha's relics were protected by mechanical robots (bhuta vahana yanta), from the kingdom of Roma visaya (Rome); until they were disarmed by King Ashoka.  
In ancient China, the 3rd century text of the Lie Zi describes an account of humanoid automata, involving a much earlier encounter between Chinese emperor King Mu of Zhou and a mechanical engineer known as Yan Shi, an 'artificer'. Yan Shi proudly presented the king with a life-size, human-shaped figure of his mechanical 'handiwork' made of leather, wood, and artificial organs. There are also accounts of flying automata in the Han Fei Zi and other texts, which attributes the 5th century BC Mohist philosopher Mozi and his contemporary Lu Ban with the invention of artificial wooden birds (ma yuan) that could successfully fly. In 1066, the Chinese inventor Su Song built a water clock in the form of a tower which featured mechanical figurines which chimed the hours.

The beginning of automata is associated with the invention of early Su Song's astronomical clock tower featured mechanical figurines that chimed the hours. His mechanism had a programmable drum machine with pegs (cams) that bumped into little levers that operated percussion instruments. The drummer could be made to play different rhythms and different drum patterns by moving the pegs to different locations.
In Renaissance Italy, Leonardo da Vinci (1452–1519) sketched plans for a humanoid robot around 1495. Da Vinci's notebooks, rediscovered in the 1950s, contained detailed drawings of a mechanical knight now known as Leonardo's robot, able to sit up, wave its arms and move its head and jaw. The design was probably based on anatomical research recorded in his Vitruvian Man. It is not known whether he attempted to build it.
In Japan, complex animal and human automata were built between the 17th to 19th centuries, with many described in the 18th century Karakuri zui (Illustrated Machinery, 1796). One such automaton was the karakuri ningyō, a mechanized puppet. Different variations of the karakuri existed: the Butai karakuri, which were used in theatre, the Zashiki karakuri, which were small and used in homes, and the Dashi karakuri which were used in religious festivals, where the puppets were used to perform reenactments of traditional myths and legends.
In France, between 1738 and 1739, Jacques de Vaucanson exhibited several life-sized automatons: a flute player, a pipe player and a duck. The mechanical duck could flap its wings, crane its neck, and swallow food from the exhibitor's hand, and it gave the illusion of digesting its food by excreting matter stored in a hidden compartment.




Remotely operated vehicles were demonstrated in the late 19th Century in the form of several types of remotely controlled torpedoes. The early 1870s saw remotely controlled torpedoes by John Ericsson (pneumatic), John Louis Lay (electric wire guided), and Victor von Scheliha (electric wire guided).
The Brennan torpedo, invented by Louis Brennan in 1877 was powered by two contra-rotating propellors that were spun by rapidly pulling out wires from drums wound inside the torpedo. Differential speed on the wires connected to the shore station allowed the torpedo to be guided to its target, making it "the world's first practical guided missile". In 1897 the British inventor Ernest Wilson was granted a patent for a torpedo remotely controlled by "Hertzian" (radio) waves and in 1898 Nikola Tesla publicly demonstrated a wireless-controlled torpedo that he hoped to sell to the US Navy.
Archibald Low, known as the "father of radio guidance systems" for his pioneering work on guided rockets and planes during the First World War. In 1917, he demonstrated a remote controlled aircraft to the Royal Flying Corps and in the same year built the first wire-guided rocket.




'Robot' was first applied as a term for artificial automata in a 1920 play R.U.R. by the Czech writer, Karel Čapek. However, Josef Čapek was named by his brother Karel as the true inventor of the term robot. The word 'robot' itself was not new, having been in Slavic language as robota (forced laborer), a term which classified those peasants obligated to compulsory service under the feudal system widespread in 19th century Europe (see: Robot Patent). Čapek's fictional story postulated the technological creation of artificial human bodies without souls, and the old theme of the feudal robota class eloquently fit the imagination of a new class of manufactured, artificial workers.



In 1928, one of the first humanoid robots was exhibited at the annual exhibition of the Model Engineers Society in London. Invented by W. H. Richards, the robot Eric's frame consisted of an aluminium body of armour with eleven electromagnets and one motor powered by a twelve-volt power source. The robot could move its hands and head and could be controlled through remote control or voice control.
Westinghouse Electric Corporation built Televox in 1926; it was a cardboard cutout connected to various devices which users could turn on and off. In 1939, the humanoid robot known as Elektro was debuted at the 1939 New York World's Fair. Seven feet tall (2.1 m) and weighing 265 pounds (120.2 kg), it could walk by voice command, speak about 700 words (using a 78-rpm record player), smoke cigarettes, blow up balloons, and move its head and arms. The body consisted of a steel gear, cam and motor skeleton covered by an aluminum skin. In 1928, Japan's first robot, Gakutensoku, was designed and constructed by biologist Makoto Nishimura.



The first electronic autonomous robots with complex behaviour were created by William Grey Walter of the Burden Neurological Institute at Bristol, England in 1948 and 1949. He wanted to prove that rich connections between a small number of brain cells could give rise to very complex behaviors - essentially that the secret of how the brain worked lay in how it was wired up. His first robots, named Elmer and Elsie, were constructed between 1948 and 1949 and were often described as tortoises due to their shape and slow rate of movement. The three-wheeled tortoise robots were capable of phototaxis, by which they could find their way to a recharging station when they ran low on battery power.
Walter stressed the importance of using purely analogue electronics to simulate brain processes at a time when his contemporaries such as Alan Turing and John von Neumann were all turning towards a view of mental processes in terms of digital computation. His work inspired subsequent generations of robotics researchers such as Rodney Brooks, Hans Moravec and Mark Tilden. Modern incarnations of Walter's turtles may be found in the form of BEAM robotics.

The first digitally operated and programmable robot was invented by George Devol in 1954 and was ultimately called the Unimate. This ultimately laid the foundations of the modern robotics industry. Devol sold the first Unimate to General Motors in 1960, and it was installed in 1961 in a plant in Trenton, New Jersey to lift hot pieces of metal from a die casting machine and stack them. Devol’s patent for the first digitally operated programmable robotic arm represents the foundation of the modern robotics industry.
The first palletizing robot was introduced in 1963 by the Fuji Yusoki Kogyo Company. In 1973, a robot with six electromechanically driven axes was patented by KUKA robotics in Germany, and the programmable universal manipulation arm was invented by Victor Scheinman in 1976, and the design was sold to Unimation.
Commercial and industrial robots are now in widespread use performing jobs more cheaply or with greater accuracy and reliability than humans. They are also employed for jobs which are too dirty, dangerous or dull to be suitable for humans. Robots are widely used in manufacturing, assembly and packing, transport, earth and space exploration, surgery, weaponry, laboratory research, and mass production of consumer and industrial goods.




Various techniques have emerged to develop the science of robotics and robots. One method is evolutionary robotics, in which a number of differing robots are submitted to tests. Those which perform best are used as a model to create a subsequent "generation" of robots. Another method is developmental robotics, which tracks changes and development within a single robot in the areas of problem-solving and other functions. Another new type of robot is just recently introduced which acts both as a smartphone and robot and is named RoboHon.
As robots become more advanced, eventually there may be a standard computer operating system designed mainly for robots. Robot Operating System is an open-source set of programs being developed at Stanford University, the Massachusetts Institute of Technology and the Technical University of Munich, Germany, among others. ROS provides ways to program a robot's navigation and limbs regardless of the specific hardware involved. It also provides high-level commands for items like image recognition and even opening doors. When ROS boots up on a robot's computer, it would obtain data on attributes such as the length and movement of robots' limbs. It would relay this data to higher-level algorithms. Microsoft is also developing a "Windows for robots" system with its Robotics Developer Studio, which has been available since 2007.
Japan hopes to have full-scale commercialization of service robots by 2025. Much technological research in Japan is led by Japanese government agencies, particularly the Trade Ministry.
Many future applications of robotics seem obvious to people, even though they are well beyond the capabilities of robots available at the time of the prediction. As early as 1982 people were confident that someday robots would: 1. clean parts by removing molding flash 2. spray paint automobiles with absolutely no human presence 3. pack things in boxes—for example, orient and nest chocolate candies in candy boxes 4. make electrical cable harness 5. load trucks with boxes—a packing problem 6. handle soft goods, such as garments and shoes 7. shear sheep 8. prosthesis 9. cook fast food and work in other service industries 10. household robot.
Generally such predictions are overly optimistic in timescale.



In 2008, Caterpillar Inc. developed a dump truck which can drive itself without any human operator. Many analysts believe that self-driving trucks may eventually revolutionize logistics. By 2014, Caterpillar had a self-driving dump truck which is expected to greatly change the process of mining. In 2015, these Caterpillar trucks were actively used in mining operations in Australia by the mining company Rio Tinto Coal Australia. Some analysts believe that within the next few decades, most trucks will be self-driving.
A literate or 'reading robot' named Marge has intelligence that comes from software. She can read newspapers, find and correct misspelled words, learn about banks like Barclays, and understand that some restaurants are better places to eat than others.
Baxter is a new robot which is different from other industrial robots because it can learn. A worker could teach Baxter how to perform a task by moving its hands in the desired motion and having Baxter memorize them. Extra dials, buttons, and controls are available on Baxter's arm for more precision and features. Any regular worker could program Baxter and it only takes a matter of minutes, unlike usual industrial robots that take extensive programs and coding in order to be used. This means Baxter needs no programming in order to operate. No software engineers are needed. This also means Baxter can be taught to perform multiple, more complicated tasks.




The word robot was introduced to the public by the Czech interwar writer Karel Čapek in his play R.U.R. (Rossum's Universal Robots), published in 1920. The play begins in a factory that uses a chemical substitute for protoplasm to manufacture living, simplified people called robots. The play does not focus in detail on the technology behind the creation of these living creatures, but in their appearance they prefigure modern ideas of androids, creatures who can be mistaken for humans. These mass-produced workers are depicted as efficient but emotionless, incapable of original thinking and indifferent to self-preservation. At issue is whether the robots are being exploited and the consequences of human dependence upon commodified labor (especially after a number of specially-formulated robots achieve self-awareness and incite robots all around the world to rise up against the humans).
Karel Čapek himself did not coin the word. He wrote a short letter in reference to an etymology in the Oxford English Dictionary in which he named his brother, the painter and writer Josef Čapek, as its actual originator.
In an article in the Czech journal Lidové noviny in 1933, he explained that he had originally wanted to call the creatures laboři ("workers", from Latin labor). However, he did not like the word, and sought advice from his brother Josef, who suggested "roboti". The word robota means literally "corvée", "serf labor", and figuratively "drudgery" or "hard work" in Czech and also (more general) "work", "labor" in many Slavic languages (e.g.: Bulgarian, Russian, Serbian, Slovak, Polish, Macedonian, Ukrainian, archaic Czech, as well as robot in Hungarian). Traditionally the robota (Hungarian robot) was the work period a serf (corvée) had to give for his lord, typically 6 months of the year. The origin of the word is the Old Church Slavonic (Old Bulgarian) rabota "servitude" ("work" in contemporary Bulgarian and Russian), which in turn comes from the Proto-Indo-European root *orbh-. Robot is cognate with the German root Arbeit (work).
The word robotics, used to describe this field of study, was coined by the science fiction writer Isaac Asimov. Asimov created the "Three Laws of Robotics" which are a recurring theme in his books. These have since been used by many others to define laws used in fiction. (The three laws are pure fiction, and no technology yet created has the ability to understand or follow them, and in fact most robots serve military purposes, which run quite contrary to the first law and often the third law. "People think about Asimov's laws, but they were set up to point out how a simple ethical system doesn't work. If you read the short stories, every single one is about a failure, and they are totally impractical," said Dr. Joanna Bryson of the University of Bath.)







Mobile robots have the capability to move around in their environment and are not fixed to one physical location. An example of a mobile robot that is in common use today is the automated guided vehicle or automatic guided vehicle (AGV). An AGV is a mobile robot that follows markers or wires in the floor, or uses vision or lasers. AGVs are discussed later in this article.
Mobile robots are also found in industry, military and security environments. They also appear as consumer products, for entertainment or to perform certain tasks like vacuum cleaning. Mobile robots are the focus of a great deal of current research and almost every major university has one or more labs that focus on mobile robot research.
Mobile robots are usually used in tightly controlled environments such as on assembly lines because they have difficulty responding to unexpected interference. Because of this most humans rarely encounter robots. However domestic robots for cleaning and maintenance are increasingly common in and around homes in developed countries. Robots can also be found in military applications.




Industrial robots usually consist of a jointed arm (multi-linked manipulator) and an end effector that is attached to a fixed surface. One of the most common type of end effector is a gripper assembly.
The International Organization for Standardization gives a definition of a manipulating industrial robot in ISO 8373:
"an automatically controlled, reprogrammable, multipurpose, manipulator programmable in three or more axes, which may be either fixed in place or mobile for use in industrial automation applications."
This definition is used by the International Federation of Robotics, the European Robotics Research Network (EURON) and many national standards committees.




Most commonly industrial robots are fixed robotic arms and manipulators used primarily for production and distribution of goods. The term "service robot" is less well-defined. The International Federation of Robotics has proposed a tentative definition, "A service robot is a robot which operates semi- or fully autonomously to perform services useful to the well-being of humans and equipment, excluding manufacturing operations."




Robots are used as educational assistants to teachers. From the 1980s, robots such as turtles were used in schools and programmed using the Logo language.
There are robot kits like Lego Mindstorms, BIOLOID, OLLO from ROBOTIS, or BotBrain Educational Robots can help children to learn about mathematics, physics, programming, and electronics. Robotics have also been introduced into the lives of elementary and high school students in the form of robot competitions with the company FIRST (For Inspiration and Recognition of Science and Technology). The organization is the foundation for the FIRST Robotics Competition, FIRST LEGO League, Junior FIRST LEGO League, and FIRST Tech Challenge competitions.
There have also been devices shaped like robots such as the teaching computer, Leachim (1974), and 2-XL (1976), a robot shaped game / teaching toy based on an 8-track tape player, both invented Michael J. Freeman.




Modular robots are a new breed of robots that are designed to increase the utilization of robots by modularizing their architecture. The functionality and effectiveness of a modular robot is easier to increase compared to conventional robots. These robots are composed of a single type of identical, several different identical module types, or similarly shaped modules, which vary in size. Their architectural structure allows hyper-redundancy for modular robots, as they can be designed with more than 8 degrees of freedom (DOF). Creating the programming, inverse kinematics and dynamics for modular robots is more complex than with traditional robots. Modular robots may be composed of L-shaped modules, cubic modules, and U and H-shaped modules. ANAT technology, an early modular robotic technology patented by Robotics Design Inc., allows the creation of modular robots from U and H shaped modules that connect in a chain, and are used to form heterogeneous and homogenous modular robot systems. These “ANAT robots” can be designed with “n” DOF as each module is a complete motorized robotic system that folds relatively to the modules connected before and after it in its chain, and therefore a single module allows one degree of freedom. The more modules that are connected to one another, the more degrees of freedom it will have. L-shaped modules can also be designed in a chain, and must become increasingly smaller as the size of the chain increases, as payloads attached to the end of the chain place a greater strain on modules that are further from the base. ANAT H-shaped modules do not suffer from this problem, as their design allows a modular robot to distribute pressure and impacts evenly amongst other attached modules, and therefore payload-carrying capacity does not decrease as the length of the arm increases. Modular robots can be manually or self-reconfigured to form a different robot, that may perform different applications. Because modular robots of the same architecture type are composed of modules that compose different modular robots, a snake-arm robot can combine with another to form a dual or quadra-arm robot, or can split into several mobile robots, and mobile robots can split into multiple smaller ones, or combine with others into a larger or different one. This allows a single modular robot the ability to be fully specialized in a single task, as well as the capacity to be specialized to perform multiple different tasks.
Modular robotic technology is currently being applied in hybrid transportation, industrial automation, duct cleaning and handling. Many research centres and universities have also studied this technology, and have developed prototypes.



A collaborative robot or cobot is a robot that can safely and effectively interact with human workers while performing simple industrial tasks. However, end-effectors and other environmental conditions may create hazards, and as such risk assessments should be done before using any industrial motion-control application.
The collaborative robots most widely used in industries today are manufactured by Universal Robots in Denmark.
Rethink Robotics—founded by Rodney Brooks, previously with iRobot—introduced Baxter in September 2012; as an industrial robot designed to safely interact with neighboring human workers, and be programmable for performing simple tasks. Baxters stop if they detect a human in the way of their robotic arms and have prominent off switches. Intended for sale to small businesses, they are promoted as the robotic analogue of the personal computer. As of May 2014, 190 companies in the US have bought Baxters and they are being used commercially in the UK.




Roughly half of all the robots in the world are in Asia, 32% in Europe, and 16% in North America, 1% in Australasia and 1% in Africa. 40% of all the robots in the world are in Japan, making Japan the country with the highest number of robots.




As robots have become more advanced and sophisticated, experts and academics have increasingly explored the questions of what ethics might govern robots' behavior, and whether robots might be able to claim any kind of social, cultural, ethical or legal rights. One scientific team has said that it is possible that a robot brain will exist by 2019. Others predict robot intelligence breakthroughs by 2050. Recent advances have made robotic behavior more sophisticated. The social impact of intelligent robots is subject of a 2010 documentary film called Plug & Pray.
Vernor Vinge has suggested that a moment may come when computers and robots are smarter than humans. He calls this "the Singularity". He suggests that it may be somewhat or possibly very dangerous for humans. This is discussed by a philosophy called Singularitarianism.
In 2009, experts attended a conference hosted by the Association for the Advancement of Artificial Intelligence (AAAI) to discuss whether computers and robots might be able to acquire any autonomy, and how much these abilities might pose a threat or hazard. They noted that some robots have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved "cockroach intelligence." They noted that self-awareness as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls. Various media sources and scientific groups have noted separate trends in differing areas which might together result in greater robotic functionalities and autonomy, and which pose some inherent concerns. In 2015, the Nao alderen robots were shown to have a capability for a degree of self-awareness. Researchers at the Rensselaer Polytechnic Institute AI and Reasoning Lab in New York conducted an experiment where a robot became aware of itself, and corrected its answer to a question once it had realised this.



Some experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions. There are also concerns about technology which might allow some armed robots to be controlled mainly by other robots. The US Navy has funded a report which indicates that, as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions. One researcher states that autonomous robots might be more humane, as they could make decisions more effectively. However, other experts question this.
One robot in particular, the EATR, has generated public concerns over its fuel source, as it can continually refuel itself using organic substances. Although the engine for the EATR is designed to run on biomass and vegetation specifically selected by its sensors, which it can find on battlefields or other local environments, the project has stated that chicken fat can also be used.
Manuel De Landa has noted that "smart missiles" and autonomous bombs equipped with artificial perception can be considered robots, as they make some of their decisions autonomously. He believes this represents an important and dangerous trend in which humans are handing over important decisions to machines.




For centuries, experts have predicted that machines would make workers obsolete and increase unemployment.
A recent example of human replacement involves Taiwanese technology company Foxconn who, in July 2011, announced a three-year plan to replace workers with more robots. At present the company uses ten thousand robots but will increase them to a million robots over a three-year period.
Lawyers have speculated that an increased prevalence of robots in the workplace could lead to the need to revise redundancy laws.




At present, there are two main types of robots, based on their use: general-purpose autonomous robots and dedicated robots.
Robots can be classified by their specificity of purpose. A robot might be designed to perform one particular task extremely well, or a range of tasks less well. Of course, all robots by their nature can be re-programmed to behave differently, but some are limited by their physical form. For example, a factory robot arm can perform jobs such as cutting, welding, gluing, or acting as a fairground ride, while a pick-and-place robot can only populate printed circuit boards.




General-purpose autonomous robots can perform a variety of functions independently. General-purpose autonomous robots typically can navigate independently in known spaces, handle their own re-charging needs, interface with electronic doors and elevators and perform other basic tasks. Like computers, general-purpose robots can link with networks, software and accessories that increase their usefulness. They may recognize people or objects, talk, provide companionship, monitor environmental quality, respond to alarms, pick up supplies and perform other useful tasks. General-purpose robots may perform a variety of functions simultaneously or they may take on different roles at different times of day. Some such robots try to mimic human beings and may even resemble people in appearance; this type of robot is called a humanoid robot. Humanoid robots are still in a very limited stage, as no humanoid robot can, as of yet, actually navigate around a room that it has never been in. Thus, humanoid robots are really quite limited, despite their intelligent behaviors in their well-known environments.






Over the last three decades, automobile factories have become dominated by robots. A typical factory contains hundreds of industrial robots working on fully automated production lines, with one robot for every ten human workers. On an automated production line, a vehicle chassis on a conveyor is welded, glued, painted and finally assembled at a sequence of robot stations.



Industrial robots are also used extensively for palletizing and packaging of manufactured goods, for example for rapidly taking drink cartons from the end of a conveyor belt and placing them into boxes, or for loading and unloading machining centers.



Mass-produced printed circuit boards (PCBs) are almost exclusively manufactured by pick-and-place robots, typically with SCARA manipulators, which remove tiny electronic components from strips or trays, and place them on to PCBs with great accuracy. Such robots can place hundreds of thousands of components per hour, far out-performing a human in speed, accuracy, and reliability.




Mobile robots, following markers or wires in the floor, or using vision or lasers, are used to transport goods around large facilities, such as warehouses, container ports, or hospitals.



Limited to tasks that could be accurately defined and had to be performed the same way every time. Very little feedback or intelligence was required, and the robots needed only the most basic exteroceptors (sensors). The limitations of these AGVs are that their paths are not easily altered and they cannot alter their paths if obstacles block them. If one AGV breaks down, it may stop the entire operation.



Developed to deploy triangulation from beacons or bar code grids for scanning on the floor or ceiling. In most factories, triangulation systems tend to require moderate to high maintenance, such as daily cleaning of all beacons or bar codes. Also, if a tall pallet or large vehicle blocks beacons or a bar code is marred, AGVs may become lost. Often such AGVs are designed to be used in human-free environments.



Such as SmartLoader, SpeciMinder, ADAM, Tug Eskorta, and MT 400 with Motivity are designed for people-friendly workspaces. They navigate by recognizing natural features. 3D scanners or other means of sensing the environment in two or three dimensions help to eliminate cumulative errors in dead-reckoning calculations of the AGV's current position. Some AGVs can create maps of their environment using scanning lasers with simultaneous localization and mapping (SLAM) and use those maps to navigate in real time with other path planning and obstacle avoidance algorithms. They are able to operate in complex environments and perform non-repetitive and non-sequential tasks such as transporting photomasks in a semiconductor lab, specimens in hospitals and goods in warehouses. For dynamic areas, such as warehouses full of pallets, AGVs require additional strategies using three-dimensional sensors such as time-of-flight or stereovision cameras.



There are many jobs which humans would rather leave to robots. The job may be boring, such as domestic cleaning, or dangerous, such as exploring inside a volcano. Other jobs are physically inaccessible, such as exploring another planet, cleaning the inside of a long pipe, or performing laparoscopic surgery.



Almost every unmanned space probe ever launched was a robot. Some were launched in the 1960s with very limited abilities, but their ability to fly and land (in the case of Luna 9) is an indication of their status as a robot. This includes the Voyager probes and the Galileo probes, among others.




Teleoperated robots, or telerobots, are devices remotely operated from a distance by a human operator rather than following a predetermined sequence of movements, but which has semi-autonomous behaviour. They are used when a human cannot be present on site to perform a job because it is dangerous, far away, or inaccessible. The robot may be in another room or another country, or may be on a very different scale to the operator. For instance, a laparoscopic surgery robot allows the surgeon to work inside a human patient on a relatively small scale compared to open surgery, significantly shortening recovery time. They can also be used to avoid exposing workers to the hazardous and tight spaces such as in duct cleaning. When disabling a bomb, the operator sends a small robot to disable it. Several authors have been using a device called the Longpen to sign books remotely. Teleoperated robot aircraft, like the Predator Unmanned Aerial Vehicle, are increasingly being used by the military. These pilotless drones can search terrain and fire on targets. Hundreds of robots such as iRobot's Packbot and the Foster-Miller TALON are being used in Iraq and Afghanistan by the U.S. military to defuse roadside bombs or improvised explosive devices (IEDs) in an activity known as explosive ordnance disposal (EOD).



Robots are used to automate picking fruit on orchards at a cost lower than that of human pickers.




Domestic robots are simple robots dedicated to a single task work in home use. They are used in simple but unwanted jobs, such as vacuum cleaning, floor washing, and lawn mowing. An example of a domestic robot is a Roomba.




Military robots include the SWORDS robot which is currently used in ground-based combat. It can use a variety of weapons and there is some discussion of giving it some degree of autonomy in battleground situations.
Unmanned combat air vehicles (UCAVs), which are an upgraded form of UAVs, can do a wide variety of missions, including combat. UCAVs are being designed such as the BAE Systems Mantis which would have the ability to fly themselves, to pick their own course and target, and to make most decisions on their own. The BAE Taranis is a UCAV built by Great Britain which can fly across continents without a pilot and has new means to avoid detection. Flight trials are expected to begin in 2011.
The AAAI has studied this topic in depth and its president has commissioned a study to look at this issue.
Some have suggested a need to build "Friendly AI", meaning that the advances which are already occurring with AI should also include an effort to make AI intrinsically friendly and humane. Several such measures reportedly already exist, with robot-heavy countries such as Japan and South Korea having begun to pass regulations requiring robots to be equipped with safety systems, and possibly sets of 'laws' akin to Asimov's Three Laws of Robotics. An official report was issued in 2009 by the Japanese government's Robot Industry Policy Committee. Chinese officials and researchers have issued a report suggesting a set of ethical rules, and a set of new legal guidelines referred to as "Robot Legal Studies." Some concern has been expressed over a possible occurrence of robots telling apparent falsehoods.



Mining robots are designed to solve a number of problems currently facing the mining industry, including skills shortages, improving productivity from declining ore grades, and achieving environmental targets. Due to the hazardous nature of mining, in particular underground mining, the prevalence of autonomous, semi-autonomous, and tele-operated robots has greatly increased in recent times. A number of vehicle manufacturers provide autonomous trains, trucks and loaders that will load material, transport it on the mine site to its destination, and unload without requiring human intervention. One of the world's largest mining corporations, Rio Tinto, has recently expanded its autonomous truck fleet to the world's largest, consisting of 150 autonomous Komatsu trucks, operating in Western Australia. Similarly, BHP has announced the expansion of its autonomous drill fleet to the world's largest, 21 autonomous Atlas Copco drills.
Drilling, longwall and rockbreaking machines are now also available as autonomous robots. The Atlas Copco Rig Control System can autonomously execute a drilling plan on a drilling rig, moving the rig into position using GPS, set up the drill rig and drill down to specified depths. Similarly, the Transmin Rocklogic system can automatically plan a path to position a rockbreaker at a selected destination. These systems greatly enhance the safety and efficiency of mining operations.



Robots in healthcare have two main functions. Those which assist an individual, such as a sufferer of a disease like Multiple Sclerosis, and those which aid in the overall systems such as pharmacies and hospitals.




Robots used in home automation have developed over time from simple basic robotic assistants, such as the Handy 1, through to semi-autonomous robots, such as FRIEND which can assist the elderly and disabled with common tasks.
The population is aging in many countries, especially Japan, meaning that there are increasing numbers of elderly people to care for, but relatively fewer young people to care for them. Humans make the best carers, but where they are unavailable, robots are gradually being introduced.
FRIEND is a semi-autonomous robot designed to support disabled and elderly people in their daily life activities, like preparing and serving a meal. FRIEND make it possible for patients who are paraplegic, have muscle diseases or serious paralysis (due to strokes etc.), to perform tasks without help from other people like therapists or nursing staff.




Script Pro manufactures a robot designed to help pharmacies fill prescriptions that consist of oral solids or medications in pill form. The pharmacist or pharmacy technician enters the prescription information into its information system. The system, upon determining whether or not the drug is in the robot, will send the information to the robot for filling. The robot has 3 different size vials to fill determined by the size of the pill. The robot technician, user, or pharmacist determines the needed size of the vial based on the tablet when the robot is stocked. Once the vial is filled it is brought up to a conveyor belt that delivers it to a holder that spins the vial and attaches the patient label. Afterwards it is set on another conveyor that delivers the patient’s medication vial to a slot labeled with the patient's name on an LED read out. The pharmacist or technician then checks the contents of the vial to ensure it’s the correct drug for the correct patient and then seals the vials and sends it out front to be picked up. The robot is a very time efficient device that the pharmacy depends on to fill prescriptions.
McKesson's Robot RX is another healthcare robotics product that helps pharmacies dispense thousands of medications daily with little or no errors. The robot can be ten feet wide and thirty feet long and can hold hundreds of different kinds of medications and thousands of doses. The pharmacy saves many resources like staff members that are otherwise unavailable in a resource scarce industry. It uses an electromechanical head coupled with a pneumatic system to capture each dose and deliver it to its either stocked or dispensed location. The head moves along a single axis while it rotates 180 degrees to pull the medications. During this process it uses barcode technology to verify its pulling the correct drug. It then delivers the drug to a patient specific bin on a conveyor belt. Once the bin is filled with all of the drugs that a particular patient needs and that the robot stocks, the bin is then released and returned out on the conveyor belt to a technician waiting to load it into a cart for delivery to the floor.




While most robots today are installed in factories or homes, performing labour or life saving jobs, many new types of robot are being developed in laboratories around the world. Much of the research in robotics focuses not on specific industrial tasks, but on investigations into new types of robot, alternative ways to think about or design robots, and new ways to manufacture them. It is expected that these new types of robot will be able to solve real world problems when they are finally realized.




One approach to designing robots is to base them on animals. BionicKangaroo was designed and engineered by studying and applying the physiology and methods of locomotion of a kangaroo.




Nanorobotics is the emerging technology field of creating machines or robots whose components are at or close to the microscopic scale of a nanometer (10−9 meters). Also known as "nanobots" or "nanites", they would be constructed from molecular machines. So far, researchers have mostly produced only parts of these complex systems, such as bearings, sensors, and synthetic molecular motors, but functioning robots have also been made such as the entrants to the Nanobot Robocup contest. Researchers also hope to be able to create entire robots as small as viruses or bacteria, which could perform tasks on a tiny scale. Possible applications include micro surgery (on the level of individual cells), utility fog, manufacturing, weaponry and cleaning. Some people have suggested that if there were nanobots which could reproduce, the earth would turn into "grey goo", while others argue that this hypothetical outcome is nonsense.




A few researchers have investigated the possibility of creating robots which can alter their physical form to suit a particular task, like the fictional T-1000. Real robots are nowhere near that sophisticated however, and mostly consist of a small number of cube shaped units, which can move relative to their neighbours. Algorithms have been designed in case any such robots become a reality.



Robots with silicone bodies and flexible actuators (air muscles, electroactive polymers, and ferrofluids) look and feel different from robots with rigid skeletons, and can have different behaviors.




Inspired by colonies of insects such as ants and bees, researchers are modeling the behavior of swarms of thousands of tiny robots which together perform a useful task, such as finding something hidden, cleaning, or spying. Each robot is quite simple, but the emergent behavior of the swarm is more complex. The whole set of robots can be considered as one single distributed system, in the same way an ant colony can be considered a superorganism, exhibiting swarm intelligence. The largest swarms so far created include the iRobot swarm, the SRI/MobileRobots CentiBots project and the Open-source Micro-robotic Project swarm, which are being used to research collective behaviors. Swarms are also more resistant to failure. Whereas one large robot may fail and ruin a mission, a swarm can continue even if several robots fail. This could make them attractive for space exploration missions, where failure is normally extremely costly.




Robotics also has application in the design of virtual reality interfaces. Specialized robots are in widespread use in the haptic research community. These robots, called "haptic interfaces", allow touch-enabled user interaction with real and virtual environments. Robotic forces allow simulating the mechanical properties of "virtual" objects, which users can experience through their sense of touch.







Robotic characters, androids (artificial men/women) or gynoids (artificial women), and cyborgs (also "bionic men/women", or humans with significant mechanical enhancements) have become a staple of science fiction.
The first reference in Western literature to mechanical servants appears in Homer's Iliad. In Book XVIII, Hephaestus, god of fire, creates new armor for the hero Achilles, assisted by robots. According to the Rieu translation, "Golden maidservants hastened to help their master. They looked like real women and could not only speak and use their limbs but were endowed with intelligence and trained in handwork by the immortal gods." Of course, the words "robot" or "android" are not used to describe them, but they are nevertheless mechanical devices human in appearance. "The first use of the word Robot was in Karel Čapek's play R.U.R. (Rossum's Universal Robots) (written in 1920)". Writer Karel Čapek was born in Czechoslovakia (Czech Republic).
Possibly the most prolific author of the twentieth century was Isaac Asimov (1920–1992) who published over five-hundred books. Asimov is probably best remembered for his science-fiction stories and especially those about robots, where he placed robots and their interaction with society at the center of many of his works. Asimov carefully considered the problem of the ideal set of instructions robots might be given in order to lower the risk to humans, and arrived at his Three Laws of Robotics: a robot may not injure a human being or, through inaction, allow a human being to come to harm; a robot must obey orders given it by human beings, except where such orders would conflict with the First Law; and a robot must protect its own existence as long as such protection does not conflict with the First or Second Law. These were introduced in his 1942 short story "Runaround", although foreshadowed in a few earlier stories. Later, Asimov added the Zeroth Law: "A robot may not harm humanity, or, by inaction, allow humanity to come to harm"; the rest of the laws are modified sequentially to acknowledge this.
According to the Oxford English Dictionary, the first passage in Asimov's short story "Liar!" (1941) that mentions the First Law is the earliest recorded use of the word robotics. Asimov was not initially aware of this; he assumed the word already existed by analogy with mechanics, hydraulics, and other similar terms denoting branches of applied knowledge.




Robots appear in many films. Most of the robots in cinema are fictional. Two of the most famous are R2-D2 and C-3PO from the Star Wars franchise.




The concept of humanoid sex robots has elicited both public attention and concern. Opponents of the concept have stated that the development of sex robots would be morally wrong. They argue that the introduction of such devices would be socially harmful, and demeaning to women and children.




Fears and concerns about robots have been repeatedly expressed in a wide range of books and films. A common theme is the development of a master race of conscious and highly intelligent robots, motivated to take over or destroy the human race. Frankenstein (1818), often called the first science fiction novel, has become synonymous with the theme of a robot or android advancing beyond its creator.
Other works with similar themes include The Mechanical Man, The Terminator, Runaway, RoboCop, the Replicators in Stargate, the Cylons in Battlestar Galactica, the Cybermen and Daleks in Doctor Who, The Matrix, Enthiran and I, Robot. Some fictional robots are programmed to kill and destroy; others gain superhuman intelligence and abilities by upgrading their own software and hardware. Examples of popular media where the robot becomes evil are 2001: A Space Odyssey, Red Planet and Enthiran.
Another common theme is the reaction, sometimes called the "uncanny valley", of unease and even revulsion at the sight of robots that mimic humans too closely.
More recently, fictional representations of artificially intelligent robots in films such as A.I. Artificial Intelligence and Ex Machina and the 2016 TV adaptation of Westworld have engaged audience sympathy for the robots themselves.




Index of robotics articles
Outline of robotics
Artificial intelligence
William Grey Walter



Robot locomotion
Simultaneous localization and mapping
Tactile sensor
Teleoperation
von Neumann machine
Wake-up robot problem



Cognitive robotics
Domestic robot
Epigenetic robotics
Evolutionary robotics
Humanoid robot
Microbotics
Robot control



AIBO
Autonomous spaceport drone ship
Driverless car
Friendly Robotics
Lely Juno family
Liquid handling robot
PatrolBot
RoboBee
Robot App Store






Čapek, Karel (1920). R.U.R., Aventinum, Prague.
Glaser, Horst Albert and Rossbach, Sabine: The Artificial Human, Frankfurt/M., Bern, New York 2011 "The Artificial Human"
TechCast Article Series, Jason Rupinski and Richard Mix, "Public Attitudes to Androids: Robot Gender, Tasks, & Pricing"
Cheney, Margaret [1989:123] (1981). Tesla, Man Out of Time. Dorset Press. New York. ISBN 0-88029-419-1
Craig, J.J. (2005). Introduction to Robotics, Pearson Prentice Hall. Upper Saddle River, NJ.
Gutkind, L. (2006). Almost Human: Making Robots Think. New York: W. W. Norton & Company, Inc.
Needham, Joseph (1986). Science and Civilization in China: Volume 2. Taipei: Caves Books Ltd.
Sotheby's New York. The Tin Toy Robot Collection of Matt Wyse (1996)
Tsai, L. W. (1999). Robot Analysis. Wiley. New York.
DeLanda, Manuel. War in the Age of Intelligent Machines. 1991. Swerve. New York.
Journal of Field Robotics




Robotics at DMOZCommunication (from Latin commūnicāre, meaning "to share") is the act of conveying intended meanings from one entity or group to another through the use of mutually understood signs and semiotic rules.
The basic steps of communication are:
The forming of communicative intent.
Message composition.
Message encoding and decoding.
Transmission of the encoded message as a sequence of signals using a specific channel or medium.
Reception of signals.
Reconstruction of the original message.
Interpretation and making sense of the reconstructed message.
The study of communication can be divided into:
Information theory which studies the quantification, storage, and communication of information in general;
Communication studies which concerns human communication;
Biosemiotics which examines the communication of organisms in general.
The channel of communication can be visual, auditory, tactile (such as in Braille) and haptic, olfactory, Kinesics, electromagnetic, or biochemical. Human communication is unique for its extensive use of abstract language.




Nonverbal communication describes the process of conveying meaning in the form of non-word messages. Examples of nonverbal communication include haptic communication, chronemic communication, gestures, body language, facial expressions, eye contact, and how one dresses. Nonverbal communication also relates to intent of a message. Examples of intent are voluntary, intentional movements like shaking a hand or winking, as well as involuntary, such as sweating. Speech also contains nonverbal elements known as paralanguage, e.g. rhythm, intonation, tempo, and stress. There may even be a pheromone component. Research has shown that up to 55% of human communication may occur through non-verbal facial expressions, and a further 38% through para-language. It affects communication most at the subconscious level and establishes trust. Likewise, written texts include nonverbal elements such as handwriting style, spatial arrangement of words and the use of emoticons to convey emotion.
Nonverbal communication demonstrates one of Wazlawick's laws: you cannot not communicate. Once proximity has formed awareness, living creatures begin interpreting any signals received. Some of the functions of nonverbal communication in humans are to complement and illustrate, to reinforce and emphasize, to replace and substitute, to control and regulate, and to contradict the denovative message.



Verbal communication is the spoken conveying of message. Human language can be defined as a system of symbols (sometimes known as lexemes) and the grammars (rules) by which the symbols are manipulated. The word "language" also refers to common properties of languages. Language learning normally occurs most intensively during human childhood. Most of the thousands of human languages use patterns of sound or gesture for symbols which enable communication with others around them. Languages tend to share certain properties, although there are exceptions. There is no defined line between a language and a dialect. Constructed languages such as Esperanto, programming languages, and various mathematical formalism is not necessarily restricted to the properties shared by human languages.



Over time the forms of and ideas about communication have evolved through the continuing progression of technology. Advances include communications psychology and media psychology, an emerging field of study.
The progression of written communication can be divided into three "information communication revolutions":
Written communication first emerged through the use of pictographs. The pictograms were made in stone, hence written communication was not yet mobile. Pictograms began to develop standardized and simplified forms.
The next step occurred when writing began to appear on paper, papyrus, clay, wax, and other media with common shared writing systems, leading to adaptable alphabets. Communication became mobile.
The final stage is characterized by the transfer of information through controlled waves of electromagnetic radiation (i.e., radio, microwave, infrared) and other electronic signals.
Communication is thus a process by which meaning is assigned and conveyed in an attempt to create shared understanding. Gregory Bateson called it "the replication of tautologies in the universe. This process, which requires a vast repertoire of skills in interpersonal processing, listening, observing, speaking, questioning, analyzing, gestures, and evaluating enables collaboration and cooperation.




Business communication is used for a wide variety of activities including, but not limited to: strategic communications planning, media relations, public relations (which can include social media, broadcast and written communications, and more), brand management, reputation management, speech-writing, customer-client relations, and internal/employee communications.
Companies with limited resources may choose to engage in only a few of these activities, while larger organizations may employ a full spectrum of communications. Since it is difficult to develop such a broad range of skills, communications professionals often specialize in one or two of these areas but usually have at least a working knowledge of most of them. By far, the most important qualifications communications professionals can possess are excellent writing ability, good 'people' skills, and the capacity to think critically and strategically.



Communication is one of the most relevant tools in political strategies, including persuasion and propaganda. In mass media research and online media research, the effort of strategist is that of getting a precise decoding, avoiding "message reactance", that is, message refusal. The reaction to a message is referred also in terms of approach to a message, as follows:
In "radical reading" the audience rejects the meanings, values, and viewpoints built into the text by its makers. Effect: message refusal.
In "dominant reading", the audience accepts the meanings, values, and viewpoints built into the text by its makers. Effect: message acceptance.
In "subordinate reading" the audience accepts, by and large, the meanings, values, and worldview built into the text by its makers. Effect: obey to the message.
Holistic approaches are used by communication campaign leaders and communication strategists in order to examine all the options, "actors" and channels that can generate change in the semiotic landscape, that is, change in perceptions, change in credibility, change in the "memetic background", change in the image of movements, of candidates, players and managers as perceived by key influencers that can have a role in generating the desired "end-state". As the European communication researcher Daniele Trevisani highlights, the shift of political communication is moving from a "mass media" approach to a holistic and semiotic approach for each specific end-state, so that, in a "holistic communication perspective", any tool becomes a potential communication tool in shaping the "infosphere" (the information environment that surrounds us). The modern political communication field is highly influenced by the framework and practices of "information operations" doctrines that derive their nature from strategic and military studies. According to this view, what is really relevant is the concept of acting on the Information Environment. The information environment is the aggregate of individuals, organizations, and systems that collect, process, disseminate, or act on information. This environment consist s of three interrelated dimensions, which continuously interact with individuals, organizations, and systems. These dimensions are known as physical, informational, and cognitive.



Family communication is the study of the communication perspective in a broadly defined family, with intimacy and trusting relationship. The main goal of family communication is to understand the interactions of family and the pattern of behaviors of family members in different circumstances. Open and honest communication creates an atmosphere that allows family members to express their differences as well as love and admiration for one another. It also helps to understand the feelings of one another.
Family communication study looks at topics such as family rules, family roles or family dialectics and how those factors could affect the communication between family members. Researchers develop theories to understand communication behaviors. Family communication study also digs deep into certain time periods of family life such as marriage, parenthood or divorce and how communication stands in those situations. It is important for family members to understand communication as a trusted way which leads to a well constructed family.



In simple terms, interpersonal communication is the communication between one person and another (or others). It is often referred to as face-to-face communication between two (or more) people. Both verbal and nonverbal communication, or body language, play a part in how one person understands another. In verbal interpersonal communication there are two types of messages being sent: a content message and a relational message. Content messages are messages about the topic at hand and relational messages are messages about the relationship itself. This means that relational messages come across in how one says something and it demonstrates a person’s feelings, whether positive or negative, towards the individual they are talking to, indicating not only how they feel about the topic at hand, but also how they feel about their relationship with the other individual.
When texting or posting something on social media the relational message is lost and can cause people to misinterpret the message. Computer-mediated communication is a largely studied topic for this reason along with many others. In the field of Interpersonal communication research, a specific model, the Four-Distances Model of Communication, describes the four main variables that can generate "relational distance" in interpersonal communication, as opposed to a sense of "closeness" or in relational terms. The variables are Role differences, Communication Codes differences, Value and Ideological differences, and Experiential differences (personal history differences and differences in personal emotional history).
The model has been used also to study interpersonal communication problems occurred in space crews inside the International Space Station, and in other cases where interpersonal communication played a critical role in the outcome of crisis events, as in the Costa Concordia disaster.



Barriers to effective communication can retard or distort the message and intention of the message being conveyed which may result in failure of the communication process or an effect that is undesirable. These include filtering, selective perception, information overload, emotions, language, silence, communication apprehension, gender differences and political correctness
This also includes a lack of expressing "knowledge-appropriate" communication, which occurs when a person uses ambiguous or complex legal words, medical jargon, or descriptions of a situation or environment that is not understood by the recipient.
Physical barriers- Physical barriers are often due to the nature of the environment. An example of this is the natural barrier which exists if staff are located in different buildings or on different sites. Likewise, poor or outdated equipment, particularly the failure of management to introduce new technology, may also cause problems. Staff shortages are another factor which frequently causes communication difficulties for an organization.
System design- System design faults refer to problems with the structures or systems in place in an organization. Examples might include an organizational structure which is unclear and therefore makes it confusing to know whom to communicate with. Other examples could be inefficient or inappropriate information systems, a lack of supervision or training, and a lack of clarity in roles and responsibilities which can lead to staff being uncertain about what is expected of them.
Attitudinal barriers- Attitudinal barriers come about as a result of problems with staff in an organization. These may be brought about, for example, by such factors as poor management, lack of consultation with employees, personality conflicts which can result in people delaying or refusing to communicate, the personal attitudes of individual employees which may be due to lack of motivation or dissatisfaction at work, brought about by insufficient training to enable them to carry out particular tasks, or simply resistance to change due to entrenched attitudes and ideas.
Ambiguity of words/phrases- Words sounding the same but having different meaning can convey a different meaning altogether. Hence the communicator must ensure that the receiver receives the same meaning. It is better if such words are avoided by using alternatives whenever possible.
Individual linguistic ability- The use of jargon, difficult or inappropriate words in communication can prevent the recipients from understanding the message. Poorly explained or misunderstood messages can also result in confusion. However, research in communication has shown that confusion can lend legitimacy to research when persuasion fails.
Physiological barriers- These may result from individuals' personal discomfort, caused—for example—by ill health, poor eyesight or hearing difficulties.
Bypassing-These happens when the communicators (sender and the receiver) do not attach the same symbolic meanings to their words. It is when the sender is expressing a thought or a word but the receiver take it in a different meaning. For example- ASAP, Rest room
Technological multi-tasking and absorbency- With a rapid increase in technologically-driven communication in the past several decades, individuals are increasingly faced with condensed communication in the form of e-mail, text, and social updates. This has, in turn, led to a notable change in the way younger generations communicate and perceive their own self-efficacy to communicate and connect with others. With the ever-constant presence of another "world" in one's pocket, individuals are multi-tasking both physically and cognitively as constant reminders of something else happening somewhere else bombard them. Though perhaps too new of an advancement to yet see long-term effects, this is a notion currently explored by such figures as Sherry Turkle.
Fear of being criticized-This is a major factor that prevents good communication. If we exercise simple practices to improve our communication skill, we can become effective communicators. For example, read an article from the newspaper or collect some news from the television and present it in front of the mirror. This will not only boost your confidence, but also improve your language and vocabulary.
Gender barriers- Most communicators whether aware or not, often have a set agenda. This is very notable among the different genders. For example, many women are found to be more critical in addressing conflict. It's also been noted that men are more than likely to withdraw from conflict when in comparison to women. This breakdown and comparison not only shows that there are many factors to communication between two specific genders, but also room for improvement as well as established guidelines for all.



Cultural differences exist within countries (tribal/regional differences, dialects etc.), between religious groups and in organisations or at an organisational level - where companies, teams and units may have different expectations, norms and idiolects. Families and family groups may also experience the effect of cultural barriers to communication within and between different family members or groups. For example: words, colours and symbols have different meanings in different cultures. In most parts of the world, nodding your head means agreement, shaking your head means no, except in some parts of the world.
Communication to a great extent is influenced by culture and cultural variables. Understanding cultural aspects of communication refers to having knowledge of different cultures in order to communicate effectively with cross culture people. Cultural aspects of communication are of great relevance in today's world which is now a global village, thanks to globalisation. Cultural aspects of communication are the cultural differences which influences communication across borders. Impact of cultural differences on communication components are explained below:
1) Verbal communication refers to form of communication which uses spoken and written words for expressing and transferring views and ideas. Language is the most important tool of verbal communication and it is the area where cultural difference play its role. All countries have different languages and to have a better understanding of different culture it is required to have knowledge of languages of different countries.
2) Non verbal communication is a very wide concept and it includes all the other forms of communication which do not uses written or spoken words. Non verbal communication takes following forms:
Paralinguistics are the voice involved in communication other than actual language and involves tones, pitch, vocal cues etc. It also include sounds from throat and all these are greatly influenced by cultural differences across borders.
Proxemics deals with the concept of space element in communication. Proxemics explains four zones of spaces namely intimate personal, social and public. This concept differs with different culture as the permissible space vary in different countries.
Artifactics studies about the non verbal signals or communication which emerges from personal accessories such as dresses or fashion accessories worn and it varies with culture as people of different countries follow different dressing codes.
Chronemics deal with the time aspects of communication and also include importance given to the time. some issues explaining this conceptpt are pauses, silences and response lag during an interaction. This aspect of communication is also influenced by cultural differences as it is well known that there is a great difference in the value given by different cultures to time.
Kinesics mainly deals with the body languages such as postures, gestures, head nods, leg movements etc. In different countries, the same gestures and postures are used to convey different messages. Sometimes even a particular kinesic indicating something good in a country may have a negative meaning in any other culture.
So in order to have an effective communication across world it is desirable to have a knowledge of cultural variables effecting communication.
According to Michael Walsh and Ghil'ad Zuckermann, Western conversational interaction is typically "dyadic", between two particular people, where eye contact is important and the speaker controls the interaction; and "contained" in a relatively short, defined time frame. However, traditional Aboriginal conversational interaction is "communal", broadcast to many people, eye contact is not important, the listener controls the interaction; and "continuous", spread over a longer, indefinite time frame.




Every information exchange between living organisms — i.e. transmission of signals that involve a living sender and receiver can be considered a form of communication; and even primitive creatures such as corals are competent to communicate. Nonhuman communication also include cell signaling, cellular communication, and chemical transmissions between primitive organisms like bacteria and within the plant and fungal kingdoms.



The broad field of animal communication encompasses most of the issues in ethology. Animal communication can be defined as any behavior of one animal that affects the current or future behavior of another animal. The study of animal communication, called zoo semiotics (distinguishable from anthroposemiotics, the study of human communication) has played an important part in the development of ethology, sociobiology, and the study of animal cognition. Animal communication, and indeed the understanding of the animal world in general, is a rapidly growing field, and even in the 21st century so far, a great share of prior understanding related to diverse fields such as personal symbolic name use, animal emotions, animal culture and learning, and even sexual conduct, long thought to be well understood, has been revolutionized. A special field of animal communication has been investigated in more detail such as vibrational communication.



Communication is observed within the plant organism, i.e. within plant cells and between plant cells, between plants of the same or related species, and between plants and non-plant organisms, especially in the root zone. Plant roots communicate with rhizome bacteria, fungi, and insects within the soil. These interactions are governed by syntactic, pragmatic, and semantic rules, and are possible because of the decentralized "nervous system" of plants. The original meaning of the word "neuron" in Greek is "vegetable fiber" and recent research has shown that most of the microorganism plant communication processes are neuron-like. Plants also communicate via volatiles when exposed to herbivory attack behavior, thus warning neighboring plants. In parallel they produce other volatiles to attract parasites which attack these herbivores. In stress situations plants can overwrite the genomes they inherited from their parents and revert to that of their grand- or great-grandparents.
Fungi communicate to coordinate and organize their growth and development such as the formation of Marcelia and fruiting bodies. Fungi communicate with their own and related species as well as with non fungal organisms in a great variety of symbiotic interactions, especially with bacteria, unicellular eukaryote, plants and insects through biochemicals of biotic origin. The biochemicals trigger the fungal organism to react in a specific manner, while if the same chemical molecules are not part of biotic messages, they do not trigger the fungal organism to react. This implies that fungal organisms can differentiate between molecules taking part in biotic messages and similar molecules being irrelevant in the situation. So far five different primary signalling molecules are known to coordinate different behavioral patterns such as filamentation, mating, growth, and pathogenicity. Behavioral coordination and production of signaling substances is achieved through interpretation processes that enables the organism to differ between self or non-self, a biotic indicator, biotic message from similar, related, or non-related species, and even filter out "noise", i.e. similar molecules without biotic content.



Communication is not a tool used only by humans, plants and animals, but it is also used by microorganisms like bacteria. The process is called quorum sensing. Through quorum sensing, bacteria are able to sense the density of cells, and regulate gene expression accordingly. This can be seen in both gram positive and gram negative bacteria. This was first observed by Fuqua et al. in marine microorganisms like V. harveyi and V. fischeri.




The first major model for communication was introduced by Claude Shannon and Warren Weaver for Bell Laboratories in 1949 The original model was designed to mirror the functioning of radio and telephone technologies. Their initial model consisted of three primary parts: sender, channel, and receiver. The sender was the part of a telephone a person spoke into, the channel was the telephone itself, and the receiver was the part of the phone where one could hear the other person. Shannon and Weaver also recognized that often there is static that interferes with one listening to a telephone conversation, which they deemed noise.
In a simple model, often referred to as the transmission model or standard view of communication, information or content (e.g. a message in natural language) is sent in some form (as spoken language) from an emisor/ sender/ encoder to a destination/ receiver/ decoder. This common conception of communication simply views communication as a means of sending and receiving information. The strengths of this model are simplicity, generality, and quantifiability. Claude Shannon and Warren Weaver structured this model based on the following elements:
An information source, which produces a message.
A transmitter, which encodes the message into signals
A channel, to which signals are adapted for transmission
A noise source, which distorts the signal while it propagates through the channel
A receiver, which 'decodes' (reconstructs) the message from the signal.
A destination, where the message arrives.
Shannon and Weaver argued that there were three levels of problems for communication within this theory.
The technical problem: how accurately can the message be transmitted?
The semantic problem: how precisely is the meaning 'conveyed'?
The effectiveness problem: how effectively does the received meaning affect behavior?
Daniel Chandler critiques the transmission model by stating:
It assumes communicators are isolated individuals.
No allowance for differing purposes.
No allowance for differing interpretations.
No allowance for unequal power relations.
No allowance for situational contexts.
In 1960, David Berlo expanded on Shannon and Weaver's (1949) linear model of communication and created the SMCR Model of Communication. The Sender-Message-Channel-Receiver Model of communication separated the model into clear parts and has been expanded upon by other scholars.
Communication is usually described along a few major dimensions: Message (what type of things are communicated), source / emisor / sender / encoder (by whom), form (in which form), channel (through which medium), destination / receiver / target / decoder (to whom), and Receiver. Wilbur Schram (1954) also indicated that we should also examine the impact that a message has (both desired and undesired) on the target of the message. Between parties, communication includes acts that confer knowledge and experiences, give advice and commands, and ask questions. These acts may take many forms, in one of the various manners of communication. The form depends on the abilities of the group communicating. Together, communication content and form make messages that are sent towards a destination. The target can be oneself, another person or being, another entity (such as a corporation or group of beings).
Communication can be seen as processes of information transmission with three levels of semiotic rules:
Pragmatic (concerned with the relations between signs/expressions and their users)
Semantic (study of relationships between signs and symbols and what they represent) and
Syntactic (formal properties of signs and symbols).
Therefore, communication is social interaction where at least two interacting agents share a common set of signs and a common set of semiotic rules. This commonly held rule in some sense ignores autocommunication, including intrapersonal communication via diaries or self-talk, both secondary phenomena that followed the primary acquisition of communicative competences within social interactions.
In light of these weaknesses, Barnlund (2008) proposed a transactional model of communication. The basic premise of the transactional model of communication is that individuals are simultaneously engaging in the sending and receiving of messages.
In a slightly more complex form a sender and a receiver are linked reciprocally. This second attitude of communication, referred to as the constitutive model or constructionist view, focuses on how an individual communicates as the determining factor of the way the message will be interpreted. Communication is viewed as a conduit; a passage in which information travels from one individual to another and this information becomes separate from the communication itself. A particular instance of communication is called a speech act. The sender's personal filters and the receiver's personal filters may vary depending upon different regional traditions, cultures, or gender; which may alter the intended meaning of message contents. In the presence of "communication noise" on the transmission channel (air, in this case), reception and decoding of content may be faulty, and thus the speech act may not achieve the desired effect. One problem with this encode-transmit-receive-decode model is that the processes of encoding and decoding imply that the sender and receiver each possess something that functions as a codebook, and that these two code books are, at the very least, similar if not identical. Although something like code books is implied by the model, they are nowhere represented in the model, which creates many conceptual difficulties.
Theories of coregulation describe communication as a creative and dynamic continuous process, rather than a discrete exchange of information. Canadian media scholar Harold Innis had the theory that people use different types of media to communicate and which one they choose to use will offer different possibilities for the shape and durability of society (Wark, McKenzie 1997). His famous example of this is using ancient Egypt and looking at the ways they built themselves out of media with very different properties stone and papyrus. Papyrus is what he called 'Space Binding'. it made possible the transmission of written orders across space, empires and enables the waging of distant military campaigns and colonial administration. The other is stone and 'Time Binding', through the construction of temples and the pyramids can sustain their authority generation to generation, through this media they can change and shape communication in their society (Wark, McKenzie 1997).



In any communication model, noise is interference with the decoding of messages sent over a channel by an encoder. There are many examples of noise:
Environmental noise. Noise that physically disrupts communication, such as standing next to loud speakers at a party, or the noise from a construction site next to a classroom making it difficult to hear the professor.
Physiological-impairment noise. Physical maladies that prevent effective communication, such as actual deafness or blindness preventing messages from being received as they were intended.
Semantic noise. Different interpretations of the meanings of certain words. For example, the word "weed" can be interpreted as an undesirable plant in a yard, or as a euphemism for marijuana.
Syntactical noise. Mistakes in grammar can disrupt communication, such as abrupt changes in verb tense during a sentence.
Organizational noise. Poorly structured communication can prevent the receiver from accurate interpretation. For example, unclear and badly stated directions can make the receiver even more lost.
Cultural noise. Stereotypical assumptions can cause misunderstandings, such as unintentionally offending a non-Christian person by wishing them a "Merry Christmas".
Psychological noise. Certain attitudes can also make communication difficult. For instance, great anger or sadness may cause someone to lose focus on the present moment. Disorders such as autism may also severely hamper effective communication.
To face communication noise, redundancy and acknowledgement must often be used. Acknowledgements are messages from the addressee informing the originator that his/her communication has been received and is understood. Message repetition and feedback about message received are necessary in the presence of noise to reduce the probability of misunderstanding.







Advice
Augmentative and alternative communication
Communication rights
Data communication
Four Cs of 21st century learning
Human communication
Inter Mirifica
Intercultural communication
Ishin-denshin
Proactive communications
Sign system
Small talk
SPEAKING
Telecommunication
Telepathy
Understanding
21st century skills
Assertion Theory






Innis, Harold. Empire and Communications. Rev. by Mary Q. Innis; foreword by Marshall McLuhan. Toronto, Ont.: University of Toronto Press, 1972. xii, 184 p. N.B.: "Here he [i.e. Innis] develops his theory that the history of empires is determined to a large extent by their means of communication."—From the back cover of the book's pbk. ed. ISBN 0-8020-6119-2 pbkInformation is that which informs. In other words, it is the answer to a question of some kind. It is thus related to data and knowledge, as data represents values attributed to parameters, and knowledge signifies understanding of real things or abstract concepts. As it regards data, the information's existence is not necessarily coupled to an observer (it exists beyond an event horizon, for example), while in the case of knowledge, the information requires a cognitive observer.
At its most fundamental, information is any propagation of cause and effect within a system. Information is conveyed either as the content of a message or through direct or indirect observation of anything. That which is perceived can be construed as a message in its own right, and in that sense, information is always conveyed as the content of a message.
Information can be encoded into various forms for transmission and interpretation (for example, information may be encoded into a sequence of signs, or transmitted via a sequence of signals). It can also be encrypted for safe storage and communication.
Information reduces uncertainty. The uncertainty of an event is measured by its probability of occurrence and is inversely proportional to that. The more uncertain an event, the more information is required to resolve uncertainty of that event. The bit is a typical unit of information, but other units such as the nat may be used. Example: information in one "fair" coin ﬂip: log2(2/1)  2 bits.
The concept that information is the message has different meanings in different contexts. Thus the concept of information becomes closely related to notions of constraint, communication, control, data, form, education, knowledge, meaning, understanding, mental stimuli, pattern, perception, representation, and entropy.




The English word was apparently derived from the Latin stem (information-) of the nominative (informatio): this noun is derived from the verb informare (to inform) in the sense of "to give form to the mind", "to discipline", "instruct", "teach". Inform itself comes (via French informer) from the Latin verb informare, which means to give form, or to form an idea of. Furthermore, Latin itself already contained the word informatio meaning concept or idea, but the extent to which this may have influenced the development of the word information in English is not clear.
The ancient Greek word for form was μορφή (morphe; cf. morph) and also εἶδος (eidos) "kind, idea, shape, set", the latter word was famously used in a technical philosophical sense by Plato (and later Aristotle) to denote the ideal identity or essence of something (see Theory of Forms). "Eidos" can also be associated with thought, proposition, or even concept.
The ancient Greek word for information is πληροφορία, which transliterates (plērophoria) from πλήρης (plērēs) "fully" and φέρω (phorein) frequentative of (pherein) to carry-through. It literally means "fully bears" or "conveys fully". In modern Greek language the word Πληροφορία is still in daily use and has the same meaning as the word information in English. In addition to its primary meaning, the word Πληροφορία as a symbol has deep roots in Aristotle's semiotic triangle. In this regard it can be interpreted to communicate information to the one decoding that specific type of sign. This is something that occurs frequently with the etymology of many words in ancient and modern Greek language where there is a very strong denotative relationship between the signifier, e.g. the word symbol that conveys a specific encoded interpretation, and the signified, e.g. a concept whose meaning the interpreter attempts to decode.




From the stance of information theory, information is taken as an ordered sequence of symbols from an alphabet, say an input alphabet χ, and an output alphabet ϒ. Information processing consists of an input-output function that maps any input sequence from χ into an output sequence from ϒ. The mapping may be probabilistic or deterministic. It may have memory or be memoryless.



Often information can be viewed as a type of input to an organism or system. Inputs are of two kinds; some inputs are important to the function of the organism (for example, food) or system (energy) by themselves. In his book Sensory Ecology Dusenbery called these causal inputs. Other inputs (information) are important only because they are associated with causal inputs and can be used to predict the occurrence of a causal input at a later time (and perhaps another place). Some information is important because of association with other information but eventually there must be a connection to a causal input. In practice, information is usually carried by weak stimuli that must be detected by specialized sensory systems and amplified by energy inputs before they can be functional to the organism or system. For example, light is often a causal input to plants but provides information to animals. The colored light reflected from a flower is too weak to do much photosynthetic work but the visual system of the bee detects it and the bee's nervous system uses the information to guide the bee to the flower, where the bee often finds nectar or pollen, which are causal inputs, serving a nutritional function.



The cognitive scientist and applied mathematician Ronaldo Vigo argues that information is a concept that involves at least two related entities in order to make quantitative sense. These are, any dimensionally defined category of objects S, and any of its subsets R. R, in essence, is a representation of S, or, in other words, conveys representational (and hence, conceptual) information about S. Vigo then defines the amount of information that R conveys about S as the rate of change in the complexity of S whenever the objects in R are removed from S. Under "Vigo information", pattern, invariance, complexity, representation, and information—five fundamental constructs of universal science—are unified under a novel mathematical framework. Among other things, the framework aims to overcome the limitations of Shannon-Weaver information when attempting to characterize and measure subjective information.



Information is any type of pattern that influences the formation or transformation of other patterns. In this sense, there is no need for a conscious mind to perceive, much less appreciate, the pattern. Consider, for example, DNA. The sequence of nucleotides is a pattern that influences the formation and development of an organism without any need for a conscious mind.
Systems theory at times seems to refer to information in this sense, assuming information does not necessarily involve any conscious mind, and patterns circulating (due to feedback) in the system can be called information. In other words, it can be said that information in this sense is something potentially perceived as representation, though not created or presented for that purpose. For example, Gregory Bateson defines "information" as a "difference that makes a difference".
If, however, the premise of "influence" implies that information has been perceived by a conscious mind and also interpreted by it, the specific context associated with this interpretation may cause the transformation of the information into knowledge. Complex definitions of both "information" and "knowledge" make such semantic and logical analysis difficult, but the condition of "transformation" is an important point in the study of information as it relates to knowledge, especially in the business discipline of knowledge management. In this practice, tools and processes are used to assist a knowledge worker in performing research and making decisions, including steps such as:
reviewing information in order to effectively derive value and meaning
referencing metadata if any is available
establishing a relevant context, often selecting from many possible contexts
deriving new knowledge from the information
making decisions or recommendations from the resulting knowledge.
Stewart (2001) argues that the transformation of information into knowledge is a critical one, lying at the core of value creation and competitive advantage for the modern enterprise.
The Danish Dictionary of Information Terms argues that information only provides an answer to a posed question. Whether the answer provides knowledge depends on the informed person. So a generalized definition of the concept should be: "Information" = An answer to a specific question".
When Marshall McLuhan speaks of media and their effects on human cultures, he refers to the structure of artifacts that in turn shape our behaviors and mindsets. Also, pheromones are often said to be "information" in this sense.




Information has a well-defined meaning in physics. In 2003 J. D. Bekenstein claimed that a growing trend in physics was to define the physical world as being made up of information itself (and thus information is defined in this way) (see Digital physics). Examples of this include the phenomenon of quantum entanglement, where particles can interact without reference to their separation or the speed of light. Material information itself cannot travel faster than light even if that information is transmitted indirectly. This could lead to all attempts at physically observing a particle with an "entangled" relationship to another being slowed down, even though the particles are not connected in any other way other than by the information they carry.
The mathematical universe hypothesis suggests a new paradigm, in which virtually everything, from particles and fields, through biological entities and consciousness, to the multiverse itself, could be described by mathematical patterns of information. By the same token, the cosmic void can be conceived of as the absence of material information in space (setting aside the virtual particles that pop in and out of existence due to quantum fluctuations, as well as the gravitational field and the dark energy). Nothingness can be understood then as that within which no matter, energy, space, time, or any other type of information could exist, which would be possible if symmetry and structure break within the manifold of the multiverse (i.e. the manifold would have tears or holes).
Another link is demonstrated by the Maxwell's demon thought experiment. In this experiment, a direct relationship between information and another physical property, entropy, is demonstrated. A consequence is that it is impossible to destroy information without increasing the entropy of a system; in practical terms this often means generating heat. Another more philosophical outcome is that information could be thought of as interchangeable with energy. Toyabe et al. experimentally showed in nature that information can be converted into work. Thus, in the study of logic gates, the theoretical lower bound of thermal energy released by an AND gate is higher than for the NOT gate (because information is destroyed in an AND gate and simply converted in a NOT gate). Physical information is of particular importance in the theory of quantum computers.
In Thermodynamics, information is any kind of event that affects the state of a dynamic system that can interpret the information.



The information cycle (addressed as a whole or in its distinct components) is of great concern to Information Technology, Information Systems, as well as Information Science. These fields deal with those processes and techniques pertaining to information capture (through sensors) and generation (through computation, formulation or composition), processing (including encoding, encryption, compression, packaging), transmission (including all telecommunication methods), presentation (including visualization / display methods), storage (such as magnetic or optical, including holographic methods), etc. Information does not cease to exist, it may only get scrambled beyond any possibility of retrieval (within Information Theory, see lossy compression; in Physics, the black hole information paradox gets solved with the aid of the holographic principle).
Information Visualization (shortened as InfoVis) depends on the computation and digital representation of data, and assists users in pattern recognition and anomaly detection.

Information Security (shortened as InfoSec) is the ongoing process of exercising due diligence to protect information, and information systems, from unauthorized access, use, disclosure, destruction, modification, disruption or distribution, through algorithms and procedures focused on monitoring and detection, as well as incident response and repair.
Information Analysis is the process of inspecting, transforming, and modelling information, by converting raw data into actionable knowledge, in support of the decision-making process.
Information Quality (shortened as InfoQ) is the potential of a dataset to achieve a specific (scientific or practical) goal using a given empirical analysis method.
Information Communication represents the convergence of informatics, telecommunication and audio-visual media & content.



It is estimated that the world's technological capacity to store information grew from 2.6 (optimally compressed) exabytes in 1986 – which is the informational equivalent to less than one 730-MB CD-ROM per person (539 MB per person) – to 295 (optimally compressed) exabytes in 2007. This is the informational equivalent of almost 61 CD-ROM per person in 2007.
The world’s combined technological capacity to receive information through one-way broadcast networks was the informational equivalent of 174 newspapers per person per day in 2007.
The world's combined effective capacity to exchange information through two-way telecommunication networks was the informational equivalent of 6 newspapers per person per day in 2007.



Records are specialized forms of information. Essentially, records are information produced consciously or as by-products of business activities or transactions and retained because of their value. Primarily, their value is as evidence of the activities of the organization but they may also be retained for their informational value. Sound records management ensures that the integrity of records is preserved for as long as they are required.
The international standard on records management, ISO 15489, defines records as "information created, received, and maintained as evidence and information by an organization or person, in pursuance of legal obligations or in the transaction of business". The International Committee on Archives (ICA) Committee on electronic records defined a record as, "a specific piece of recorded information generated, collected or received in the initiation, conduct or completion of an activity and that comprises sufficient content, context and structure to provide proof or evidence of that activity".
Records may be maintained to retain corporate memory of the organization or to meet legal, fiscal or accountability requirements imposed on the organization. Willis (2005) expressed the view that sound management of business records and information delivered "...six key requirements for good corporate governance...transparency; accountability; due process; compliance; meeting statutory and common law requirements; and security of personal and corporate information."



Beynon-Davies explains the multi-faceted concept of information in terms of signs and signal-sign systems. Signs themselves can be considered in terms of four inter-dependent levels, layers or branches of semiotics: pragmatics, semantics, syntax, and empirics. These four layers serve to connect the social world on the one hand with the physical or technical world on the other.
Pragmatics is concerned with the purpose of communication. Pragmatics links the issue of signs with the context within which signs are used. The focus of pragmatics is on the intentions of living agents underlying communicative behaviour. In other words, pragmatics link language to action.
Semantics is concerned with the meaning of a message conveyed in a communicative act. Semantics considers the content of communication. Semantics is the study of the meaning of signs - the association between signs and behaviour. Semantics can be considered as the study of the link between symbols and their referents or concepts – particularly the way in which signs relate to human behavior.
Syntax is concerned with the formalism used to represent a message. Syntax as an area studies the form of communication in terms of the logic and grammar of sign systems. Syntax is devoted to the study of the form rather than the content of signs and sign-systems.
Nielsen (2008) discusses the relationship between semiotics and information in relation to dictionaries. The concept of lexicographic information costs is introduced and refers to the efforts users of dictionaries need to make in order to, first, find the data sought and, secondly, understand the data so that they can generate information.
Communication normally exists within the context of some social situation. The social situation sets the context for the intentions conveyed (pragmatics) and the form in which communication takes place. In a communicative situation intentions are expressed through messages which comprise collections of inter-related signs taken from a language which is mutually understood by the agents involved in the communication. Mutual understanding implies that agents involved understand the chosen language in terms of its agreed syntax (syntactics) and semantics. The sender codes the message in the language and sends the message as signals along some communication channel (empirics). The chosen communication channel will have inherent properties which determine outcomes such as the speed with which communication can take place and over what distance.









Alan Liu (2004). The Laws of Cool: Knowledge Work and the Culture of Information, University of Chicago Press
Bekenstein, Jacob D. (2003, August). Information in the holographic universe. Scientific American.
Gleick, James (2011). The Information: A History, a Theory, a Flood. Pantheon, New York, NY.
Shu-Kun Lin (2008). 'Gibbs Paradox and the Concepts of Information, Symmetry, Similarity and Their Relationship', Entropy, 10 (1), 1-5. Available online at Entropy journal website.
Luciano Floridi, (2005). 'Is Information Meaningful Data?', Philosophy and Phenomenological Research, 70 (2), pp. 351 – 370. Available online at PhilSci Archive
Luciano Floridi, (2005). 'Semantic Conceptions of Information', The Stanford Encyclopedia of Philosophy (Winter 2005 Edition), Edward N. Zalta (ed.). Available online at Stanford University
Luciano Floridi, (2010). Information: A Very Short Introduction, Oxford University Press, Oxford.
Robert K. Logan. What is Information? - Propagating Organization in the Biosphere, the Symbolosphere, the Technosphere and the Econosphere,
Toronto: DEMO Publishing.
Sandro Nielsen: 'The Effect of Lexicographical Information Costs on Dictionary Making and Use', Lexikos 18/2008, 170-189.
Stewart, Thomas, (2001). Wealth of Knowledge. Doubleday, New York, NY, 379 p.
Young, Paul. The Nature of Information (1987). Greenwood Publishing Group, Westport, Ct. ISBN 0-275-92698-2.



Semantic Conceptions of Information Review by Luciano Floridi for the Stanford Encyclopedia of Philosophy
Principia Cybernetica entry on negentropy
Fisher Information, a New Paradigm for Science: Introduction, Uncertainty principles, Wave equations, Ideas of Escher, Kant, Plato and Wheeler. This essay is continually revised in the light of ongoing research.
How Much Information? 2003 an attempt to estimate how much new information is created each year (study was produced by faculty and students at the School of Information Management and Systems at the University of California at Berkeley)
(Danish) Informationsordbogen.dk The Danish Dictionary of Information Terms / InformationsordbogenMedicine (British English /ˈmɛdsᵻn/; American English /ˈmɛdᵻsᵻn/) is the science and practice of the diagnosis, treatment, and prevention of disease. The word "medicine" is derived from Latin medicus, meaning "a physician". Medicine encompasses a variety of health care practices evolved to maintain and restore health by the prevention and treatment of illness. Contemporary medicine applies biomedical sciences, biomedical research, genetics, and medical technology to diagnose, treat, and prevent injury and disease, typically through pharmaceuticals or surgery, but also through therapies as diverse as psychotherapy, external splints and traction, medical devices, biologics, and ionizing radiation, amongst others.
Medicine has existed for thousands of years, during most of which it was an art (an area of skill and knowledge) frequently having connections to the religious and philosophical beliefs of local culture. For example, a medicine man would apply herbs and say prayers for healing, or an ancient philosopher and physician would apply bloodletting according to the theories of humorism. In recent centuries, since the advent of modern science, most medicine has become a combination of art and science (both basic and applied, under the umbrella of medical science). While stitching technique for sutures is an art learned through practice, the knowledge of what happens at the cellular and molecular level in the tissues being stitched arises through science.
Prescientific forms of medicine are now known as traditional medicine and folk medicine. They remain commonly used with or instead of scientific medicine and are thus called alternative medicine. For example, evidence on the effectiveness of acupuncture is "variable and inconsistent" for any condition, but is generally safe when done by an appropriately trained practitioner. In contrast, treatments outside the bounds of safety and efficacy are termed quackery.




Medical availability and clinical practice varies across the world due to regional differences in culture and technology. Modern scientific medicine is highly developed in the Western world, while in developing countries such as parts of Africa or Asia, the population may rely more heavily on traditional medicine with limited evidence and efficacy and no required formal training for practitioners. Even in the developed world however, evidence-based medicine is not universally used in clinical practice; for example, a 2007 survey of literature reviews found that about 49% of the interventions lacked sufficient evidence to support either benefit or harm.
In modern clinical practice, doctors personally assess patients in order to diagnose, treat, and prevent disease using clinical judgment. The doctor-patient relationship typically begins an interaction with an examination of the patient's medical history and medical record, followed by a medical interview and a physical examination. Basic diagnostic medical devices (e.g. stethoscope, tongue depressor) are typically used. After examination for signs and interviewing for symptoms, the doctor may order medical tests (e.g. blood tests), take a biopsy, or prescribe pharmaceutical drugs or other therapies. Differential diagnosis methods help to rule out conditions based on the information provided. During the encounter, properly informing the patient of all relevant facts is an important part of the relationship and the development of trust. The medical encounter is then documented in the medical record, which is a legal document in many jurisdictions. Follow-ups may be shorter but follow the same general procedure, and specialists follow a similar process. The diagnosis and treatment may take only a few minutes or a few weeks depending upon the complexity of the issue.
The components of the medical interview and encounter are:
Chief complaint (CC): the reason for the current medical visit. These are the 'symptoms.' They are in the patient's own words and are recorded along with the duration of each one. Also called 'chief concern' or 'presenting complaint'.
History of present illness (HPI): the chronological order of events of symptoms and further clarification of each symptom. Distinguishable from history of previous illness, often called past medical history (PMH). Medical history comprises HPI and PMH.
Current activity: occupation, hobbies, what the patient actually does.
Medications (Rx): what drugs the patient takes including prescribed, over-the-counter, and home remedies, as well as alternative and herbal medicines/herbal remedies. Allergies are also recorded.
Past medical history (PMH/PMHx): concurrent medical problems, past hospitalizations and operations, injuries, past infectious diseases and/or vaccinations, history of known allergies.
Social history (SH): birthplace, residences, marital history, social and economic status, habits (including diet, medications, tobacco, alcohol).
Family history (FH): listing of diseases in the family that may impact the patient. A family tree is sometimes used.
Review of systems (ROS) or systems inquiry: a set of additional questions to ask, which may be missed on HPI: a general enquiry (have you noticed any weight loss, change in sleep quality, fevers, lumps and bumps? etc.), followed by questions on the body's main organ systems (heart, lungs, digestive tract, urinary tract, etc.).
The physical examination is the examination of the patient for medical signs of disease, which are objective and observable, in contrast to symptoms which are volunteered by the patient and not necessarily objectively observable. The healthcare provider uses the senses of sight, hearing, touch, and sometimes smell (e.g., in infection, uremia, diabetic ketoacidosis). Four actions are the basis of physical examination: inspection, palpation (feel), percussion (tap to determine resonance characteristics), and auscultation (listen), generally in that order although auscultation occurs prior to percussion and palpation for abdominal assessments.
The clinical examination involves the study of:
Vital signs including height, weight, body temperature, blood pressure, pulse, respiration rate, and hemoglobin oxygen saturation
General appearance of the patient and specific indicators of disease (nutritional status, presence of jaundice, pallor or clubbing)
Skin
Head, eye, ear, nose, and throat (HEENT)
Cardiovascular (heart and blood vessels)
Respiratory (large airways and lungs)
Abdomen and rectum
Genitalia (and pregnancy if the patient is or could be pregnant)
Musculoskeletal (including spine and extremities)
Neurological (consciousness, awareness, brain, vision, cranial nerves, spinal cord and peripheral nerves)
Psychiatric (orientation, mental state, evidence of abnormal perception or thought).
It is to likely focus on areas of interest highlighted in the medical history and may not include everything listed above.
The treatment plan may include ordering additional medical laboratory tests and medical imaging studies, starting therapy, referral to a specialist, or watchful observation. Follow-up may be advised. Depending upon the health insurance plan and the managed care system, various forms of "utilization review", such as prior authorization of tests, may place barriers on accessing expensive services.
The medical decision-making (MDM) process involves analysis and synthesis of all the above data to come up with a list of possible diagnoses (the differential diagnoses), along with an idea of what needs to be done to obtain a definitive diagnosis that would explain the patient's problem.
On subsequent visits, the process may be repeated in an abbreviated manner to obtain any new history, symptoms, physical findings, and lab or imaging results or specialist consultations.




Contemporary medicine is in general conducted within health care systems. Legal, credentialing and financing frameworks are established by individual governments, augmented on occasion by international organizations, such as churches. The characteristics of any given health care system have significant impact on the way medical care is provided.
From ancient times, Christian emphasis on practical charity gave rise to the development of systematic nursing and hospitals and the Catholic Church today remains the largest non-government provider of medical services in the world. Advanced industrial countries (with the exception of the United States) and many developing countries provide medical services through a system of universal health care that aims to guarantee care for all through a single-payer health care system, or compulsory private or co-operative health insurance. This is intended to ensure that the entire population has access to medical care on the basis of need rather than ability to pay. Delivery may be via private medical practices or by state-owned hospitals and clinics, or by charities, most commonly by a combination of all three.
Most tribal societies provide no guarantee of healthcare for the population as a whole. In such societies, healthcare is available to those that can afford to pay for it or have self-insured it (either directly or as part of an employment contract) or who may be covered by care financed by the government or tribe directly.

Transparency of information is another factor defining a delivery system. Access to information on conditions, treatments, quality, and pricing greatly affects the choice by patients/consumers and, therefore, the incentives of medical professionals. While the US healthcare system has come under fire for lack of openness, new legislation may encourage greater openness. There is a perceived tension between the need for transparency on the one hand and such issues as patient confidentiality and the possible exploitation of information for commercial gain on the other.




Provision of medical care is classified into primary, secondary, and tertiary care categories.

Primary care medical services are provided by physicians, physician assistants, nurse practitioners, or other health professionals who have first contact with a patient seeking medical treatment or care. These occur in physician offices, clinics, nursing homes, schools, home visits, and other places close to patients. About 90% of medical visits can be treated by the primary care provider. These include treatment of acute and chronic illnesses, preventive care and health education for all ages and both sexes.
Secondary care medical services are provided by medical specialists in their offices or clinics or at local community hospitals for a patient referred by a primary care provider who first diagnosed or treated the patient. Referrals are made for those patients who required the expertise or procedures performed by specialists. These include both ambulatory care and inpatient services, emergency rooms, intensive care medicine, surgery services, physical therapy, labor and delivery, endoscopy units, diagnostic laboratory and medical imaging services, hospice centers, etc. Some primary care providers may also take care of hospitalized patients and deliver babies in a secondary care setting.
Tertiary care medical services are provided by specialist hospitals or regional centers equipped with diagnostic and treatment facilities not generally available at local hospitals. These include trauma centers, burn treatment centers, advanced neonatology unit services, organ transplants, high-risk pregnancy, radiation oncology, etc.
Modern medical care also depends on information – still delivered in many health care settings on paper records, but increasingly nowadays by electronic means.
In low-income countries, modern healthcare is often too expensive for the average person. International healthcare policy researchers have advocated that "user fees" be removed in these areas to ensure access, although even after removal, significant costs and barriers remain.



Working together as an interdisciplinary team, many highly trained health professionals besides medical practitioners are involved in the delivery of modern health care. Examples include: nurses, emergency medical technicians and paramedics, laboratory scientists, pharmacists, podiatrists, physiotherapists, respiratory therapists, speech therapists, occupational therapists, radiographers, dietitians, and bioengineers, surgeons, surgeon's assistant, surgical technologist.
The scope and sciences underpinning human medicine overlap many other fields. Dentistry, while considered by some a separate discipline from medicine, is a medical field.
A patient admitted to the hospital is usually under the care of a specific team based on their main presenting problem, e.g., the cardiology team, who then may interact with other specialties, e.g., surgical, radiology, to help diagnose or treat the main problem or any subsequent complications/developments.
Physicians have many specializations and subspecializations into certain branches of medicine, which are listed below. There are variations from country to country regarding which specialties certain subspecialties are in.
The main branches of medicine are:
Basic sciences of medicine; this is what every physician is educated in, and some return to in biomedical research
Medical specialties
Interdisciplinary fields, where different medical specialties are mixed to function in certain occasions.



Anatomy is the study of the physical structure of organisms. In contrast to macroscopic or gross anatomy, cytology and histology are concerned with microscopic structures.
Biochemistry is the study of the chemistry taking place in living organisms, especially the structure and function of their chemical components.
Biomechanics is the study of the structure and function of biological systems by means of the methods of Mechanics.
Biostatistics is the application of statistics to biological fields in the broadest sense. A knowledge of biostatistics is essential in the planning, evaluation, and interpretation of medical research. It is also fundamental to epidemiology and evidence-based medicine.
Biophysics is an interdisciplinary science that uses the methods of physics and physical chemistry to study biological systems.
Cytology is the microscopic study of individual cells.

Embryology is the study of the early development of organisms.
Endocrinology is the study of hormones and their effect throughout the body of animals.
Epidemiology is the study of the demographics of disease processes, and includes, but is not limited to, the study of epidemics.
Genetics is the study of genes, and their role in biological inheritance.
Histology is the study of the structures of biological tissues by light microscopy, electron microscopy and immunohistochemistry.
Immunology is the study of the immune system, which includes the innate and adaptive immune system in humans, for example.
Medical physics is the study of the applications of physics principles in medicine.
Microbiology is the study of microorganisms, including protozoa, bacteria, fungi, and viruses.
Molecular biology is the study of molecular underpinnings of the process of replication, transcription and translation of the genetic material.
Neuroscience includes those disciplines of science that are related to the study of the nervous system. A main focus of neuroscience is the biology and physiology of the human brain and spinal cord. Some related clinical specialties include neurology, neurosurgery and psychiatry.
Nutrition science (theoretical focus) and dietetics (practical focus) is the study of the relationship of food and drink to health and disease, especially in determining an optimal diet. Medical nutrition therapy is done by dietitians and is prescribed for diabetes, cardiovascular diseases, weight and eating disorders, allergies, malnutrition, and neoplastic diseases.
Pathology as a science is the study of disease—the causes, course, progression and resolution thereof.
Pharmacology is the study of drugs and their actions.
Photobiology is the study of the interactions between non-ionizing radiation and living organisms.
Physiology is the study of the normal functioning of the body and the underlying regulatory mechanisms.
Radiobiology is the study of the interactions between ionizing radiation and living organisms.
Toxicology is the study of hazardous effects of drugs and poisons.




In the broadest meaning of "medicine", there are many different specialties. In the UK, most specialities have their own body or college, which have its own entrance examination. These are collectively known as the Royal Colleges, although not all currently use the term "Royal". The development of a speciality is often driven by new technology (such as the development of effective anaesthetics) or ways of working (such as emergency departments); the new specialty leads to the formation of a unifying body of doctors and the prestige of administering their own examination.
Within medical circles, specialities usually fit into one of two broad categories: "Medicine" and "Surgery." "Medicine" refers to the practice of non-operative medicine, and most of its subspecialties require preliminary training in Internal Medicine. In the UK, this was traditionally evidenced by passing the examination for the Membership of the Royal College of Physicians (MRCP) or the equivalent college in Scotland or Ireland. "Surgery" refers to the practice of operative medicine, and most subspecialties in this area require preliminary training in General Surgery, which in the UK leads to membership of the Royal College of Surgeons of England (MRCS). At present, some specialties of medicine do not fit easily into either of these categories, such as radiology, pathology, or anesthesia. Most of these have branched from one or other of the two camps above; for example anaesthesia developed first as a faculty of the Royal College of Surgeons (for which MRCS/FRCS would have been required) before becoming the Royal College of Anaesthetists and membership of the college is attained by sitting for the examination of the Fellowship of the Royal College of Anesthetists (FRCA).




Surgery is an ancient medical specialty that uses operative manual and instrumental techniques on a patient to investigate and/or treat a pathological condition such as disease or injury, to help improve bodily function or appearance or to repair unwanted ruptured areas (for example, a perforated ear drum). Surgeons must also manage pre-operative, post-operative, and potential surgical candidates on the hospital wards. Surgery has many sub-specialties, including general surgery, ophthalmic surgery, cardiovascular surgery, colorectal surgery, neurosurgery, oral and maxillofacial surgery, oncologic surgery, orthopedic surgery, otolaryngology, plastic surgery, podiatric surgery, transplant surgery, trauma surgery, urology, vascular surgery, and pediatric surgery. In some centers, anesthesiology is part of the division of surgery (for historical and logistical reasons), although it is not a surgical discipline. Other medical specialties may employ surgical procedures, such as ophthalmology and dermatology, but are not considered surgical sub-specialties per se.
Surgical training in the U.S. requires a minimum of five years of residency after medical school. Sub-specialties of surgery often require seven or more years. In addition, fellowships can last an additional one to three years. Because post-residency fellowships can be competitive, many trainees devote two additional years to research. Thus in some cases surgical training will not finish until more than a decade after medical school. Furthermore, surgical training can be very difficult and time-consuming.




Internal medicine is the medical specialty dealing with the prevention, diagnosis, and treatment of adult diseases. According to some sources, an emphasis on internal structures is implied. In North America, specialists in internal medicine are commonly called "internists." Elsewhere, especially in Commonwealth nations, such specialists are often called physicians. These terms, internist or physician (in the narrow sense, common outside North America), generally exclude practitioners of gynecology and obstetrics, pathology, psychiatry, and especially surgery and its subspecialities.
Because their patients are often seriously ill or require complex investigations, internists do much of their work in hospitals. Formerly, many internists were not subspecialized; such general physicians would see any complex nonsurgical problem; this style of practice has become much less common. In modern urban practice, most internists are subspecialists: that is, they generally limit their medical practice to problems of one organ system or to one particular area of medical knowledge. For example, gastroenterologists and nephrologists specialize respectively in diseases of the gut and the kidneys.
In the Commonwealth of Nations and some other countries, specialist pediatricians and geriatricians are also described as specialist physicians (or internists) who have subspecialized by age of patient rather than by organ system. Elsewhere, especially in North America, general pediatrics is often a form of primary care.
There are many subspecialities (or subdisciplines) of internal medicine:

Training in internal medicine (as opposed to surgical training), varies considerably across the world: see the articles on medical education and physician for more details. In North America, it requires at least three years of residency training after medical school, which can then be followed by a one- to three-year fellowship in the subspecialties listed above. In general, resident work hours in medicine are less than those in surgery, averaging about 60 hours per week in the USA. This difference does not apply in the UK where all doctors are now required by law to work less than 48 hours per week on average.



Clinical laboratory sciences are the clinical diagnostic services that apply laboratory techniques to diagnosis and management of patients. In the United States, these services are supervised by a pathologist. The personnel that work in these medical laboratory departments are technically trained staff who do not hold medical degrees, but who usually hold an undergraduate medical technology degree, who actually perform the tests, assays, and procedures needed for providing the specific services. Subspecialties include transfusion medicine, cellular pathology, clinical chemistry, hematology, clinical microbiology and clinical immunology.
Pathology as a medical specialty is the branch of medicine that deals with the study of diseases and the morphologic, physiologic changes produced by them. As a diagnostic specialty, pathology can be considered the basis of modern scientific medical knowledge and plays a large role in evidence-based medicine. Many modern molecular tests such as flow cytometry, polymerase chain reaction (PCR), immunohistochemistry, cytogenetics, gene rearrangements studies and fluorescent in situ hybridization (FISH) fall within the territory of pathology.
Diagnostic radiology is concerned with imaging of the body, e.g. by x-rays, x-ray computed tomography, ultrasonography, and nuclear magnetic resonance tomography. Interventional radiologists can access areas in the body under imaging for an intervention or diagnostic sampling.
Nuclear medicine is concerned with studying human organ systems by administering radiolabelled substances (radiopharmaceuticals) to the body, which can then be imaged outside the body by a gamma camera or a PET scanner. Each radiopharmaceutical consists of two parts: a tracer that is specific for the function under study (e.g., neurotransmitter pathway, metabolic pathway, blood flow, or other), and a radionuclide (usually either a gamma-emitter or a positron emitter). There is a degree of overlap between nuclear medicine and radiology, as evidenced by the emergence of combined devices such as the PET/CT scanner.
Clinical neurophysiology is concerned with testing the physiology or function of the central and peripheral aspects of the nervous system. These kinds of tests can be divided into recordings of: (1) spontaneous or continuously running electrical activity, or (2) stimulus evoked responses. Subspecialties include electroencephalography, electromyography, evoked potential, nerve conduction study and polysomnography. Sometimes these tests are performed by techs without a medical degree, but the interpretation of these tests is done by a medical professional.



The followings are some major medical specialties that do not directly fit into any of the above-mentioned groups:
Anesthesiology (also known as anaesthetics): concerned with the perioperative management of the surgical patient. The anesthesiologist's role during surgery is to prevent derangement in the vital organs' (i.e. brain, heart, kidneys) functions and postoperative pain. Outside of the operating room, the anesthesiology physician also serves the same function in the labor & delivery ward, and some are specialized in critical medicine.
Dermatology is concerned with the skin and its diseases. In the UK, dermatology is a subspecialty of general medicine.
Emergency medicine is concerned with the diagnosis and treatment of acute or life-threatening conditions, including trauma, surgical, medical, pediatric, and psychiatric emergencies.
Family medicine, family practice, general practice or primary care is, in many countries, the first port-of-call for patients with non-emergency medical problems. Family physicians often provide services across a broad range of settings including office based practices, emergency room coverage, inpatient care, and nursing home care.

Obstetrics and gynecology (often abbreviated as OB/GYN (American English) or Obs & Gynae (British English)) are concerned respectively with childbirth and the female reproductive and associated organs. Reproductive medicine and fertility medicine are generally practiced by gynecological specialists.
Medical genetics is concerned with the diagnosis and management of hereditary disorders.
Neurology is concerned with diseases of the nervous system. In the UK, neurology is a subspecialty of general medicine.
Ophthalmology is exclusively concerned with the eye and ocular adnexa, combining conservative and surgical therapy.
Pediatrics (AE) or paediatrics (BE) is devoted to the care of infants, children, and adolescents. Like internal medicine, there are many pediatric subspecialties for specific age ranges, organ systems, disease classes, and sites of care delivery.
Pharmaceutical medicine is the medical scientific discipline concerned with the discovery, development, evaluation, registration, monitoring and medical aspects of marketing of medicines for the benefit of patients and public health.
Physical medicine and rehabilitation (or physiatry) is concerned with functional improvement after injury, illness, or congenital disorders.
Podiatric medicine is the study of, diagnosis, and medical & surgical treatment of disorders of the foot, ankle, lower limb, hip and lower back.
Psychiatry is the branch of medicine concerned with the bio-psycho-social study of the etiology, diagnosis, treatment and prevention of cognitive, perceptual, emotional and behavioral disorders. Related non-medical fields include psychotherapy and clinical psychology.
Preventive medicine is the branch of medicine concerned with preventing disease.
Community health or public health is an aspect of health services concerned with threats to the overall health of a community based on population health analysis.



Some interdisciplinary sub-specialties of medicine include:
Aerospace medicine deals with medical problems related to flying and space travel.
Addiction medicine deals with the treatment of addiction.
Medical ethics deals with ethical and moral principles that apply values and judgments to the practice of medicine.
Biomedical Engineering is a field dealing with the application of engineering principles to medical practice.
Clinical pharmacology is concerned with how systems of therapeutics interact with patients.
Conservation medicine studies the relationship between human and animal health, and environmental conditions. Also known as ecological medicine, environmental medicine, or medical geology.
Disaster medicine deals with medical aspects of emergency preparedness, disaster mitigation and management.
Diving medicine (or hyperbaric medicine) is the prevention and treatment of diving-related problems.
Evolutionary medicine is a perspective on medicine derived through applying evolutionary theory.
Forensic medicine deals with medical questions in legal context, such as determination of the time and cause of death, type of weapon used to inflict trauma, reconstruction of the facial features using remains of deceased (skull) thus aiding identification.
Gender-based medicine studies the biological and physiological differences between the human sexes and how that affects differences in disease.
Hospice and Palliative Medicine is a relatively modern branch of clinical medicine that deals with pain and symptom relief and emotional support in patients with terminal illnesses including cancer and heart failure.
Hospital medicine is the general medical care of hospitalized patients. Physicians whose primary professional focus is hospital medicine are called hospitalists in the USA and Canada. The term Most Responsible Physician (MRP) or attending physician is also used interchangeably to describe this role.
Laser medicine involves the use of lasers in the diagnostics and/or treatment of various conditions.
Medical humanities includes the humanities (literature, philosophy, ethics, history and religion), social science (anthropology, cultural studies, psychology, sociology), and the arts (literature, theater, film, and visual arts) and their application to medical education and practice.
Health informatics is a relatively recent field that deal with the application of computers and information technology to medicine.
Nosology is the classification of diseases for various purposes.
Nosokinetics is the science/subject of measuring and modelling the process of care in health and social care systems.
Occupational medicine's principal role is the provision of health advice to organizations and individuals to ensure that the highest standards of health and safety at work can be achieved and maintained.
Pain management (also called pain medicine, or algiatry) is the medical discipline concerned with the relief of pain.
Pharmacogenomics is a form of individualized medicine.
Podiatric medicine is the study of, diagnosis, and medical treatment of disorders of the foot, ankle, lower limb, hip and lower back.
Sexual medicine is concerned with diagnosing, assessing and treating all disorders related to sexuality.
Sports medicine deals with the treatment and prevention and rehabilitation of sports/exercise injuries such as muscle spasms, muscle tears, injuries to ligaments (ligament tears or ruptures) and their repair in athletes, amateur and professional.
Therapeutics is the field, more commonly referenced in earlier periods of history, of the various remedies that can be used to treat disease and promote health.
Travel medicine or emporiatrics deals with health problems of international travelers or travelers across highly different environments.
Tropical medicine deals with the prevention and treatment of tropical diseases. It is studied separately in temperate climates where those diseases are quite unfamiliar to medical practitioners and their local clinical needs.
Urgent care focuses on delivery of unscheduled, walk-in care outside of the hospital emergency department for injuries and illnesses that are not severe enough to require care in an emergency department. In some jurisdictions this function is combined with the emergency room.
Veterinary medicine; veterinarians apply similar techniques as physicians to the care of animals.
Wilderness medicine entails the practice of medicine in the wild, where conventional medical facilities may not be available.
Many other health science fields, e.g. dietetics




Medical education and training varies around the world. It typically involves entry level education at a university medical school, followed by a period of supervised practice or internship, and/or residency. This can be followed by postgraduate vocational training. A variety of teaching methods have been employed in medical education, still itself a focus of active research. In Canada and the United States of America, a Doctor of Medicine degree, often abbreviated M.D., or a Doctor of Osteopathic Medicine degree, often abbreviated as D.O. and unique to the United States, must be completed in and delivered from a recognized university.
Since knowledge, techniques, and medical technology continue to evolve at a rapid rate, many regulatory authorities require continuing medical education. Medical practitioners upgrade their knowledge in various ways, including medical journals, seminars, conferences, and online programs.

In most countries, it is a legal requirement for a medical doctor to be licensed or registered. In general, this entails a medical degree from a university and accreditation by a medical board or an equivalent national organization, which may ask the applicant to pass exams. This restricts the considerable legal authority of the medical profession to physicians that are trained and qualified by national standards. It is also intended as an assurance to patients and as a safeguard against charlatans that practice inadequate medicine for personal gain. While the laws generally require medical doctors to be trained in "evidence based", Western, or Hippocratic Medicine, they are not intended to discourage different paradigms of health.
In the European Union, the profession of doctor of medicine is regulated. A profession is said to be regulated when access and exercise is subject to the possession of a specific professional qualification. The regulated professions database contains a list of regulated professions for doctor of medicine in the EU member states, EEA countries and Switzerland. This list is covered by the Directive 2005/36/EC.
Doctors who are negligent or intentionally harmful in their care of patients can face charges of medical malpractice and be subject to civil, criminal, or professional sanctions.




Medical ethics is a system of moral principles that apply values and judgments to the practice of medicine. As a scholarly discipline, medical ethics encompasses its practical application in clinical settings as well as work on its history, philosophy, theology, and sociology. Six of the values that commonly apply to medical ethics discussions are:
autonomy - the patient has the right to refuse or choose their treatment. (Voluntas aegroti suprema lex.)
beneficence - a practitioner should act in the best interest of the patient. (Salus aegroti suprema lex.)
justice - concerns the distribution of scarce health resources, and the decision of who gets what treatment (fairness and equality).
non-maleficence - "first, do no harm" (primum non-nocere).
respect for persons - the patient (and the person treating the patient) have the right to be treated with dignity.
truthfulness and honesty - the concept of informed consent has increased in importance since the historical events of the Doctors' Trial of the Nuremberg trials, Tuskegee syphilis experiment, and others.
Values such as these do not give answers as to how to handle a particular situation, but provide a useful framework for understanding conflicts. When moral values are in conflict, the result may be an ethical dilemma or crisis. Sometimes, no good solution to a dilemma in medical ethics exists, and occasionally, the values of the medical community (i.e., the hospital and its staff) conflict with the values of the individual patient, family, or larger non-medical community. Conflicts can also arise between health care providers, or among family members. For example, some argue that the principles of autonomy and beneficence clash when patients refuse blood transfusions, considering them life-saving; and truth-telling was not emphasized to a large extent before the HIV era.






Prehistoric medicine incorporated plants (herbalism), animal parts, and minerals. In many cases these materials were used ritually as magical substances by priests, shamans, or medicine men. Well-known spiritual systems include animism (the notion of inanimate objects having spirits), spiritualism (an appeal to gods or communion with ancestor spirits); shamanism (the vesting of an individual with mystic powers); and divination (magically obtaining the truth). The field of medical anthropology examines the ways in which culture and society are organized around or impacted by issues of health, health care and related issues.
Early records on medicine have been discovered from ancient Egyptian medicine, Babylonian Medicine, Ayurvedic medicine (in the Indian subcontinent), classical Chinese medicine (predecessor to the modern traditional Chinese medicine), and ancient Greek medicine and Roman medicine.
In Egypt, Imhotep (3rd millennium BC) is the first physician in history known by name. The oldest Egyptian medical text is the Kahun Gynaecological Papyrus from around 2000 BCE, which describes gynaecological diseases. The Edwin Smith Papyrus dating back to 1600 BCE is an early work on surgery, while the Ebers Papyrus dating back to 1500 BCE is akin to a textbook on medicine.
In China, archaeological evidence of medicine in Chinese dates back to the Bronze Age Shang Dynasty, based on seeds for herbalism and tools presumed to have been used for surgery. The Huangdi Neijing, the progenitor of Chinese medicine, is a medical text written beginning in the 2nd century BCE and compiled in the 3rd century.
In India, the surgeon Sushruta described numerous surgical operations, including the earliest forms of plastic surgery. Earliest records of dedicated hospitals come from Mihintale in Sri Lanka where evidence of dedicated medicinal treatment facilities for patients are found.
In Greece, the Greek physician Hippocrates, the "father of western medicine", laid the foundation for a rational approach to medicine. Hippocrates introduced the Hippocratic Oath for physicians, which is still relevant and in use today, and was the first to categorize illnesses as acute, chronic, endemic and epidemic, and use terms such as, "exacerbation, relapse, resolution, crisis, paroxysm, peak, and convalescence". The Greek physician Galen was also one of the greatest surgeons of the ancient world and performed many audacious operations, including brain and eye surgeries. After the fall of the Western Roman Empire and the onset of the Early Middle Ages, the Greek tradition of medicine went into decline in Western Europe, although it continued uninterrupted in the Eastern Roman (Byzantine) Empire.
Most of our knowledge of ancient Hebrew medicine during the 1st millennium BC comes from the Torah, i.e. the Five Books of Moses, which contain various health related laws and rituals. The Hebrew contribution to the development of modern medicine started in the Byzantine Era, with the physician Asaph the Jew.




After 750 CE, the Muslim world had the works of Hippocrates, Galen and Sushruta translated into Arabic, and Islamic physicians engaged in some significant medical research. Notable Islamic medical pioneers include the Persian polymath, Avicenna, who, along with Imhotep and Hippocrates, has also been called the "father of medicine". He wrote The Canon of Medicine, considered one of the most famous books in the history of medicine. Others include Abulcasis, Avenzoar, Ibn al-Nafis, and Averroes. Rhazes was one of the first to question the Greek theory of humorism, which nevertheless remained influential in both medieval Western and medieval Islamic medicine. Al-Risalah al-Dhahabiah by Ali al-Ridha, the eighth Imam of Shia Muslims, is revered as the most precious Islamic literature in the Science of Medicine. The Persian Bimaristan hospitals were an early example of public hospitals.
In Europe, Charlemagne decreed that a hospital should be attached to each cathedral and monastery and the historian Geoffrey Blainey likened the activities of the Catholic Church in health care during the Middle Ages to an early version of a welfare state: "It conducted hospitals for the old and orphanages for the young; hospices for the sick of all ages; places for the lepers; and hostels or inns where pilgrims could buy a cheap bed and meal". It supplied food to the population during famine and distributed food to the poor. This welfare system the church funded through collecting taxes on a large scale and possessing large farmlands and estates. The Benedictine order was noted for setting up hospitals and infirmaries in their monasteries, growing medical herbs and becoming the chief medical care givers of their districts, as at the great Abbey of Cluny. The Church also established a network of cathedral schools and universities where medicine was studied. The Schola Medica Salernitana in Salerno, looking to the learning of Greek and Arab physicians, grew to be the finest medical school in Medieval Europe.

However, the fourteenth and fifteenth century Black Death devastated both the Middle East and Europe, and it has even been argued that Western Europe was generally more effective in recovering from the pandemic than the Middle East. In the early modern period, important early figures in medicine and anatomy emerged in Europe, including Gabriele Falloppio and William Harvey.
The major shift in medical thinking was the gradual rejection, especially during the Black Death in the 14th and 15th centuries, of what may be called the 'traditional authority' approach to science and medicine. This was the notion that because some prominent person in the past said something must be so, then that was the way it was, and anything one observed to the contrary was an anomaly (which was paralleled by a similar shift in European society in general – see Copernicus's rejection of Ptolemy's theories on astronomy). Physicians like Vesalius improved upon or disproved some of the theories from the past. The main tomes used both by medicine students and expert physicians were Materia Medica and Pharmacopoeia.
Andreas Vesalius was the author of De humani corporis fabrica, an important book on human anatomy. Bacteria and microorganisms were first observed with a microscope by Antonie van Leeuwenhoek in 1676, initiating the scientific field microbiology. Independently from Ibn al-Nafis, Michael Servetus rediscovered the pulmonary circulation, but this discovery did not reach the public because it was written down for the first time in the "Manuscript of Paris" in 1546, and later published in the theological work for which he paid with his life in 1553. Later this was described by Renaldus Columbus and Andrea Cesalpino. Herman Boerhaave is sometimes referred to as a "father of physiology" due to his exemplary teaching in Leiden and textbook 'Institutiones medicae' (1708). Pierre Fauchard has been called "the father of modern dentistry".




Veterinary medicine was, for the first time, truly separated from human medicine in 1761, when the French veterinarian Claude Bourgelat founded the world's first veterinary school in Lyon, France. Before this, medical doctors treated both humans and other animals.
Modern scientific biomedical research (where results are testable and reproducible) began to replace early Western traditions based on herbalism, the Greek "four humours" and other such pre-modern notions. The modern era really began with Edward Jenner's discovery of the smallpox vaccine at the end of the 18th century (inspired by the method of inoculation earlier practiced in Asia), Robert Koch's discoveries around 1880 of the transmission of disease by bacteria, and then the discovery of antibiotics around 1900.
The post-18th century modernity period brought more groundbreaking researchers from Europe. From Germany and Austria, doctors Rudolf Virchow, Wilhelm Conrad Röntgen, Karl Landsteiner and Otto Loewi made notable contributions. In the United Kingdom, Alexander Fleming, Joseph Lister, Francis Crick and Florence Nightingale are considered important. Spanish doctor Santiago Ramón y Cajal is considered the father of modern neuroscience.
From New Zealand and Australia came Maurice Wilkins, Howard Florey, and Frank Macfarlane Burnet.
In the United States, William Williams Keen, William Coley, James D. Watson, Italy (Salvador Luria), Switzerland (Alexandre Yersin), Japan (Kitasato Shibasaburō), and France (Jean-Martin Charcot, Claude Bernard, Paul Broca) and others did significant work. Russian Nikolai Korotkov also did significant work, as did Sir William Osler and Harvey Cushing.

As science and technology developed, medicine became more reliant upon medications. Throughout history and in Europe right until the late 18th century, not only animal and plant products were used as medicine, but also human body parts and fluids. Pharmacology developed in part from herbalism and some drugs are still derived from plants (atropine, ephedrine, warfarin, aspirin, digoxin, vinca alkaloids, taxol, hyoscine, etc.). Vaccines were discovered by Edward Jenner and Louis Pasteur.
The first antibiotic was arsphenamine (Salvarsan) discovered by Paul Ehrlich in 1908 after he observed that bacteria took up toxic dyes that human cells did not. The first major class of antibiotics was the sulfa drugs, derived by German chemists originally from azo dyes.
Pharmacology has become increasingly sophisticated; modern biotechnology allows drugs targeted towards specific physiological processes to be developed, sometimes designed for compatibility with the body to reduce side-effects. Genomics and knowledge of human genetics is having some influence on medicine, as the causative genes of most monogenic genetic disorders have now been identified, and the development of techniques in molecular biology and genetics are influencing medical technology, practice and decision-making.
Evidence-based medicine is a contemporary movement to establish the most effective algorithms of practice (ways of doing things) through the use of systematic reviews and meta-analysis. The movement is facilitated by modern global information science, which allows as much of the available evidence as possible to be collected and analyzed according to standard protocols that are then disseminated to healthcare providers. The Cochrane Collaboration leads this movement. A 2001 review of 160 Cochrane systematic reviews revealed that, according to two readers, 21.3% of the reviews concluded insufficient evidence, 20% concluded evidence of no effect, and 22.5% concluded positive effect.




Traditional medicine (also known as indigenous or folk medicine) comprises knowledge systems that developed over generations within various societies before the era of modern medicine. The World Health Organization (WHO) defines traditional medicine as "the sum total of the knowledge, skills, and practices based on the theories, beliefs, and experiences indigenous to different cultures, whether explicable or not, used in the maintenance of health as well as in the prevention, diagnosis, improvement or treatment of physical and mental illness."
In some Asian and African countries, up to 80% of the population relies on traditional medicine for their primary health care needs. When adopted outside of its traditional culture, traditional medicine is often called alternative medicine. Practices known as traditional medicines include Ayurveda, Siddha medicine, Unani, ancient Iranian medicine, Irani, Islamic medicine, traditional Chinese medicine, traditional Korean medicine, acupuncture, Muti, Ifá, and traditional African medicine.
The WHO notes however that "inappropriate use of traditional medicines or practices can have negative or dangerous effects" and that "further research is needed to ascertain the efficacy and safety" of several of the practices and medicinal plants used by traditional medicine systems. The line between alternative medicine and quackery is a contentious subject.
Traditional medicine may include formalized aspects of folk medicine, that is to say longstanding remedies passed on and practised by lay people. Folk medicine consists of the healing practices and ideas of body physiology and health preservation known to some in a culture, transmitted informally as general knowledge, and practiced or applied by anyone in the culture having prior experience. Folk medicine may also be referred to as traditional medicine, alternative medicine, indigenous medicine, or natural medicine. These terms are often considered interchangeable, even though some authors may prefer one or the other because of certain overtones they may be willing to highlight. In fact, out of these terms perhaps only indigenous medicine and traditional medicine have the same meaning as folk medicine, while the others should be understood rather in a modern or modernized context.





Health is the level of functional and metabolic efficiency of a living organism. In humans it is the ability of individuals or communities to adapt and self-manage when facing physical, mental or social changes. The World Health Organization (WHO) defined health in its broader sense in its 1948 constitution as "a state of complete physical, mental, and social well-being and not merely the absence of disease or infirmity." This definition has been subject to controversy, in particular as lacking operational value, the ambiguity in developing cohesive health strategies, and because of the problem created by use of the word "complete". Other definitions have been proposed, among which a recent definition that correlates health and personal satisfaction.  Classification systems such as the WHO Family of International Classifications, including the International Classification of Functioning, Disability and Health (ICF) and the International Classification of Diseases (ICD), are commonly used to define and measure the components of health.



The definition of health has evolved over time. In keeping with the biomedical perspective, early definitions of health focused on the theme of the body's ability to function; health was seen as a state of normal function that could be disrupted from time to time by disease. An example of such a definition of health is: "a state characterized by anatomic, physiologic, and psychological integrity; ability to perform personally valued family, work, and community roles; ability to deal with physical, biologic, psychological, and social stress". Then, in 1948, in a radical departure from previous definitions, the World Health Organization (WHO) proposed a definition that aimed higher, linking health to well-being, in terms of "physical, mental, and social well-being, and not merely the absence of disease and infirmity". Although this definition was welcomed by some as being innovative, it was also criticized as being vague, excessively broad, and was not construed as measurable. For a long time it was set aside as an impractical ideal and most discussions of health returned to the practicality of the biomedical model.
Just as there was a shift from viewing disease as a state to thinking of it as a process, the same shift happened in definitions of health. Again, the WHO played a leading role when it fostered the development of the health promotion movement in the 1980s. This brought in a new conception of health, not as a state, but in dynamic terms of resiliency, in other words, as "a resource for living". The 1984 WHO revised definition of health defined it as "the extent to which an individual or group is able to realize aspirations and satisfy needs, and to change or cope with the environment. Health is a resource for everyday life, not the objective of living; it is a positive concept, emphasizing social and personal resources, as well as physical capacities". Thus, health referred to the ability to maintain homeostasis and recover from insults. Mental, intellectual, emotional, and social health referred to a person's ability to handle stress, to acquire skills, to maintain relationships, all of which form resources for resiliency and independent living.
Since the late 1970s, the federal Healthy People Initiative has been a visible component of the United States’ approach to improving population health. In each decade, a new version of Healthy People is issued, featuring updated goals and identifying topic areas and quantifiable objectives for health improvement during the succeeding ten years, with assessment at that point of progress or lack thereof. Progress has been limited for many objectives, leading to concerns about the effectiveness of Healthy People in shaping outcomes in the context of a decentralized and uncoordinated US health system. Healthy People 2020 gives more prominence to health promotion and preventive approaches, and adds a substantive focus on the importance of addressing societal determinants of health. A new expanded digital interface facilitates use and dissemination rather than bulky printed books as produced in the past. The impact of these changes to Healthy People will be determined in the coming years.
Systematic activities to prevent or cure health problems and promote good health in humans are undertaken by health care providers. Applications with regard to animal health are covered by the veterinary sciences. The term "healthy" is also widely used in the context of many types of non-living organizations and their impacts for the benefit of humans, such as in the sense of healthy communities, healthy cities or healthy environments. In addition to health care interventions and a person's surroundings, a number of other factors are known to influence the health status of individuals, including their background, lifestyle, and economic, social conditions, and spirituality; these are referred to as "determinants of health." Studies have shown that high levels of stress can affect human health.




Generally, the context in which an individual lives is of great importance for both his health status and quality of their life. It is increasingly recognized that health is maintained and improved not only through the advancement and application of health science, but also through the efforts and intelligent lifestyle choices of the individual and society. According to the World Health Organization, the main determinants of health include the social and economic environment, the physical environment, and the person's individual characteristics and behaviors.
More specifically, key factors that have been found to influence whether people are healthy or unhealthy include the following:

An increasing number of studies and reports from different organizations and contexts examine the linkages between health and different factors, including lifestyles, environments, health care organization, and health policy – such as the 1974 Lalonde report from Canada; the Alameda County Study in California; and the series of World Health Reports of the World Health Organization, which focuses on global health issues including access to health care and improving public health outcomes, especially in developing countries.
The concept of the "health field," as distinct from medical care, emerged from the Lalonde report from Canada. The report identified three interdependent fields as key determinants of an individual's health. These are:
Lifestyle: the aggregation of personal decisions (i.e., over which the individual has control) that can be said to contribute to, or cause, illness or death;
Environmental: all matters related to health external to the human body and over which the individual has little or no control;
Biomedical: all aspects of health, physical and mental, developed within the human body as influenced by genetic make-up.
The maintenance and promotion of health is achieved through different combination of physical, mental, and social well-being, together sometimes referred to as the "health triangle." The WHO's 1986 Ottawa Charter for Health Promotion further stated that health is not just a state, but also "a resource for everyday life, not the objective of living. Health is a positive concept emphasizing social and personal resources, as well as physical capacities."
Focusing more on lifestyle issues and their relationships with functional health, data from the Alameda County Study suggested that people can improve their health via exercise, enough sleep, maintaining a healthy body weight, limiting alcohol use, and avoiding smoking. Health and illness can co-exist, as even people with multiple chronic diseases or terminal illnesses can consider themselves healthy.
The environment is often cited as an important factor influencing the health status of individuals. This includes characteristics of the natural environment, the built environment, and the social environment. Factors such as clean water and air, adequate housing, and safe communities and roads all have been found to contribute to good health, especially to the health of infants and children. Some studies have shown that a lack of neighborhood recreational spaces including natural environment leads to lower levels of personal satisfaction and higher levels of obesity, linked to lower overall health and well being. This suggests that the positive health benefits of natural space in urban neighborhoods should be taken into account in public policy and land use.
Genetics, or inherited traits from parents, also play a role in determining the health status of individuals and populations. This can encompass both the predisposition to certain diseases and health conditions, as well as the habits and behaviors individuals develop through the lifestyle of their families. For example, genetics may play a role in the manner in which people cope with stress, either mental, emotional or physical. For example, obesity is a very large problem in the United States that contributes to bad mental health and causes stress in a lot of people's lives. (One difficulty is the issue raised by the debate over the relative strengths of genetics and other factors; interactions between genetics and environment may be of particular importance.)



There are a lot of types of health issues common with many people across the globe. Disease is one of the most common. According to GlobalIssues.org, approximately 36 million people die each year from non-communicable (not contagious) disease including cardiovascular disease cancer, diabetes, and chronic lung disease (Shah, 2014).
As for communicable diseases, both viral and bacterial, AIDS/HIV, tuberculosis, and malaria are the most common also causing millions of deaths every year (2014).
Another health issue that causes death or contributes to other health problems is malnutrition majorly among children. One of the groups malnutrition affects most is young children. Approximately 7.5 million children under the age of 5 die from malnutrition, and it is usually brought on by not having the money to find or make food (2014).
Bodily injuries are also a common health issue worldwide. These injuries, including broken bones, fractures, and burns can reduce a person's quality of life or can cause fatalities including infections that resulted from the injury or the severity injury in general (Moffett, 2013).
Some contributing factors to poor health are lifestyle choices. These include smoking cigarettes, and also can include a poor diet, whether it is overeating or an overly constrictive diet. Inactivity can also contribute to health issues and also a lack of sleep, excessive alcohol consumption, and neglect of oral hygiene (2013). There are also genetic disorders that are inherited by the person and can vary in how much they affect the person and when they surface (2013).
The one health issue that is the most unfortunate because the majority of these health issues are preventable is that approximately 1 billion people lack access to health care systems (Shah, 2014). It is easy to say that the most common and harmful health issue is that a lot of people do not have access to quality remedies.




The World Health Organization describes mental health as "a state of well-being in which the individual realizes his or her own abilities, can cope with the normal stresses of life, can work productively and fruitfully, and is able to make a contribution to his or her community". Mental Health is not just the absence of mental illness.
Mental illness is described as 'the spectrum of cognitive, emotional, and behavioral conditions that interfere with social and emotional well-being and the lives and productivity of people. Having a mental illness can seriously impair, temporarily or permanently, the mental functioning of a person. Other terms include: 'mental health problem', 'illness', 'disorder', 'dysfunction'.
Roughly a quarter of all adults 18 and over in the US suffer from a diagnosable mental illness. Mental illnesses are the leading cause of disability in the US and Canada. Examples include, schizophrenia, ADHD, major depressive disorder, bipolar disorder, anxiety disorder, post-traumatic stress disorder and autism.
Many teens suffer from mental health issues in response to the pressures of society and social problems they encounter. Some of the key mental health issues seen in teens are: depression, eating disorders, and drug abuse. There are many ways to prevent these health issues from occurring such as communicating well with a teen suffering from mental health issues. Mental health can be treated and be attentive to teens' behavior.



Achieving and maintaining health is an ongoing process, shaped by both the evolution of health care knowledge and practices as well as personal strategies and organized interventions for staying healthy.




An important way to maintain your personal health is to have a healthy diet. A healthy diet includes a variety of plant-based and animal-based foods that provide nutrients to your body. Such nutrients give you energy and keep your body running. Nutrients help build and strengthen bones, muscles, and tendons and also regulate body processes (i.e. blood pressure). The food guide pyramid is a pyramid-shaped guide of healthy foods divided into sections. Each section shows the recommended intake for each food group (i.e. Protein, Fat, Carbohydrates, and Sugars). Making healthy food choices is important because it can lower your risk of heart disease, developing some types of cancer, and it will contribute to maintaining a healthy weight.
The Mediterranean diet is commonly associated with health-promoting effects due to the fact that it contains some bioactive compounds like phenolic compounds, isoprenoids and alkaloids.




Physical exercise enhances or maintains physical fitness and overall health and wellness. It strengthens muscles and improves the cardiovascular system.




Sleep is an essential component to maintaining health. In children, sleep is also vital for growth and development. Ongoing sleep deprivation has been linked to an increased risk for some chronic health problems. In addition, sleep deprivation has been shown to correlate with both increased susceptibility to illness and slower recovery times from illness. In one study, people with chronic insufficient sleep, set as six hours of sleep a night or less, were found to be four times more likely to catch a cold compared to those who reported sleeping for seven hours or more a night. Due to the role of sleep in regulating metabolism, insufficient sleep may also play a role in weight gain or, conversely, in impeding weight loss. Additionally, in 2007, the International Agency for Research on Cancer, which is the cancer research agency for the World Health Organization, declared that "shiftwork that involves circadian disruption is probably carcinogenic to humans," speaking to the dangers of long-term nighttime work due to its intrusion on sleep. In 2015, the National Sleep Foundation released updated recommendations for sleep duration requirements based on age and concluded that "Individuals who habitually sleep outside the normal range may be exhibiting signs or symptoms of serious health problems or, if done volitionally, may be compromising their health and well-being."




Health science is the branch of science focused on health. There are two main approaches to health science: the study and research of the body and health-related issues to understand how humans (and animals) function, and the application of that knowledge to improve health and to prevent and cure diseases and other physical and mental impairments. The science builds on many sub-fields, including biology, biochemistry, physics, epidemiology, pharmacology, medical sociology. Applied health sciences endeavor to better understand and improve human health through applications in areas such as health education, biomedical engineering, biotechnology and public health.
Organized interventions to improve health based on the principles and procedures developed through the health sciences are provided by practitioners trained in medicine, nursing, nutrition, pharmacy, social work, psychology, occupational therapy, physical therapy and other health care professions. Clinical practitioners focus mainly on the health of individuals, while public health practitioners consider the overall health of communities and populations. Workplace wellness programs are increasingly adopted by companies for their value in improving the health and well-being of their employees, as are school health services in order to improve the health and well-being of children.




Public health has been described as "the science and art of preventing disease, prolonging life and promoting health through the organized efforts and informed choices of society, organizations, public and private, communities and individuals." It is concerned with threats to the overall health of a community based on population health analysis. The population in question can be as small as a handful of people or as large as all the inhabitants of several continents (for instance, in the case of a pandemic). Public health has many sub-fields, but typically includes the interdisciplinary categories of epidemiology, biostatistics and health services. Environmental health, community health, behavioral health, and occupational health are also important areas of public health.
The focus of public health interventions is to prevent and manage diseases, injuries and other health conditions through surveillance of cases and the promotion of healthy behavior, communities, and (in aspects relevant to human health) environments. Its aim is to prevent health problems from happening or re-occurring by implementing educational programs, developing policies, administering services and conducting research. In many cases, treating a disease or controlling a pathogen can be vital to preventing it in others, such as during an outbreak. Vaccination programs and distribution of condoms to prevent the spread of communicable diseases are examples of common preventive public health measures, as are educational campaigns to promote vaccination and the use of condoms (including overcoming resistance to such).
Public health also takes various actions to limit the health disparities between different areas of the country and, in some cases, the continent or world. One issue is the access of individuals and communities to health care in terms of financial, geographical or socio-cultural constraints to accessing and using services. Applications of the public health system include the areas of maternal and child health, health services administration, emergency response, and prevention and control of infectious and chronic diseases.
The great positive impact of public health programs is widely acknowledged. Due in part to the policies and actions developed through public health, the 20th century registered a decrease in the mortality rates for infants and children and a continual increase in life expectancy in most parts of the world. For example, it is estimated that life expectancy has increased for Americans by thirty years since 1900, and worldwide by six years since 1990.




Personal health depends partially on the active, passive, and assisted cues people observe and adopt about their own health. These include personal actions for preventing or minimizing the effects of a disease, usually a chronic condition, through integrative care. They also include personal hygiene practices to prevent infection and illness, such as bathing and washing hands with soap; brushing and flossing teeth; storing, preparing and handling food safely; and many others. The information gleaned from personal observations of daily living – such as about sleep patterns, exercise behavior, nutritional intake and environmental features – may be used to inform personal decisions and actions (e.g., "I feel tired in the morning so I am going to try sleeping on a different pillow"), as well as clinical decisions and treatment plans (e.g., a patient who notices his or her shoes are tighter than usual may be having exacerbation of left-sided heart failure, and may require diuretic medication to reduce fluid overload).
Personal health also depends partially on the social structure of a person's life. The maintenance of strong social relationships, volunteering, and other social activities have been linked to positive mental health and also increased longevity. One American study among seniors over age 70, found that frequent volunteering was associated with reduced risk of dying compared with older persons who did not volunteer, regardless of physical health status. Another study from Singapore reported that volunteering retirees had significantly better cognitive performance scores, fewer depressive symptoms, and better mental well-being and life satisfaction than non-volunteering retirees.
Prolonged psychological stress may negatively impact health, and has been cited as a factor in cognitive impairment with aging, depressive illness, and expression of disease. Stress management is the application of methods to either reduce stress or increase tolerance to stress. Relaxation techniques are physical methods used to relieve stress. Psychological methods include cognitive therapy, meditation, and positive thinking, which work by reducing response to stress. Improving relevant skills, such as problem solving and time management skills, reduces uncertainty and builds confidence, which also reduces the reaction to stress-causing situations where those skills are applicable.




In addition to safety risks, many jobs also present risks of disease, illness and other long-term health problems. Among the most common occupational diseases are various forms of pneumoconiosis, including silicosis and coal worker's pneumoconiosis (black lung disease). Asthma is another respiratory illness that many workers are vulnerable to. Workers may also be vulnerable to skin diseases, including eczema, dermatitis, urticaria, sunburn, and skin cancer. Other occupational diseases of concern include carpal tunnel syndrome and lead poisoning.
As the number of service sector jobs has risen in developed countries, more and more jobs have become sedentary, presenting a different array of health problems than those associated with manufacturing and the primary sector. Contemporary problems, such as the growing rate of obesity and issues relating to stress and overwork in many countries, have further complicated the interaction between work and health.
Many governments view occupational health as a social challenge and have formed public organizations to ensure the health and safety of workers. Examples of these include the British Health and Safety Executive and in the United States, the National Institute for Occupational Safety and Health, which conducts research on occupational health and safety, and the Occupational Safety and Health Administration, which handles regulation and policy relating to worker safety and health.



Men's health
Women's health
Youth health
Population health
Public health
Global burden of disease
Health care
Health system
Medicine
Human enhancement
One Health






World Health Organization
UK National Health Service
OECD Health Statistics
Health and Medical Information from the University of ColoradoEthics or moral philosophy is a branch of philosophy that involves systematizing, defending, and recommending concepts of right and wrong conduct. The term ethics derives from the Ancient Greek word ἠθικός ethikos, which is derived from the word ἦθος ethos (habit, "custom"). The branch of philosophy axiology comprises the sub-branches of ethics and aesthetics, each concerned with values.
As a branch of philosophy, ethics investigates the questions "What is the best way for people to live?" and "What actions are right or wrong in particular circumstances?" In practice, ethics seeks to resolve questions of human morality, by defining concepts such as good and evil, right and wrong, virtue and vice, justice and crime. As a field of intellectual enquiry, moral philosophy also is related to the fields of moral psychology, descriptive ethics, and value theory.
Three major areas of study within ethics recognised today are:
Meta-ethics, concerning the theoretical meaning and reference of moral propositions, and how their truth values (if any) can be determined
Normative ethics, concerning the practical means of determining a moral course of action
Applied ethics, concerning what a person is obligated (or permitted) to do in a specific situation or a particular domain of action



Rushworth Kidder states that "standard definitions of ethics have typically included such phrases as 'the science of the ideal human character' or 'the science of moral duty'". Richard William Paul and Linda Elder define ethics as "a set of concepts and principles that guide us in determining what behavior helps or harms sentient creatures". The Cambridge Dictionary of Philosophy states that the word ethics is "commonly used interchangeably with 'morality' ... and sometimes it is used more narrowly to mean the moral principles of a particular tradition, group or individual." Paul and Elder state that most people confuse ethics with behaving in accordance with social conventions, religious beliefs and the law and don't treat ethics as a stand-alone concept.
The word "ethics" in English refers to several things. It can refer to philosophical ethics or moral philosophy—a project that attempts to use reason in order to answer various kinds of ethical questions. As the English philosopher Bernard Williams writes, attempting to explain moral philosophy: "What makes an inquiry a philosophical one is reflective generality and a style of argument that claims to be rationally persuasive." And Williams describes the content of this area of inquiry as addressing the very broad question, "how one should live" Ethics can also refer to a common human ability to think about ethical problems that is not particular to philosophy. As bioethicist Larry Churchill has written: "Ethics, understood as the capacity to think critically about moral values and direct our actions in terms of such values, is a generic human capacity." Ethics can also be used to describe a particular person's own idiosyncratic principles or habits. For example: "Joe has strange ethics."
The English word ethics is derived from an Ancient Greek word êthikos, which means "relating to one's character". The Ancient Greek adjective êthikos is itself derived from another Greek word, the noun êthos meaning "character, disposition".




Meta-ethics asks how we understand, know about, and what we mean when we talk about what is right and what is wrong. An ethical question fixed on some particular practical question—such as, "Should I eat this particular piece of chocolate cake?"—cannot be a meta-ethical question. A meta-ethical question is abstract and relates to a wide range of more specific practical questions. For example, "Is it ever possible to have secure knowledge of what is right and wrong?" would be a meta-ethical question.
Meta-ethics has always accompanied philosophical ethics. For example, Aristotle implies that less precise knowledge is possible in ethics than in other spheres of inquiry, and he regards ethical knowledge as depending upon habit and acculturation in a way that makes it distinctive from other kinds of knowledge. Meta-ethics is also important in G.E. Moore's Principia Ethica from 1903. In it he first wrote about what he called the naturalistic fallacy. Moore was seen to reject naturalism in ethics, in his Open Question Argument. This made thinkers look again at second order questions about ethics. Earlier, the Scottish philosopher David Hume had put forward a similar view on the difference between facts and values.
Studies of how we know in ethics divide into cognitivism and non-cognitivism; this is similar to the contrast between descriptivists and non-descriptivists. Non-cognitivism is the claim that when we judge something as right or wrong, this is neither true nor false. We may for example be only expressing our emotional feelings about these things. Cognitivism can then be seen as the claim that when we talk about right and wrong, we are talking about matters of fact.
The ontology of ethics is about value-bearing things or properties, i.e. the kind of things or stuff referred to by ethical propositions. Non-descriptivists and non-cognitivists believe that ethics does not need a specific ontology, since ethical propositions do not refer. This is known as an anti-realist position. Realists on the other hand must explain what kind of entities, properties or states are relevant for ethics, how they have value, and why they guide and motivate our actions.




Normative ethics is the study of ethical action. It is the branch of ethics that investigates the set of questions that arise when considering how one ought to act, morally speaking. Normative ethics is distinct from meta-ethics because it examines standards for the rightness and wrongness of actions, while meta-ethics studies the meaning of moral language and the metaphysics of moral facts. Normative ethics is also distinct from descriptive ethics, as the latter is an empirical investigation of people's moral beliefs. To put it another way, descriptive ethics would be concerned with determining what proportion of people believe that killing is always wrong, while normative ethics is concerned with whether it is correct to hold such a belief. Hence, normative ethics is sometimes called prescriptive, rather than descriptive. However, on certain versions of the meta-ethical view called moral realism, moral facts are both descriptive and prescriptive at the same time.
Traditionally, normative ethics (also known as moral theory) was the study of what makes actions right and wrong. These theories offered an overarching moral principle one could appeal to in resolving difficult moral decisions.
At the turn of the 20th century, moral theories became more complex and are no longer concerned solely with rightness and wrongness, but are interested in many different kinds of moral status. During the middle of the century, the study of normative ethics declined as meta-ethics grew in prominence. This focus on meta-ethics was in part caused by an intense linguistic focus in analytic philosophy and by the popularity of logical positivism.
In 1971 John Rawls published A Theory of Justice, noteworthy in its pursuit of moral arguments and eschewing of meta-ethics. This publication set the trend for renewed interest in normative ethics.




Virtue ethics describes the character of a moral agent as a driving force for ethical behavior, and is used to describe the ethics of Socrates, Aristotle, and other early Greek philosophers. Socrates (469–399 BC) was one of the first Greek philosophers to encourage both scholars and the common citizen to turn their attention from the outside world to the condition of humankind. In this view, knowledge bearing on human life was placed highest, while all other knowledge were secondary. Self-knowledge was considered necessary for success and inherently an essential good. A self-aware person will act completely within his capabilities to his pinnacle, while an ignorant person will flounder and encounter difficulty. To Socrates, a person must become aware of every fact (and its context) relevant to his existence, if he wishes to attain self-knowledge. He posited that people will naturally do what is good, if they know what is right. Evil or bad actions are the result of ignorance. If a criminal was truly aware of the intellectual and spiritual consequences of his actions, he would neither commit nor even consider committing those actions. Any person who knows what is truly right will automatically do it, according to Socrates. While he correlated knowledge with virtue, he similarly equated virtue with joy. The truly wise man will know what is right, do what is good, and therefore be happy.
Aristotle (384–323 BC) posited an ethical system that may be termed "self-realizationism". In Aristotle's view, when a person acts in accordance with his nature and realizes his full potential, he will do good and be content. At birth, a baby is not a person, but a potential person. To become a "real" person, the child's inherent potential must be realized. Unhappiness and frustration are caused by the unrealized potential of a person, leading to failed goals and a poor life. Aristotle said, "Nature does nothing in vain." Therefore, it is imperative for people to act in accordance with their nature and develop their latent talents in order to be content and complete. Happiness was held to be the ultimate goal. All other things, such as civic life or wealth, are merely means to the end. Self-realization, the awareness of one's nature and the development of one's talents, is the surest path to happiness.
Aristotle asserted that man had three natures: vegetable (physical/metabolism), animal (emotional/appetite) and rational (mental/conceptual). Physical nature can be assuaged through exercise and care, emotional nature through indulgence of instinct and urges, and mental through human reason and developed potential. Rational development was considered the most important, as essential to philosophical self-awareness and as uniquely human. Moderation was encouraged, with the extremes seen as degraded and immoral. For example, courage is the moderate virtue between the extremes of cowardice and recklessness. Man should not simply live, but live well with conduct governed by moderate virtue. This is regarded as difficult, as virtue denotes doing the right thing, to the right person, at the right time, to the proper extent, in the correct fashion, for the right reason.




The Stoic philosopher Epictetus posited that the greatest good was contentment and serenity. Peace of mind, or Apatheia, was of the highest value; self-mastery over one's desires and emotions leads to spiritual peace. The "unconquerable will" is central to this philosophy. The individual's will should be independent and inviolate. Allowing a person to disturb the mental equilibrium is in essence offering yourself in slavery. If a person is free to anger you at will, you have no control over your internal world, and therefore no freedom. Freedom from material attachments is also necessary. If a thing breaks, the person should not be upset, but realize it was a thing that could break. Similarly, if someone should die, those close to them should hold to their serenity because the loved one was made of flesh and blood destined to death. Stoic philosophy says to accept things that cannot be changed, resigning oneself to existence and enduring in a rational fashion. Death is not feared. People do not "lose" their life, but instead "return", for they are returning to God (who initially gave what the person is as a person). Epictetus said difficult problems in life should not be avoided, but rather embraced. They are spiritual exercises needed for the health of the spirit, just as physical exercise is required for the health of the body. He also stated that sex and sexual desire are to be avoided as the greatest threat to the integrity and equilibrium of a man's mind. Abstinence is highly desirable. Epictetus said remaining abstinent in the face of temptation was a victory for which a man could be proud.



Modern virtue ethics was popularized during the late 20th century in large part as a response to G. E. M. Anscombe's "Modern Moral Philosophy". Anscombe argues that consequentialist and deontological ethics are only feasible as universal theories if the two schools ground themselves in divine law. As a deeply devoted Christian herself, Anscombe proposed that either those who do not give ethical credence to notions of divine law take up virtue ethics, which does not necessitate universal laws as agents themselves are investigated for virtue or vice and held up to "universal standards", or that those who wish to be utilitarian or consequentialist ground their theories in religious conviction. Alasdair MacIntyre, who wrote the book After Virtue, was a key contributor and proponent of modern virtue ethics, although MacIntyre supports a relativistic account of virtue based on cultural norms, not objective standards. Martha Nussbaum, a contemporary virtue ethicist, objects to MacIntyre's relativism, among that of others, and responds to relativist objections to form an objective account in her work "Non-Relative Virtues: An Aristotelian Approach". Complete Conduct Principles for the 21st Century blended the Eastern virtue ethics and the Western virtue ethics, with some modifications to suit the 21st Century, and formed a part of contemporary virtue ethics.




Hedonism posits that the principal ethic is maximizing pleasure and minimizing pain. There are several schools of Hedonist thought ranging from those advocating the indulgence of even momentary desires to those teaching a pursuit of spiritual bliss. In their consideration of consequences, they range from those advocating self-gratification regardless of the pain and expense to others, to those stating that the most ethical pursuit maximizes pleasure and happiness for the most people.



Founded by Aristippus of Cyrene, Cyrenaics supported immediate gratification or pleasure. "Eat, drink and be merry, for tomorrow we die." Even fleeting desires should be indulged, for fear the opportunity should be forever lost. There was little to no concern with the future, the present dominating in the pursuit for immediate pleasure. Cyrenaic hedonism encouraged the pursuit of enjoyment and indulgence without hesitation, believing pleasure to be the only good.




Epicurean ethics is a hedonist form of virtue ethics. Epicurus "presented a sustained argument that pleasure, correctly understood, will coincide with virtue". He rejected the extremism of the Cyrenaics, believing some pleasures and indulgences to be detrimental to human beings. Epicureans observed that indiscriminate indulgence sometimes resulted in negative consequences. Some experiences were therefore rejected out of hand, and some unpleasant experiences endured in the present to ensure a better life in the future. To Epicurus the summum bonum, or greatest good, was prudence, exercised through moderation and caution. Excessive indulgence can be destructive to pleasure and can even lead to pain. For example, eating one food too often will cause a person to lose taste for it. Eating too much food at once will lead to discomfort and ill-health. Pain and fear were to be avoided. Living was essentially good, barring pain and illness. Death was not to be feared. Fear was considered the source of most unhappiness. Conquering the fear of death would naturally lead to a happier life. Epicurus reasoned if there was an afterlife and immortality, the fear of death was irrational. If there was no life after death, then the person would not be alive to suffer, fear or worry; he would be non-existent in death. It is irrational to fret over circumstances that do not exist, such as one's state in death in the absence of an afterlife.




State consequentialism, also known as Mohist consequentialism, is an ethical theory that evaluates the moral worth of an action based on how much it contributes to the basic goods of a state. The Stanford Encyclopedia of Philosophy describes Mohist consequentialism, dating back to the 5th century BC, as "a remarkably sophisticated version based on a plurality of intrinsic goods taken as constitutive of human welfare". Unlike utilitarianism, which views pleasure as a moral good, "the basic goods in Mohist consequentialist thinking are ... order, material wealth, and increase in population". During Mozi's era, war and famines were common, and population growth was seen as a moral necessity for a harmonious society. The "material wealth" of Mohist consequentialism refers to basic needs like shelter and clothing, and the "order" of Mohist consequentialism refers to Mozi's stance against warfare and violence, which he viewed as pointless and a threat to social stability.
Stanford sinologist David Shepherd Nivison, in The Cambridge History of Ancient China, writes that the moral goods of Mohism "are interrelated: more basic wealth, then more reproduction; more people, then more production and wealth ... if people have plenty, they would be good, filial, kind, and so on unproblematically." The Mohists believed that morality is based on "promoting the benefit of all under heaven and eliminating harm to all under heaven". In contrast to Bentham's views, state consequentialism is not utilitarian because it is not hedonistic or individualistic. The importance of outcomes that are good for the community outweigh the importance of individual pleasure and pain.




Consequentialism refers to moral theories that hold that the consequences of a particular action form the basis for any valid moral judgment about that action (or create a structure for judgment, see rule consequentialism). Thus, from a consequentialist standpoint, a morally right action is one that produces a good outcome, or consequence. This view is often expressed as the aphorism "The ends justify the means".
The term "consequentialism" was coined by G. E. M. Anscombe in her essay "Modern Moral Philosophy" in 1958, to describe what she saw as the central error of certain moral theories, such as those propounded by Mill and Sidgwick. Since then, the term has become common in English-language ethical theory.
The defining feature of consequentialist moral theories is the weight given to the consequences in evaluating the rightness and wrongness of actions. In consequentialist theories, the consequences of an action or rule generally outweigh other considerations. Apart from this basic outline, there is little else that can be unequivocally said about consequentialism as such. However, there are some questions that many consequentialist theories address:
What sort of consequences count as good consequences?
Who is the primary beneficiary of moral action?
How are the consequences judged and who judges them?
One way to divide various consequentialisms is by the types of consequences that are taken to matter most, that is, which consequences count as good states of affairs. According to utilitarianism, a good action is one that results in an increase in a positive effect, and the best action is one that results in that effect for the greatest number. Closely related is eudaimonic consequentialism, according to which a full, flourishing life, which may or may not be the same as enjoying a great deal of pleasure, is the ultimate aim. Similarly, one might adopt an aesthetic consequentialism, in which the ultimate aim is to produce beauty. However, one might fix on non-psychological goods as the relevant effect. Thus, one might pursue an increase in material equality or political liberty instead of something like the more ephemeral "pleasure". Other theories adopt a package of several goods, all to be promoted equally. Whether a particular consequentialist theory focuses on a single good or many, conflicts and tensions between different good states of affairs are to be expected and must be adjudicated.




Utilitarianism is an ethical theory that argues the proper course of action is one that maximizes a positive effect, such as "happiness", "welfare", or the ability to live according to personal preferences. Jeremy Bentham and John Stuart Mill are influential proponents of this school of thought. In A Fragment on Government Bentham says 'it is the greatest happiness of the greatest number that is the measure of right and wrong' and describes this as a fundamental axiom. In An Introduction to the Principles of Morals and Legislation he talks of 'the principle of utility' but later prefers "the greatest happiness principle".
Utilitarianism is the paradigmatic example of a consequentialist moral theory. This form of utilitarianism holds that what matters is the aggregate positive effect of everyone and not only of any one person. John Stuart Mill, in his exposition of utilitarianism, proposed a hierarchy of pleasures, meaning that the pursuit of certain kinds of pleasure is more highly valued than the pursuit of other pleasures. Other noteworthy proponents of utilitarianism are neuroscientist Sam Harris, author of The Moral Landscape, and moral philosopher Peter Singer, author of, amongst other works, Practical Ethics.
There are two types of utilitarianism, act utilitarianism and rule utilitarianism. In act utilitarianism the principle of utility is applied directly to each alternative act in a situation of choice. The right act is then defined as the one which brings about the best results (or the least amount of bad results). In rule utilitarianism the principle of utility is used to determine the validity of rules of conduct (moral principles). A rule like promise-keeping is established by looking at the consequences of a world in which people broke promises at will and a world in which promises were binding. Right and wrong are then defined as following or breaking those rules.




Deontological ethics or deontology (from Greek δέον, deon, "obligation, duty"; and -λογία, -logia) is an approach to ethics that determines goodness or rightness from examining acts, or the rules and duties that the person doing the act strove to fulfill. This is in contrast to consequentialism, in which rightness is based on the consequences of an act, and not the act by itself. In deontology, an act may be considered right even if the act produces a bad consequence, if it follows the rule that "one should do unto others as they would have done unto them", and even if the person who does the act lacks virtue and had a bad intention in doing the act. According to deontology, people have a duty to act in a way that does those things that are inherently good as acts ("truth-telling" for example), or follow an objectively obligatory rule (as in rule utilitarianism). For deontologists, the ends or consequences of people's actions are not important in and of themselves, and people's intentions are not important in and of themselves.
Immanuel Kant's theory of ethics is considered deontological for several different reasons. First, Kant argues that to act in the morally right way, people must act from duty (deon). Second, Kant argued that it was not the consequences of actions that make them right or wrong but the motives (maxime) of the person who carries out the action. Kant's argument that to act in the morally right way, one must act from duty, begins with an argument that the highest good must be both good in itself, and good without qualification. Something is 'good in itself' when it is intrinsically good, and 'good without qualification' when the addition of that thing never makes a situation ethically worse. Kant then argues that those things that are usually thought to be good, such as intelligence, perseverance and pleasure, fail to be either intrinsically good or good without qualification. Pleasure, for example, appears to not be good without qualification, because when people take pleasure in watching someone suffer, they make the situation ethically worse. He concludes that there is only one thing that is truly good:

Nothing in the world—indeed nothing even beyond the world—can possibly be conceived which could be called good without qualification except a good will.




Associated with the pragmatists, Charles Sanders Peirce, William James, and especially John Dewey, pragmatic ethics holds that moral correctness evolves similarly to scientific knowledge: socially over the course of many lifetimes. Thus, we should prioritize social reform over attempts to account for consequences, individual virtue or duty (although these may be worthwhile attempts, provided social reform is provided for).




Role ethics is an ethical theory based on family roles. Unlike virtue ethics, role ethics is not individualistic. Morality is derived from a person's relationship with their community. Confucian ethics is an example of role ethics. Confucian roles center around the concept of filial piety or xiao, a respect for family members. According to Roger Ames and Henry Rosemont, "Confucian normativity is defined by living one's family roles to maximum effect." Morality is determined through a person's fulfillment of a role, such as that of a parent or a child. Confucian roles are not rational, and originate through the xin, or human emotions.




Anarchist ethics is an ethical theory based on the studies of anarchist thinkers. The biggest contributor to the anarchist ethics is the Russian zoologist, geographer, economist and political activist Peter Kropotkin. The anarchist ethics is a big and vague field which can depend upon different historical situations and different anarchist thinkers, but as Peter Kropotkin explains, "any “bourgeois” or “proletarian” ethics rests, after all, on the common basis, on the common ethnological foundation, which at times exerts a very strong inﬂuence on the principles of the class or group morality." Still, most of the anarchist ethics schools are based on three fundamental ideas, which are: "solidarity, equality and justice". Kropotkin argues that Ethics is evolutionary and is inherited as a sort of a social instinct through History, and by so, he rejects any religious and transcendental explanation of ethics. Kropotkin suggests that the principle of equality which lies at the basis of anarchism is the same as the Golden rule:

This principle of treating others as one wishes to be treated oneself, what is it but the very same principle as equality, the fundamental principle of anarchism? And how can any one manage to believe himself an anarchist unless he practices it? We do not wish to be ruled. And by this very fact, do we not declare that we ourselves wish to rule nobody? We do not wish to be deceived, we wish always to be told nothing but the truth. And by this very fact, do we not de- clare that we ourselves do not wish to deceive anybody, that we promise to always tell the truth, nothing but the truth, the whole truth? We do not wish to have the fruits of our labor stolen from us. And by that very fact, do we not declare that we respect the fruits of others' labor? By what right indeed can we demand that we should be treated in one fashion, reserving it to ourselves to treat others in a fashion entirely different? Our sense of equality revolts at such an idea.




The 20th century saw a remarkable expansion and evolution of critical theory, following on earlier Marxist Theory efforts to locate individuals within larger structural frameworks of ideology and action.
Antihumanists such as Louis Althusser and Michel Foucault and structuralists such as Roland Barthes challenged the possibilities of individual agency and the coherence of the notion of the 'individual' itself. As critical theory developed in the later 20th century, post-structuralism sought to problematize human relationships to knowledge and 'objective' reality. Jacques Derrida argued that access to meaning and the 'real' was always deferred, and sought to demonstrate via recourse to the linguistic realm that "there is nothing outside context" ("il n'y a pas de hors-texte" is often mistranslated as "there is nothing outside the text"); at the same time, Jean Baudrillard theorised that signs and symbols or simulacra mask reality (and eventually the absence of reality itself), particularly in the consumer world.
Post-structuralism and postmodernism argue that ethics must study the complex and relational conditions of actions. A simple alignment of ideas of right and particular acts is not possible. There will always be an ethical remainder that cannot be taken into account or often even recognized. Such theorists find narrative (or, following Nietzsche and Foucault, genealogy) to be a helpful tool for understanding ethics because narrative is always about particular lived experiences in all their complexity rather than the assignment of an idea or norm to separate and individuated actions.
Zygmunt Bauman says Postmodernity is best described as Modernity without illusion, the illusion being the belief that humanity can be repaired by some ethic principle. Postmodernity can be seen in this light as accepting the messy nature of humanity as unchangeable.
David Couzens Hoy states that Emmanuel Levinas's writings on the face of the Other and Derrida's meditations on the relevance of death to ethics are signs of the "ethical turn" in Continental philosophy that occurred in the 1980s and 1990s. Hoy describes post-critique ethics as the "obligations that present themselves as necessarily to be fulfilled but are neither forced on one or are enforceable" (2004, p. 103).
Hoy's post-critique model uses the term ethical resistance. Examples of this would be an individual's resistance to consumerism in a retreat to a simpler but perhaps harder lifestyle, or an individual's resistance to a terminal illness. Hoy describes Levinas's account as "not the attempt to use power against itself, or to mobilize sectors of the population to exert their political power; the ethical resistance is instead the resistance of the powerless"(2004, p. 8).
Hoy concludes that

The ethical resistance of the powerless others to our capacity to exert power over them is therefore what imposes unenforceable obligations on us. The obligations are unenforceable precisely because of the other's lack of power. That actions are at once obligatory and at the same time unenforceable is what put them in the category of the ethical. Obligations that were enforced would, by the virtue of the force behind them, not be freely undertaken and would not be in the realm of the ethical. (2004, p.184)

In present-day terms the powerless may include the unborn, the terminally sick, the aged, the insane, and non-human animals. It is in these areas that ethical action in Hoy's sense will apply. Until legislation or the state apparatus enforces a moral order that addresses the causes of resistance these issues will remain in the ethical realm. For example, should animal experimentation become illegal in a society, it will no longer be an ethical issue on Hoy's definition. Likewise one hundred and fifty years ago, not having a black slave in America would have been an ethical choice. This later issue has been absorbed into the fabric of an enforceable social order and is therefore no longer an ethical issue in Hoy's sense.




Applied ethics is a discipline of philosophy that attempts to apply ethical theory to real-life situations. The discipline has many specialized fields, such as engineering ethics, bioethics, geoethics, public service ethics and business ethics.



Applied ethics is used in some aspects of determining public policy, as well as by individuals facing difficult decisions. The sort of questions addressed by applied ethics include: "Is getting an abortion immoral?" "Is euthanasia immoral?" "Is affirmative action right or wrong?" "What are human rights, and how do we determine them?" "Do animals have rights as well?" and "Do individuals have the right of self determination?"
A more specific question could be: "If someone else can make better out of his/her life than I can, is it then moral to sacrifice myself for them if needed?" Without these questions there is no clear fulcrum on which to balance law, politics, and the practice of arbitration—in fact, no common assumptions of all participants—so the ability to formulate the questions are prior to rights balancing. But not all questions studied in applied ethics concern public policy. For example, making ethical judgments regarding questions such as, "Is lying always wrong?" and, "If not, when is it permissible?" is prior to any etiquette.
People in-general are more comfortable with dichotomies (two opposites). However, in ethics the issues are most often multifaceted and the best proposed actions address many different areas concurrently. In ethical decisions the answer is almost never a "yes or no", "right or wrong" statement. Many buttons are pushed so that the overall condition is improved and not to the benefit of any particular faction.







Bioethics is the study of controversial ethics brought about by advances in biology and medicine. Bioethicists are concerned with the ethical questions that arise in the relationships among life sciences, biotechnology, medicine, politics, law, and philosophy. It also includes the study of the more commonplace questions of values ("the ethics of the ordinary") that arise in primary care and other branches of medicine.
Bioethics also needs to address emerging biotechnologies that affect basic biology and future humans. These developments include cloning, gene therapy, human genetic engineering, astroethics and life in space, and manipulation of basic biology through altered DNA, RNA and proteins,e.g.- "three parent baby,where baby is born from genetically modified embryos, would have DNA from a mother, a father and from a female donor. Correspondingly, new bioethics also need to address life at its core. For example, biotic ethics value organic gene/protein life itself and seek to propagate it. With such life-centered principles, ethics may secure a cosmological future for life.




Business ethics (also corporate ethics) is a form of applied ethics or professional ethics that examines ethical principles and moral or ethical problems that arise in a business environment, including fields like Medical ethics. It applies to all aspects of business conduct and is relevant to the conduct of individuals and entire organizations.
Business ethics has both normative and descriptive dimensions. As a corporate practice and a career specialization, the field is primarily normative. Academics attempting to understand business behavior employ descriptive methods. The range and quantity of business ethical issues reflects the interaction of profit-maximizing behavior with non-economic concerns. Interest in business ethics accelerated dramatically during the 1980s and 1990s, both within major corporations and within academia. For example, today most major corporations promote their commitment to non-economic values under headings such as ethics codes and social responsibility charters. Adam Smith said, "People of the same trade seldom meet together, even for merriment and diversion, but the conversation ends in a conspiracy against the public, or in some contrivance to raise prices." Governments use laws and regulations to point business behavior in what they perceive to be beneficial directions. Ethics implicitly regulates areas and details of behavior that lie beyond governmental control. The emergence of large corporations with limited relationships and sensitivity to the communities in which they operate accelerated the development of formal ethics regimes.




In Moral Machines: Teaching Robots Right from Wrong, Wendell Wallach and Colin Allen conclude that issues in machine ethics will likely drive advancement in understanding of human ethics by forcing us to address gaps in modern normative theory and by providing a platform for experimental investigation. The effort to actually program a machine or artificial agent to behave as though instilled with a sense of ethics requires new specificity in our normative theories, especially regarding aspects customarily considered common-sense. For example, machines, unlike humans, can support a wide selection of learning algorithms, and controversy has arisen over the relative ethical merits of these options. This may reopen classic debates of normative ethics framed in new (highly technical) terms.




Military ethics are concerned with questions regarding the application of force and the ethos of the soldier and are often understood as applied professional ethics. Just war theory is generally seen to set the background terms of military ethics. However individual countries and traditions have different fields of attention.
Military ethics involves multiple subareas, including the following among others:
what, if any, should be the laws of war.
justification for the initiation of military force.
decisions about who may be targeted in warfare.
decisions on choice of weaponry, and what collateral effects such weaponry may have.
standards for handling military prisoners.
methods of dealing with violations of the laws of war.




Political ethics (also known as political morality or public ethics) is the practice of making moral judgements about political action and political agents.




Public sector ethics is a set of principles that guide public officials in their service to their constituents, including their decision-making on behalf of their constituents. Fundamental to the concept of public sector ethics is the notion that decisions and actions are based on what best serves the public's interests, as opposed to the official's personal interests (including financial interests) or self-serving political interests.



Publication ethics is the set of principles that guide the writing and publishing process for all professional publications. In order to follow the set of principles, authors should verify that the publication does not contain plagiarism or publication bias. As a way to avoid misconduct in research these principles can also be applied to experiments which are referenced or analyzed in publications by ensuring the data is recorded, honestly and accurately.
Plagiarism is the failure to give credit to another author’s work or ideas, when it is used in the publication. It is the obligation of the editor of the journal to ensure the article does not contain any plagiarism before it is published. If a publication which has already been published is proven to contain plagiarism, then the editor of the journal can proceed to have the article retracted.
Publication bias occurs when the publication is one-sided or "prejudiced against results". In best practice, an author should try to include information from all parties involved, or affected by the topic. If an author is prejudiced against certain results, than it can "lead to erroneous conclusions being drawn.”
Misconduct in research can occur when information from an experiment is falsely recorded or altered. Falsely recorded information occurs when the researcher "fakes" information or data, which was not used when conducting the actual experiment. By faking the data, the researcher can alter the results from the experiment to better fit the hypothesis they originally predicted. When conducting medical research, it is important to honor the healthcare rights of a patient by protecting their anonymity in the publication.



Relational ethics are related to an ethics of care. They are used in qualitative research, especially ethnography and autoethnography. Researchers who employ relational ethics value and respect the connection between themselves and the people they study, and "between researchers and the communities in which they live and work" (Ellis, 2007, p. 4). Relational ethics also help researchers understand difficult issues such as conducting research on intimate others that have died and developing friendships with their participants. Relational ethics in close personal relationships form a central concept of contextual therapy.




Moral psychology is a field of study that began as an issue in philosophy and that is now properly considered part of the discipline of psychology. Some use the term "moral psychology" relatively narrowly to refer to the study of moral development. However, others tend to use the term more broadly to include any topics at the intersection of ethics and psychology (and philosophy of mind). Such topics are ones that involve the mind and are relevant to moral issues. Some of the main topics of the field are moral responsibility, moral development, moral character (especially as related to virtue ethics), altruism, psychological egoism, moral luck, and moral disagreement.




Evolutionary ethics concerns approaches to ethics (morality) based on the role of evolution in shaping human psychology and behavior. Such approaches may be based in scientific fields such as evolutionary psychology or sociobiology, with a focus on understanding and explaining observed ethical preferences and choices.




Descriptive ethics is on the less philosophical end of the spectrum, since it seeks to gather particular information about how people live and draw general conclusions based on observed patterns. Abstract and theoretical questions that are more clearly philosophical—such as, "Is ethical knowledge possible?"—are not central to descriptive ethics. Descriptive ethics offers a value-free approach to ethics, which defines it as a social science rather than a humanity. Its examination of ethics doesn't start with a preconceived theory, but rather investigates observations of actual choices made by moral agents in practice. Some philosophers rely on descriptive ethics and choices made and unchallenged by a society or culture to derive categories, which typically vary by context. This can lead to situational ethics and situated ethics. These philosophers often view aesthetics, etiquette, and arbitration as more fundamental, percolating "bottom up" to imply the existence of, rather than explicitly prescribe, theories of value or of conduct. The study of descriptive ethics may include examinations of the following:
Ethical codes applied by various groups. Some consider aesthetics itself the basis of ethics—and a personal moral core developed through art and storytelling as very influential in one's later ethical choices.
Informal theories of etiquette that tend to be less rigorous and more situational. Some consider etiquette a simple negative ethics, i.e., where can one evade an uncomfortable truth without doing wrong? One notable advocate of this view is Judith Martin ("Miss Manners"). According to this view, ethics is more a summary of common sense social decisions.
Practices in arbitration and law, e.g., the claim that ethics itself is a matter of balancing "right versus right", i.e., putting priorities on two things that are both right, but that must be traded off carefully in each situation.
Observed choices made by ordinary people, without expert aid or advice, who vote, buy, and decide what is worth valuing. This is a major concern of sociology, political science, and economics.




Contemporary ethics
Corporate social responsibility
Declaration of Geneva
Declaration of Helsinki
Deductive reasoning
Descriptive ethics
Dharma
Ethical movement
Ethics paper
Index of ethics articles—alphabetical list of ethics-related articles
Moral psychology
Outline of ethics—list of ethics-related articles, arranged by sub-topic
Practical philosophy
Science of morality
Theory of justification






Hoy, D. (2005). Critical Resistance from Poststructuralism to Postcritique. Massachusetts Institute of Technology, Cambridge, Massachusetts.
Lyon, D. (1999). Postmodernity (2nd ed.). Open University Press, Buckingham.
Singer, P. (2000). Writings on an Ethical Life. Harper Collins Publishers, London.



Aristotle, Nicomachean Ethics
The London Philosophy Study Guide offers many suggestions on what to read, depending on the student's familiarity with the subject: Ethics
Encyclopedia of Ethics. Lawrence C. Becker and Charlotte B. Becker, editors. Second edition in three volumes. New York: Routledge, 2002. A scholarly encyclopedia with over 500 signed, peer-reviewed articles, mostly on topics and figures of, or of special interest in, Western philosophy.
Azurmendi, J. 1998: "The violence and the search for new values" in Euskal Herria krisian, (Elkar, 1999), pp. 11–116. ISBN 84-8331-572-6
Blackburn, S. (2001). Being good: A short introduction to ethics. Oxford: Oxford University Press.
De Finance, Joseph, An Ethical Inquiry, Rome, Editrice Pontificia Università Gregoriana, 1991.
De La Torre, Miguel A., "Doing Christian Ethics from the Margins", Orbis Books, 2004.
Derrida, J. 1995, The Gift of Death, translated by David Wills, University of Chicago Press, Chicago.
Fagothey, Austin, Right and Reason, Tan Books & Publishers, Rockford, Illinois, 2000.
Levinas, E. 1969, Totality and infinity, an essay on exteriority, translated by Alphonso Lingis, Duquesne University Press, Pittsburgh.
Perle, Stephen (March 11, 2004). "Morality and Ethics: An Introduction". Retrieved February 13, 2007. , Butchvarov, Panayot. Skepticism in Ethics (1989).
Solomon, R.C., Morality and the Good Life: An Introduction to Ethics Through Classical Sources, New York: McGraw-Hill Book Company, 1984.
Vendemiati, Aldo, In the First Person, An Outline of General Ethics, Rome, Urbaniana University Press, 2004.
John Paul II, Encyclical Letter Veritatis Splendor, August 6, 1993.
D'Urance, Michel, Jalons pour une éthique rebelle, Aléthéia, Paris, 2005.
John Newton, Ph.D. Complete Conduct Principles for the 21st Century, 2000. ISBN 0-9673705-7-4.
Guy Cools & Pascal Gielen, The Ethics of Art. Valiz: Amsterdam, 2014.
Lafollette, Hugh [ed.]: Ethics in Practice: An Anthology. Wiley Blackwell, 4th edition, Oxford 2014. ISBN 978-0470671832
An entire issue of Pacific Island Studies devoted to studying "Constructing Moral Communities" in Pacific islands, 2002, vol. 25: Link
Paul R. Ehrlich (May 2016), Conference on population, environment, ethics: where we stand now (video, 93 min), University of Lausanne




Meta-Ethics at PhilPapers
Normative Ethics at PhilPapers
Applied Ethics at PhilPapers
Ethics at the Indiana Philosophy Ontology Project
"Ethics". Internet Encyclopedia of Philosophy. 
An Introduction to Ethics by Paul Newall, aimed at beginners.
Ethics, 2d ed., 1973. by William Frankena
Ethics Bites, Open University podcast series podcast exploring ethical dilemmas in everyday life.
National Reference Center for Bioethics Literature World's largest library for ethical issues in medicine and biomedical research
Ethics entry in Encyclopædia Britannica by Peter Singer
The Philosophy of Ethics on Philosophy Archive
Kirby Laing Institute for Christian Ethics Resources, events, and research on a range of ethical subjects from a Christian perspective.
International Association for Geoethics (IAGETH)
International Association for Promoting Geoethics (IAPG)
Markkula Center for Applied Ethics at Santa Clara University Resources for analyzing real-world ethical issues and tools to address them.Laughter is a physical reaction in humans and some other species of primate, consisting typically of rhythmical, often audible contractions of the diaphragm and other parts of the respiratory system. It is a response to certain external or internal stimuli. Laughter can arise from such activities as being tickled, or from humorous stories or thoughts. Most commonly, it is considered a visual expression of a number of positive emotional states, such as joy, mirth, happiness, relief, etc. On some occasions, however, it may be caused by contrary emotional states such as embarrassment, apology, or confusion such as nervous laughter or courtesy laugh. Age, gender, education, language, and culture are all factors as to whether a person will experience laughter in a given situation.
Laughter is a part of human behavior regulated by the brain, helping humans clarify their intentions in social interaction and providing an emotional context to conversations. Laughter is used as a signal for being part of a group—it signals acceptance and positive interactions with others. Laughter is sometimes seen as contagious, and the laughter of one person can itself provoke laughter from others as a positive feedback. This may account in part for the popularity of laugh tracks in situation comedy television shows.
The study of humor and laughter, and its psychological and physiological effects on the human body, is called gelotology.



Laughter might be thought of as an audible expression or appearance of excitement, an inward feeling of joy and happiness. It may ensue from jokes, tickling, and other stimuli completely unrelated to psychological state, such as nitrous oxide. One group of researchers speculated that noises from infants as early as 17 days old may be vocal laughing sounds or laughter, however the weight of the evidence supports its appearance at 15 weeks to four months of age.
Laughter researcher Robert Provine said: "Laughter is a mechanism everyone has; laughter is part of universal human vocabulary. There are thousands of languages, hundreds of thousands of dialects, but everyone speaks laughter in pretty much the same way." Babies have the ability to laugh before they ever speak. Children who are born blind and deaf still retain the ability to laugh.
Provine argues that "Laughter is primitive, an unconscious vocalization." Provine argues that it probably is genetic. In a study of the "Giggle Twins", two happy twins who were separated at birth and only reunited 43 years later, Provine reports that "until they met each other, neither of these exceptionally happy ladies had known anyone who laughed as much as they did." They reported this even though they both had been brought together by their adoptive parents, who they indicated were "undemonstrative and dour." He indicates that the twins "inherited some aspects of their laugh sound and pattern, readiness to laugh, and maybe even taste in humor."
Norman Cousins developed a recovery program incorporating megadoses of Vitamin C, along with a positive attitude, love, faith, hope, and laughter induced by Marx Brothers films. "I made the joyous discovery that ten minutes of genuine belly laughter had an anesthetic effect and would give me at least two hours of pain-free sleep," he reported. "When the pain-killing effect of the laughter wore off, we would switch on the motion picture projector again and not infrequently, it would lead to another pain-free interval."
Scientists have noted the similarity in forms of laughter induced by tickling among various primates, which suggests that laughter derives from a common origin among primate species.
A very rare neurological condition has been observed whereby the sufferer is unable to laugh out loud, a condition known as aphonogelia.




Neurophysiology indicates that laughter is linked with the activation of the ventromedial prefrontal cortex, that produces endorphins. Scientists have shown that parts of the limbic system are involved in laughter. This system is involved in emotions and helps us with functions necessary for humans' survival. The structures in the limbic system that are involved in laughter are the hippocampus and the amygdala.
The December 7, 1984, Journal of the American Medical Association describes the neurological causes of laughter as follows:
"Although there is no known 'laugh center' in the brain, its neural mechanism has been the subject of much, albeit inconclusive, speculation. It is evident that its expression depends on neural paths arising in close association with the telencephalic and diencephalic centers concerned with respiration. Wilson considered the mechanism to be in the region of the mesial thalamus, hypothalamus, and subthalamus. Kelly and co-workers, in turn, postulated that the tegmentum near the periaqueductal grey contains the integrating mechanism for emotional expression. Thus, supranuclear pathways, including those from the limbic system that Papez hypothesised to mediate emotional expressions such as laughter, probably come into synaptic relation in the reticular core of the brain stem. So while purely emotional responses such as laughter are mediated by subcortical structures, especially the hypothalamus, and are stereotyped, the cerebral cortex can modulate or suppress them."



A link between laughter and healthy function of blood vessels was first reported in 2005 by researchers at the University of Maryland Medical Center with the fact that laughter causes the dilatation of the inner lining of blood vessels, the endothelium, and increases blood flow. Drs. Michael Miller (University of Maryland) and William Fry (Stanford), theorize that beta-endorphin like compounds released by the hypothalamus activate receptors on the endothelial surface to release nitric oxide, thereby resulting in dilation of vessels. Other cardioprotective properties of nitric oxide include reduction of inflammation and decreased platelet aggregation.
Laughter has proven beneficial effects on various other aspects of biochemistry. It has been shown to lead to reductions in stress hormones such as cortisol and epinephrine. When laughing the brain also releases endorphins that can relieve some physical pain. Laughter also boosts the number of antibody-producing cells and enhances the effectiveness of T-cells, leading to a stronger immune system. A 2000 study found that people with heart disease were 40 percent less likely to laugh and be able to recognize humor in a variety of situations, compared to people of the same age without heart disease.



A number of studies using methods of conversation analysis and discourse analysis have documented the systematic workings of laughter in a variety of interactions, from casual conversations to interviews,meetings, and therapy sessions. Working with recorded interactions, researchers have created detailed transcripts that indicate not only the presence of laughter but also features of its production and placement.
These studies challenge several widely held assumptions about the nature of laughter. Contrary to notions that it is spontaneous and involuntary, research documents that laughter is sequentially-organized and precisely placed relative to surrounding talk. Far more than merely a response to humor, laughter often works to manage delicate and serious moments. More than simply an external behavior “caused” by an inner state, laughter is highly communicative and helps accomplish actions and regulate relationships.




Common causes for laughter are sensations of joy and humor; however, other situations may cause laughter as well.
A general theory that explains laughter is called the relief theory. Sigmund Freud summarized it in his theory that laughter releases tension and "psychic energy". This theory is one of the justifications of the beliefs that laughter is beneficial for one's health. This theory explains why laughter can be used as a coping mechanism when one is upset, angry or sad.
Philosopher John Morreall theorizes that human laughter may have its biological origins as a kind of shared expression of relief at the passing of danger. Friedrich Nietzsche, by contrast, suggested laughter to be a reaction to the sense of existential loneliness and mortality that only humans feel.
For example: a joke creates an inconsistency and the audience automatically try to understand what the inconsistency means; if they are successful in solving this 'cognitive riddle' and they realize that the surprise was not dangerous, they laugh with relief. Otherwise, if the inconsistency is not resolved, there is no laugh, as Mack Sennett pointed out: "when the audience is confused, it doesn't laugh." This is one of the basic laws of a comedian, referred to "exactness". It is important to note that sometimes the inconsistency may be resolved and there may still be no laugh. Because laughter is a social mechanism, an audience may not feel as if they are in danger, and the laugh may not occur. In addition, the extent of the inconsistency (and aspects of it timing and rhythm) has to do with the amount of danger the audience feels, and how hard or long they laugh.
Laughter can also be brought on by tickling. Although most people find it unpleasant, being tickled often causes heavy laughter, thought to be an (often uncontrollable) reflex of the body.




Laughter can be classified according to:
intensity: the chuckle, the titter, the giggle, the chortle, the cackle, the belly laugh, the sputtering burst.
the overtness: snicker, snigger, guffaw.
the respiratory pattern involved: snort.
the emotion it is expressed with: relief, mirth, joy, happiness, embarrassment, apology, confusion, nervous laughter, paradoxical laughter, courtesy laugh, evil laughter.
the sequence of notes or pitches it produces. It may be subjectively measured on the Andreoli scale for heartiness, with a higher measure denoting greater robustness, generally in a manly aspect.



A normal laugh has the structure of "ha-ha-ha" or "ho-ho-ho." It is unnatural, and one is physically unable, to have a laugh structure of "ha-ho-ha-ho." The usual variations of a laugh most often occur in the first or final note in a sequence- therefore, "ho-ha-ha" or "ha-ha-ho" laughs are possible. Normal note durations with unusually long or short "inter-note intervals" do not happen due to the result of the limitations of our vocal cords. This basic structure allows one to recognize a laugh despite individual variants.
It has also been determined that eyes moisten during laughter as a reflex from the tear glands.



Laughter is not always a pleasant experience and is associated with several negative phenomena. Excessive laughter can lead to cataplexy, and unpleasant laughter spells, excessive elation, and fits of laughter can all be considered negative aspects of laughter. Unpleasant laughter spells, or "sham mirth," usually occur in people who have a neurological condition, including patients with pseudobulbar palsy, multiple sclerosis and Parkinson's disease. These patients appear to be laughing out of amusement but report that they are feeling undesirable sensations "at the time of the punch line."
Excessive elation is a common symptom associated with manic-depressive psychoses and mania/hypomania. Those who suffer from schizophrenic psychoses seem to suffer the opposite—they do not understand humor or get any joy out of it. A fit describes an abnormal time when one cannot control the laughter or one’s body, sometimes leading to seizures or a brief period of unconsciousness. Some believe that fits of laughter represent a form of epilepsy.



Laughter has been used as a therapeutic tool for many years because it is a natural form of medicine. Laughter is available to everyone and it provides benefits to a person's physical, emotional, and social well being. Some of the benefits of using laughter therapy are that it can relieve stress and relax the whole body. It can also boost the immune system and release endorphins to relieve pain. Additionally, laughter can help prevent heart disease by increasing blood flow and improving the function of blood vessels. Some of the emotional benefits include diminishing anxiety or fear, improving overall mood, and adding joy to one's life. Laughter is also known to reduce allergic reactions in a preliminary study related to dust mite allergy sufferers.
Laughter therapy also has some social benefits, such as strengthening relationships, improving teamwork and reducing conflicts, and making oneself more attractive to others. Therefore, whether a person is trying to cope with a terminal illness or just trying to manage their stress or anxiety levels, laughter therapy can be a significant enhancement to their life.




Laughter in literature, although considered understudied by some, is a subject that has received attention in the written word for millennia. The use of humor and laughter in literary works has been studied and analyzed by many thinkers and writers, from the Ancient Greek philosophers onward. Henri Bergson's Laughter: An Essay on the Meaning of the Comic (Le rire, 1901) is a notable 20th-century contribution.






For Herodotus, laughers can be distinguished into three types:
Those who are innocent of wrongdoing, but ignorant of their own vulnerability
Those who are mad
Those who are overconfident
According to Donald Lateiner, Herodotus reports about laughter for valid literary and historiological reasons. "Herodotus believes either that both nature (better, the gods' direction of it) and human nature coincide sufficiently, or that the latter is but an aspect or analogue of the former, so that to the recipient the outcome is suggested." When reporting laughter, Herodotus does so in the conviction that it tells the reader something about the future and/or the character of the person laughing. It is also in this sense that it is not coincidental that in about 80% of the times when Herodotus speaks about laughter it is followed by a retribution. "Men whose laughter deserves report are marked, because laughter connotes scornful disdain, disdain feeling of superiority, and this feeling and the actions which stem from it attract the wrath of the gods."






Thomas Hobbes understood the superiority of the laughter in a much wider sense than the aesthetic and quasi-moral sense of Aristotle, the seeds of the superiority theory are definitely Greek. In Hobbes' own words: "The passion of laughter is nothing else but sudden glory arising from sudden conception of some eminency in ourselves, by comparison with the infirmity of others, or with our own formerly."



Philosopher Arthur Schopenhauer devotes the 13th chapter of the first part of his major work, The World as Will and Representation, to laughter.



Friedrich Nietzsche distinguishes two different purposes for the use of laughter. In a positive sense, "man uses the comical as a therapy against the restraining jacket of logic morality and reason. He needs from time to time a harmless demotion from reason and hardship and in this sense laughter has a positive character for Nietzsche." Laughter can, however, also have a negative connotation when it is used for the expression of social conflict. This is expressed, for instance, in The Gay Science: "Laughter -- Laughter means to be schadenfroh, but with clear conscience."
"Possibly Nietzsche's works would have had a totally different effect, if the playful, ironical and joking in his writings would have been factored in better"



In Laughter: An Essay on the Meaning of the Comic, French philosopher Henri Bergson, renowned for his philosophical studies on materiality, memory, life and consciousness, tries to determine the laws of the comic and to understand the fundamental causes of comic situations. His method consists in determining the causes of comic instead of analyzing its effects. He also deals with laughter in relation to human life, collective imagination and art, to have a better knowledge of society. One of the theories of the essay is that laughter, as a collective activity, has a social and moral role, in forcing people to eliminate their vices. It is a factor of uniformity of behaviours, as it condemns ludicrous and eccentric behaviours.

In this essay, Bergson also asserts that there is a central cause that all comic situations are derived from: that of mechanism applied to life. The fundamental source of comic is the presence of inflexibility and rigidness in life. For Bergson, the essence of life is movement, elasticity and flexibility, and every comic situation is due the presence of rigidity and inelasticity in life. Hence, for Bergson the source of the comic is not ugliness but rigidity. All the examples taken by Bergson (such as a man falling in the street, one person's imitation of another, the automatic application of conventions and rules, absent-mindedness, repetitive gestures of a speaker, the resemblance between two faces) are comic situations because they give the impression that life is subject to rigidity, automatism and mechanism.
Bergson closes by noting that most comic situations are not laughable because they are part of collective habits. He defines laughter as an intellectual activity that requires an immediate approach to a comic situation, detached from any form of emotion or sensibility. A situation is laughable when the attention and the imagination are focused on the resistance and rigidity of the body. Thus somebody is laughable when he or she gives the impression of being a thing or a machine.



Anthony Ludovici developed the thoughts of Hobbes and Darwin even further in The Secret of Laughter. His conviction is that there's something sinister in laughter, and that the modern omnipressence of humour and the idolatry of it are signs of societal weakness, as instinctive resort to humour became a sort of escapism from responsibility and action. Ludovici considered laughter to be an evolutionary trait and he offered many examples of different triggers for laughter with their own distinct explanations. 



Death from laughter
Evil laughter
Gelotology
Laughter in animals
Laughter Yoga
Nervous laughter
Paradoxical laughter
Pathological laughing and crying






Bachorowski, J.-A., Smoski, M.J., & Owren, M.J. The acoustic features of human laughter. Journal of the Acoustical Society of America, 110 (1581) 2001
Bakhtin, Mikhail (1941). Rabelais and His World. Bloomington: Indiana University Press. ISBN 0-253-34830-7. 
Chapman, Antony J.; Foot, Hugh C.; Derks, Peter (editors), Humor and Laughter: Theory, Research, and Applications, Transaction Publishers, 1996. ISBN 1-56000-837-7. Books.google.com
Cousins, Norman, Anatomy of an Illness As Perceived by the Patient, 1979.
Davila-Ross, M., Allcock, B., Thomas, C., and Bard K.A. (2011) Aping expressions? Chimpanzees produce distinct laugh types when responding to laughter of others. Emotion doi:10.1037/a0022594
Fried, I., Wilson, C.L., MacDonald, K.A., and Behnke EJ. Electric current stimulates laughter. Nature, 391:650, 1998 (see patient AK)
Goel, V. & Dolan, R. J. The functional anatomy of humor: segregating cognitive and affective components. Nature Neuroscience 3, 237 - 238 (2001).
Greig, John Young Thomson, The Psychology of Comedy and Laughter, New York, Dodd, Mead and company, 1923.
Marteinson, Peter, On the Problem of the Comic: A Philosophical Study on the Origins of Laughter, Legas Press, Ottawa, 2006. utoronto.ca
Miller, M; Mangano, C; Park, Y; Goel, R; Plotnick, GD; Vogel, RA (2006). "Impact of cinematic viewing on endothelial function". Heart. 92 (2): 261–2. doi:10.1136/hrt.2005.061424. 
Provine, R. R., Laughter. American Scientist, V84, 38:45, 1996. ucla.edu
Provine, Robert R. (2001). Laughter: A Scientific Investigation. ISBN 978-0141002255. 
Quentin Skinner (2004). "Hobbes and the Classical Theory of Laughter" (PDF). Retrieved 2006-10-23.  included in book: Sorell, Tom; Luc Foisneau (2004). "6" (PDF). Leviathan After 350 Years. Oxford University Press. pp. 139–66. ISBN 0-19-926461-9. ISBN 0-19-926461-9. 
Raskin, Victor, Semantic Mechanisms of Humor (1985).
MacDonald, C., "A Chuckle a Day Keeps the Doctor Away: Therapeutic Humor & Laughter" Journal of Psychosocial Nursing and Mental Health Services(2004) V42, 3:18-25. psychnurse.org
Kawakami, K., et al., Origins of smile and laughter: A preliminary study Early Human Development (2006) 82, 61-66. kyoto-u.ac.jp
Johnson, S., Emotions and the Brain Discover (2003) V24, N4. discover.com
Panksepp, J., Burgdorf, J.,"Laughing" rats and the evolutionary antecedents of human joy? Physiology & Behavior (2003) 79:533-547. psych.umn.edu
Milius, S., Don't look now, but is that dog laughing? Science News (2001) V160 4:55. sciencenews.org
Simonet, P., et al., Dog Laughter: Recorded playback reduces stress related behavior in shelter dogs 7th International Conference on Environmental Enrichment (2005). petalk.org
Discover Health (2004) Humor & Laughter: Health Benefits and Online Sources, helpguide.org
Klein, A. The Courage to Laugh: Humor, Hope and Healing in the Face of Death and Dying. Los Angeles, CA: Tarcher/Putman, 1998.
Ron Jenkins Subversive laughter (New York, Free Press, 1994), 13ff
Bogard, M. Laughter and its Effects on Groups. New York, New York: Bullish Press, 2008.
Humor Theory. The formulae of laughter by Igor Krichtafovitch, Outskitspress, 2006, ISBN 978-1-59800-222-5
Hans-Georg Moeller und Günter Wohlfart (Hrsg.): Laughter in Eastern and Western Philosophies. Verlag Karl Alber, Freiburg / München 2010. ISBN 978-3-495-48385-5



The Origins of Laughter, chass.utoronto.ca
Human laughter up to 16 million years old, cosmosmagazine.com
More information about Gelotology from the University of Washington, faculty.Washington.edu
WNYC's Radio Lab radio show: Is Laughter just a Human Thing?, wnyc.org
Transcriptions of laughter, writtensound.com
Recordings of people laughing, 99 audio examples of human laughter
Comprehensive summary of research on the benefits of laughterSport (UK) or sports (US) are all usually forms of competitive physical activity or games which, through casual or organised participation, aim to use, maintain or improve physical ability and skills while providing enjoyment to participants, and in some cases, entertainment for spectators. Usually the contest or game is between two sides, each attempting to exceed the other. Some sports allow a tie game; others provide tie-breaking methods, to ensure one winner and one loser. A number of such two-sided contests may be arranged in a tournament producing a champion. Many sports leagues make an annual champion by arranging games in a regular sports season, followed in some cases by playoffs. Hundreds of sports exist, from those between single contestants, through to those with hundreds of simultaneous participants, either in teams or competing as individuals. In certain sports such as racing, many contestants may compete, each against each other, with one winner.
Sport is generally recognised as system of activities which are based in physical athleticism or physical dexterity, with the largest major competitions such as the Olympic Games admitting only sports meeting this definition, and other organisations such as the Council of Europe using definitions precluding activities without a physical element from classification as sports. However, a number of competitive, but non-physical, activities claim recognition as mind sports. The International Olympic Committee (through ARISF) recognises both chess and bridge as bona fide sports, and SportAccord, the international sports federation association, recognises five non-physical sports: bridge, chess Pictures of Chess, draughts (checkers), Go, and xiangqi, and limits the number of mind games which can be admitted as sports.
Sports are usually governed by a set of rules or customs, which serve to ensure fair competition, and allow consistent adjudication of the winner. Winning can be determined by physical events such as scoring goals or crossing a line first. It can also be determined by judges who are scoring elements of the sporting performance, including objective or subjective measures such as technical performance or artistic impression.
Records of performance are often kept, and for popular sports, this information may be widely announced or reported in sport news. Sport is also a major source of entertainment for non-participants, with spectator sport drawing large crowds to sport venues, and reaching wider audiences through broadcasting. Sports betting is in some cases severely regulated, and in some cases is central to the sport.
According to A.T. Kearney, a consultancy, the global sporting industry is worth up to $620 billion as of 2013. The world's most accessible and practised sport is running, while association football is its most popular spectator sport.






The word "Sport" comes from the Old French desport meaning "leisure", with the oldest definition in English from around 1300 being "anything humans find amusing or entertaining".
Other meanings include gambling and events staged for the purpose of gambling; hunting; and games and diversions, including ones that require exercise. Roget's defines the noun sport as an "activity engaged in for relaxation and amusement" with synonyms including diversion and recreation.



The singular term "sport" is used in most English dialects to describe the overall concept (e.g. "children taking part in sport"), with "sports" used to describe multiple activities (e.g. "football and rugby are the most popular sports in England"). American English uses "sports" for both terms.




The precise definition of what separates a sport from other leisure activities varies between sources. The closest to an international agreement on a definition is provided by SportAccord, which is the association for all the largest international sports federations (including association football, athletics, cycling, tennis, equestrian sports, and more), and is therefore the de facto representative of international sport.
SportAccord uses the following criteria, determining that a sport should:
have an element of competition
be in no way harmful to any living creature
not rely on equipment provided by a single supplier (excluding proprietary games such as arena football)
not rely on any "luck" element specifically designed into the sport.
They also recognise that sport can be primarily physical (such as rugby or athletics), primarily mind (such as chess or go), predominantly motorised (such as Formula 1 or powerboating), primarily co-ordination (such as billiard sports), or primarily animal-supported (such as equestrian sport).
The inclusion of mind sports within sport definitions has not been universally accepted, leading to legal challenges from governing bodies in regards to being denied funding available to sports. Whilst SportAccord recognises a small number of mind sports, it is not open to admitting any further mind sports.
There has been an increase in the application of the term "sport" to a wider set of non-physical challenges such as video games, also called esports, especially due to the large scale of participation and organised competition, but these are not widely recognised by mainstream sports organisations. According to Council of Europe, European Sports Charter, article 2.i, " "Sport" means all forms of physical activity which, through casual or organised participation, aim at expressing or improving physical fitness and mental well-being, forming social relationships or obtaining results in competition at all levels.".



There are opposing views on the necessity of competition as a defining element of a sport, with almost all professional sport involving competition, and governing bodies requiring competition as a prerequisite of recognition by the International Olympic Committee (IOC) or SportAccord.
Other bodies advocate widening the definition of sport to include all physical activity. For instance, the Council of Europe include all forms of physical exercise, including those competed just for fun.
In order to widen participation, and reduce the impact of losing on less able participants, there has been an introduction of non-competitive physical activity to traditionally competitive events such as school sports days, although moves like this are often controversial.
In competitive events, participants are graded or classified based on their "result" and often divided into groups of comparable performance, (e.g. gender, weight and age). The measurement of the result may be objective or subjective, and corrected with "handicaps" or penalties. In a race, for example, the time to complete the course is an objective measurement. In gymnastics or diving the result is decided by a panel of judges, and therefore subjective. There are many shades of judging between boxing and mixed martial arts, where victory is assigned by judges if neither competitor has lost at the end of the match time.




Artifacts and structures suggest sport in China as early as 2000 BC. Gymnastics appears to have been popular in China's ancient past. Monuments to the Pharaohs indicate that a number of sports, including swimming and fishing, were well-developed and regulated several thousands of years ago in ancient Egypt. Other Egyptian sports included javelin throwing, high jump, and wrestling. Ancient Persian sports such as the traditional Iranian martial art of Zourkhaneh had a close connection to warfare skills. Among other sports that originate in ancient Persia are polo and jousting.

A wide range of sports were already established by the time of Ancient Greece and the military culture and the development of sports in Greece influenced one another considerably. Sports became such a prominent part of their culture that the Greeks created the Olympic Games, which in ancient times were held every four years in a small village in the Peloponnesus called Olympia.
Sports have been increasingly organised and regulated from the time of the ancient Olympics up to the present century. Industrialisation has brought increased leisure time, letting people attend and follow spectator sports and participate in athletic activities. These trends continued with the advent of mass media and global communication. Professionalism became prevalent, further adding to the increase in sport's popularity, as sports fans followed the exploits of professional athletes — all while enjoying the exercise and competition associated with amateur participation in sports. Since the turn of the 21st century, there has been increasing debate about whether transgender sportpersons should be able to participate in sport events that conform with their post-transition gender identity.







Sportsmanship is an attitude that strives for fair play, courtesy toward teammates and opponents, ethical behaviour and integrity, and grace in victory or defeat.
Sportsmanship expresses an aspiration or ethos that the activity will be enjoyed for its own sake. The well-known sentiment by sports journalist Grantland Rice, that it's "not that you won or lost but how you played the game", and the modern Olympic creed expressed by its founder Pierre de Coubertin: "The most important thing... is not winning but taking part" are typical expressions of this sentiment.




Key principles of sport include that the result should not be predetermined, and that both sides should have equal opportunity to win. Rules are in place to ensure that fair play to occur, but participants can break these rules in order to gain advantage.
Participants may choose to cheat in order to satisfy their desire to win, or in order to achieve an ulterior motive. The widespread existence of gambling on the results of sports fixtures creates the motivation for match fixing, where a participant or participants deliberately work to ensure a given outcome.




The competitive nature of sport encourages some participants to attempt to enhance their performance through the use of medicines, or through other means such as increasing the volume of blood in their bodies through artificial means.
All sports recognised by the IOC or SportAccord are required to implement a testing programme, looking for a list of banned drugs, with suspensions or bans being placed on participants who test positive for banned substances.



Violence in sports involves crossing the line between fair competition and intentional aggressive violence. Athletes, coaches, fans, and parents sometimes unleash violent behaviour on people or property, in misguided shows of loyalty, dominance, anger, or celebration. Rioting or hooliganism by fans in particular is a problem at some national and international sporting contests.







Female participation in sports continues to rise alongside the opportunity for involvement and the value of sports for child development and physical fitness. Despite gains during the last three decades, a gap persists in the enrollment figures between male and female players. Female players account for 39% of the total participation in US interscholastic athletics. Gender balance has been accelerating from a 32% increase in 1973–74 to a 63% increase in 1994–95. Hessel (2000).



Youth sports present children with opportunities for fun, socialization, forming peer relationships, physical fitness, and athletic scholarships. Activists for education and the war on drugs encourage youth sports as a means to increase educational participation and to fight the illegal drug trade. According to the Center for Injury Research and Policy at Nationwide Children's Hospital, the biggest risk for youth sports is death or serious injury including concussion. These risks come from running, basketball, association football, volleyball, gridiron, gymnastics, and ice hockey.




Disabled sports also adaptive sports or parasports, are sports played by persons with a disability, including physical and intellectual disabilities. As many of these based on existing sports modified to meet the needs of persons with a disability, they are sometimes referred to as adapted sports. However, not all disabled sports are adapted; several sports that have been specifically created for persons with a disability have no equivalent in able-bodied sports.




The competition element of sport, along with the aesthetic appeal of some sports, result in the popularity of people attending to watch sport being played. This has led to the specific phenomenon of spectator sport.
Both amateur and professional sports attract spectators, both in person at the sport venue, and through broadcast mediums including radio, television and internet broadcast. Both attendance in person and viewing remotely can incur a sometimes substantial charge, such as an entrance ticket, or pay-per-view television broadcast.
It is common for popular sports to attract large broadcast audiences, leading to rival broadcasters bidding large amounts of money for the rights to show certain fixtures. The football World Cup attracts a global television audience of hundreds of millions; the 2006 final alone attracted an estimated worldwide audience of well over 700 million and the 2011 Cricket World Cup Final attracted an estimated audience of 135 million in India alone .
In the United States, the championship game of the NFL, the Super Bowl, has become one of the most watched television broadcasts of the year. Super Bowl Sunday is a de facto national holiday in America; the viewership being so great that in 2015, advertising space was reported as being sold at $4.5m for a 30-second slot.







Sport can be undertaken on an amateur, professional or semi-professional basis, depending on whether participants are incentivised for participation (usually through payment of a wage or salary). Amateur participation in sport at lower levels is often called "grassroots sport".
The popularity of spectator sport as a recreation for non-participants has led to sport becoming a major business in its own right, and this has incentivised a high paying professional sport culture, where high performing participants are rewarded with pay far in excess of average wages, which can run into millions of dollars.
Some sports, or individual competitions within a sport, retain a policy of allowing only amateur sport. The Olympic Games started with a principle of amateur competition with those who practiced a sport professionally considered to have an unfair advantage over those who practiced it merely as a hobby. From 1971, Olympic athletes were allowed to receive compensation and sponsorship, and from 1986, the IOC decided to make all professional athletes eligible for the Olympics, with the exceptions of boxing, and wrestling.



Technology plays an important part in modern sports. With it being a necessary part of some sports (such as motorsport), it is used in others to improve performance. Some sports also use it to allow off-field decision making.
Sports science is a widespread academic discipline, and can be applied to areas including athlete performance, such as the use of video analysis to fine-tune technique, or to equipment, such as improved running shoes or competitive swimwear. Sports engineering emerged as a discipline in 1998 with an increasing focus not just on materials design but also the use of technology in sport, from analytics and big data to wearable technology. In order to control the impact of technology on fair play, governing bodies frequently have specific rules that are set to control the impact of technical advantage between participants. For example, in 2010, full-body, non-textile swimsuits were banned by FINA, as they were enhancing swimmers' performances.
The increase in technology has also allowed many decisions in sports matches to be taken, or reviewed, off-field, with another official using instant replays to make decisions. In some sports, players can now challenge decisions made by officials. In football, Goal-line technology makes decisions on whether a ball has crossed the goal line or not. The technology is not compulsory, but was used in the 2014 FIFA World Cup in Brazil, and the 2015 FIFA Women's World Cup in Canada, as well as in the Premier League from 2013–14, and the Bundesliga from 2015–16. In the NFL, a referee can ask for a review from the replay booth, or a head coach can issue a challenge to review the play using replays. The final decision rests with the referee. A video referee (commonly known as a Television Match Official or TMO) can also use replays to help decision-making in rugby (both league and union). In international cricket, an umpire can ask the Third umpire for a decision, and the third umpire makes the final decision. Since 2008, a decision review system for players to review decisions has been introduced and used in ICC-run tournaments, and optionally in other matches. Depending on the host broadcaster, a number of different technologies are used during an umpire or player review, including instant replays, Hawk-Eye, Hot Spot and Real Time Snickometer. Hawk-Eye is also used in tennis to challenge umpiring decisions.




Sports and politics can influence each other greatly.
Benito Mussolini used the 1934 FIFA World Cup, which was held in Italy, to showcase Fascist Italy. Adolf Hitler also used the 1936 Summer Olympics held in Berlin, and the 1936 Winter Olympics held in Garmisch-Partenkirchen, to promote the Nazi ideology of the superiority of the Aryan race, and inferiority of the Jews and other "undesirables". Germany used the Olympics to give of itself a peaceful image while it was very actively preparing the war.
When apartheid was the official policy in South Africa, many sports people, particularly in rugby union, adopted the conscientious approach that they should not appear in competitive sports there. Some feel this was an effective contribution to the eventual demolition of the policy of apartheid, others feel that it may have prolonged and reinforced its worst effects.
In the history of Ireland, Gaelic sports were connected with cultural nationalism. Until the mid 20th century a person could have been banned from playing Gaelic football, hurling, or other sports administered by the Gaelic Athletic Association (GAA) if she/he played or supported football, or other games seen to be of British origin. Until recently the GAA continued to ban the playing of football and rugby union at Gaelic venues. This ban, also known as Rule 42, is still enforced, but was modified to allow football and rugby to be played in Croke Park while Lansdowne Road was redeveloped into Aviva Stadium. Until recently, under Rule 21, the GAA also banned members of the British security forces and members of the RUC from playing Gaelic games, but the advent of the Good Friday Agreement in 1998 led to the eventual removal of the ban.
Nationalism is often evident in the pursuit of sports, or in its reporting: people compete in national teams, or commentators and audiences can adopt a partisan view. On occasion, such tensions can lead to violent confrontation among players or spectators within and beyond the sporting venue, as in the Football War. These trends are seen by many as contrary to the fundamental ethos of sports being carried on for its own sake and for the enjoyment of its participants.
A very famous case when sports and politics collided was the 1972 Olympics in Munich. Masked men entered the hotel of the Israeli olympic team and killed many of their men. This was known as the Munich massacre.
A study of US elections has shown that the result of sports events can affect the results. A study published in the Proceedings of the National Academy of Sciences showed that when the home team wins the game before the election, the incumbent candidates can increase their share of the vote by 1.5 percent. A loss had the opposite effect, and the effect is greater for higher-profile teams or unexpected wins and losses. Also, when Washington Redskins win their final game before an election, then the incumbent President is more likely to win, and if the Redskins lose, then the opposition candidate is more likely to win; this has become known as the Redskins Rule.



Étienne de La Boétie, in his essay Discourse on Voluntary Servitude describes athletic spectacles as means for tyrants to control their subjects by distracting them.

Do not imagine that there is any bird more easily caught by decoy, nor any fish sooner fixed on the hook by wormy bait, than are all these poor fools neatly tricked into servitude by the slightest feather passed, so to speak, before their mouths. Truly it is a marvelous thing that they let themselves be caught so quickly at the slightest tickling of their fancy. Plays, farces, spectacles, gladiators, strange beasts, medals, pictures, and other such opiates, these were for ancient peoples the bait toward slavery, the price of their liberty, the instruments of tyranny. By these practices and enticements the ancient dictators so successfully lulled their subjects under the yoke, that the stupefied peoples, fascinated by the pastimes and vain pleasures flashed before their eyes, learned subservience as naïvely, but not so creditably, as little children learn to read by looking at bright picture books.



The practice of athletic competitions has been criticized by some Christian thinkers as a form of idolatry, in which "human beings extol themselves, adore themselves, sacrifice themselves and reward themselves." Sports are seen by these critics as a manifestation of "collective pride" and "national self-deification" in which feats of human power are idolized at the expense of divine worship.
Tertullian condemns the athletic performances of his day, insisting "the entire apparatus of the shows is based upon idolatry." The shows, says Tertullian, excite passions foreign to the calm temperament cultivated by the Christian:

God has enjoined us to deal calmly, gently, quietly, and peacefully with the Holy Spirit, because these things are alone in keeping with the goodness of His nature, with His tenderness and sensitiveness. ... Well, how shall this be made to accord with the shows? For the show always leads to spiritual agitation, since where there is pleasure, there is keenness of feeling giving pleasure its zest; and where there is keenness of feeling, there is rivalry giving in turn its zest to that. Then, too, where you have rivalry, you have rage, bitterness, wrath and grief, with all bad things which flow from them—the whole entirely out of keeping with the religion of Christ.




Related topics




European Commission (2007), The White Paper on Sport.
Council of Europe (2001), The Europien sport charter.



The Meaning of Sports by Michael Mandel (PublicAffairs, ISBN 1-58648-252-1).
Journal of the Philosophy of Sport
Sullivan, George. The Complete Sports Dictionary. New York: Scholastic Book Services, 1979. 199 p. ISBN 0-590-05731-6


In modern English, a casino is a facility which houses and accommodates certain types of gambling activities. The industry that deals in casinos is called the gaming industry. Casinos are most commonly built near or combined with hotels, restaurants, retail shopping, cruise ships or other tourist attractions. There is much debate over whether or not the social and economic consequences of casino gambling outweigh the initial revenue that may be generated. Some casinos are also known for hosting live entertainment events, such as stand-up comedy, concerts, and sporting events.



The term "casino" is a confusing linguistic false friend for translators.
Casino is of Italian origin; the root casa (house) originally meant a small country villa, summerhouse, or social club. During the 19th century, the term casino came to include other public buildings where pleasurable activities took place; such edifices were usually built on the grounds of a larger Italian villa or palazzo, and were used to host civic town functions, including dancing, gambling, music listening, and sports; examples in Italy include Villa Farnese and Villa Giulia, and in the US the Newport Casino in Newport, Rhode Island. In modern-day Italian, the term casino designates a bordello (also called casa chiusa, literally "closed house"), while the gambling house is spelled casinò with an accent.
Not all casinos were used for gaming. The Catalina Casino, a famous landmark overlooking Avalon Harbor on Santa Catalina Island, California, has never been used for traditional games of chance, which were already outlawed in California by the time it was built. The Copenhagen Casino was a theatre, known for the mass public meetings often held in its hall during the 1848 Revolution, which made Denmark a constitutional monarchy. Until 1937, it was a well-known Danish theatre. The Hanko Casino in Hanko, Finland—one of that town's most conspicuous landmarks—was never used for gambling. Rather, it was a banquet hall for the Russian nobility which frequented this spa resort in the late 19th century and is now used as a restaurant.
In military and non-military usage in German and Spanish, a casino or kasino is an officers' mess. In Italian—the source-language of the word—a casino is either a brothel, a mess, or a noisy environment, while a gaming house is called a casinò.



The precise origin of gambling is unknown. It is generally believed that gambling in some form or another has been seen in almost every society in history. From the Ancient Greeks and Romans to Napoleon's France and Elizabethan England, much of history is filled with stories of entertainment based on games of chance.
The first known European gambling house, not called a casino although meeting the modern definition, was the Ridotto, established in Venice, Italy in 1638 by the Great Council of Venice to provide controlled gambling during the carnival season. It was closed in 1770 as the city government felt it was impoverishing the local gentry.
In American history, early gambling establishments were known as saloons. The creation and importance of saloons was greatly influenced by four major cities: New Orleans, St. Louis, Chicago and San Francisco. It was in the saloons that travelers could find people to talk to, drink with, and often gamble with. During the early 20th century in America, gambling became outlawed and banned by state legislation and social reformers of the time. However, in 1931, gambling was legalized throughout the state of Nevada. America's first legalized casinos were set up in those places. In 1978 New Jersey allowed gambling in Atlantic City, now America's second largest gambling city.




Most jurisdictions worldwide have a minimum gambling age (16 to 21 years of age in most countries which permit the operation of casinos).
Customers gamble by playing games of chance, in some cases with an element of skill, such as craps, roulette, baccarat, blackjack, and video poker. Most games played have mathematically determined odds that ensure the house has at all times an overall advantage over the players. This can be expressed more precisely by the notion of expected value, which is uniformly negative (from the player's perspective). This advantage is called the house edge. In games such as poker where players play against each other, the house takes a commission called the rake. Casinos sometimes give out complimentary items or comps to gamblers.
Payout is the percentage of funds ("winnings") returned to players.
Casinos in the United States say that a player staking money won from the casino is playing with the house's money.
Video Lottery Machines (slot machines) have become one of the most popular forms of gambling in casinos. As of 2011 investigative reports have started calling into question whether the modern-day slot-machine is addictive.



Casino design—regarded as a psychological exercise—is an intricate process that involves optimising floor plan, décor and atmospherics to encourage consumer gambling.
Factors influencing consumer gambling tendencies include sound, odour and lighting. Natasha Dow Schüll, an anthropologist at the Massachusetts Institute of Technology, highlights the audio directors at Silicon Gaming’s decision to make its slot machines resonate in, “the universally pleasant tone of C, sampling existing casino soundscapes to create a sound that would please but not clash”.
Dr Alan Hirsch, founder of the Smell & Taste Treatment and Research Foundation in Chicago, studied the impact of certain scents on gamblers, discerning that a pleasant albeit unidentifiable odour released by Las Vegas slots machines generated approximately 50% more in daily revenue. He suggested that the scent acted as an aphrodisiac, facilitating a more aggressive form of gambling.
Casino designer Roger Thomas is credited with implementing a successful, disruptive design for the Las Vegas Wynn Resorts’ casinos in 2008. He broke casino design convention by introducing natural sunlight and flora to appeal to a female demographic. Thomas inserted skylights and antique clocks, defying the commonplace notion that a casino should be a timeless space.



The following lists major casino markets in the world with casino revenue of over $1 billion USD as published in PricewaterhouseCoopers's report on the outlook for the global casino market:









According to Bloomberg, accumulated revenue of biggest casino operator companies worldwide amounted almost 55 billion US dollars as per 2011. SJM Holdings ltd. was the leading company in this field and earned 9.7 billion in 2011, followed by Las Vegas Sands Corp. (7.4 bn). The third biggest casino operator company (based on revenue) was Caesars Entertainment with revenue of 6.2 bn US dollar.



While there are casinos in many places, a few places have become well-known specifically for gambling. Perhaps the place almost defined by its casino is Monte Carlo, but other places are known as gambling centers.




Monte Carlo has a famous casino popular with well-off visitors and is a tourist attraction in its own right. A song and a film named The Man Who Broke the Bank at Monte Carlo need no explanation—they clearly refer to the casino.
Monte Carlo's Casino has also been depicted in many books including Ben Mezrich's Busting Vegas, where a group of Massachusetts Institute of Technology students beat the casino out of nearly $1 000 000. This book is based on real people and events; however, many of those events are contested by main character Semyon Dukach.
The casino has made Monte Carlo so well known for games of chance that mathematical methods for solving various problems using many quasi-random numbers—numbers with the statistical distribution of numbers generated by chance—are formally known as Monte Carlo methods. Monte Carlo was part of the plot in a few James Bond novels and films.




The former Portuguese colony of Macau, a special administrative region of China since 1999, is a popular destination for visitors who wish to gamble. This started in Portuguese times, when Macau was popular with visitors from nearby British Hong Kong where gambling was more closely regulated. The Venetian Macao is currently the largest casino in the world. Macau also surpassed Las Vegas as the largest gambling market in the world.




Singapore is an up-and-coming destination for visitors wanting to gamble, although there are currently only two casinos (both foreign owned), in Singapore. The Marina Bay Sands is the most expensive standalone casino in the world, at a price of US$8 billion, and is among the world's ten most expensive buildings. The Resorts World Sentosa has the world's largest oceanarium.




With currently over 900 casinos, the United States has the largest number of casinos in the world. The number continues to steadily grow as more states seek to legalize casinos. 38 states now have some form of casino gambling. Relatively small places such as Las Vegas are best known for gambling; larger cities such as Chicago are not defined by their casinos in spite of the large turnover.
The Las Vegas Valley has the largest concentration of casinos in the United States. Based on revenue, Atlantic City, New Jersey ranks second, and the Chicago region third.
Top American casino markets by revenue (2009 annual revenues):
Las Vegas Strip $5.550 billion
Atlantic City $3.943 billion
Chicago region $2.092 billion
Connecticut $1.448 billion
Detroit $1.36 billion
St. Louis $1.050 billion
Tunica Resorts, Mississippi $997.02 million
Biloxi, Mississippi $833.50 million
Shreveport, Louisiana $779.65 million
Boulder Strip (Las Vegas) $774.33 million
Reno, Nevada $715.23 million
New Orleans $653.05 million
Downtown Las Vegas $523.82 million
Laughlin, Nevada $492.51 million
The Nevada Gaming Control Board divides Clark County, which is coextensive with the Las Vegas metropolitan area, into seven regions for reporting purposes.
Indian gaming has been responsible for a rise in the number of casinos outside of Las Vegas and Atlantic City.




Given the large amounts of currency handled within a casino, both patrons and staff may be tempted to cheat and steal, in collusion or independently; most casinos have security measures to prevent this. Security cameras located throughout the casino are the most basic measure.
Modern casino security is usually divided between a physical security force and a specialized surveillance department. The physical security force usually patrols the casino and responds to calls for assistance and reports of suspicious or definite criminal activity. A specialized surveillance department operates the casino's closed circuit television system, known in the industry as the eye in the sky. Both of these specialized casino security departments work very closely with each other to ensure the safety of both guests and the casino's assets, and have been quite successful in preventing crime. Some casinos also have catwalks in the ceiling above the casino floor, which allow surveillance personnel to look directly down, through one way glass, on the activities at the tables and slot machines.
When it opened in 1989, The Mirage was the first casino to use cameras full-time on all table games.
In addition to cameras and other technological measures, casinos also enforce security through rules of conduct and behavior; for example, players at card games are required to keep the cards they are holding in their hands visible at all times.



Over the past few decades, casinos have developed many different marketing techniques for attracting and maintaining loyal patrons. Many casinos use a loyalty rewards program used to track players' spending habits and target their patrons more effectively, by sending mailings with free slot play and other promotions.



One area of controversy surrounding casinos is their relationship to crime rates. Economic studies that show a positive relationship between casinos and crime usually fail to consider the visiting population at risk when they calculate the crime rate in casino areas. Such studies thus count the crimes committed by visitors, but do not count visitors in the population measure, and this overstates the crime rates in casino areas. Part of the reason this methodology is used, despite it leading to an overstatement of crime rates is that reliable data on tourist count are often not available. In a 2004 report by the US Department of Justice, researchers interviewed people who had been arrested in Las Vegas and Des Moines and found that the percentage of problem or pathological gamblers among the arrestees was three to five times higher than in the general population. According to some police reports, incidences of reported crime often double and triple in communities within three years of a casino opening.












Worldwide Casino Listings at DMOZA statue is a sculpture representing one or more people or animals (including abstract concepts allegorically represented as people or animals), normally full-length, as opposed to a bust, and at least close to life-size, or larger. A small statue, usually small enough to be picked up, is called a statuette or figurine, while one that is more than twice life-size is called a colossal statue.
The definition of a statue is not always clear-cut; equestrian statues, of a person on a horse, are certainly included, and in many cases, such as a Madonna and Child or a Pietà, a sculpture of two people will also be.
Statues have been produced in many cultures from prehistory to the present; the oldest known statue dating to about 30,000 years ago. The world's tallest statue, Spring Temple Buddha, is 128 metres (420 ft), and is located in Lushan County, Henan, China.
Many statues are built on commission to commemorate a historical event, or the life of an influential person. Many statues are intended as public art, exhibited outdoors or in public buildings. Some statues gain fame in their own right, separate from the person or concept they represent, as with the Statue of Liberty.



Ancient statues often survive showing the bare surface of the material of which they are made. For example, many people associate Greek classical art with white marble sculpture, but there is evidence that many statues were painted in bright colours. Most of the colour was weathered off over time; small remnants were removed during cleaning; in some cases small traces remained which could be identified. A travelling exhibition of 20 coloured replicas of Greek and Roman works, alongside 35 original statues and reliefs, was held in Europe and the United States in 2008: Gods in Color: Painted Sculpture of Classical Antiquity. Details such as whether the paint was applied in one or two coats, how finely the pigments were ground, or exactly which binding medium would have been used in each case—all elements that would affect the appearance of a finished piece—are not known. Richter goes so far as to say of classical Greek sculpture, "`All stone sculpture, whether limestone or marble, was painted, either wholly or in part." 
Medieval statues were also usually painted, with some still retaining their original pigments. The colouring of statues ceased during the Renaissance, as excavated classical sculptures, which had lost their colouring, became regarded as the best models.






The Löwenmensch figurine from the Swabian Alps in Germany is the oldest known statue in the world, and dates to 30,000-40,000 years ago. The Venus of Hohle Fels, from the same area, is somewhat later. Throughout history, statues have been associated with cult images in many religious traditions, from Ancient Egypt, Ancient Greece, and Ancient Rome to the present.
Egyptian statues showing kings as sphinxes have existed since the Old Kingdom, the oldest being for Djedefre (c. 2500 BC). The oldest statue of a striding pharaoh dates from the reign of Senwosret I (c. 1950 BC) and is the Egyptian Museum, Cairo. The Middle Kingdom of Egypt (starting around 2000 BC) witnessed the growth of block statues which then became the most popular form until the Ptolemaic period (c. 300 BC).
The oldest statue of a deity in Rome was the bronze statue of Ceres in 485 BC. The oldest statue in Rome is now the statue of Diana on the Aventine.
The wonders of the world include several statues from antiquity, with the Colossus of Rhodes and the Statue of Zeus at Olympia among the Seven Wonders of the Ancient World.



While Byzantine art flourished in various forms, sculpture and statue making witnessed a general decline; although statues of emperors continued to appear. An example was the statue of Justinian (6th century) which stood in the square across from the Hagia Sophia until the fall of Constantinople in the 15th century. Part of the decline in statue making in the Byzantine period can be attributed to the mistrust the Church placed in the art form, given that it viewed sculpture in general as a method for making and worshiping idols. While making statues was not subject to a general ban, it was hardly encouraged in this period. Justinian was one of the last Emperors to have a full-size statue made, and secular statues of any size became virtually non-existent after iconoclasm; and the artistic skill for making statues was lost in the process.



Starting with the work of Maillol around 1900, the human figures embodied in statues began to move away from the various schools of realism that had held them bound for thousands of years. The Futurist and Cubist schools took this metamorphism even further until statues, often still nominally representing humans, had lost all but the most rudimentary relationship to the human form. By the 1920s and 1930s statues began to appear that were completely abstract in design and execution.
The notion that the position of the hooves of horses in equestrian statues indicated the rider's cause of death has been disproved.












UK Public Monument and Sculpture AssociationNominative determinism is the hypothesis that people tend to gravitate towards areas of work that fit their name. The term was first used in the magazine New Scientist in 1994, after the magazine's humorous Feedback column noted several studies carried out by researchers with remarkably fitting surnames. These included a book on polar explorations by Daniel Snowman and an article on urology by researchers named Splatt and Weedon. These and other examples led to light-hearted speculation that some sort of psychological effect was at work. Since the term appeared, nominative determinism has been an irregularly recurring topic in New Scientist, as readers continue to submit examples. Nominative determinism differs from the related concept aptronym, and its synonyms aptonym, namephreak, and Perfect Fit Last Name, in that it focusses on causality. An aptronym merely means the name is fitting, without saying anything about why it has come to fit.
The idea that people are drawn to professions that fit their name was suggested by psychologist Carl Jung, citing as an example Sigmund Freud who studied pleasure and whose name means "joy". A few recent empirical studies have indicated that certain professions are disproportionately represented by people with appropriate surnames, though the methods of these studies have been challenged. One explanation for nominative determinism is implicit egotism, which states that humans have an unconscious preference for things they associate with themselves. An alternative explanation is genetic: a person might be named Smith or Taylor because that was originally their occupation, and they would pass on their genes to their descendants, including an aptitude for activities involving strength in the case of Smith, or dexterity in the case of Taylor.



Before people could gravitate towards areas of work that matched their name, many people were given names that matched their area of work. The way people are named has changed over time. In pre-urban times people were only known by a single name, for example, the Anglo-Saxon name Beornheard. Single names were chosen for their meaning or given as nicknames. In England it was not until after the Norman conquest that surnames were added. Surnames were created to fit the person, mostly from patronyms (e.g., son of William becomes John Williamson), occupational descriptions (e.g., John Carpenter), character or traits (e.g., John Long), or location (e.g., John from Acton became John Acton). Names were not initially hereditary; only by the mid-14th century did they gradually become so. Surnames relating to trades or craft were the first to become hereditary, as the craft often persisted within the family for generations. The appropriateness of occupational names has decreased over time, because tradesmen did not always follow their fathers: an early example from the 14th century is "Roger Carpenter the pepperer."
Another aspect of naming was the importance attached to the wider meaning contained in a name. In 17th-century England it was believed that choosing a name for a child should be done carefully. Children should live according to the message contained in, or the meaning of their names. In 1652 William Jenkyn, an English clergyman, argued that first names should be "as a thread tyed about the finger to make us mindful of the errand we came into the world to do for our Master." In 1623, at a time when Puritan names such as Faith, Fortitude and Grace were appearing for the first time, English historian William Camden wrote that names should be chosen with "good and gracious significations", as they might inspire the bearer to good actions. With the rise of the British Empire the English naming system and English surnames spread across large portions of the globe.
By the beginning of the 20th century, Smith and Taylor were two of the three most frequently occurring English surnames; both were occupational, though few smiths and tailors remained. When a correspondence between a name and an occupation did occur, it became worthy of note. In an 1888 issue of the Kentish Note Book magazine a list appeared with "several carriers by the name of Carter; a hosier named Hosegood; an auctioneer named Sales; and a draper named Cuff". Since then, a variety of terms for the concept of a close relationship between name and occupation have emerged. The term aptronym is thought to have been coined in the early 20th century by the American newspaper columnist Franklin P. Adams. Linguist Frank Nuessel coined aptonym, without an 'r', in 1992. Other synonyms include euonym, Perfect Fit Last Name (PFLN), and namephreak. In literary science a name that particularly suits a character is called a charactonym. Notable authors who frequently used charactonyms as a stylistic technique are Charles Dickens, (e.g., Scrooge, the tightfisted miser), and William Shakespeare, (e.g., the lost baby Perdita in The Winter's Tale). Unlike nominative determinism the concept of aptronym and its synonyms do not say anything about causality, i.e. why the name has come to fit.
Because of the potentially humorous nature of aptronyms a number of newspapers have collected them. San Francisco Chronicle columnist Herb Caen reported irregularly on reader-submitted gems, including substitute teacher Mr. Fillin, piano teacher Patience Scales, and the Vatican's spokesman on the evils of rock 'n roll, Cardinal Rapsong. Similarly, the journalist Bob Levey on occasion listed examples sent in by readers of his column in the Washington Post: a food industry consultant named Faith Popcorn, a lieutenant called Sergeant, and a tax accountant called Shelby Goldgrab. Dutch newspaper Het Parool had an irregularly featured column called "Nomen est omen" with Dutch examples. Individual name collectors have also published books of aptronyms. Onomastic scholar R.M. Rennick called for more verification of aptronyms appearing in newspaper columns and books. Lists of aptronyms in science, medicine, and law are more reliable as they tend to be drawn from easily verifiable sources.



Nominative determinism, literally "name-driven outcome", is the hypothesis that people tend to gravitate towards areas of work which reflect their names. The name fits because people, possibly subconsciously, made themselves fit. Nominative determinism differs from the concept of aptronyms in that it focusses on causality.
The term has its origin in the "Feedback" column of the British magazine New Scientist in 1994. A series of events raised the suspicion of its editor, John Hoyland, who wrote in the 5 November issue:
"We recently came across a new book, Pole Positions—The Polar Regions and the Future of the Planet, by Daniel Snowman. Then, a couple of weeks later, we received a copy of London Under London—A Subterranean Guide, one of the authors of which is Richard Trench. So it was interesting to see Jen Hunt of the University of Manchester stating in the October issue of The Psychologist: "Authors gravitate to the area of research which fits their surname." Hunt's example is an article on incontinence in the British Journal of Urology by A. J. Splatt and D. Weedon.
We feel it's time to open up this whole issue to rigorous scrutiny. You are invited to send in examples of the phenomenon in the fields of science and technology (with references that check out, please) together with any hypotheses you may have on how it comes about."
Feedback editors John Hoyland and Mike Holderness subsequently adopted the term nominative determinism as suggested by reader C. R. Cavonius. The term first appeared in the 17 December issue. Even though the magazine tried to ban the topic numerous times over the decades since, readers kept sending in curious examples. These included the US navy spokesman put up to answer journalists' questions about the Guantanamo Bay detention camp, one Lieutenant Mike Kafka; authors of the book The Imperial Animal Lionel Tiger and Robin Fox; and the UK Association of Chief Police Officers' spokesman on knife crime, Alfred Hitchcock.
As used in New Scientist the term nominative determinism only applies to work. In contributions to other newspapers New Scientist writers have stuck to this definition, with the exception of editor Roger Highfield in a column in the Evening Standard, in which he included "key attributes of life".
Prior to 1994 other terms for the suspected psychological effect were used sporadically. Onomastic determinism was used as early as 1970 by Roberta Frank. German psychologist Wilhelm Stekel spoke of "Die Verpflichtung des Namens" (the obligation of the name) in 1911. Outside of science, cognomen syndrome was used by playwright Tom Stoppard in his 1972 play Jumpers. In Ancient Rome the predictive power of a person's name was captured by the Latin proverb "nomen est omen", meaning the name is a sign. This saying is still in use today in English and other languages such as French, German, Italian, Dutch, and Slovenian.
New Scientist coined the term nominative contradeterminism for people who move away from their name, creating a contradiction between name and occupation. Examples include Andrew Waterhouse, a professor of wine, would-be doctor Thomas Edward Kill, who subsequently changed his name to Jirgensohn, and the Archbishop of Manila, Cardinal Sin. The synonym inaptronym is also sometimes used.






The first scientists to discuss the concept that names had a determining effect were early 20th century German psychologists. Wilhelm Stekel spoke of the "obligation of the name" in the context of compulsive behaviour and choice of occupation; Karl Abraham wrote that the determining power of names might be partially caused by inheriting a trait from an ancestor who was given a fitting name. He made the further inference that families with fitting names might then try to live up to their names in some way. In 1952 Carl Jung referred to Stekel's work in his theory of synchronicity (events without causal relationship that yet seem to be meaningfully related):
"We find ourselves in something of a quandary when it comes to making up our minds about the phenomenon which Stekel calls the 'compulsion of the name'. What he means by this is the sometimes quite gross coincidence between a man's name and his peculiarities or profession. For instance ... Herr Feist (Mr Stout) is the food minister, Herr Rosstäuscher (Mr Horsetrader) is a lawyer, Herr Kalberer (Mr Calver) is an obstetrician ... Are these the whimsicalities of chance, or the suggestive effects of the name, as Stekel seems to suggest, or are they 'meaningful coincidences'?"
Jung listed striking instances among psychologists—including himself: "Herr Freud (Joy) champions the pleasure principle, Herr Adler (Eagle) the will to power, Herr Jung (Young) the idea of rebirth ..."
In 1975 psychologist Lawrence Casler called for empirical research into the relative frequencies of career-appropriate names to establish if there is an effect at work or whether we are being "seduced by Lady Luck". He proposed three possible explanations for nominative determinism: 1) one's self-image and self-expectation being internally influenced by one's name; 2) the name acting as a social stimulus, creating expectations in others that are then communicated to the individual; 3) genetics – attributes suited to a particular career being passed down the generations alongside the appropriate occupational surname.
In 2002 the researchers Pelham, Mirenberg, and Jones explored Casler's first explanation, arguing that people have a basic desire to feel good about themselves and behave according to that desire. These automatic positive associations would influence feelings about almost anything associated with the self. Given the mere ownership effect, which states that people like things more if they own them, the researchers theorised that people would develop an affection for objects and concepts that are associated with the self, such as their name. They called this unconscious power implicit egotism. Uri Simonsohn suggested that implicit egotism only applies to cases where people are nearly indifferent between options, and therefore it would not apply to major decisions such as career choices. Low-stakes decisions such as choosing a charity would show an effect. Raymond Smeets theorised that if implicit egotism stems from a positive evaluation of the self, then people with low self-esteem would not gravitate towards choices associated with the self, but possibly away from them. A lab experiment confirmed this.




Those with fitting names give differing accounts of the effect of their name on their career choices. Igor Judge, former Lord Chief Justice of England and Wales, said he has no recollection of anyone commenting on his destined profession when he was a child, adding "I'm absolutely convinced in my case it is entirely coincidental and I can't think of any evidence in my life that suggests otherwise." James Counsell on the other hand, having chosen a career in law just like his father, his sibling, and two distant relatives, reported having been spurred on to join the bar from an early age and he cannot remember ever wanting to do anything else. Sue Yoo, an American lawyer, said that when she was younger people urged her to become a lawyer because of her name, which she thinks may have helped her decision. Weather reporter Storm Field was not sure about the influence of his name; his father, also a weather reporter, was his driving force. Psychology professor Lewis Lipsitt, a lifelong collector of aptronyms, was lecturing about nominative determinism in class when a student pointed out that Lipsitt himself was subject to the effect since he studied babies' sucking behaviour. Lipsitt said "that had never occurred to me." Church of England vicar Reverend Michael Vickers, who denied being a Vickers had anything to do with him becoming a vicar, suggesting instead that in some cases "perhaps people are actually escaping from their name, rather than moving towards their job".

While reports by owners of fitting names are of interest, some scientists have questioned their value in deciding whether nominative determinism is a real effect. Instead, they argue that the claim that a name affects life decisions is an extraordinary one that requires extraordinary evidence. To select only those cases that seem to give evidence for nominative determinism is to ignore those that do not. Analysis of large numbers of names is therefore needed. In 2002 Pelham, Mirenberg, and Jones analysed various databases containing first names, surnames, occupations, cities and states. In one study they concluded that people named Dennis gravitate towards dentistry. They did this by retrieving the number of dentists called Dennis (482) from a database of US dentists. They then used the 1990 Census to find out which male first name was the next most popular after Dennis: Walter. The likelihood of a US male being called Dennis was 0.415% and the likelihood of a US male being called Walter was 0.416%. The researchers then retrieved the number of dentists called Walter (257). Comparing the relative frequencies of Dennis and Walter led them to their conclusion that the name Dennis is over-represented in dentistry. However, in 2011, Uri Simonsohn published a paper in which he criticized Pelham et al. for not considering confounding factors and reported on how the popularity of Dennis and Walter as baby names has varied over the decades. Given Walter was a relatively old-fashioned name it was far more likely for Pelham et al. to find people named Dennis to have any job, not just that of dentist, and people named Walter to be retired. Simonsohn did indeed find a disproportionally high number of Dennis lawyers compared to Walter lawyers.
Aware of Simonsohn's critical analyses of their earlier methods, Pelham and Mauricio published a new study in 2015, describing how they now controlled for gender, ethnicity, and education confounds. In one study they looked at census data and concluded that men disproportionately worked in eleven occupations whose titles matched their surnames, for example, baker, carpenter, and farmer.
In 2009 Michalos reported the results of an analysis of the occurrences of people with the surname Counsell registered as independent barristers in England and Wales versus those with the name in England and Wales as whole. Given the low frequency of the name in England and Wales as a whole he expected to find no one registered, but three barristers named Counsell were found.
In 2015 researchers Limb, Limb, Limb and Limb published a paper on their study into the effect of surnames on medical specialisation. They looked at 313,445 entries in the medical register from the General Medical Council, and identified surnames that were apt for the speciality, for example, Limb for an orthopaedic surgeon, and Doctor for medicine in general. They found that the frequency of names relevant to medicine and to subspecialties was much greater than expected by chance. Specialties that had the largest proportion of names specifically relevant to that specialty were those for which the English language has provided a wide range of alternative terms for the same anatomical parts (or functions thereof). Specifically, these were genitourinary medicine (e.g., Hardwick and Woodcock) and urology (e.g., Burns, Cox, Ball). Neurologists had names relevant to medicine in general, but far fewer had names directly relevant to their specialty (1 in every 302). Limb, Limb, Limb and Limb did not report on looking for any confounding variables. In 2010 Abel came to a similar conclusion. In one study he compared doctors and lawyers whose first or last names began with three-letter combinations representative of their professions, for example, "doc," "law," and likewise found a significant relationship between name and profession. Abel also found that the initial letters of physicians' last names were significantly related to their subspecialty. For example, Raymonds were more likely to be radiologists than dermatologists. Two separate studies by Krajick and Neimi in 2005, both analysing large samples of names of scientists, showed 1.35% of geologists having names referring to their field, and in political science, 1.26%.
As for Casler's third possible explanation for nominative determinism, genetics, researchers Voracek, Rieder, Stieger, and Swami found some evidence for it in 2015. They reported that today's Smiths still tend to have the physical capabilities of their ancestors who were smiths. People called Smith reported above-average aptitude for strength-related activities. A similar aptitude for dexterity-related activities among people with the surname Tailor, or equivalent spellings thereof, was found, but it was not statistically significant. In the researchers' view a genetic-social hypothesis appears more viable than the hypothesis of implicit egotism effects.















Abel, Ernest L. (2010). "Influence of Names on Career Choices in Medicine". Names: A Journal of Onomastics. 58 (2): 65–74. doi:10.1179/002777310X12682237914945. 
Abraham, Karl (1979). "On determining the power of names (1911)". Clinical Papers and Essays on Psychoanalysis. London: Karnac Books. pp. 31–32. ISBN 978-1-78181-144-3. 
Alter, Adam (2013). Drunk Tank Pink: And Other Unexpected Forces That Shape How We Think, Feel, and Behave. London: Penguin Press. ISBN 978-1-78074-264-9. 
American Council of Learned Societies (1998). Surnames in the United States Census of 1790: An Analysis of National Origins of the Population. Baltimore, Maryland: Genealogical Publishing Company. ISBN 978-0-8063-0004-7. 
Bateson, Patrick; Martin, Paul (2001). Design for a Life: How Biology and Psychology Shape Human Behavior. New York: Touchstone. ISBN 978-0-684-86933-9. 
Bennett, H. J. (1992). "A piece of my mind. Calling Dr Doctor". JAMA. 268 (21): 3060. doi:10.1001/jama.268.21.3060. PMID 1306061. 
Camden, William (1984). Dunn, R. D., ed. Remains Concerning Britain. Toronto: University of Toronto Press. ISBN 978-0-8020-2457-2. 
Casler, Lawrence (1975). "Put the Blame on Name" (PDF). Psychological Reports. 36 (2): 467–472. doi:10.2466/pr0.1975.36.2.467. 
Cavill, Paul (2016). "Language-based approaches to names in literature". In Hough, Carole. The Oxford Handbook of Names and Naming. Oxford Handbooks in Linguistics Series. Oxford: Oxford University Press. ISBN 978-0-19-965643-1. 
Christenfeld, N.; Phillips, D. P.; Glynn, L. M. (1999). "What's in a name: Mortality and the power of symbols". Journal of Psychosomatic Research. 47 (3): 241–254. doi:10.1016/S0022-3999(99)00035-5. PMID 10576473. 
Cole, Kristen (9 February 2001). "Is work calling your name?". George Street Journal. 25 (17). 
Conrad, Barnaby (1999). The World of Herb Caen: San Francisco 1938–1997. San Francisco: Chronicle Books. ISBN 978-0-8118-2575-7. 
Danesi, Marcel (2012). Linguistic Anthropology: A Brief Introduction. Toronto: Canadian Scholars' Press. ISBN 978-1-55130-489-2. 
Dickson, Paul (1996). What's in a name?: Reflections of an Irrepressible Name Collector. Springfield, Massachusetts: Merriam-Webster. ISBN 978-0-87779-613-8. 
Duša, Zdravko; Kenda, Marjetka (2011). Nomen est omen: Čiginj in njegova imena (in Slovenian). Volče, Slovenia: Myra Locatelli. ISBN 978-961-92522-5-3. 
Flugel, Ingeborg (1930). "On the significance of names". British Journal of Medical Psychology. 10 (2): 208–213. doi:10.1111/j.2044-8341.1930.tb01017.x. 
Feedback (5 November 1994a). "Feedback". New Scientist (1950). Archived from the original on 24 September 2016. 
Feedback (17 December 1994b). "Feedback". New Scientist (1956). Archived from the original on 26 September 2016. 
Feedback (16 November 1996). "Feedback". New Scientist (2056). Archived from the original on 6 October 2016. 
Feedback (16 October 1999). "Feedback". New Scientist (2208). Archived from the original on 8 October 2016. 
Feedback (20 May 2000). "Feedback". New Scientist (2239). Archived from the original on 11 October 2016. 
Feedback (10 July 2004). "Feedback". New Scientist (2455). Archived from the original on 14 October 2016. 
Feedback (9 November 2005). "Feedback". New Scientist (2525). Archived from the original on 15 October 2016. 
Feedback (4 October 2006). "Feedback". New Scientist (2572). Archived from the original on 16 October 2016. 
Feedback (9 May 2007). "Feedback". New Scientist (2603). Archived from the original on 17 October 2016. 
Feedback (19 February 2014a). "Feedback: All shall have pills". New Scientist (2957). Archived from the original on 25 October 2016. 
Feedback (23 July 2014b). "Feedback: A whole world of Tweet". New Scientist (2979). Archived from the original on 28 October 2016. 
Feedback (3 June 2015). "Feedback: Scatological scorecards". New Scientist (3024). Archived from the original on 1 November 2016. 
Fibbi, Rosita; Kaya, Bülent; Piguet, Etienne (2003). Nomen est omen: quand s' appeler Pierre, Afrim ou Mehmet fait la différence (PDF) (in French). Bern: Fonds National Suisse, Direction du programme PNR43. ISBN 978-3-908117-69-8. 
Fowler, Alastair (2012). Literary Names: Personal Names in English Literature. Oxford: Oxford University Press. ISBN 978-0-19-165099-4. 
Frank, Roberta (1970). "Onomastic Play in Kormakr's Verse: the name Steingerðr". Mediaeval Scandinavia. 3: 7–34. 
Gerber, Sophia (2006). Nomen est omen - Morfologia dei nomi propri di persona italiani (in Italian). Munich: GRIN Verlag. ISBN 978-3-638-56044-3. 
Hoekstra, Hans (2001). Naam & Faam. Amsterdam: Nijgh & Van Ditmar. ISBN 978-90-388-3107-7. 
Hunt, Jen (1994). "The Psychology of Reference Hunting" (PDF). The Psychologist. 7 (10): 480. 
Jenkyn, William (1652). Exposition of the epistle of Jude (PDF). Bungay, England: John Guilds and son. 
Joubert, Charles E. (1985). "Factors Related To Individuals Attitudes Toward Their Names". Psychological Reports. 57 (3): 983–986. doi:10.2466/pr0.1985.57.3.983. 
Jung, Carl (1972). Synchronicity – An Acausal Connecting Principle. London: Routledge and Kegan Paul. ISBN 978-0-7100-7397-6. 
Keaney, John J.; Groarke, John D.; Galvin, Zita; McGorrian, Catherine; McCann, Hugh A.; Sugrue, Declan; Keelan, Edward; Galvin, Joseph; Blake, Gavin; Mahon, Niall G.; O'Neill, James (2013). "The Brady Bunch? New evidence for nominative determinism in patients' health: retrospective, population based cohort study". British Medical Journal. doi:10.1136/bmj.f6627. 
Krajick, Kevin (2005). "The 'Name Number' for Geology, and for Other Professions". Annals of Improbable Research. 11 (2): 14–15. doi:10.3142/107951405781748030. 
Lederer, Richard (2010). Crazy English. New York: Simon and Schuster. ISBN 978-1-4391-3894-6. 
Limb, C.; Limb, R.; Limb, C.; Limb, D. (2015). "Nominative determinism in hospital medicine". The Bulletin. 97 (1): 24–26. doi:10.1308/147363515X14134529299420. 
McKeown, J. C. (2010). A Cabinet of Roman Curiosities: Strange Tales and Surprising Facts from the World's Greatest Empire. Oxford: Oxford University Press. ISBN 978-0-19-975278-2. 
Merriam-Webster (1995). Merriam-Webster's Encyclopedia of Literature. Springfield, Massachusetts: Merriam-Webster. ISBN 978-0-87779-042-6. 
Michalos, Christina (2009). "In the Name of the Law". Counsel. 25 (4): 16–18. 
Morrison, Stilian; Smith, Gary (2005). "Monogrammatic determinism?". Psychosomatic Medicine. 67 (5): 820–824. doi:10.1097/01.psy.0000181283.51771.8a. 
Neimi, Richard (2005). "The Name Number (s) for Political Science" (PDF). Annals of Improbable Research. 11 (4): 13–14. 
Nevid, Jeffrey S.; Rathus, Spencer A. (2009). Psychology and the Challenges of Life - Adjustment and Growth. Hoboken, New Jersey: John Wiley & Sons. ISBN 978-0-470-38362-9. 
Nuessel, Frank (1992). The Study of Names. Santa Barbara, California: Greenwood Press. ISBN 978-0-313-28356-7. 
Nuttin, Jozef M. (1985). "Narcissism beyond Gestalt and awareness: The name–letter effect". European Journal of Social Psychology. 15 (3): 353–361. doi:10.1002/ejsp.2420150309. 
Pelham, B.; Mirenberg, Matthew C.; Jones, John T. (2002). "Why Susie sells seashells by the seashore: Implicit egotism and major life decisions". Journal of Personality and Social Psychology. 82 (4): 469–487. doi:10.1037/0022-3514.82.4.469. PMID 11999918. 
Pelham, Brett; Carvallo, Mauricio (2011). "The surprising potency of implicit egotism: A reply to Simonsohn". Journal of Personality and Social Psychology. 101 (1): 25–30. doi:10.1037/a0023526. 
Pelham, Brett; Mauricio, Carvallo (2015). "When Tex and Tess Carpenter Build Houses in Texas: Moderators of Implicit Egotism". Self and Identity. 4 (6): 692–723. doi:10.1080/15298868.2015.1070745. 
Ratzan, Lee (2004). Understanding Information Systems: What They Do and why We Need Them. Chicago: American Library Association. ISBN 978-0-8389-0868-6. 
Rennick, R.M. (1982). "The Alleged "Hogg Sisters," or Simple Ground Rules for Collectors of "Odd" Names". Names. 30 (3): 193–198. doi:10.1179/nam.1982.30.3.193. 
Room, Adrian (1996). An Alphabetical Guide to the Language of Name Studies. Lanham, Maryland: Scarecrow Press. ISBN 978-0-8108-3169-8. 
Safire, William (2004). No Uncertain Terms: More Writing from the Popular "On Language" Column in The New York Times Magazine. New York: Simon and Schuster. ISBN 978-0-7432-5812-8. 
Salway, Benet (1994). "What's in a Name? A Survey of Roman Onomastic Practice from c. 700 B.C. to A.D. 700". The Journal of Roman Studies. 84: 124–145. doi:10.2307/300873. JSTOR 300873. 
Schaffer-Suchomel, Joachim (2009). Nomen est Omen: Die verborgene Botschaft der Vornamen - Von Adam bis Zarah (in German). Munich: Goldmann Arkana. ISBN 978-3-641-01672-2. 
Simonsohn, Uri (2011). "Spurious? Name similarity effects (implicit egotism) in marriage, job, and moving decisions". Journal of Personality and Social Psychology. 101 (1): 1–24. doi:10.1037/a0021990. PMID 21299311. 
Simonsohn, Uri (2011b). "In defense of diligence: A rejoinder to Pelham and Carvallo (2011)". Journal of Personality and Social Psychology. 101 (1): 31–33. doi:10.1037/a0023232. 
Slovenko, R. (1983). "The Destiny Of A Name". Journal of Psychiatry and Law. 11 (2): 227–270. 
Smeets, Raymond (2009). On the Preference for Self-related Entities: The Role of Positive Self-associations in Implicit Egotism Effects. Nijmegen, the Netherlands: UB Nijmegen. ISBN 978-90-90-24290-3. 
Smith-Bannister, Scott (1997). Names and Naming Patterns in England, 1538–1700. Oxford: Clarendon Press. ISBN 978-0-19-820663-7. 
Snowman, Daniel (1993). Pole Positions: Polar Regions and the Future of the Planet. London: Hodder & Stoughton. ISBN 978-0-340-54068-8. 
Splatt, A. J.; Weedon, D. (1977). "The Urethral Syndrome: Experience with the Richardson Urethroplasty". British Journal of Urology. 49 (2): 173–176. doi:10.1111/j.1464-410X.1977.tb04095.x. PMID 870138. 
Stekel, Wilhelm (1911). "Die Verpflichtung des Namens". Zeitschrift für Psychotherapie und medizinische Psychologie (in German). 3: 110–114. 
Stoppard, Tom (1972). Jumpers. London: Faber & Faber. ISBN 978-0-571-14569-0. 
Trench, Richard (1993). London Under London: A Subterranean Guide (2 ed.). London: John Murray. ISBN 978-0-7195-5288-5. 
Voracek, Martin; Rieder, Stephan; Stieger, Stefan; Swami, Viren (2015). "What's in a Surname? Physique, Aptitude, and Sports Type Comparisons between Tailors and Smiths". PLoS ONE. 10 (7): 699–702. Bibcode:2015PLoSO..1031795V. doi:10.1371/journal.pone.0131795. 
Weekley, Ernest (1914). The Romance of Names (PDF). London: John Murray. 
Wilson, Stephen (2003). The Means Of Naming: A Social History. London: Routledge. ISBN 978-1-135-36836-4. 



Colls, Tom (20 December 2011). "When the name fits the job". BBC Radio 4. Archived from the original on 13 September 2016. Retrieved 12 September 2016. 
Highfield, Roger (24 March 2011). "The name game - the weird science of nominative determinism". The Evening Standard. Archived from the original on 29 September 2016. Retrieved 12 September 2016. 
Hoekstra, Hans (16 April 2011). "Nomen est omen". Het Parool (in Dutch). Archived from the original (PDF) on 1 October 2016. Retrieved 12 September 2016. 
Levey, Bob (20 November 1985). "Bob Levey's Washington". The Washington Post. Retrieved 12 September 2016. 
Levey, Bob (29 August 2000). "Bob Levey's Washington". The Washington Post. Archived from the original on 23 September 2016. Retrieved 12 September 2016. 
Mount, Harry (27 December 2011). "From Doctor De'ath to Cardinal Sin, do we pick jobs to suit our names?". The Daily Mail. Archived from the original on 14 September 2016. Retrieved 12 September 2016. 
Nelson, Graham. "Meet ordinary humans whose names shaped their destiny". The Huffington Post. Archived from the original on 20 September 2016. Retrieved 12 September 2016. 
Nunn, Gary (31 October 2014). "Reckless by name, reckless by nature?". The Guardian. Archived from the original on 16 September 2016. Retrieved 12 September 2016. 
Silverman, Rachel Emma; Light, Joe (21 June 2011). "Dr. Chopp, Meet Congressman Weiner". The Wall Street Journal. Archived from the original on 24 September 2016. Retrieved 24 September 2016. 
Telegraph staff (20 December 2011). "A person's surname can influence their career, experts claim". The Telegraph. Archived from the original on 15 September 2016. Retrieved 12 September 2016.Fun is the enjoyment of pleasure, particularly in leisure activities. Fun is an experience — short-term, often unexpected, informal, not cerebral and generally purposeless. It is an enjoyable distraction, diverting the mind and body from any serious task or contributing an extra dimension to it. Although particularly associated with recreation and play, fun may be encountered during work, social functions, and even seemingly mundane activities of daily living. It may often have little to no logical basis, and opinions on whether or not an activity is fun may differ. A distinction between enjoyment and fun is difficult but possible to articulate, fun being a more spontaneous, playful, or active event. There are psychological and physiological implications to the experience of fun.



The word is associated with sports, high merriment, and amusement. Although its etymology is uncertain, it may be derived from fonne (fool) and fonnen (the one fooling the other). Its meaning in 1727 was "cheat, trick, hoax", a meaning still retained in the phrase "to make fun of".

The landlady was going to reply, but was prevented by the peace-making sergeant, sorely to the displeasure of Partridge, who was a great lover of what is called fun, and a great promoter of those harmless quarrels which tend rather to the production of comical than tragical incidents.Henry Fielding, The History of Tom Jones, a Foundling (1749)

The way the word "fun" is used demonstrates its distinctive elusiveness. Expressions such as "Have fun!" and "That was fun!" indicate that fun is pleasant, personal, and to some extent unpredictable. Expressions such as "I was making fun of myself" convey the sense that fun is something that can be amusing and not to be taken seriously. The adjective "funny" has two meanings which often need to be clarified between a speaker and listener. One meaning is "amusing, jocular, droll" and the other meaning is "odd, quirky, peculiar". These differences indicate the evanescent and experiential nature of fun and the difficulty of distinguishing "fun" from "enjoyment".
Fun's evanescence can be seen when an activity regarded as fun becomes goal-oriented. Many physical activities and individual sports are regarded as fun until the participant seeks to win a competition, at which point, much of the fun may disappear as the individual's focus tightens. Surfing is an example. If you are a "mellow soul" (not in a competition or engaging in extreme sport) "once you're riding waves, you're guaranteed to be having ... fun".
The pleasure of fun can be seen by the numerous efforts to harness its positive associations. For example, there are many books on serious subjects, about skills such as music, mathematics and languages, normally quite difficult to master, which have "fun" added to the title.



Many physical activities provide opportunities to play and have fun.




According to Johan Huizinga, fun is "an absolutely primary category of life, familiar to everybody at a glance right down to the animal level." Psychological studies reveal both the importance of fun and its effect on the perception of time, which is sometimes said to be shortened when one is having fun. As the adage states: "Time flies when you're having fun".
It has been suggested that games, toys, and activities perceived as fun are often challenging in some way. When a person is challenged to think consciously, overcome challenge and learn something new, they are more likely to enjoy a new experience and view it as fun. A change from routine activities appears to be at the core of this perception, since people spend much of a typical day engaged in activities that are routine and require limited conscious thinking. Routine information is processed by the brain as a "chunked pattern": "We rarely look at the real world", according to game designer Raph Koster, "we instead recognize something we have chunked, and leave it at that. [...] One might argue that the essence of much of art is in forcing us to see things as they really are rather than as we assume them to be". Since it helps people to relax, fun is sometimes regarded as a "social lubricant", important in adding "to one's pleasure in life" and helping to "act as a buffer against stress".
For children, fun is strongly related to play and they have great capacity to extract the fun from it in a spontaneous and inventive way. Play "involves the capacity to have fun - to be able to return, at least for a little while, to never-never land and enjoy it."



Some scientists have identified areas of the brain associated with the perception of novelty, which are stimulated when faced with "unusual or surprising circumstances". Information is initially received in the hippocampus, the site of long-term memory consolidation, where the brain attempts to match the new information with recognizable patterns stored in long-term memory. When it is unable to do this, the brain releases dopamine, a chemical which stimulates the amygdala, the site of emotion, and creates a pleasurable feeling that is associated with the new memory. In other words, fun is created by stimulating the brain with novelty.



In the modern world, fun is sold as a consumer product in the form of games, novelties, television, toys and other amusements. Marxist sociologists such as the Frankfurt School criticise mass-manufactured fun as too calculated and empty to be fully satisfying. Bill Griffith satirises this dysphoria when his cartoon character Zippy the Pinhead asks mechanically, "Are we having fun yet?" In the Beatles song "She's Leaving Home" fun is called "the one thing that money can't buy."



Amusement
Entertainment
Epicurus
Happiness
Hedonism






Yates, Vicki (2008). Having Fun. Heinemann-Raintree Library. ISBN 1403498326. Retrieved 4 February 2013. 
Raph Koster (2011), Theory of Fun for Game Design, O'Reilly Media, Inc., ISBN 9781932111972 


Psychology is the study of behavior and mind, embracing all aspects of conscious and unconscious experience as well as thought. It is an academic discipline and a social science which seeks to understand individuals and groups by establishing general principles and researching specific cases. In this field, a professional practitioner or researcher is called a psychologist and can be classified as a social, behavioral, or cognitive scientist. Psychologists attempt to understand the role of mental functions in individual and social behavior, while also exploring the physiological and biological processes that underlie cognitive functions and behaviors.
Psychologists explore behavior and mental processes, including perception, cognition, attention, emotion (affect), intelligence, phenomenology, motivation (conation), brain functioning, and personality. This extends to interaction between people, such as interpersonal relationships, including psychological resilience, family resilience, and other areas. Psychologists of diverse orientations also consider the unconscious mind. Psychologists employ empirical methods to infer causal and correlational relationships between psychosocial variables. In addition, or in opposition, to employing empirical and deductive methods, some—especially clinical and counseling psychologists—at times rely upon symbolic interpretation and other inductive techniques. Psychology has been described as a "hub science", with psychological findings linking to research and perspectives from the social sciences, natural sciences, medicine, humanities, and philosophy.
While psychological knowledge is often applied to the assessment and treatment of mental health problems, it is also directed towards understanding and solving problems in several spheres of human activity. By many accounts psychology ultimately aims to benefit society. The majority of psychologists are involved in some kind of therapeutic role, practicing in clinical, counseling, or school settings. Many do scientific research on a wide range of topics related to mental processes and behavior, and typically work in university psychology departments or teach in other academic settings (e.g., medical schools, hospitals). Some are employed in industrial and organizational settings, or in other areas such as human development and aging, sports, health, and the media, as well as in forensic investigation and other aspects of law.



The word psychology derives from Greek roots meaning study of the psyche, or soul (ψυχή psukhē, "breath, spirit, soul" and -λογία -logia, "study of" or "research"). The Latin word psychologia was first used by the Croatian humanist and Latinist Marko Marulić in his book, Psichiologia de ratione animae humanae in the late 15th century or early 16th century. The earliest known reference to the word psychology in English was by Steven Blankaart in 1694 in The Physical Dictionary which refers to "Anatomy, which treats the Body, and Psychology, which treats of the Soul."
In 1890, William James defined psychology as "the science of mental life, both of its phenomena and their conditions". This definition enjoyed widespread currency for decades. However, this meaning was contested, notably by radical behaviorists such as John Watson, who in his 1913 manifesto defined the discipline of psychology as the acquisition of information useful to the control of behavior. Also since James defined it, the term more strongly connotes techniques of scientific experimentation. Folk psychology refers to the understanding of ordinary people, as contrasted with that of psychology professionals.




The ancient civilizations of Egypt, Greece, China, India, and Persia all engaged in the philosophical study of psychology. Historians note that Greek philosophers, including Thales, Plato, and Aristotle (especially in his De Anima treatise), addressed the workings of the mind. As early as the 4th century BC, Greek physician Hippocrates theorized that mental disorders had physical rather than supernatural causes.
In China, psychological understanding grew from the philosophical works of Laozi and Confucius, and later from the doctrines of Buddhism. This body of knowledge involves insights drawn from introspection and observation, as well as techniques for focused thinking and acting. It frames the universe as a division of, and interaction between, physical reality and mental reality, with an emphasis on purifying the mind in order to increase virtue and power. An ancient text known as The Yellow Emperor's Classic of Internal Medicine identifies the brain as the nexus of wisdom and sensation, includes theories of personality based on yin–yang balance, and analyzes mental disorder in terms of physiological and social disequilibria. Chinese scholarship focused on the brain advanced in the Qing Dynasty with the work of Western-educated Fang Yizhi (1611–1671), Liu Zhi (1660–1730), and Wang Qingren (1768–1831). Wang Qingren emphasized the importance of the brain as the center of the nervous system, linked mental disorder with brain diseases, investigated the causes of dreams and insomnia, and advanced a theory of hemispheric lateralization in brain function.
Distinctions in types of awareness appear in the ancient thought of India, influenced by Hinduism. A central idea of the Upanishads is the distinction between a person's transient mundane self and their eternal unchanging soul. Divergent Hindu doctrines, and Buddhism, have challenged this hierarchy of selves, but have all emphasized the importance of reaching higher awareness. Yoga is a range of techniques used in pursuit of this goal. Much of the Sanskrit corpus was suppressed under the British East India Company followed by the British Raj in the 1800s. However, Indian doctrines influenced Western thinking via the Theosophical Society, a New Age group which became popular among Euro-American intellectuals.
Psychology was a popular topic in Enlightenment Europe. In Germany, Gottfried Wilhelm Leibniz (1646–1716) applied his principles of calculus to the mind, arguing that mental activity took place on an indivisible continuum—most notably, that among an infinity of human perceptions and desires, the difference between conscious and unconscious awareness is only a matter of degree. Christian Wolff identified psychology as its own science, writing Psychologia empirica in 1732 and Psychologia rationalis in 1734. This notion advanced further under Immanuel Kant, who established the idea of anthropology, with psychology as an important subdivision. However, Kant explicitly and notoriously rejected the idea of experimental psychology, writing that "the empirical doctrine of the soul can also never approach chemistry even as a systematic art of analysis or experimental doctrine, for in the manifold of inner observation can be separated only by mere division in thought, and cannot then be held separate and recombined at will (but still less does another thinking subject suffer himself to be experimented upon to suit our purpose), and even observation by itself already changes and displaces the state of the observed object." Having consulted philosophers Hegel and Herbart, in 1825 the Prussian state established psychology as a mandatory discipline in its rapidly expanding and highly influential educational system. However, this discipline did not yet embrace experimentation. In England, early psychology involved phrenology and the response to social problems including alcoholism, violence, and the country's well-populated mental asylums.




Gustav Fechner began conducting psychophysics research in Leipzig in the 1830s, articulating the principle that human perception of a stimulus varies logarithmically according to its intensity. Fechner's 1860 Elements of Psychophysics challenged Kant's stricture against quantitative study of the mind. In Heidelberg, Hermann von Helmholtz conducted parallel research on sensory perception, and trained physiologist Wilhelm Wundt. Wundt, in turn, came to Leipzig University, establishing the psychological laboratory which brought experimental psychology to the world. Wundt focused on breaking down mental processes into the most basic components, motivated in part by an analogy to recent advances in chemistry, and its successful investigation of the elements and structure of material. Paul Flechsig and Emil Kraepelin soon created another influential psychology laboratory at Leipzig, this one focused on more on experimental psychiatry.
Psychologists in Germany, Denmark, Austria, England, and the United States soon followed Wundt in setting up laboratories. G. Stanley Hall who studied with Wundt, formed a psychology lab at Johns Hopkins University in Maryland, which became internationally influential. Hall, in turn, trained Yujiro Motora, who brought experimental psychology, emphasizing psychophysics, to the Imperial University of Tokyo. Wundt assistant Hugo Münsterberg taught psychology at Harvard to students such as Narendra Nath Sen Gupta—who, in 1905, founded a psychology department and laboratory at the University of Calcutta. Wundt students Walter Dill Scott, Lightner Witmer, and James McKeen Cattell worked on developing tests for mental ability. Catell, who also studied with eugenicist Francis Galton, went on to found the Psychological Corporation. Wittmer focused on mental testing of children; Scott, on selection of employees.
Another student of Wundt, Edward Titchener, created the psychology program at Cornell University and advanced a doctrine of "structuralist" psychology. Structuralism sought to analyze and classify different aspects of the mind, primarily through the method of introspection. William James, John Dewey and Harvey Carr advanced a more expansive doctrine called functionalism, attuned more to human–environment actions. In 1890 James wrote an influential book, The Principles of Psychology, which expanded on the realm of structuralism, memorably described the human "stream of consciousness", and interested many American students in the emerging discipline. Dewey integrated psychology with social issues, most notably by promoting the cause progressive education to assimilate immigrants and inculcate moral values in children.
A different strain of experimentalism, with more connection to physiology, emerged in South America, under the leadership of Horacio G. Piñero at the University of Buenos Aires. Russia, too, placed greater emphasis on the biological basis for psychology, beginning with Ivan Sechenov's 1873 essay, "Who Is to Develop Psychology and How?" Sechenov advanced the idea of brain reflexes and aggressively promoted a deterministic viewpoint on human behavior.
Wolfgang Kohler, Max Wertheimer and Kurt Koffka co-founded the school of Gestalt psychology (not to be confused with the Gestalt therapy of Fritz Perls). This approach is based upon the idea that individuals experience things as unified wholes. Rather than breaking down thoughts and behavior into smaller elements, as in structuralism, the Gestaltists maintained that whole of experience is important, and differs from the sum of its parts. Other 19th-century contributors to the field include the German psychologist Hermann Ebbinghaus, a pioneer in the experimental study of memory, who developed quantitative models of learning and forgetting at the University of Berlin, and the Russian-Soviet physiologist Ivan Pavlov, who discovered in dogs a learning process that was later termed "classical conditioning" and applied to human beings.



One of the earliest psychology societies was La Société de Psychologie Physiologique in France, which lasted 1885–1893. The first meeting of the International Congress of Psychology took place in Paris, in August 1889, amidst the World's Fair celebrating the centennial of the French Revolution. William James was one of three Americans among the four hundred attendees. The American Psychological Association was founded soon after, in 1892. The International Congress continued to be held, at different locations in Europe, with wider international participation. The Sixth Congress, Geneva 1909, included presentations in Russian, Chinese, and Japanese, as well as Esperanto. After a hiatus for World War I, the Seventh Congress met in Oxford, with substantially greater participation from the war-victorious Anglo-Americans. In 1929, the Congress took place at Yale University in New Haven, Connecticut, attended by hundreds of members of the American Psychological Association Tokyo Imperial University led the way in bringing the new psychology to the East, and from Japan these ideas diffused into China.
American psychology gained status during World War I, during which a standing committee headed by Robert Yerkes administered mental tests ("Army Alpha" and "Army Beta") to almost 1.8 million GIs. Subsequent funding for behavioral research came in large part from the Rockefeller family, via the Social Science Research Council. Rockefeller charities funded the National Committee on Mental Hygiene, which promoted the concept of mental illness and lobbied for psychological supervision of child development. Through the Bureau of Social Hygiene and later funding of Alfred Kinsey, Rockefeller foundations established sex research as a viable discipline in the U.S. Under the influence of the Carnegie-funded Eugenics Record Office, the Draper-funded Pioneer Fund, and other institutions, the eugenics movement also had a significant impact on American psychology; in the 1910s and 1920s, eugenics became a standard topic in psychology classes.
During World War II and the Cold War, the U.S. military and intelligence agencies established themselves as leading funders of psychology—through the armed forces and in the new Office of Strategic Services intelligence agency. University of Michigan psychologist Dorwin Cartwright reported that university researchers began large-scale propaganda research in 1939–1941, and "the last few months of the war saw a social psychologist become chiefly responsible for determining the week-by-week-propaganda policy for the United States Government." Cartwright also wrote that psychologists had significant roles in managing the domestic economy. The Army rolled out its new General Classification Test and engaged in massive studies of troop morale. In the 1950s, the Rockefeller Foundation and Ford Foundation collaborated with the Central Intelligence Agency to fund research on psychological warfare. In 1965, public controversy called attention to the Army's Project Camelot—the "Manhattan Project" of social science—an effort which enlisted psychologists and anthropologists to analyze foreign countries for strategic purposes.
In Germany after World War I, psychology held institutional power through the military, and subsequently expanded along with the rest of the military under the Third Reich. Under the direction of Hermann Göring's cousin Matthias Göring, the Berlin Psychoanalytic Institute was renamed the Göring Institute. Freudian psychoanalysts were expelled and persecuted under the anti-Jewish policies of the Nazi Party, and all psychologists had to distance themselves from Freud and Adler. The Göring Institute was well-financed throughout the war with a mandate to create a "New German Psychotherapy". This psychotherapy aimed to align suitable Germans with the overall goals of the Reich; as described by one physician: "Despite the importance of analysis, spiritual guidance and the active cooperation of the patient represent the best way to overcome individual mental problems and to subordinate them to the requirements of the Volk and the Gemeinschaft." Psychologists were to provide Seelenführung, leadership of the mind, to integrate people into the new vision of a German community. Harald Schultz-Hencke melded psychology with the Nazi theory of biology and racial origins, criticizing psychoanalysis as a study of the weak and deformed. Johannes Heinrich Schultz, a German psychologist recognized for developing the technique of autogenic training, prominently advocated sterilization and euthanasia of men considered genetically undesirable, and devised techniques for facilitating this process. After the war, some new institutions were created and some psychologists were discredited due to Nazi affiliation. Alexander Mitscherlich founded a prominent applied psychoanalysis journal called Psyche and with funding from the Rockefeller Foundation established the first clinical psychosomatic medicine division at Heidelberg University. In 1970, psychology was integrated into the required studies of medical students.
After the Russian Revolution, psychology was heavily promoted by the Bolsheviks as a way to engineer the "New Man" of socialism. Thus, university psychology departments trained large numbers of students, for whom positions were made available at schools, workplaces, cultural institutions, and in the military. An especial focus was pedology, the study of child development, regarding which Lev Vygotsky became a prominent writer. The Bolsheviks also promoted free love and embranced the doctrine of psychoanalysis as an antidote to sexual repression. Although pedology and intelligence testing fell out of favor in 1936, psychology maintained its privileged position as an instrument of the Soviet state. Stalinist purges took a heavy toll and instilled a climate of fear in the profession, as elsewhere in Soviet society. Following World War II, Jewish psychologists past and present (including Vygotsky, A. R. Luria, and Aron Zalkind) were denounced; Ivan Pavlov (posthumously) and Stalin himself were aggrandized as heroes of Soviet psychology. Soviet academics was speedily liberalized during the Khrushchev Thaw, and cybernetics, linguistics, genetics, and other topics became acceptable again. There emerged a new field called "engineering psychology" which studied mental aspects of complex jobs (such as pilot and cosmonaut). Interdisciplinary studies became popular and scholars such as Georgy Shchedrovitsky developed systems theory approaches to human behavior.
Twentieth-century Chinese psychology originally modeled the United States, with translations from American authors like William James, the establishment of university psychology departments and journals, and the establishment of groups including the Chinese Association of Psychological Testing (1930) and the Chinese Psychological Society (1937). Chinese psychologists were encouraged to focus on education and language learning, with the aspiration that education would enable modernization and nationalization. John Dewey, who lectured to Chinese audiences in 1918–1920, had a significant influence on this doctrine. Chancellor T'sai Yuan-p'ei introduced him at Peking University as a greater thinker than Confucius. Kuo Zing-yang who received a PhD at the University of California, Berkeley, became President of Zhejiang University and popularized behaviorism. After the Chinese Communist Party gained control of the country, the Stalinist USSR became the leading influence, with Marxism–Leninism the leading social doctrine and Pavlovian conditioning the approved concept of behavior change. Chinese psychologists elaborated on Lenin's model of a "reflective" consciousness, envisioning an "active consciousness" (tzu-chueh neng-tung-li) able to transcend material conditions through hard work and ideological struggle. They developed a concept of "recognition" (jen-shih) which referred the interface between individual perceptions and the socially accepted worldview. (Failure to correspond with party doctrine was "incorrect recognition".) Psychology education was centralized under the Chinese Academy of Sciences, supervised by the State Council. In 1951 the Academy created a Psychology Research Office, which in 1956 became the Institute of Psychology. Most leading psychologists were educated in the United States, and the first concern of the Academy was re-education of these psychologists in the Soviet doctrines. Child psychology and pedagogy for nationally cohesive education remained a central goal of the discipline.






In 1920, Édouard Claparède and Pierre Bovet created a new applied psychology organization called the International Congress of Psychotechnics Applied to Vocational Guidance, later called the International Congress of Psychotechnics and then the International Association of Applied Psychology. The IAAP is considered the oldest international psychology association. Today, at least 65 international groups deal with specialized aspects of psychology. In response to male predominance in the field, female psychologists in the U.S. formed National Council of Women Psychologists in 1941. This organization became the International Council of Women Psychologists after World War II, and the International Council of Psychologists in 1959. Several associations including the Association of Black Psychologists and the Asian American Psychological Association have arisen to promote non-European racial groups in the profession.
The world federation of national psychological societies is the International Union of Psychological Science (IUPsyS), founded in 1951 under the auspices of UNESCO, the United Nations cultural and scientific authority. Psychology departments have since proliferated around the world, based primarily on the Euro-American model. Since 1966, the Union has published the International Journal of Psychology. IAAP and IUPsyS agreed in 1976 each to hold a congress every four years, on a staggered basis.
The International Union recognizes 66 national psychology associations and at least 15 others exist. The American Psychological Association is the oldest and largest. Its membership has increased from 5,000 in 1945 to 100,000 in the present day. The APA includes 54 divisions, which since 1960 have steadily proliferated to include more specialties. Some of these divisions, such as the Society for the Psychological Study of Social Issues and the American Psychology–Law Society, began as autonomous groups.
The Interamerican Society of Psychology, founded in 1951, aspires to promote psychology and coordinate psychologists across the Western Hemisphere. It holds the Interamerican Congress of Psychology and had 1000 members in year 2000. The European Federation of Professional Psychology Associations, founded in 1981, represents 30 national associations with a total of 100,000 individual members. At least 30 other international groups organize psychologists in different regions.
In some places, governments legally regulate who can provide psychological services or represent themselves as a "psychologist". The American Psychological Association defines a psychologist as someone with a doctoral degree in psychology.



Early practitioners of experimental psychology distinguished themselves from parapsychology, which in the late nineteenth century enjoyed great popularity (including the interest of scholars such as William James), and indeed constituted the bulk of what people called "psychology". Parapsychology, hypnotism, and psychism were major topics of the early International Congresses. But students of these fields were eventually ostractized, and more or less banished from the Congress in 1900–1905. Parapsychology persisted for a time at Imperial University, with publications such as Clairvoyance and Thoughtography by Tomokichi Fukurai, but here too it was mostly shunned by 1913.
As a discipline, psychology has long sought to fend off accusations that it is a "soft" science. Philosopher of science Thomas Kuhn's 1962 critique implied psychology overall was in a pre-paradigm state, lacking the agreement on overarching theory found in mature sciences such as chemistry and physics. Because some areas of psychology rely on research methods such as surveys and questionnaires, critics asserted that psychology is not an objective science. Skeptics have suggested that personality, thinking, and emotion, cannot be directly measured and are often inferred from subjective self-reports, which may be problematic. Experimental psychologists have devised a variety of ways to indirectly measure these elusive phenomenological entities.
Divisions still exist within the field, with some psychologists more oriented towards the unique experiences of individual humans, which cannot be understood only as data points within a larger population. Critics inside and outside the field have argued that mainstream psychology has become increasingly dominated by a "cult of empiricism" which limits the scope of its study by using only methods derived from the physical sciences. Feminist critiques along these lines have argued that claims to scientific objectivity obscure the values and agenda of (historically mostly male) researchers. Jean Grimshaw, for example, argues that mainstream psychological research has advanced a patriarchal agenda through its efforts to control behavior.







Psychologists generally consider the organism the basis of the mind, and therefore a vitally related area of study. Psychiatrists and neuropsychologists work at the interface of mind and body. Biological psychology, also known as physiological psychology, or neuropsychology is the study of the biological substrates of behavior and mental processes. Key research topics in this field include comparative psychology, which studies humans in relation to other animals, and perception which involves the physical mechanics of sensation as well as neural and mental processing. For centuries, a leading question in biological psychology has been whether and how mental functions might be localized in the brain. From Phineas Gage to H. M. and Clive Wearing, individual people with mental issues traceable to physical damage have inspired new discoveries in this area. Modern neuropsychology could be said to originate in the 1870s, when in France Paul Broca traced production of speech to the left frontal gyrus, thereby also demonstrating hemispheric lateralization of brain function. Soon after, Carl Wernicke identified a related area necessary for the understanding of speech.
The contemporary field of behavioral neuroscience focuses on physical causes underpinning behavior. For example, physiological psychologists use animal models, typically rats, to study the neural, genetic, and cellular mechanisms that underlie specific behaviors such as learning and memory and fear responses. Cognitive neuroscientists investigate the neural correlates of psychological processes in humans using neural imaging tools, and neuropsychologists conduct psychological assessments to determine, for instance, specific aspects and extent of cognitive deficit caused by brain damage or disease. The biopsychosocial model is an integrated perspective toward understanding consciousness, behavior, and social interaction. It assumes that any given behavior or mental process affects and is affected by dynamically interrelated biological, psychological, and social factors.
Evolutionary psychology examines cognition and personality traits from an evolutionary perspective. This perspective suggests that psychological adaptations evolved to solve recurrent problems in human ancestral environments. Evolutionary psychology offers complementary explanations for the mostly proximate or developmental explanations developed by other areas of psychology: that is, it focuses mostly on ultimate or "why?" questions, rather than proximate or "how?" questions. "How?" questions are more directly tackled by behavioral genetics research, which aims to understand how genes and environment impact behavior.
The search for biological origins of psychological phenomena has long involved debates about the importance of race, and especially the relationship between race and intelligence. The idea of white supremacy and indeed the modern concept of race itself arose during the process of world conquest by Europeans. Carl von Linnaeus's four-fold classification of humans classifies Europeans as intelligent and severe, Americans as contented and free, Asians as ritualistic, and Africans as lazy and capricious. Race was also used to justify the construction of socially specific mental disorders such as drapetomania and dysaesthesia aethiopica—the behavior of uncooperative African slaves. After the creation of experimental psychology, "ethnical psychology" emerged as a subdiscipline, based on the assumption that studying primitive races would provide an important link between animal behavior and the psychology of more evolved humans.




Psychologists take human behavior as a main area of study. Much of the research in this area began with tests on mammals, based on the idea that humans exhibit similar fundamental tendencies. Behavioral research ever aspires to improve the effectiveness of techniques for behavior modification.
Early behavioral researchers studied stimulus–response pairings, now known as classical conditioning. They demonstrated that behaviors could be linked through repeated association with stimuli eliciting pain or pleasure. Ivan Pavlov—known best for inducing dogs to salivate in the presence of a stimulus previous linked with food—became a leading figure in the Soviet Union and inspired followers to use his methods on humans. In the United States, Edward Lee Thorndike initiated "connectionism" studies by trapping animals in "puzzle boxes" and rewarding them for escaping. Thorndike wrote in 1911: "There can be no moral warrant for studying man's nature unless the study will enable us to control his acts." From 1910–1913 the American Psychological Association went through a sea change of opinion, away from mentalism and towards "behavioralism", and in 1913 John B. Watson coined the term behaviorism for this school of thought. Watson's famous Little Albert experiment in 1920 demonstrated that repeated use of upsetting loud noises could instill phobias (aversions to other stimuli) in an infant human. Karl Lashley, a close collaborator with Watson, examined biological manifestations of learning in the brain.
Embraced and extended by Clark L. Hull, Edwin Guthrie, and others, behaviorism became a widely used research paradigm. A new method of "instrumental" or "operant" conditioning added the concepts of reinforcement and punishment to the model of behavior change. Radical behaviorists avoided discussing the inner workings of the mind, especially the unconscious mind, which they considered impossible to assess scientifically. Operant conditioning was first described by Miller and Kanorski and popularized in the U.S. by B. F. Skinner, who emerged as a leading intellectual of the behaviorist movement.
Noam Chomsky delivered an influential critique of radical behaviorism on the grounds that it could not adequately explain the complex mental process of language acquisition. Martin Seligman and colleagues discovered that the conditioning of dogs led to outcomes ("learned helplessness") that opposed the predictions of behaviorism. Skinner's behaviorism did not die, perhaps in part because it generated successful practical applications. Edward C. Tolman advanced a hybrid "cognitive behaviorial" model, most notably with his 1948 publication discussing the cognitive maps used by rats to guess at the location of food at the end of a modified maze.
The Association for Behavior Analysis International was founded in 1974 and by 2003 had members from 42 countries. The field has been especially influential in Latin America, where it has a regional organization known as ALAMOC: La Asociación Latinoamericana de Análisis y Modificación del Comportamiento. Behaviorism also gained a strong foothold in Japan, where it gave rise to the Japanese Society of Animal Psychology (1933), the Japanese Association of Special Education (1963), the Japanese Society of Biofeedback Research (1973), the Japanese Association for Behavior Therapy (1976), the Japanese Association for Behavior Analysis (1979), and the Japanese Association for Behavioral Science Research (1994). Today the field of behaviorism is also commonly referred to as behavior modification or behavior analysis.




Cognitive psychology studies cognition, the mental processes underlying mental activity. Perception, attention, reasoning, thinking, problem solving, memory, learning, language, and emotion are areas of research. Classical cognitive psychology is associated with a school of thought known as cognitivism, whose adherents argue for an information processing model of mental function, informed by functionalism and experimental psychology.
On a broader level, cognitive science is an interdisciplinary enterprise of cognitive psychologists, cognitive neuroscientists, researchers in artificial intelligence, linguists, human–computer interaction, computational neuroscience, logicians and social scientists. Computer simulations are sometimes used to model phenomena of interest.

Starting in the 1950s, the experimental techniques developed by Wundt, James, Ebbinghaus, and others re-emerged as experimental psychology became increasingly cognitivist—concerned with information and its processing—and, eventually, constituted a part of the wider cognitive science. Some called this development the cognitive revolution because it rejected the anti-mentalist dogma of behaviorism as well as the strictures of psychoanalysis.
Social learning theorists, such as Albert Bandura, argued that the child's environment could make contributions of its own to the behaviors of an observant subject.

Technological advances also renewed interest in mental states and representations. English neuroscientist Charles Sherrington and Canadian psychologist Donald O. Hebb used experimental methods to link psychological phenomena with the structure and function of the brain. The rise of computer science, cybernetics and artificial intelligence suggested the value of comparatively studying information processing in humans and machines. Research in cognition had proven practical since World War II, when it aided in the understanding of weapons operation.
A popular and representative topic in this area is cognitive bias, or irrational thought. Psychologists (and economists) have classified and described a sizeable catalogue of biases which recur frequently in human thought. The availability heuristic, for example, is the tendency to overestimate the importance of something which happens to come readily to mind.
Elements of behaviorism and cognitive psychology were synthesized to form cognitive behavioral therapy, a form of psychotherapy modified from techniques developed by American psychologist Albert Ellis and American psychiatrist Aaron T. Beck. Cognitive psychology was subsumed along with other disciplines, such as philosophy of mind, computer science, and neuroscience, under the cover discipline of cognitive science.




Social psychology is the study of how humans think about each other and how they relate to each other. Social psychologists study such topics as the influence of others on an individual's behavior (e.g. conformity, persuasion), and the formation of beliefs, attitudes, and stereotypes about other people. Social cognition fuses elements of social and cognitive psychology in order to understand how people process, remember, or distort social information. The study of group dynamics reveals information about the nature and potential optimization of leadership, communication, and other phenomena that emerge at least at the microsocial level. In recent years, many social psychologists have become increasingly interested in implicit measures, mediational models, and the interaction of both person and social variables in accounting for behavior. The study of human society is therefore a potentially valuable source of information about the causes of psychiatric disorder. Some sociological concepts applied to psychiatric disorders are the social role, sick role, social class, life event, culture, migration, social, and total institution.



Psychoanalysis comprises a method of investigating the mind and interpreting experience; a systematized set of theories about human behavior; and a form of psychotherapy to treat psychological or emotional distress, especially conflict originating in the unconscious mind. This school of thought originated in the 1890s with Austrian medical doctors including Josef Breuer (physician), Alfred Adler (physician), Otto Rank (psychoanalyst), and most prominently Sigmund Freud (neurologist). Freud's psychoanalytic theory was largely based on interpretive methods, introspection and clinical observations. It became very well known, largely because it tackled subjects such as sexuality, repression, and the unconscious. These subjects were largely taboo at the time, and Freud provided a catalyst for their open discussion in polite society. Clinically, Freud helped to pioneer the method of free association and a therapeutic interest in dream interpretation.

Swiss psychiatrist Carl Jung, influenced by Freud, elaborated a theory of the collective unconscious—a primordial force present in all humans, featuring archetypes which exerted a profound influence on the mind. Jung's competing vision formed the basis for analytical psychology, which later led to the archetypal and process-oriented schools. Other well-known psychoanalytic scholars of the mid-20th century include Erik Erikson, Melanie Klein, D. W. Winnicott, Karen Horney, Erich Fromm, John Bowlby, and Sigmund Freud's daughter, Anna Freud. Throughout the 20th century, psychoanalysis evolved into diverse schools of thought which could be called Neo-Freudian. Among these schools are ego psychology, object relations, and interpersonal, Lacanian, and relational psychoanalysis.
Psychologists such as Hans Eysenck and philosophers including Karl Popper criticized psychoanalysis. Popper argued that psychoanalysis had been misrepresented as a scientific discipline, whereas Eysenck said that psychoanalytic tenets had been contradicted by experimental data. By the end of 20th century, psychology departments in American universities mostly marginalized Freudian theory, dismissing it as a "desiccated and dead" historical artifact. However, researchers in the emerging field of neuro-psychoanalysis today defend some of Freud's ideas on scientific grounds, while scholars of the humanities maintain that Freud was not a "scientist at all, but ... an interpreter".




Humanistic psychology developed in the 1950s as a movement within academic psychology, in reaction to both behaviorism and psychoanalysis. The humanistic approach sought to glimpse the whole person, not just fragmented parts of the personality or isolated cognitions. Humanism focused on uniquely human issues, such as free will, personal growth, self-actualization, self-identity, death, aloneness, freedom, and meaning. It emphasized subjective meaning, rejection of determinism, and concern for positive growth rather than pathology. Some founders of the humanistic school of thought were American psychologists Abraham Maslow, who formulated a hierarchy of human needs, and Carl Rogers, who created and developed client-centered therapy. Later, positive psychology opened up humanistic themes to scientific modes of exploration.
The American Association for Humanistic Psychology, formed in 1963, declared:

Humanistic psychology is primarily an orientation toward the whole of psychology rather than a distinct area or school. It stands for respect for the worth of persons, respect for differences of approach, open-mindedness as to acceptable methods, and interest in exploration of new aspects of human behavior. As a "third force" in contemporary psychology, it is concerned with topics having little place in existing theories and systems: e.g., love, creativity, self, growth, organism, basic need-gratification, self-actualization, higher values, being, becoming, spontaneity, play, humor, affection, naturalness, warmth, ego-transcendence, objectivity, autonomy, responsibility, meaning, fair-play, transcendental experience, peak experience, courage, and related concepts.

In the 1950s and 1960s, influenced by philosophers Søren Kierkegaard and Martin Heidegger and, psychoanalytically trained American psychologist Rollo May pioneered an existential branch of psychology, which included existential psychotherapy: a method based on the belief that inner conflict within a person is due to that individual's confrontation with the givens of existence. Swiss psychoanalyst Ludwig Binswanger and American psychologist George Kelly may also be said to belong to the existential school. Existential psychologists differed from more "humanistic" psychologists in their relatively neutral view of human nature and their relatively positive assessment of anxiety. Existential psychologists emphasized the humanistic themes of death, free will, and meaning, suggesting that meaning can be shaped by myths, or narrative patterns, and that it can be encouraged by an acceptance of the free will requisite to an authentic, albeit often anxious, regard for death and other future prospects.
Austrian existential psychiatrist and Holocaust survivor Viktor Frankl drew evidence of meaning's therapeutic power from reflections garnered from his own internment. He created a variation of existential psychotherapy called logotherapy, a type of existentialist analysis that focuses on a will to meaning (in one's life), as opposed to Adler's Nietzschean doctrine of will to power or Freud's will to pleasure.






Personality psychology is concerned with enduring patterns of behavior, thought, and emotion—commonly referred to as personality—in individuals. Theories of personality vary across different psychological schools and orientations. They carry different assumptions about such issues as the role of the unconscious and the importance of childhood experience. According to Freud, personality is based on the dynamic interactions of the id, ego, and super-ego. Trait theorists, in contrast, attempt to analyze personality in terms of a discrete number of key traits by the statistical method of factor analysis. The number of proposed traits has varied widely. An early model, proposed by Hans Eysenck, suggested that there are three traits which comprise human personality: extraversion–introversion, neuroticism, and psychoticism. Raymond Cattell proposed a theory of 16 personality factors. Dimensional models of personality are receiving increasing support, and some version of dimensional assessment will be included in the forthcoming DSM-V.
Myriad approach to systematically assess different personality types, with the Woodworth Personal Data Sheet, developed during World War I, an early example of the modern technique. The Myers–Briggs Type Indicator sought to assess people according to the personality theories of Carl Jung. Behaviorist resistance to introspection led to the development of the Strong Vocational Interest Blank and Minnesota Multiphasic Personality Inventory, tests which ask more empirical questions and focus less on the psychodynamics of the respondent.



Study of the unconscious mind, a part of the psyche outside the awareness of the individual which nevertheless influenced thoughts and behavior was a hallmark of early psychology. In one of the first psychology experiments conducted in the United States, C. S. Peirce and Joseph Jastrow found in 1884 that subjects could choose the minutely heavier of two weights even if consciously uncertain of the difference. Freud popularized this concept, with terms like Freudian slip entering popular culture, to mean an uncensored intrusion of unconscious thought into one's speech and action. His 1901 text The Psychopathology of Everyday Life catalogues hundreds of everyday events which Freud explains in terms of unconscious influence. Pierre Janet advanced the idea of a subconscious mind, which could contain autonomous mental elements unavailable to the scrutiny of the subject.
Behaviorism notwithstanding, the unconscious mind has maintained its importance in psychology. Cognitive psychologists have used a "filter" model of attention, according to which much information processing takes place below the threshold of consciousness, and only certain processes, limited by nature and by simultaneous quantity, make their way through the filter. Copious research has shown that subconscious priming of certain ideas can covertly influence thoughts and behavior. A significant hurdle in this research is proving that a subject's conscious mind has not grasped a certain stimulus, due to the unreliability of self-reporting. For this reason, some psychologists prefer to distinguish between implicit and explicit memory. In another approach, one can also describe a subliminal stimulus as meeting an objective but not a subjective threshold.
The automaticity model, which became widespread following exposition by John Bargh and others in the 1980s, describes sophisticated processes for executing goals which can be selected and performed over an extended duration without conscious awareness. Some experimental data suggests that the brain begins to consider taking actions before the mind becomes aware of them. This influence of unconscious forces on people's choices naturally bears on philosophical questions free will. John Bargh, Daniel Wegner, and Ellen Langer are some prominent contemporary psychologists who describe free will as an illusion.




Psychologists such as William James initially used the term motivation to refer to intention, in a sense similar to the concept of will in European philosophy. With the steady rise of Darwinian and Freudian thinking, instinct also came to be seen as a primary source of motivation. According to drive theory, the forces of instinct combine into a single source of energy which exerts a constant influence. Psychoanalysis, like biology, regarded these forces as physical demands made by the organism on the nervous system. However, they believed that these forces, especially the sexual instincts, could become entangled and transmuted within the psyche. Classical psychoanalysis conceives of a struggle between the pleasure principle and the reality principle, roughly corresponding to id and ego. Later, in Beyond the Pleasure Principle, Freud introduced the concept of the death drive, a compulsion towards aggression, destruction, and psychic repetition of traumatic events. Meanwhile, behaviorist researchers used simple dichotomous models (pleasure/pain, reward/punishment) and well-established principles such as the idea that a thirsty creature will take pleasure in drinking. Clark Hull formalized the latter idea with his drive reduction model.
Hunger, thirst, fear, sexual desire, and thermoregulation all seem to constitute fundamental motivations for animals. Humans also seem to exhibit a more complex set of motivations—though theoretically these could be explained as resulting from primordial instincts—including desires for belonging, self-image, self-consistency, truth, love, and control.
Motivation can be modulated or manipulated in many different ways. Researchers have found that eating, for example, depends not only on the organism's fundamental need for homeostasis—an important factor causing the experience of hunger—but also on circadian rhythms, food availability, food palatability, and cost. Abstract motivations are also malleable, as evidenced by such phenomena as goal contagion: the adoption of goals, sometimes unconsciously, based on inferences about the goals of others. Vohs and Baumeister suggest that contrary to the need-desire-fulfilment cycle of animal instincts, human motivations sometimes obey a "getting begets wanting" rule: the more you get a reward such as self-esteem, love, drugs, or money, the more you want it. They suggest that this principle can even apply to food, drink, sex, and sleep.




Mainly focusing on the development of the human mind through the life span, developmental psychology seeks to understand how people come to perceive, understand, and act within the world and how these processes change as they age. This may focus on cognitive, affective, moral, social, or neural development. Researchers who study children use a number of unique research methods to make observations in natural settings or to engage them in experimental tasks. Such tasks often resemble specially designed games and activities that are both enjoyable for the child and scientifically useful, and researchers have even devised clever methods to study the mental processes of infants. In addition to studying children, developmental psychologists also study aging and processes throughout the life span, especially at other times of rapid change (such as adolescence and old age). Developmental psychologists draw on the full range of psychological theories to inform their research.




All researched psychological traits are influenced by both genes and environment, to varying degrees. These two sources of influence are often confounded in observational research of individuals or families. An example is the transmission of depression from a depressed mother to her offspring. Theory may hold that the offspring, by virtue of having a depressed mother in his or her (the offspring's) environment, is at risk for developing depression. However, risk for depression is also influenced to some extent by genes. The mother may both carry genes that contribute to her depression but will also have passed those genes on to her offspring thus increasing the offspring's risk for depression. Genes and environment in this simple transmission model are completely confounded. Experimental and quasi-experimental behavioral genetic research uses genetic methodologies to disentangle this confound and understand the nature and origins of individual differences in behavior. Traditionally this research has been conducted using twin studies and adoption studies, two designs where genetic and environmental influences can be partially un-confounded. More recently, the availability of microarray molecular genetic or genome sequencing technologies allows researchers to measure participant DNA variation directly, and test whether individual genetic variants within genes are associated with psychological traits and psychopathology through methods including genome-wide association studies. One goal of such research is similar to that in positional cloning and its success in Huntington's: once a causal gene is discovered biological research can be conducted to understand how that gene influences the phenotype. One major result of genetic association studies is the general finding that psychological traits and psychopathology, as well as complex medical diseases, are highly polygenic, where a large number (on the order of hundreds to thousands) of genetic variants, each of small effect, contribute to individual differences in the behavioral trait or propensity to the disorder. Active research continues to understand the genetic and environmental bases of behavior and their interaction.




Psychology encompasses many subfields and includes different approaches to the study of mental processes and behavior:



Psychological testing has ancient origins, such as examinations for the Chinese civil service dating back to 2200 BC. Written exams began during the Han dynasty (202 BC.–AD. 200). By 1370, the Chinese system required a stratified series of tests, involving essay writing and knowledge of diverse topics. The system was ended in 1906. In Europe, mental assessment took a more physiological approach, with theories of physiognomy—judgment of character based on the face—described by Aristotle in 4th century BC Greece. Physiognomy remained current through the Enlightenment, and added the doctrine of phrenology: a study of mind and intelligence based on simple assessment of neuroanatomy.
When experimental psychology came to Britain, Francis Galton was a leading practitioner, and, with his procedures for measuring reaction time and sensation, is considered an inventor of modern mental testing (also known as psychometrics). James McKeen Cattell, a student of Wundt and Galton, brought the concept to the United States, and in fact coined the term "mental test". In 1901, Cattell's student Clark Wissler published discouraging results, suggesting that mental testing of Columbia and Barnard students failed to predict their academic performance. In response to 1904 orders from the Minister of Public Instruction, French psychologists Alfred Binet and Théodore Simon elaborated a new test of intelligence in 1905–1911, using a range of questions diverse in their nature and difficulty. Binet and Simon introduced the concept of mental age and referred to the lowest scorers on their test as idiots. Henry H. Goddard put the Binet-Simon scale to work and introduced classifications of mental level such as imbecile and feebleminded. In 1916 (after Binet's death), Stanford professor Lewis M. Terman modified the Binet-Simon scale (renamed the Stanford–Binet scale) and introduced the intelligence quotient as a score report. From this test, Terman concluded that mental retardation "represents the level of intelligence which is very, very common among Spanish-Indians and Mexican families of the Southwest and also among negroes. Their dullness seems to be racial."
Following the Army Alpha and Army Beta tests for soldiers in World War I, mental testing became popular in the US, where it was soon applied to school children. The federally created National Intelligence Test was administered to 7 million children in the 1920s, and in 1926 the College Entrance Examination Board created the Scholastic Aptitude Test to standardize college admissions. The results of intelligence tests were used to argue for segregated schools and economic functions—i.e. the preferential training of Black Americans for manual labor. These practices were criticized by black intellectuals such a Horace Mann Bond and Allison Davis. Eugenicists used mental testing to justify and organize compulsory sterilization of individuals classified as mentally retarded. In the United States, tens of thousands of men and women were sterilized. Setting a precedent which has never been overturned, the U.S. Supreme Court affirmed the constitutionality of this practice in the 1907 case Buck v. Bell.
Today mental testing is a routine phenomenon for people of all ages in Western societies. Modern testing aspires to criteria including standardization of procedure, consistency of results, output of an interpretable score, statistical norms describing population outcomes, and, ideally, effective prediction of behavior and life outcomes outside of testing situations.



The provision of psychological health services is generally called clinical psychology in the U.S. The definitions of this term are various and may include school psychology and counseling psychology. Practitioners typically includes people who have graduated from doctoral programs in clinical psychology but may also include others. In Canada, the above groups usually fall within the larger category of professional psychology. In Canada and the US, practitioners get bachelor's degrees and doctorates, then spend one year in an internship and one year in postdoctoral education. In Mexico and most other Latin American and European countries, psychologists do not get bachelor's and doctorate degrees; instead, they take a three-year professional course following high school. Clinical psychology is at present the largest specialization within psychology. It includes the study and application of psychology for the purpose of understanding, preventing, and relieving psychologically based distress, dysfunction or mental illness and to promote subjective well-being and personal development. Central to its practice are psychological assessment and psychotherapy although clinical psychologists may also engage in research, teaching, consultation, forensic testimony, and program development and administration.
Credit for the first psychology clinic in the United States typically goes to Lightner Witmer, who established his practice in Philadelphia in 1896. Another modern psychotherapist was Morton Prince. For the most part, in the first part of the twentieth century, most mental health care in the United States was performed by specialized medical doctors called psychiatrists. Psychology entered the field with its refinements of mental testing, which promised to improve diagnosis of mental problems. For their part, some psychiatrists became interested in using psychoanalysis and other forms of psychodynamic psychotherapy to understand and treat the mentally ill. In this type of treatment, a specially trained therapist develops a close relationship with the patient, who discusses wishes, dreams, social relationships, and other aspects of mental life. The therapist seeks to uncover repressed material and to understand why the patient creates defenses against certain thoughts and feelings. An important aspect of the therapeutic relationship is transference, in which deep unconscious feelings in a patient reorient themselves and become manifest in relation to the therapist.
Psychiatric psychotherapy blurred the distinction between psychiatry and psychology, and this trend continued with the rise of community mental health facilities and behavioral therapy, a thoroughly non-psychodynamic model which used behaviorist learning theory to change the actions of patients. A key aspect of behavior therapy is empirical evaluation of the treatment's effectiveness. In the 1970s, cognitive-behavior therapy arose, using similar methods and now including the cognitive constructs which had gained popularity in theoretical psychology. A key practice in behavioral and cognitive-behavioral therapy is exposing patients to things they fear, based on the premise that their responses (fear, panic, anxiety) can be deconditioned.
Mental health care today involves psychologists and social workers in increasing numbers. In 1977, National Institute of Mental Health director Bertram Brown described this shift as a source of "intense competition and role confusion". Graduate programs issuing doctorates in psychology (PsyD) emerged in the 1950s and underwent rapid increase through the 1980s. This degree is intended to train practitioners who might conduct scientific research.
Some clinical psychologists may focus on the clinical management of patients with brain injury—this area is known as clinical neuropsychology. In many countries, clinical psychology is a regulated mental health profession. The emerging field of disaster psychology (see crisis intervention) involves professionals who respond to large-scale traumatic events.
The work performed by clinical psychologists tends to be influenced by various therapeutic approaches, all of which involve a formal relationship between professional and client (usually an individual, couple, family, or small group). Typically, these approaches encourage new ways of thinking, feeling, or behaving. Four major theoretical perspectives are psychodynamic, cognitive behavioral, existential–humanistic, and systems or family therapy. There has been a growing movement to integrate the various therapeutic approaches, especially with an increased understanding of issues regarding culture, gender, spirituality, and sexual orientation. With the advent of more robust research findings regarding psychotherapy, there is evidence that most of the major therapies have equal effectiveness, with the key common element being a strong therapeutic alliance. Because of this, more training programs and psychologists are now adopting an eclectic therapeutic orientation.
Diagnosis in clinical psychology usually follows the Diagnostic and Statistical Manual of Mental Disorders (DSM), a handbook first published by the American Psychiatric Association in 1952. New editions over time have increased in size and focused more on medical language. The study of mental illnesses is called abnormal psychology.




Educational psychology is the study of how humans learn in educational settings, the effectiveness of educational interventions, the psychology of teaching, and the social psychology of schools as organizations. The work of child psychologists such as Lev Vygotsky, Jean Piaget, and Jerome Bruner has been influential in creating teaching methods and educational practices. Educational psychology is often included in teacher education programs in places such as North America, Australia, and New Zealand.
School psychology combines principles from educational psychology and clinical psychology to understand and treat students with learning disabilities; to foster the intellectual growth of gifted students; to facilitate prosocial behaviors in adolescents; and otherwise to promote safe, supportive, and effective learning environments. School psychologists are trained in educational and behavioral assessment, intervention, prevention, and consultation, and many have extensive training in research.



Industrialists soon brought the nascent field of psychology to bear on the study of scientific management techniques for improving workplace efficiency. This field was at first called economic psychology or business psychology; later, industrial psychology, employment psychology, or psychotechnology. An important early study examined workers at Western Electric's Hawthorne plant in Cicero, Illinois from 1924–1932. With funding from the Laura Spelman Rockefeller Fund and guidance from Australian psychologist Elton Mayo, Western Electric experimented on thousands of factory workers to assess their responses to illumination, breaks, food, and wages. The researchers came to focus on workers' responses to observation itself, and the term Hawthorne effect is now used to describe the fact that people work harder when they think they're being watched.
The name industrial and organizational psychology (I–O) arose in the 1960s and became enshrined as the Society for Industrial and Organizational Psychology, Division 14 of the American Psychological Association, in 1973. The goal is to optimize human potential in the workplace. Personnel psychology, a subfield of I–O psychology, applies the methods and principles of psychology in selecting and evaluating workers. I–O psychology's other subfield, organizational psychology, examines the effects of work environments and management styles on worker motivation, job satisfaction, and productivity. The majority of I–O psychologists work outside of academia, for private and public organizations and as consultants. A psychology consultant working in business today might expect to provide executives with information and ideas about their industry, their target markets, and the organization of their company.



One role for psychologists in the military is to evaluate and counsel soldiers and other personnel. In the U.S., this function began during World War I, when Robert Yerkes established the School of Military Psychology at Fort Oglethorpe in Georgia, to provide psychological training for military staff military. Today, U.S Army psychology includes psychological screening, clinical psychotherapy, suicide prevention, and treatment for post-traumatic stress, as well as other aspects of health and workplace psychology such as smoking cessation.
Psychologists may also work on a diverse set of campaigns known broadly as psychological warfare. Psychologically warfare chiefly involves the use of propaganda to influence enemy soldiers and civilians. In the case of so-called black propaganda the propaganda is designed to seem like it originates from a different source. The CIA's MKULTRA program involved more individualized efforts at mind control, involving techniques such as hypnosis, torture, and covert involuntary administration of LSD. The U.S. military used the name Psychological Operations (PSYOP) until 2010, when these were reclassified as Military Information Support Operations (MISO), part of Information Operations (IO). Psychologists are sometimes involved in assisting the interrogation and torture of suspects, though this has sometimes been denied by those involved and sometimes opposed by others.



Medical facilities increasingly employ psychologists to perform various roles. A prominent aspect of health psychology is the psychoeducation of patients: instructing them in how to follow a medical regimen. Health psychologists can also educate doctors and conduct research on patient compliance.
Psychologists in the field of public health use a wide variety of interventions to influence human behavior. These range from public relations campaigns and outreach to governmental laws and policies. Psychologists study the composite influence of all these different tools in an effort to influence whole populations of people.
Black American psychologists Kenneth and Mamie Clark studied the psychological impact of segregation and testified with their findings in the desegregation case Brown v. Board of Education (1954).
Positive psychology is the study of factors which contribute to human happiness and well-being, focusing more on people who are currently health. In 2010 Clinical Psychological Review published a special issue devoted to positive psychological interventions, such as gratitude journaling and the physical expression of gratitude. Positive psychological interventions have been limited in scope, but their effects are thought to be superior to that of placebos, especially with regard to helping people with body image problems.




Quantitative psychological research lends itself to the statistical testing of hypotheses. Although the field makes abundant use of randomized and controlled experiments in laboratory settings, such research can only assess a limited range of short-term phenomena. Thus, psychologists also rely on creative statistical methods to glean knowledge from clinical trials and population data. These include the Pearson product–moment correlation coefficient, the analysis of variance, multiple linear regression, logistic regression, structural equation modeling, and hierarchical linear modeling. The measurement and operationalization of important constructs is an essential part of these research designs.




A true experiment with random allocation of subjects to conditions allows researchers to make strong inferences about causal relationships. In an experiment, the researcher alters parameters of influence, called independent variables, and measures resulting changes of interest, called dependent variables. Prototypical experimental research is conducted in a laboratory with a carefully controlled environment.
Repeated-measures experiments are those which take place through intervention on multiple occasions. In research on the effectiveness of psychotherapy, experimenters often compare a given treatment with placebo treatments, or compare different treatments against each other. Treatment type is the independent variable. The dependent variables are outcomes, ideally assessed in several ways by different professionals. Using crossover design, researchers can further increase the strength of their results by testing both of two treatments on two groups of subjects.
Quasi-experimental design refers especially to situations precluding random assignment to different conditions. Researchers can use common sense to consider how much the nonrandom assignment threatens the study's validity. For example, in research on the best way to affect reading achievement in the first three grades of school, school administrators may not permit educational psychologists to randomly assign children to phonics and whole language classrooms, in which case the psychologists must work with preexisting classroom assignments. Psychologists will compare the achievement of children attending phonics and whole language classes.
Experimental researchers typically use a statistical hypothesis testing model which involves making predictions before conducting the experiment, then assessing how well the data supports the predictions. (These predictions may originate from a more abstract scientific hypothesis about how the phenomenon under study actually works.) Analysis of variance (ANOVA) statistical techniques are used to distinguish unique results of the experiment from the null hypothesis that variations result from random fluctuations in data. In psychology, the widely usd standard ascribes statistical significance to results which have less than 5% probability of being explained by random variation.



Statistical surveys are used in psychology for measuring attitudes and traits, monitoring changes in mood, checking the validity of experimental manipulations, and for other psychological topics. Most commonly, psychologists use paper-and-pencil surveys. However, surveys are also conducted over the phone or through e-mail. Web-based surveys are increasingly used to conveniently reach many subjects.
Neuropsychological tests, such as the Wechsler scales and Wisconsin Card Sorting Test, are mostly questionnaires or simple tasks used which assess a specific type of mental function in the respondent. These can be used in experiments, as in the case of lesion experiments evaluating the results of damage to a specific part of the brain.
Observational studies analyze uncontrolled data in search of correlations; multivariate statistics are typically used to interpret the more complex situation. Cross-sectional observational studies use data from a single point in time, whereas longitudinal studies are used to study trends across the life span. Longitudinal studies track the same people, and therefore detect more individual, rather than cultural, differences. However, they suffer from lack of controls and from confounding factors such as selective attrition (the bias introduced when a certain type of subject disproportionately leaves a study).
Exploratory data analysis refers to a variety of practices which researchers can use to visualize and analyze existing sets of data. In Peirce's three modes of inference, exploratory data anlysis corresponds to abduction, or hypothesis formation. Meta-analysis is the technique of integrating the results from multiple studies and interpreting the statistical properties of the pooled dataset.




A classic and popular tool used to relate mental and neural activity is the electroencephalogram (EEG), a technique using amplified electrodes on a person's scalp to measure voltage changes in different parts of the brain. Hans Berger, the first researcher to use EEG on an unopened skull, quickly found that brains exhibit signature "brain waves": electric oscillations which correspond to different states of consciousness. Researchers subsequently refined statistical methods for synthesizing the electrode data, and identified unique brain wave patterns such as the delta wave observed during non-REM sleep.
Newer functional neuroimaging techniques include functional magnetic resonance imaging and positron emission tomography, both of which track the flow of blood through the brain. These technologies provide more localized information about activity in the brain and create representations of the brain with widespread appeal. They also provide insight which avoids the classic problems of subjective self-reporting. It remains challenging to draw hard conclusions about where in the brain specific thoughts originate—or even how usefully such localization corresponds with reality. However, neuroimaging has delivered unmistakable results showing the existence of correlations between mind and brain. Some of these draw on a systemic neural network model rather than a localized function model.
Psychiatric interventions such as transcranial magnetic stimulation and of course drugs also provide information about brain–mind interactions. Psychopharmacology is the study of drug-induced mental effects.



Computational modeling is a tool used in mathematical psychology and cognitive psychology to simulate behavior. This method has several advantages. Since modern computers process information quickly, simulations can be run in a short time, allowing for high statistical power. Modeling also allows psychologists to visualize hypotheses about the functional organization of mental events that couldn't be directly observed in a human. Connectionism uses neural networks to simulate the brain. Another method is symbolic modeling, which represents many mental objects using variables and rules. Other types of modeling include dynamic systems and stochastic modeling.




Animal experiments aid in investigating many aspects of human psychology, including perception, emotion, learning, memory, and thought, to name a few. In the 1890s, Russian physiologist Ivan Pavlov famously used dogs to demonstrate classical conditioning. Non-human primates, cats, dogs, pigeons, rats, and other rodents are often used in psychological experiments. Ideally, controlled experiments introduce only one independent variable at a time, in order to ascertain its unique effects upon dependent variables. These conditions are approximated best in laboratory settings. In contrast, human environments and genetic backgrounds vary so widely, and depend upon so many factors, that it is difficult to control important variables for human subjects. Of course, there are pitfalls in generalizing findings from animal studies to humans through animal models.
Comparative psychology refers to the scientific study of the behavior and mental processes of non-human animals, especially as these relate to the phylogenetic history, adaptive significance, and development of behavior. Research in this area explores the behavior of many species, from insects to primates. It is closely related to other disciplines that study animal behavior such as ethology. Research in comparative psychology sometimes appears to shed light on human behavior, but some attempts to connect the two have been quite controversial, for example the Sociobiology of E. O. Wilson. Animal models are often used to study neural processes related to human behavior, e.g. in cognitive neuroscience.



Research designed to answer questions about the current state of affairs such as the thoughts, feelings, and behaviors of individuals is known as descriptive research. Descriptive research can be qualitative or quantitative in orientation. Qualitative research is descriptive research that is focused on observing and describing events as they occur, with the goal of capturing all of the richness of everyday behavior and with the hope of discovering and understanding phenomena that might have been missed if only more cursory examinations have been made.
Qualitative psychological research methods include interviews, first-hand observation, and participant observation. Creswell (2003) identifies five main possibilities for qualitative research, including narrative, phenomenology, ethnography, case study, and grounded theory. Qualitative researchers sometimes aim to enrich interpretations or critiques of symbols, subjective experiences, or social structures. Sometimes hermeneutic and critical aims can give rise to quantitative research, as in Erich Fromm's study of Nazi voting or Stanley Milgram's studies of obedience to authority.

Just as Jane Goodall studied chimpanzee social and family life by careful observation of chimpanzee behavior in the field, psychologists conduct naturalistic observation of ongoing human social, professional, and family life. Sometimes the participants are aware they are being observed, and other times the participants do not know they are being observed. Strict ethical guidelines must be followed when covert observation is being carried out.




In 1959, statistician Theodore Sterling examined the results of psychological studies and discovered that 97% of them supported their initial hypotheses, implying a possible publication bias. Similarly, Fanelli (2010) found that 91.5% of psychiatry/psychology studies confirmed the effects they were looking for, and concluded that the odds of this happening (a positive result) was around five times higher than in fields such as space- or geosciences. Fanelli argues that this is because researchers in "softer" sciences have fewer constraints to their conscious and unconscious biases.
Some popular media outlets have in recent years spotlighted a replication crisis in psychology, arguing that many findings in the field cannot be reproduced. Repeats of some famous studies have not reached the same conclusions, and some researchers have been accused of outright fraud in their results. Focus on this issue has led to renewed efforts in the discipline to re-test important findings.
Some critics view statistical hypothesis testing as misplaced. Psychologist and statistician Jacob Cohen wrote in 1994 that psychologists routinely confuse statistical significance with practical importance, enthusiastically reporting great certainty in unimportant facts. Some psychologists have responded with an increased use of effect size statistics, rather than sole reliance on the Fisherian p < .05 significance criterion (whereby an observed difference is deemed "statistically significant" if an effect of that size or larger would occur with 5% -or less- probability in independent replications, assuming the truth of the null-hypothesis of no difference between the treatments).
In 2010, a group of researchers reported a systemic bias in psychology studies towards WEIRD ("western, educated, industrialized, rich and democratic") subjects. Although only 1/8 people worldwide fall into the WEIRD classification, the researchers claimed that 60–90% of psychology studies are performed on WEIRD subjects. The article gave examples of results that differ significantly between WEIRD subjects and tribal cultures, including the Müller-Lyer illusion.
Some observers perceive a gap between scientific theory and its application—in particular, the application of unsupported or unsound clinical practices. Critics say there has been an increase in the number of mental health training programs that do not instill scientific competence. One skeptic asserts that practices, such as "facilitated communication for infantile autism"; memory-recovery techniques including body work; and other therapies, such as rebirthing and reparenting, may be dubious or even dangerous, despite their popularity. In 1984, Allen Neuringer made a similar point regarding the experimental analysis of behavior. Psychologists, sometimes divided along the lines of laboratory vs. clinic, continue to debate these issues.



Ethical standards in the discipline have changed over time. Some famous past studies are today considered unethical and in violation of established codes (Ethics Code of the American Psychological Association, the Canadian Code of Conduct for Research Involving Humans, and the Belmont Report).
The most important contemporary standards are informed and voluntary consent. After World War II, the Nuremberg Code was established because of Nazi abuses of experimental subjects. Later, most countries (and scientific journals) adopted the Declaration of Helsinki. In the U.S., the National Institutes of Health established the Institutional Review Board in 1966, and in 1974 adopted the National Research Act (HR 7724). All of these measures encouraged researchers to obtain informed consent from human participants in experimental studies. A number of influential studies led to the establishment of this rule; such studies included the MIT and Fernald School radioisotope studies, the Thalidomide tragedy, the Willowbrook hepatitis study, and Stanley Milgram's studies of obedience to authority.



University psychology departments have ethics committees dedicated to the rights and well-being of research subjects. Researchers in psychology must gain approval of their research projects before conducting any experiment to protect the interests of human participants and laboratory animals.
The ethics code of the American Psychological Association originated in 1951 as "Ethical Standards of Psychologists." This code has guided the formation of licensing laws in most American states. It has changed multiple times over the decades since its adoption. In 1989 the APA revised its policies on advertising and referral fees to negotiate the end of an investigation by the Federal Trade Commission. The 1992 incarnation was the first to distinguish between "aspirational" ethical standards and "enforceable" ones. Members of the public have a 5-year window to file ethics complaints about APA members with the APA ethics committee; members of the APA have a 3-year window.
Some of the ethical issues considered most important are the requirement to practice only within the area of competence, to maintain confidentiality with the patients, and to avoid sexual relations with them. Another important principle is informed consent, the idea that a patient or research subject must understand and freely choose a procedure they are undergoing. Some of the most common complaints against clinical psychologists include sexual misconduct, and involvement in child custody evaluations.



Current ethical guidelines state that using non-human animals for scientific purposes is only acceptable when the harm (physical or psychological) done to animals is outweighed by the benefits of the research. Keeping this in mind, psychologists can use certain research techniques on animals that could not be used on humans.
An experiment by Stanley Milgram raised questions about the ethics of scientific experimentation because of the extreme emotional stress suffered by the participants. It measured the willingness of study participants to obey an authority figure who instructed them to perform acts that conflicted with their personal conscience.
Harry Harlow drew condemnation for his "pit of despair" experiments on rhesus macaque monkeys at the University of Wisconsin–Madison in the 1970s. The aim of the research was to produce an animal model of clinical depression. Harlow also devised what he called a "rape rack", to which the female isolates were tied in normal monkey mating posture. In 1974, American literary critic Wayne C. Booth wrote that, "Harry Harlow and his colleagues go on torturing their nonhuman primates decade after decade, invariably proving what we all knew in advance—that social creatures can be destroyed by destroying their social ties." He writes that Harlow made no mention of the criticism of the morality of his work.






Baker, David B. (ed.). The Oxford Handbook of the History of Psychology. Oxford University Press (Oxford Library of Psychology), 2012. ISBN 9780195366556
Brock, Adrian C. (ed.). Internationalizing the History of Psychology. New York University Press, 2006. ISBN 9780814799444
Chin, Robert, and Ai-li S. Chin. Psychological Research in Communist China: 1949–1966. Cambridge: M.I.T. Press, 1969. ISBN 978-0-262-03032-8
Cina, Carol. "Social Science for Whom? A Structural History of Social Psychology." Doctoral dissertation, accepted by the State University of New York at Stony Brook, 1981.
Cocks, Geoffrey. Psychotherapy in the Third Reich: The Göring Institute, second edition. New Brunswick, NJ: Transaction Publishers, 1997. ISBN 1-56000-904-7
Forgas, Joseph P., Kipling D. Williams, & Simon M. Laham. Social Motivation: Conscious and Unconscious Processes. Cambridge University Press, 2005. ISBN 0-521-83254-3
Gregory, Robert J. Psychological Testing: History, Principles, and Applications. Sixth edition. Boston: Allyn & Bacon (Pearson), 2011. ISBN 978-0-205-78214-7
Guthrie, Robert. Even the Rat was White: A Historical View of Psychology. Second edition. Boston, Allyn and Bacon (Viacon), 1998. ISBN 0-205-14993-6
Leahey, A History of Modern Psychology. Third Edition. Upper Saddle River, NJ: Prentice Hall (Pearson), 2001.
Luria, A. R. (1973). The Working Brain: An Introduction to Neuropsychology. Translated by Basil Haigh. Basic Books. ISBN 0-465-09208-X
Herman, Ellen. "Psychology as Politics: How Psychological Experts Transformed Public Life in the United States 1940–1970." Doctoral dissertation accepted by Brandeis University, 1993.
Hock, Roger R. Forty Studies That Changed Psychology: Explorations Into the History of Psychological Research. Fourth edition. Upper Saddle River, NJ: Prentice Hall, 2002. ISBN 978-0-13-032263-0
Kozulin, Alex. Psychology in Utopia: Toward a Social History of Soviet Psychology. Cambridge: MIT Press, 1984. ISBN 0-262-11087-3
Morgan, Robert D., Tara L. Kuther, & Corey J. Habben. Life After Graduate School in Psychology: Insider's Advice from New Psychologists. New York: Psychology Press (Taylor & Francis Group), 2005. ISBN 1-84169-410-X
Severin, Frank T. (ed.). Humanistic Viewpoints in Psychology: A Book of Readings. New York: McGraw Hill, 1965. ISBN
Shah, James Y., and Wendi L. Gardner. Handbook of Motivation Science. New York: The Guilford Press, 2008. ISBN 978-1-59385-568-0
Teo, Thomas. The Critique of Psychology: From Kant to Postcolonial Theory. New York: Springer, 2005. ISBN 978-0-387-25355-8
Wallace, Edwin R., IV, & John Gach (eds.), History of Psychiatry and Medical Psychology; New York: Springer, 2008; ISBN 978-0-387-34708-0
Weiner, Bernard. Human Motivation. Hoboken, NJ: Taylor and Francis, 2013. ISBN 9780805807110
Weiner, Irving B. Handbook of Psychology. Hoboken, NJ: John Wiley & Sons, 2003. ISBN 0-471-17669-9
Volume 1: History of Psychology. Donald K. Freedheim, ed. ISBN 0-471-38320-1
Volume 2: Research Methods in Psychology. John A. Schinka & Wayne F. Velicer, eds. ISBN 0-471-38513-1
Volume 3: Biological Psychology. Michela Gallagher & Randy J. Nelson, eds. ISBN 0-471-38403-8
Volume 4: Experimental Psychology. Alice F. Healy & Robert W. Proctor, eds. ISBN 0-471-39262-6
Volume 8: Clinical Psychology. George Stricker, Thomas A. Widiger, eds. ISBN 0-471-39263-4



Badcock, Christopher R. (2015). "Nature-Nurture Controversy, History of". In Wright, James D. International Encyclopedia of the Social & Behavioral Sciences (Second ed.). Elsevier. pp. 340–344. doi:10.1016/B978-0-08-097086-8.03136-6. ISBN 978-0-08-097087-5. Retrieved 9 April 2015. Lay summary – Penn Libraries News Center (8 April 2015).  – via ScienceDirect (Subscription may be required or content may be available in libraries.)
Cascio, Wayne F. (2015). "Industrial–Organizational Psychology: Science and Practice". In Wright, James D. International Encyclopedia of the Social & Behavioral Sciences (Second ed.). Elsevier. pp. 879–884. doi:10.1016/B978-0-08-097086-8.22007-2. ISBN 978-0-08-097087-5. Retrieved 9 April 2015. Lay summary – Penn Libraries News Center (8 April 2015).  – via ScienceDirect (Subscription may be required or content may be available in libraries.)
Chryssochoou, Xenia (2015). "Social Psychology". In Wright, James D. International Encyclopedia of the Social & Behavioral Sciences (Second ed.). Elsevier. pp. 532–537. doi:10.1016/B978-0-08-097086-8.24095-6. ISBN 978-0-08-097087-5. Retrieved 9 April 2015. Lay summary – Penn Libraries News Center (8 April 2015).  – via ScienceDirect (Subscription may be required or content may be available in libraries.)
Deakin, Nicholas (2015). "Philosophy, Psychiatry, and Psychology". In Wright, James D. International Encyclopedia of the Social & Behavioral Sciences (Second ed.). Elsevier. pp. 31–36. doi:10.1016/B978-0-08-097086-8.27049-9. ISBN 978-0-08-097087-5. Retrieved 9 April 2015. Lay summary – Penn Libraries News Center (8 April 2015).  – via ScienceDirect (Subscription may be required or content may be available in libraries.)
Demetriou, Andreas (2015). "Intelligence in Cultural, Social and Educational Context". In Wright, James D. International Encyclopedia of the Social & Behavioral Sciences (Second ed.). Elsevier. pp. 313–322. doi:10.1016/B978-0-08-097086-8.92147-0. ISBN 978-0-08-097087-5. Retrieved 9 April 2015. Lay summary – Penn Libraries News Center (8 April 2015).  – via ScienceDirect (Subscription may be required or content may be available in libraries.)
Gelso, Charles J. (2015). "Counseling Psychology". In Wright, James D. International Encyclopedia of the Social & Behavioral Sciences (Second ed.). Elsevier. pp. 69–72. doi:10.1016/B978-0-08-097086-8.21073-8. ISBN 978-0-08-097087-5. Retrieved 9 April 2015. Lay summary – Penn Libraries News Center (8 April 2015).  – via ScienceDirect (Subscription may be required or content may be available in libraries.)
Henley, Tracy B. (2015). "Psychology, History of (Early Period)". In Wright, James D. International Encyclopedia of the Social & Behavioral Sciences (Second ed.). Elsevier. pp. 406–411. doi:10.1016/B978-0-08-097086-8.03235-9. ISBN 978-0-08-097087-5. Retrieved 9 April 2015. Lay summary – Penn Libraries News Center (8 April 2015).  – via ScienceDirect (Subscription may be required or content may be available in libraries.)
Knowland, Victoria C. P.; Purser, Harry; Thomas, Michael S. C. (2015). "Cross-Sectional Methodologies in Developmental Psychology". In Wright, James D. International Encyclopedia of the Social & Behavioral Sciences (Second ed.). Elsevier. pp. 354–360. doi:10.1016/B978-0-08-097086-8.23235-2. ISBN 978-0-08-097087-5. Retrieved 9 April 2015. Lay summary – Penn Libraries News Center (8 April 2015).  – via ScienceDirect (Subscription may be required or content may be available in libraries.)
Louw, Dap (2015). "Forensic Psychology". In Wright, James D. International Encyclopedia of the Social & Behavioral Sciences (Second ed.). Elsevier. pp. 351–356. doi:10.1016/B978-0-08-097086-8.21074-X. ISBN 978-0-08-097087-5. Retrieved 9 April 2015. Lay summary – Penn Libraries News Center (8 April 2015).  – via ScienceDirect (Subscription may be required or content may be available in libraries.)
McWilliams, Spencer A. (2015). "Psychology, History of (Twentieth Century)". In Wright, James D. International Encyclopedia of the Social & Behavioral Sciences (Second ed.). Elsevier. pp. 412–417. doi:10.1016/B978-0-08-097086-8.03046-4. ISBN 978-0-08-097087-5. Retrieved 8 April 2015. Lay summary – Penn Libraries News Center (8 April 2015).  – via ScienceDirect (Subscription may be required or content may be available in libraries.)
Pe-Pua, Rogelia (2015). "Indigenous Psychology". In Wright, James D. International Encyclopedia of the Social & Behavioral Sciences (Second ed.). Elsevier. pp. 788–794. doi:10.1016/B978-0-08-097086-8.24067-1. ISBN 978-0-08-097087-5. Retrieved 9 April 2015. Lay summary – Penn Libraries News Center (8 April 2015).  – via ScienceDirect (Subscription may be required or content may be available in libraries.)
Peterson, Roger L.; Peterson, Donald R.; Abrams, Jules C.; Stricker, George; Ducheny, Kelly (2015). "Training in Clinical Psychology in the United States: Practitioner Model". In Wright, James D. International Encyclopedia of the Social & Behavioral Sciences (Second ed.). Elsevier. pp. 517–523. doi:10.1016/B978-0-08-097086-8.21086-6. ISBN 978-0-08-097087-5. Retrieved 9 April 2015. Lay summary – Penn Libraries News Center (8 April 2015).  – via ScienceDirect (Subscription may be required or content may be available in libraries.)
Poortinga, Ype H. (2015). "Cross-Cultural Psychology". In Wright, James D. International Encyclopedia of the Social & Behavioral Sciences (Second ed.). Elsevier. pp. 311–317. doi:10.1016/B978-0-08-097086-8.24011-7. ISBN 978-0-08-097087-5. Retrieved 9 April 2015. Lay summary – Penn Libraries News Center (8 April 2015).  – via ScienceDirect (Subscription may be required or content may be available in libraries.)
Spinath, Frank M.; Spinath, Birgit; Borkenau, Peter (2015). "Developmental Behavioral Genetics and Education". In Wright, James D. International Encyclopedia of the Social & Behavioral Sciences (Second ed.). Elsevier. pp. 320–325. doi:10.1016/B978-0-08-097086-8.92009-9. ISBN 978-0-08-097087-5. Retrieved 9 April 2015. Lay summary – Penn Libraries News Center (8 April 2015).  – via ScienceDirect (Subscription may be required or content may be available in libraries.)
Smith, Edward E. (2015). "Cognitive Psychology: History". In Wright, James D. International Encyclopedia of the Social & Behavioral Sciences (Second ed.). Elsevier. pp. 103–109. doi:10.1016/B978-0-08-097086-8.03028-2. ISBN 978-0-08-097087-5. Retrieved 9 April 2015. Lay summary – Penn Libraries News Center (8 April 2015).  – via ScienceDirect (Subscription may be required or content may be available in libraries.)
Staerklé, Christian (2015). "Political Psychology". In Wright, James D. International Encyclopedia of the Social & Behavioral Sciences (Second ed.). Elsevier. pp. 427–433. doi:10.1016/B978-0-08-097086-8.24079-8. ISBN 978-0-08-097087-5. Retrieved 9 April 2015. Lay summary – Penn Libraries News Center (8 April 2015).  – via ScienceDirect (Subscription may be required or content may be available in libraries.)
Gelder, Mayou & Geddes (2005). Psychiatry. New York, NY: Oxford University Press Inc.



Psychology at DMOZ
American Psychological Association
Association for Psychological Science
Discovering Psychology. (2001). The History of Psychology:Contemporary Foundations
The Florida State University PSYCHOLOGY.(2011).HISTORYParrots, also known as psittacines /ˈsɪtəsaɪnz/, are birds of the roughly 393 species in 92 genera that make up the order Psittaciformes, found in most tropical and subtropical regions. The order is subdivided into three superfamilies: the Psittacoidea ("true" parrots), the Cacatuoidea (cockatoos), and the Strigopoidea (New Zealand parrots). Parrots have a generally pantropical distribution with several species inhabiting temperate regions in the Southern Hemisphere, as well. The greatest diversity of parrots is in South America and Australasia.
Characteristic features of parrots include a strong, curved bill, an upright stance, strong legs, and clawed zygodactyl feet. Many parrots are vividly coloured, and some are multi-coloured. Most parrots exhibit little or no sexual dimorphism in the visual spectrum. They form the most variably sized bird order in terms of length. The most important components of most parrots' diets are seeds, nuts, fruit, buds, and other plant material. A few species sometimes eat animals and carrion, while the lories and lorikeets are specialised for feeding on floral nectar and soft fruits. Almost all parrots nest in tree hollows (or nest boxes in captivity), and lay white eggs from which hatch altricial (helpless) young.
Parrots, along with ravens, crows, jays, and magpies, are among the most intelligent birds, and the ability of some species to imitate human voices enhances their popularity as pets. Some parrots are intelligent and talk at the level of a four-to-five year old human. Trapping wild parrots for the pet trade, as well as hunting, habitat loss, and competition from invasive species, has diminished wild populations, with parrots being subjected to more exploitation than any other group of birds. Measures taken to conserve the habitats of some high-profile charismatic species have also protected many of the less charismatic species living in the same ecosystems.







Psittaciform diversity in South America and Australasia suggests that the order may have evolved in Gondwana, centred in Australasia. The scarcity of parrots in the fossil record, however, presents difficulties in confirming the hypothesis, and there is currently a higher amount of fossil remains from the northern hemisphere in the early Cenozoic. Molecular studies suggest that parrots evolved approximately 59 million years ago (Mya) (range 66–51 Mya) in Gondwana. The three major clades of Neotropical parrots originated about 50 Mya (range 57–41 Mya).
A single 15 mm (0.6 in) fragment from a large lower bill (UCMP 143274), found in deposits from the Lance Creek Formation in Niobrara County, Wyoming, had been thought to be the oldest parrot fossil and is presumed to have originated from the Late Cretaceous period, which makes it about 70 million years old. However, other studies suggest that this fossil is not from a bird, but from a caenagnathid oviraptorosaur (a non-avian dinosaur with a birdlike beak), as several details of the fossil used to support its identity as a parrot are not actually exclusive to parrots, and it is dissimilar to the earliest-known unequivocal parrot fossils. Likewise, the earliest parrots do not have the specialised crushing bills of modern species.
It is now generally assumed that the Psittaciformes, or their common ancestors with several related bird orders, were present somewhere in the world around the Cretaceous–Paleogene extinction event (K-Pg extinction), some 66 Mya. If so, they probably had not evolved their morphological autapomorphies yet, but were generalised arboreal birds. The combined evidence supported the hypothesis of Psittaciformes being "near passerines", i.e. the mostly land-living birds that emerged in close proximity to the K-Pg extinction. Analysis of transposable element insertions observed in the genomes of passerines and parrots, but not in the genomes of other birds, provides strong evidence that parrots are the sister group of passerines, forming a clade Psittacopasserae, to the exclusion of the next closest group, the falcons.
Europe is the origin of the first undeniable parrot fossils, which date from about 50 Mya. The climate there and then was tropical, consistent with the Paleocene-Eocene thermal maximum. Initially, a neoavian named Mopsitta tanta, uncovered in Denmark's Early Eocene Fur Formation and dated to 54 Mya, was assigned to the Psittaciformes; it was described from a single humerus. However, the rather nondescript bone is not unequivocally psittaciform, and more recently it was pointed out that it may rather belong to a newly discovered ibis of the genus Rhynchaeites, whose fossil legs were found in the same deposits.

Fossils assignable to Psittaciformes (though not yet the present-day parrots) date from slightly later in the Eocene, starting around 50 Mya. Several fairly complete skeletons of parrot-like birds have been found in England and Germany. Some uncertainty remains, but on the whole it seems more likely that these are not direct ancestors of the modern parrots, but related lineages that evolved in the Northern Hemisphere and have since died out. These are probably not "missing links" between ancestral and modern parrots, but rather psittaciform lineages that evolved parallel to true parrots and cockatoos and had their own peculiar autapomorphies:
Psittacopes
Serudaptus
Pseudasturidae
Pseudasturides

Vastanavidae
Vastanavis

Quercypsittidae
Quercypsitta

MesselasturidaeMesselastur
Tynskya

The earliest records of modern parrots date to about 23–20 Mya. The fossil record—mainly from Europe—consists of bones clearly recognisable as belonging to parrots of modern type. The Southern Hemisphere does not have nearly as rich a fossil record for the period of interest as the Northern, and contains no known parrot-like remains earlier than the early to middle Miocene, around 20 Mya. At this point, however, is found the first unambiguous parrot fossil (as opposed to a parrot-like one), an upper jaw that is indistinguishable from that of modern cockatoos.



The Psittaciformes comprise three main lineages: Strigopoidea, Psittacoidea and Cacatuoidea. The Strigopoidea were considered part of the Psittacoidea, but recent studies place this group of New Zealand species at the base of the parrot tree next to the remaining members of the Psittacoidea, as well as all members of the Cacatuoidea. The Cacatuoidea are quite distinct, having a movable head crest, a different arrangement of the carotid arteries, a gall bladder, differences in the skull bones, and lack the Dyck texture feathers that—in the Psittacidae—scatter light to produce the vibrant colours of so many parrots. Colourful feathers with high levels of psittacofulvin resist the feather-degrading bacterium Bacillus licheniformis better than white ones. Lorikeets were previously regarded as a third family, Loriidae, but are now considered a tribe (Loriini) within the subfamily Lorinae, family Cacatuoidea. The two other tribes in the subfamily are the closely related fig parrots (two genera in the tribe Cyclopsittini) and budgerigar (tribe Melopsittacini).




The order Psittaciformes consists of roughly 393 species belonging to 92 genera. The following classification is based on the most recent proposal as of 2012.

Superfamily Strigopoidea: New Zealand parrots
Family Nestoridae: two genera with two living (kea and New Zealand kaka) and several extinct species of the New Zealand region
Family Strigopidae: the flightless, critically endangered kakapo of New Zealand
Superfamily Cacatuoidea: cockatoos
Family Cacatuidae
Subfamily Nymphicinae: one genus with one species, the cockatiel.
Subfamily Calyptorhynchinae: the black cockatoos
Subfamily Cacatuinae
Tribe Microglossini: one genus with one species, the black palm cockatoo
Tribe Cacatuini: four genera of white, pink, and grey species

Superfamily Psittacoidea: true parrots
Family Psittacidae
Subfamily Psittacinae: two African genera, Psittacus and Poicephalus
Subfamily Arinae
Tribe Arini: 18 genera
Tribe Androglossini: seven genera.

Family Psittaculidae
Subfamily Psittrichasinae: one species, Pesquet's parrot
Subfamily Coracopsinae: one genus with several species.
Subfamily Platycercinae
Tribe Pezoporini: ground parrots and allies
Tribe Platycercini: broad-tailed parrots

Subfamily Psittacellinae: one genus (Psittacella) with several species
Subfamily Loriinae
Tribe Loriini: lories and lorikeets
Tribe Melopsittacini: one genus with one species, the budgerigar
Tribe Cyclopsittini: fig parrots

Subfamily Agapornithinae: three genera
Subfamily Psittaculinae
Tribe Polytelini: three genera
Tribe Psittaculini: Asian psittacines
Tribe Micropsittini: pygmy parrots




Extant species range in size from the buff-faced pygmy parrot, at under 10 g (0.4 oz) in weight and 8 cm (3.1 in) in length, to the hyacinth macaw, at 1 m (3.3 ft) in length, and the kakapo, at 4.0 kg (8.8 lb) in weight. Among the superfamilies, the three extant Strigopoidea species are all large parrots, and the cockatoos tend to be large birds, as well. The Psittacoidea parrots are far more variable, ranging the full spectrum of sizes shown by the family.
The most obvious physical characteristic is the strong, curved, broad bill. The upper mandible is prominent, curves downward, and comes to a point. It is not fused to the skull, which allows it to move independently, and contributes to the tremendous biting pressure the birds are able to exert. A large macaw, for example, has a bite force of 35 kg/cm2 (500 lb/sq in), close to that of a large dog. The lower mandible is shorter, with a sharp, upward-facing cutting edge, which moves against the flat portion of the upper mandible in an anvil-like fashion. Touch receptors occur along the inner edges of the kerantinised bill, which are collectively known as the "bill tip organ", allowing for highly dexterous manipulations. Seed-eating parrots have a strong tongue (containing similar touch receptors to those in the bill tip organ), which helps to manipulate seeds or position nuts in the bill so that the mandibles can apply an appropriate cracking force. The head is large, with eyes positioned high and laterally in the skull, so the visual field of parrots is unlike any other birds. Without turning its head, a parrot can see from just below its bill tip, all above its head, and quite far behind its head. Parrots also have quite a wide frontal binocular field for a bird, although this is nowhere near as large as primate binocular visual fields.
Parrots have strong zygodactyl feet with sharp, elongated claws, which are used for climbing and swinging. Most species are capable of using their feet to manipulate food and other objects with a high degree of dexterity, in a similar manner to a human using their hands. A study conducted with Australian parrots has demonstrated that they exhibit "handedness", a distinct preference with regards to the foot used to pick up food, with adult parrots being almost exclusively "left-footed" or "right-footed", and with the prevalence of each preference within the population varying by species.
Cockatoo species have a mobile crest of feathers on the top of their heads, which they can raise for display, and retract. No other parrots can do so, but the Pacific lorikeets in the genera Vini and Phigys can ruffle the feathers of the crown and nape, and the red-fan parrot (or hawk-headed parrot) has a prominent feather neck frill that it can raise and lower at will. The predominant colour of plumage in parrots is green, though most species have some red or another colour in small quantities. Cockatoos are the main exception to this, having lost the green and blue plumage colours in their evolutionary history; they are now predominately black or white with some red, pink, or yellow. Strong sexual dimorphism in plumage is not typical among parrots, with some notable exceptions, the most striking being the eclectus parrot. However it has been shown that some parrot species exhibit sexually dimorphic plumage in the ultraviolet spectrum, normally invisible to humans.




Parrots are found on all tropical and subtropical continents and regions including Australia and Oceania, South Asia, Southeast Asia, Central America, South America, and Africa. Some Caribbean and Pacific islands are home to endemic species. By far the greatest number of parrot species come from Australasia and South America. The lories and lorikeets range from Sulawesi and the Philippines in the north to Australia and across the Pacific as far as French Polynesia, with the greatest diversity being found in and around New Guinea. The subfamily Arinae encompasses all the neotropical parrots, including the amazons, macaws, and conures, and ranges from northern Mexico and the Bahamas to Tierra del Fuego in the southern tip of South America. The pygmy parrots, tribe Micropsittini, form a small genus restricted to New Guinea and the Solomon Islands. The superfamily Strigopoidea contains three living species of aberrant parrots from New Zealand. The broad-tailed parrots, subfamily Platycercinae, are restricted to Australia, New Zealand, and the Pacific islands as far eastwards as Fiji. The true parrot superfamily, Psittacoidea, includes a range of species from Australia and New Guinea to South Asia and Africa. The centre of cockatoo biodiversity is Australia and New Guinea, although some species reach the Solomon Islands (and one formerly occurred in New Caledonia), Wallacea and the Philippines.
Several parrots inhabit the cool, temperate regions of South America and New Zealand. One, the Carolina parakeet, lived in temperate North America, but was hunted to extinction in the early 20th century. Many parrots have been introduced to areas with temperate climates, and have established stable populations in parts of the United States (including New York City), the United Kingdom, Belgium and Spain, as well as in Greece.
Few parrots are wholly sedentary or fully migratory. Most fall somewhere between the two extremes, making poorly understood regional movements, with some adopting an entirely nomadic lifestyle. Only three species are migratory – the orange-bellied, blue-winged and swift parrots.



Numerous challenges are found in studying wild parrots, as they are difficult to catch and once caught, they are difficult to mark. Most wild bird studies rely on banding or wing tagging, but parrots chew off such attachments. Parrots also tend to range widely, and consequently many gaps occur in knowledge of their behaviour. Some parrots have a strong, direct flight. Most species spend much of their time perched or climbing in tree canopies. They often use their bills for climbing by gripping or hooking on branches and other supports. On the ground, parrots often walk with a rolling gait.




The diet of parrots consists of seeds, fruit, nectar, pollen, buds, and sometimes arthropods and other animal prey. The most important of these for most true parrots and cockatoos are seeds; the evolution of the large and powerful bill can be explained primarily as an adaptation to opening and consuming seeds. All true parrots except the Pesquet's parrot employ the same method to obtain the seed from the husk; the seed is held between the mandibles and the lower mandible crushes the husk, whereupon the seed is rotated in the bill and the remaining husk is removed. A foot is sometimes used to help hold large seeds in place. Parrots are seed predators rather than seed dispersers, and in many cases where species are recorded as consuming fruit, they are only eating the fruit to get at the seed. As seeds often have poisons that protect them, parrots carefully remove seed coats and other chemically defended fruit parts prior to ingestion. Many species in the Americas, Africa, and Papua New Guinea consume clay, which releases minerals and absorbs toxic compounds from the gut.

The lories and lorikeets, hanging parrots, and swift parrot are primarily nectar and pollen consumers, and have tongues with brush tips to collect this source of food, as well as some specialised gut adaptations to accommodate this diet. Many other species also consume nectar when it becomes available.
In addition to feeding on seeds and flowers, some parrot species prey on animals, especially invertebrate larvae. Golden-winged parakeets prey on water snails, the kea of New Zealand hunts adult sheep (though uncommon), and the Antipodes parakeet, another New Zealand parrot, enters the burrows of nesting grey-backed storm petrels and kills the incubating adults. Some cockatoos and the kākā excavate branches and wood to obtain grubs; the bulk of the yellow-tailed black cockatoo's diet is made up of insects.
Some extinct parrots had carnivorous diets. Pseudasturids were probably cuckoo or puffbird-like insectivores, while messelasturids were raptor-like carnivores.



With few exceptions, parrots are monogamous breeders who nest in cavities and hold no territories other than their nesting sites. The pair bonds of the parrots and cockatoos are strong and a pair remains close during the nonbreeding season, even if they join larger flocks. As with many birds, pair bond formation is preceded by courtship displays; these are relatively simple in the case of cockatoos. In Psittacidae parrots' common breeding displays, usually undertaken by the male, include slow, deliberate steps known as a "parade" or "stately walk" and the "eye-blaze", where the pupil of the eye constricts to reveal the edge of the iris. Allopreening is used by the pair to help maintain the bond. Cooperative breeding, where birds other than the breeding pair help raise the young and is common in some bird families, is extremely rare in parrots, and has only unambiguously been demonstrated in the El Oro parakeet and the golden parakeet (which may also exhibit polygamous, or group breeding, behaviour with multiple females contributing to the clutch).

Only the monk parakeet and five species of lovebirds build nests in trees, and three Australian and New Zealand ground parrots nest on the ground. All other parrots and cockatoos nest in cavities, either tree hollows or cavities dug into cliffs, banks, or the ground. The use of holes in cliffs is more common in the Americas. Many species use termite nests, possibly to reduce the conspicuousness of the nesting site or to create a favourable microclimate. In most cases, both parents participate in the nest excavation. The length of the burrow varies with species, but is usually between 0.5 and 2 m (1.6 and 6.6 ft) in length. The nests of cockatoos are often lined with sticks, wood chips, and other plant material. In the larger species of parrots and cockatoos, the availability of nesting hollows may be limited, leading to intense competition for them both within the species and between species, as well as with other bird families. The intensity of this competition can limit breeding success in some cases. Hollows created artificially by arborists have proven successful in boosting breeding rates in these areas. Some species are colonial, with the burrowing parrot nesting in colonies up to 70,000 strong. Coloniality is not as common in parrots as might be expected, possibly because most species adopt old cavities rather than excavate their own.
The eggs of parrots are white. In most species, the female undertakes all the incubation, although incubation is shared in cockatoos, the blue lorikeet, and the vernal hanging parrot. The female remains in the nest for almost all of the incubation period and is fed both by the male and during short breaks. Incubation varies from 17 to 35 days, with larger species having longer incubation periods. The newly born young are altricial, either lacking feathers or with sparse white down. The young spend three weeks to four months in the nest, depending on species, and may receive parental care for several months thereafter.
As typical of K-selected species, the macaws and other larger parrot species have low reproductive rates. They require several years to reach maturity, produce one or very few young per year, and do not necessarily breed every year.




Studies with captive birds have given insight into which birds are the most intelligent. While parrots are able to mimic human speech, studies with the African grey parrot have shown that some are able to associate words with their meanings and form simple sentences. Along with crows, ravens, and jays (family Corvidae), parrots are considered the most intelligent of birds. The brain-to body size ratio of psittacines and corvines is comparable to that of higher primates. One argument against the supposed intelligent capabilities of bird species is that birds have a relatively small cerebral cortex, which is the part of the brain considered the main area of intelligence in other animals. However, birds use a different part of the brain, the mediorostral HVC as the seat of their intelligence. These species tend to have the largest hyperstriata, and Harvey J. Karten, a neuroscientist at the University of California, San Diego, who studied bird physiology, has discovered that the lower part of the avian brain is functionally similar to that in humans. Not only have parrots demonstrated intelligence through scientific testing of their language-using ability, but also some species of parrots such as the kea are also highly skilled at using tools and solving puzzles.
Learning in early life is apparently important to all parrots, and much of that learning is social learning. Social interactions are often practised with siblings, and in several species, creches are formed with several broods, and these, too, are important for learning social skills. Foraging behaviour is generally learnt from parents, and can be a very protracted affair. Suprageneralists and specialists generally become independent of their parents much quicker than partly specialised species who may have to learn skills over long periods as various resources become seasonally available. Play forms a large part of learning in parrots; it can be solitary, and related to motor skills, or social. Species may engage in play fights or wild flights to practice predator evasion. An absence of stimuli can delay the development of young birds, as demonstrated by a group of vasa parrots kept in tiny cages with domesticated chickens from the age of 3 months; at 9 months, these birds still behaved in the same way as 3-month-olds, but had adopted some chicken behaviour. In a similar fashion, captive birds in zoo collections or pets can, if deprived of stimuli, develop stereotyped behaviours and harmful behaviours like self plucking. Aviculturists working with parrots have identified the need for environmental enrichment to keep parrots stimulated.




Many parrots can imitate human speech or other sounds. A study by Irene Pepperberg suggested a high learning ability in an African grey parrot named Alex. Alex was trained to use words to identify objects, describe them, count them, and even answer complex questions such as "How many red squares?" with over 80% accuracy. N'kisi, another African grey, has been shown to have a vocabulary around a thousand words, and has displayed an ability to invent, as well as use words in context and in the correct tense.
Parrots do not have vocal cords, so sound is accomplished by expelling air across the mouth of the bifurcated trachea, in the organ called the syrinx. Different sounds are produced by changing the depth and shape of the trachea. African grey parrots of all subspecies are known for their superior ability to imitate sounds and human speech. This ability has made them prized as pets from ancient times to the present. In the Masnavi, written by Rumi of Persia in 1250, the author describes an ancient method for training parrots to speak.
Although most parrot species are able to imitate, some of the amazon parrots are generally regarded as the next-best imitators and speakers of the parrot world. The question of why birds imitate remains open, but those that do often score very high on tests designed to measure problem-solving ability. Wild African grey parrots have been observed imitating other birds.



The journal Animal Cognition stated that some birds preferred to work alone, while others like to work together as with African grey parrots. With two parrots, they know the order of tasks or when they should do something together at once, but they have trouble exchanging roles. With three parrots, one parrot usually prefers to cooperate with one of the other two, but all of them are cooperating to solve the task.







Parrots may not make good pets for most people because of their natural wild instincts such as screaming and chewing. Although parrots can be very affectionate and cute when immature, they often become aggressive when mature (partly due to mishandling and poor training) and may bite, causing serious injury. For this reason, parrot rescue groups estimate that most parrots are surrendered and rehomed through at least five homes before reaching their permanent destinations or before dying prematurely from unintentional or intentional neglect and abuse. The parrots' ability to mimic human words and their bright colours and beauty prompt impulse buying from unsuspecting consumers. The domesticated budgerigar, a small parrot, is the most popular of all pet bird species. In 1992, the newspaper USA Today published that 11 million pet birds were in the United States alone, many of them parrots. Europeans kept birds matching the description of the rose-ringed parakeet (or called the ring-necked parrot), documented particularly in a first-century account by Pliny the Elder. As they have been prized for thousands of years for their beauty and ability to talk, they have also often been misunderstood. For example, author Wolfgang de Grahl says in his 1987 book The Grey Parrot that some importers had parrots drink only coffee while they were shipped by boat, believing that pure water was detrimental and that their actions would increase survival rates during shipping. Nowadays, it is commonly accepted that the caffeine in coffee is toxic to birds.
Pet parrots may be kept in a cage or aviary; though generally, tame parrots should be allowed out regularly on a stand or gym. Depending on locality, parrots may be either wild-caught or be captive-bred, though in most areas without native parrots, pet parrots are captive-bred. Parrot species that are commonly kept as pets include conures, macaws, amazon parrots, cockatoos, African greys, lovebirds, cockatiels, budgerigars, caiques, parakeets, and Eclectus, Pionus, and Poicephalus species. Temperaments and personalities vary even within a species, just as with dog breeds. African grey parrots are thought to be excellent talkers, but not all African grey parrots want to talk, though they have the capability to do so. Noise level, talking ability, cuddliness with people, and care needs can sometimes depend on how the bird is cared for and the attention he/she regularly receives.

Parrots invariably require an enormous amount of attention, care, and intellectual stimulation to thrive, akin to that required by a three-year-old child, which many people find themselves unable to provide in the long term. Parrots that are bred for pets may be hand fed or otherwise accustomed to interacting with people from a young age to help ensure they become tame and trusting. However, even when hand fed, parrots revert to biting and aggression during hormonal surges and if mishandled or neglected. Parrots are not low-maintenance pets; they require feeding, grooming, veterinary care, training, environmental enrichment through the provision of toys, exercise, and social interaction (with other parrots or humans) for good health.
Some large parrot species, including large cockatoos, amazons, and macaws, have very long lifespans, with 80 years being reported, and record ages of over 100. Small parrots, such as lovebirds, hanging parrots, and budgies, have shorter lifespans up to 15–20 years. Some parrot species can be quite loud, and many of the larger parrots can be destructive and require a very large cage, and a regular supply of new toys, branches, or other items to chew up. The intelligence of parrots means they are quick to learn tricks and other behaviours—both good and bad—that get them what they want, such as attention or treats.
The popularity, longevity, and intelligence of many of the larger kinds of pet parrots and their wild traits such as screaming, has led to many birds needing to be rehomed during the course of their long lifespans. A common problem is that large parrots that are cuddly and gentle as juveniles mature into intelligent, complex, often demanding adults who can outlive their owners, and can also become aggressive or even dangerous. Due to an increasing number of homeless parrots, they are being euthanised like dogs and cats, and parrot adoption centres and sanctuaries are becoming more common. Parrots do not often do well in captivity, causing some parrots to go insane and develop repetitive behaviours, such as swaying and screaming, or they become riddled with intense fear. Feather destruction and self-mutilation, although not commonly seen in the wild, occur frequently in captivity.




The popularity of parrots as pets has led to a thriving—and often illegal—trade in the birds, and some species are now threatened with extinction. A combination of trapping of wild birds and damage to parrot habitats makes survival difficult or even impossible for some species of parrot. Importation of wild-caught parrots into the US and Europe is illegal after the Wild Bird Population Act was passed in 1992.
The trade continues unabated in some countries. A report published in January 2007 presents a clear picture of the wild-caught parrot trade in Mexico, stating: "The majority of parrots captured in Mexico stay in the country for the domestic trade. A small percentage of this capture, 4% to 14%, is smuggled into the USA."
The scale of the problem can be seen in the Tony Silva case of 1996, in which a parrot expert and former director at Tenerife's Loro Parque (Europe's largest parrot park) was jailed in the United States for 82 months and fined $100,000 for smuggling hyacinth macaws (Such birds command a very high price.) The case led to calls for greater protection and control over trade in the birds. Different nations have different methods of handling internal and international trade. Australia has banned the export of its native birds since 1960. Following years of campaigning by hundreds of NGOs and outbreaks of avian flu, in July 2007, the European Union halted the importation of all wild birds with a permanent ban on their import. Prior to an earlier temporary ban started in late October 2005, the European Union (EU) was importing about two million live birds a year, about 90% of the international market: hundreds of thousands of these were parrots. No national laws protect feral parrot populations in the U.S. Mexico has a licensing system for capturing and selling native birds.




Parrots have featured in human writings, story, art, humor, religion, and music for thousands of years. From Aesop's fable "The parrot and the cat" and the Roman poet Ovid's "The Dead Parrot" to Monty Python's "Dead Parrot sketch", parrots have existed in the consciousness of many cultures. Recent books about parrots in human culture include Parrot Culture.
In ancient times and current, parrot feathers have been used in ceremonies and for decoration. They also have a long history as pets, stretching back thousands of years, and were often kept as a symbol of royalty or wealth. In Polynesian legend as current in the Marquesas Islands, the hero Laka/Aka is mentioned as having undertaken a long and dangerous voyage to Aotona in what are now the Cook Islands, to obtain the highly prized feathers of a red parrot as gifts for his son and daughter. On the voyage, 100 of his 140 rowers died of hunger on their way, but the survivors reached Aotona and captured enough parrots to fill 140 bags with their feathers. Parrots have also been considered sacred. The Moche people of ancient Peru worshipped birds and often depicted parrots in their art. Parrots are popular in Buddhist scripture and many writings about them exist. For example, Amitābha once changed himself into a parrot to aid in converting people. Another old story tells how after a forest caught fire, the parrot was so concerned, it carried water to try to put out the flames. The ruler of heaven was so moved upon seeing the parrot's act, he sent rain to put out the fire. In Chinese Buddhist iconography, a parrot is sometimes depicted hovering on the upper right side Guan Yin clasping a pearl or prayer beads in its beak.
Parrots are used as symbols of nations and nationalism. A parrot is found on the flag of Dominica and two parrots on their coat of arms. The St. Vincent parrot is the national bird of St. Vincent and the Grenadines, a Caribbean nation.
Sayings about parrots colour the modern English language. The verb "parrot" in the dictionary means "to repeat by rote". Also clichés such as the British expression "sick as a parrot" are given; although this refers to extreme disappointment rather than illness, it may originate from the disease of psittacosis, which can be passed to humans. The first occurrence of a related expression is in Aphra Behn's 1681 play The False Count. Fans of Jimmy Buffett are known as parrotheads. Parrots feature in many media. Magazines are devoted to parrots as pets, and to the conservation of parrots. Fictional films include Home Alone 3  and Rio, and documentaries include The Wild Parrots of Telegraph Hill.




Escaped parrots of several species have become established in the wild outside their natural ranges and in some cases outside the natural range of parrots. Among the earliest instances were pet red shining-parrots from Fiji, which established a population on the islands of southern Tonga. These introductions were prehistoric and red-shining parrots were recorded in Tonga by Captain Cook in the 1770s. Escapees first began breeding in cities in California, Texas, and Florida in the 1950s (with unproven earlier claims dating back to the 1920s in Texas and Florida). They have proved surprisingly hardy in adapting to conditions in Europe and North America. They sometimes even multiply to the point of becoming a nuisance or pest, and a threat to local ecosystems, and control measures have been used on some feral populations.
Feral parrot flocks can be formed after mass escapes of newly imported, wild-caught parrots from airports or quarantine facilities. Large groups of escapees have the protection of a flock and possess the skills to survive and breed in the wild. Some feral parakeets may have descended from escaped zoo birds. Escaped or released pets rarely contribute to establishing feral populations. Escapes typically involve only one or a few birds at a time, so the birds do not have the protection of a flock and often do not have a mate. Most captive-born birds do not possess the necessary survival skills to find food or avoid predators and often do not survive long without human caretakers. However, in areas where there are existing feral parrot populations, escaped pets may sometimes successfully join these flocks. The most common era or years that feral parrots were released to non-native environments was from the 1890s to the 1940s, during the wild-caught parrot era. In the psittacosis "parrot fever" panic of 1930, a city health commissioner urged everyone who owned a parrot to put them down, but owners abandoned their parrots on the streets.




Many parrot species are in decline and several are extinct. Of the 350 or so living species, 130 are listed as near threatened or worse by the International Union for Conservation of Nature (IUCN), and 16 of which are currently considered critically endangered. Several reasons are given for the decline of so many species, the principal threats being habitat loss and degradation, hunting, and for certain species, the wild-bird trade. Parrots are persecuted because, in some areas, they are (or have been) hunted for food and feathers, and as agricultural pests. For a time, Argentina offered a bounty on Monk parakeets (an agricultural pest), resulting in hundreds of thousands of birds being killed, though apparently this did not greatly affect the overall population.
Capture for the pet trade is a threat to many of the rarer or slower-to-breed parrots. Habitat loss or degradation, most often for agriculture, is a threat to many species. Parrots, being cavity nesters, are vulnerable to the loss of nesting sites and to competition with introduced species for those sites. The loss of old trees is a particular problem in some areas, particularly in Australia, where suitable nesting trees must be centuries old. Many parrots occur only on islands and are vulnerable to introduced species such as rats and cats, as they lack the appropriate antipredator behaviours needed to deal with mammalian predators. Controlling such predators can help in maintaining or increasing the numbers of endangered species. Insular species, such as the Puerto Rican amazon, which have small populations in restricted habitats, are also vulnerable to (unpredictable) natural events such as hurricanes.
Many active conservation groups have as their goal the conservation of wild parrot populations. One of the largest is the World Parrot Trust, an international organisation. The group gives assistance to worthwhile projects, as well as producing a magazine (PsittaScene) and raising funds through donations and memberships, often from pet parrot owners. They state they have helped conservation work in 22 countries. On a smaller scale, local parrot clubs raise money to donate to a conservation cause. Zoo and wildlife centres usually provide public education, to change habits that cause damage to wild populations. Recent conservation measures to conserve the habitats of some of the high-profile charismatic parrot species has also protected many of the less charismatic species living in the ecosystem. A popular attraction that many zoos employ is a feeding station for lories and lorikeets, where visitors feed small parrots with cups of liquid food. This is usually done in association with educational signs and lectures. Birdwatching-based ecotourism can be beneficial to economies.
Several projects aimed specifically at parrot conservation have met with success. Translocation of vulnerable kakapo, followed by intensive management and supplementary feeding, has increased the population from 50 individuals to 123. In New Caledonia, the Ouvea parakeet was threatened by trapping for the pet trade and loss of habitat. Community-based conservation, which eliminated the threat of poaching, has allowed the population to increase from around 600 birds in 1993 to over 2000 birds in 2009.
As of 2009, the IUCN recognises 19 species of parrot as extinct since 1600 (the date used to denote modern extinctions). This does not include species like the New Caledonian lorikeet, which has not been officially seen for 100 years, yet is still listed as critically endangered.
Trade, export, and import of all wild-caught parrots is regulated and only permitted under special licensed circumstances in countries party to the Convention on the International Trade in Endangered Species, that came into force in 1975 to regulate the international trade of all endangered wild-caught animal and plant species. In 1975, 24 parrot species were included on Appendix I of CITES, thus prohibiting commercial international trade in these birds. Since that initial listing, continuing threats from international trade led CITES to add an additional 32 parrot varieties to Appendix I. All the other parrot species are protected on Appendix II of CITES. In addition, individual countries may have laws to regulate trade in certain species; for example, the EU has banned parrot trade, whereas Mexico has a licensing system for capturing parrots.




List of parrots
Parrots of New Zealand






Cameron, Matt (2007). Cockatoos. Collingwood, VIC, Australia: CSIRO Publishing. ISBN 978-0-643-09232-7. 




Parrot videos on the Internet Bird CollectionThe domestic cat (Latin: Felis catus) is a small, typically furry, carnivorous mammal. They are often called house cats when kept as indoor pets or simply cats when there is no need to distinguish them from other felids and felines. Cats are often valued by humans for companionship and for their ability to hunt vermin. There are more than 70 cat breeds; different associations proclaim different numbers according to their standards.
Cats are similar in anatomy to the other felids, with a strong, flexible body, quick reflexes, sharp retractable claws, and teeth adapted to killing small prey. Cat senses fit a crepuscular and predatory ecological niche. Cats can hear sounds too faint or too high in frequency for human ears, such as those made by mice and other small animals. They can see in near darkness. Like most other mammals, cats have poorer color vision and a better sense of smell than humans. Cats, despite being solitary hunters, are a social species and cat communication includes the use of a variety of vocalizations (mewing, purring, trilling, hissing, growling, and grunting), as well as cat pheromones and types of cat-specific body language.
Cats have a high breeding rate. Under controlled breeding, they can be bred and shown as registered pedigree pets, a hobby known as cat fancy. Failure to control the breeding of pet cats by neutering and the abandonment of former household pets has resulted in large numbers of feral cats worldwide, requiring population control. In certain areas outside cats' native range, this has contributed, along with habitat destruction and other factors, to the extinction of many bird species. Cats have been known to extirpate a bird species within specific regions and may have contributed to the extinction of isolated island populations. Cats are thought to be primarily, though not solely, responsible for the extinction of 33 species of birds, and the presence of feral and free ranging cats makes some locations unsuitable for attempted species reintroduction in otherwise suitable locations.
Since cats were venerated in ancient Egypt, they were commonly believed to have been domesticated there, but there may have been instances of domestication as early as the Neolithic from around 9,500 years ago (7,500 BC). A genetic study in 2007 concluded that domestic cats are descended from Near Eastern wildcats, having diverged around 8,000 BC in West Asia. A 2016 study found that leopard cats were undergoing domestication independently in China around 5,500 BC, though this line of partially domesticated cats leaves no trace in the domesticated populations of today.
As of a 2007 study, cats are the second most popular pet in the US by number of pets owned, behind freshwater fish. In a 2010 study they were ranked the third most popular pet in the UK, after fish and dogs, with around 8 million being owned.




The felids are a rapidly evolving family of mammals that share a common ancestor only 10–15 million years ago and include lions, tigers, cougars and many others. Within this family, domestic cats (Felis catus) are part of the genus Felis, which is a group of small cats containing about seven species (depending upon classification scheme). Members of the genus are found worldwide and include the jungle cat (Felis chaus) of southeast Asia, European wildcat (F. silvestris silvestris), African wildcat (F. s. lybica), the Chinese mountain cat (F. bieti), and the Arabian sand cat (F. margarita), among others.
The domestic cat was first classified as Felis catus by Carl Linnaeus in the 10th edition of his Systema Naturae published in 1758. Because of modern phylogenetics, domestic cats are usually regarded as another subspecies of the wildcat, F. silvestris. This has resulted in mixed usage of the terms, as the domestic cat can be called by its subspecies name, Felis silvestris catus. Wildcats have also been referred to as various subspecies of F. catus, but in 2003, the International Commission on Zoological Nomenclature fixed the name for wildcats as F. silvestris. The most common name in use for the domestic cat remains F. catus, following a convention for domesticated animals of using the earliest (the senior) synonym proposed. Sometimes, the domestic cat has been called Felis domesticus or Felis domestica, as proposed by German naturalist J. C. P. Erxleben in 1777 but these are not valid taxonomic names and have been used only rarely in scientific literature, because Linnaeus's binomial takes precedence. A population of Transcaucasian black feral cats was once classified as Felis daemon (Satunin 1904) but now this population is considered to be a part of domestic cat.
All the cats in this genus share a common ancestor that probably lived around 6–7 million years ago in Asia. The exact relationships within the Felidae are close but still uncertain, e.g. the Chinese mountain cat is sometimes classified (under the name Felis silvestris bieti) as a subspecies of the wildcat, like the North African variety F. s. lybica.
In comparison to dogs, cats have not undergone major changes during the domestication process, as the form and behavior of the domestic cat is not radically different from those of wildcats and domestic cats are perfectly capable of surviving in the wild. Fully domesticated house cats often interbreed with feral F. catus populations, producing hybrids such as the Kellas cat. This limited evolution during domestication means that hybridisation can occur with many other felids, notably the Asian leopard cat. Several natural behaviors and characteristics of wildcats may have predisposed them for domestication as pets. These traits include their small size, social nature, obvious body language, love of play and relatively high intelligence. Several small felid species may have an inborn tendency towards tameness.
Cats have either a mutualistic or commensal relationship with humans. Two main theories are given about how cats were domesticated. In one, people deliberately tamed cats in a process of artificial selection as they were useful predators of vermin. This has been criticized as implausible, because the reward for such an effort may have been too little; cats generally do not carry out commands and although they do eat rodents, other species such as ferrets or terriers may be better at controlling these pests. The alternative idea is that cats were simply tolerated by people and gradually diverged from their wild relatives through natural selection, as they adapted to hunting the vermin found around humans in towns and villages.



The English word 'cat' (Old English catt) is in origin a loanword, introduced to many languages of Europe from Latin cattus and Byzantine Greek κάττα, including Portuguese and Spanish gato, French chat, German Katze, Lithuanian katė, and Old Church Slavonic kotka, among others. The ultimate source of the word is Afroasiatic, presumably from Late Egyptian čaute, the feminine of čaus "wildcat". An alternative word with cognates in many languages is English 'puss' ('pussycat'). Attested only from the 16th century, it may have been introduced from Dutch poes or from Low German puuskatte, related to Swedish kattepus, or Norwegian pus, pusekatt. Similar forms exist in Lithuanian puižė and Irish puiscín. The etymology of this word is unknown, but it may have simply arisen from a sound used to attract a cat.
A group of cats is referred to as a "clowder" or a "glaring", a male cat is called a "tom" or "tomcat" (or a "gib", if neutered), an unaltered female is called a "queen", and a juvenile cat is referred to as a "kitten". The male progenitor of a cat, especially a pedigreed cat, is its "sire", and its female progenitor is its "dam". In Early Modern English, the word 'kitten' was interchangeable with the now-obsolete word 'catling'.
A pedigreed cat is one whose ancestry is recorded by a cat fancier organization. A purebred cat is one whose ancestry contains only individuals of the same breed. Many pedigreed and especially purebred cats are exhibited as show cats. Cats of unrecorded, mixed ancestry are referred to as domestic short-haired or domestic long-haired cats, by coat type, or commonly as random-bred, moggies (chiefly British), or (using terms borrowed from dog breeding) mongrels or mutt-cats.
While the African wildcat is the ancestral subspecies from which domestic cats are descended, and wildcats and domestic cats can completely interbreed (Being subspecies of the same species), several intermediate stages occur between domestic pet and pedigree cats on one hand and those entirely wild animals on the other. The semiferal cat, a mostly outdoor cat, is not owned by any one individual, but is generally friendly to people and may be fed by several households. Feral cats are associated with human habitation areas and may be fed by people or forage for food, but are typically wary of human interaction.







Domestic cats are similar in size to the other members of the genus Felis, typically weighing between 4 and 5 kg (9 and 10 lb). Some breeds, however, such as the Maine Coon, can occasionally exceed 11 kg (24 lb). Conversely, very small cats, less than 2 kg (4 lb), have been reported. The world record for the largest cat is 21 kg (50 lb). The smallest adult cat ever officially recorded weighed around 1 kg (2 lb). Feral cats tend to be lighter as they have more limited access to food than house cats. In the Boston area, the average feral adult male will weigh 4 kg (9 lb) and average feral female 3 kg (7 lb). Cats average about 23–25 cm (9–10 in) in height and 46 cm (18 in) in head/body length (males being larger than females), with tails averaging 30 cm (12 in) in length.
Cats have seven cervical vertebrae, as do almost all mammals; 13 thoracic vertebrae (humans have 12); seven lumbar vertebrae (humans have five); three sacral vertebrae like most mammals (humans have five); and a variable number of caudal vertebrae in the tail (humans retain three to five caudal vertebrae, fused into an internal coccyx). The extra lumbar and thoracic vertebrae account for the cat's spinal mobility and flexibility. Attached to the spine are 13 ribs, the shoulder, and the pelvis.  Unlike human arms, cat forelimbs are attached to the shoulder by free-floating clavicle bones which allow them to pass their body through any space into which they can fit their head.

The cat skull is unusual among mammals in having very large eye sockets and a powerful and specialized jaw. Within the jaw, cats have teeth adapted for killing prey and tearing meat. When it overpowers its prey, a cat delivers a lethal neck bite with its two long canine teeth, inserting them between two of the prey's vertebrae and severing its spinal cord, causing irreversible paralysis and death. Compared to other felines, domestic cats have narrowly spaced canine teeth, which is an adaptation to their preferred prey of small rodents, which have small vertebrae. The premolar and first molar together compose the carnassial pair on each side of the mouth, which efficiently shears meat into small pieces, like a pair of scissors. These are vital in feeding, since cats' small molars cannot chew food effectively, and cats are largely incapable of mastication. Though cats tend to have better teeth than most humans, with decay generally less likely because of a thicker protective layer of enamel, a less damaging saliva, less retention of food particles between teeth, and a diet mostly devoid of sugar, they are nonetheless subject to occasional tooth loss and infection.
Cats, like dogs, are digitigrades. They walk directly on their toes, with the bones of their feet making up the lower part of the visible leg. Cats are capable of walking very precisely, because like all felines, they directly register; that is, they place each hind paw (almost) directly in the print of the corresponding fore paw, minimizing noise and visible tracks. This also provides sure footing for their hind paws when they navigate rough terrain. Unlike most mammals, when cats walk, they use a "pacing" gait; that is, they move the two legs on one side of the body before the legs on the other side. This trait is shared with camels and giraffes. As a walk speeds up into a trot, a cat's gait changes to be a "diagonal" gait, similar to that of most other mammals (and many other land animals, such as lizards): the diagonally opposite hind and fore legs move simultaneously.
Like almost all members of the Felidae, cats have protractable and retractable claws. In their normal, relaxed position, the claws are sheathed with the skin and fur around the paw's toe pads. This keeps the claws sharp by preventing wear from contact with the ground and allows the silent stalking of prey. The claws on the fore feet are typically sharper than those on the hind feet. Cats can voluntarily extend their claws on one or more paws. They may extend their claws in hunting or self-defense, climbing, kneading, or for extra traction on soft surfaces. Most cats have five claws on their front paws, and four on their rear paws. The fifth front claw (the dewclaw) is proximal to the other claws. More proximally is a protrusion which appears to be a sixth "finger". This special feature of the front paws, on the inside of the wrists, is the carpal pad, also found on the paws of big cats and dogs. It has no function in normal walking, but is thought to be an antiskidding device used while jumping. Some breeds of cats are prone to polydactyly (extra toes and claws). These are particularly common along the northeast coast of North America.



Cats are familiar and easily kept animals, and their physiology has been particularly well studied; it generally resembles those of other carnivorous mammals, but displays several unusual features probably attributable to cats' descent from desert-dwelling species. For instance, cats are able to tolerate quite high temperatures: Humans generally start to feel uncomfortable when their skin temperature passes about 38 °C (100 °F), but cats show no discomfort until their skin reaches around 52 °C (126 °F), and can tolerate temperatures of up to 56 °C (133 °F) if they have access to water.

Cats conserve heat by reducing the flow of blood to their skin and lose heat by evaporation through their mouths. Cats have minimal ability to sweat, with glands located primarily in their paw pads, and pant for heat relief only at very high temperatures (but may also pant when stressed). A cat's body temperature does not vary throughout the day; this is part of cats' general lack of circadian rhythms and may reflect their tendency to be active both during the day and at night. Cats' feces are comparatively dry and their urine is highly concentrated, both of which are adaptations to allow cats to retain as much water as possible. Their kidneys are so efficient, they can survive on a diet consisting only of meat, with no additional water, and can even rehydrate by drinking seawater. While domestic cats are able to swim, they are generally reluctant to enter water as it quickly leads to exhaustion.
Cats are obligate carnivores: their physiology has evolved to efficiently process meat, and they have difficulty digesting plant matter. In contrast to omnivores such as rats, which only require about 4% protein in their diet, about 20% of a cat's diet must be protein. Cats are unusually dependent on a constant supply of the amino acid arginine, and a diet lacking arginine causes marked weight loss and can be rapidly fatal. Another unusual feature is that the cat cannot produce taurine, with taurine deficiency causing macular degeneration, wherein the cat's retina slowly degenerates, causing irreversible blindness.
A cat's gastrointestinal tract is adapted to meat eating, being much shorter than that of omnivores and having low levels of several of the digestive enzymes needed to digest carbohydrates. These traits severely limit the cat's ability to digest and use plant-derived nutrients, as well as certain fatty acids. Despite the cat's meat-oriented physiology, several vegetarian or vegan cat foods have been marketed that are supplemented with chemically synthesized taurine and other nutrients, in attempts to produce a complete diet. However, some of these products still fail to provide all the nutrients cats require, and diets containing no animal products pose the risk of causing severe nutritional deficiencies. However, vets in the United States have expressed concern that many domestic cats are overfed.
Cats do eat grass occasionally. A proposed explanation is that cats use grass as a source of folic acid. Another proposed explanation is that it is used to supply dietary fiber.




Cats have excellent night vision and can see at only one-sixth the light level required for human vision. This is partly the result of cat eyes having a tapetum lucidum, which reflects any light that passes through the retina back into the eye, thereby increasing the eye's sensitivity to dim light. Another adaptation to dim light is the large pupils of cats' eyes. Unlike some big cats, such as tigers, domestic cats have slit pupils. These slit pupils can focus bright light without chromatic aberration, and are needed since the domestic cat's pupils are much larger, relative to their eyes, than the pupils of the big cats. At low light levels a cat's pupils will expand to cover most of the exposed surface of its eyes. However, domestic cats have rather poor color vision and (like most nonprimate mammals) have only two types of cones, optimized for sensitivity to blue and yellowish green; they have limited ability to distinguish between red and green. A 1993 paper reported a response to middle wavelengths from a system other than the rods which might be due to a third type of cone. However, this appears to be an adaptation to low light levels rather than representing true trichromatic vision.
Cats have excellent hearing and can detect an extremely broad range of frequencies. They can hear higher-pitched sounds than either dogs or humans, detecting frequencies from 55 Hz to 79,000 Hz, a range of 10.5 octaves, while humans and dogs both have ranges of about 9 octaves. Cats can hear ultrasound, which is important in hunting because many species of rodents make ultrasonic calls. However, they do not communicate using ultrasound like rodents do. Cats' hearing is also sensitive and among the best of any mammal, being most acute in the range of 500 Hz to 32 kHz. This sensitivity is further enhanced by the cat's large movable outer ears (their pinnae), which both amplify sounds and help detect the direction of a noise.
Cats have an acute sense of smell, due in part to their well-developed olfactory bulb and a large surface of olfactory mucosa, about 5.8 cm2 (0.90 in2) in area, which is about twice that of humans. Cats are sensitive to pheromones such as 3-mercapto-3-methylbutan-1-ol, which they use to communicate through urine spraying and marking with scent glands. Many cats also respond strongly to plants that contain nepetalactone, especially catnip, as they can detect that substance at less than one part per billion. About 70—80% of cats are affected by nepetalactone. This response is also produced by other plants, such as silver vine (Actinidia polygama) and the herb valerian; it may be caused by the smell of these plants mimicking a pheromone and stimulating cats' social or sexual behaviors.
Cats have relatively few taste buds compared to humans (470 or so versus more than 9,000 on the human tongue). Domestic and wild cats share a gene mutation that keeps their sweet taste buds from binding to sugary molecules, leaving them with no ability to taste sweetness. Their taste buds instead respond to amino acids, bitter tastes, and acids. Cats and many other animals have a Jacobson's organ located in their mouths that allows them to taste-smell certain aromas in a way which humans have no experience of. Cats also have a distinct temperature preference for their food, preferring food with a temperature around 100 °F (38 °C) which is similar to that of a fresh kill and routinely rejecting food presented cold or refrigerated (which would signal to the cat that the "prey" item is long dead and therefore possibly toxic or decomposing).

To aid with navigation and sensation, cats have dozens of movable whiskers (vibrissae) over their body, especially their faces. These provide information on the width of gaps and on the location of objects in the dark, both by touching objects directly and by sensing air currents; they also trigger protective blink reflexes to protect the eyes from damage.
Most breeds of cat have a noted fondness for settling in high places, or perching. In the wild, a higher place may serve as a concealed site from which to hunt; domestic cats may strike prey by pouncing from a perch such as a tree branch, as does a leopard. Another possible explanation is that height gives the cat a better observation point, allowing it to survey its territory. During a fall from a high place, a cat can reflexively twist its body and right itself using its acute sense of balance and flexibility. This is known as the cat righting reflex. An individual cat always rights itself in the same way, provided it has the time to do so, during a fall. The height required for this to occur is around 90 cm (3.0 ft). Cats without a tail (e.g. Manx cats) also have this ability, since a cat mostly moves its hind legs and relies on conservation of angular momentum to set up for landing, and the tail is little used for this feat.




The average lifespan of pet cats has risen in recent years. In the early 1980s it was about seven years, rising to 9.4 years in 1995 and 12–15 years in 2014. However, cats have been reported as surviving into their 30s, with the oldest known cat, Creme Puff, dying at a verified age of 38.
Spaying or neutering increases life expectancy: one study found neutered male cats live twice as long as intact males, while spayed female cats live 62% longer than intact females. Having a cat neutered confers health benefits, because castrated males cannot develop testicular cancer, spayed females cannot develop uterine or ovarian cancer, and both have a reduced risk of mammary cancer.
Despite widespread concern about the welfare of free-roaming cats, the lifespans of neutered feral cats in managed colonies compare favorably with those of pet cats. Neutered cats in managed colonies can also live long lives.



Cats can suffer from a wide range of health problems, including infectious diseases, parasites, injuries, and chronic disease. Vaccinations are available for many of these diseases, and domestic cats are regularly given treatments to eliminate parasites such as worms and fleas.



In addition to obvious dangers such as rodenticides, insecticides, and herbicides, cats may be poisoned by many chemicals usually considered safe by their human guardians, because their livers are less effective at some forms of detoxification than those of many other animals, including humans and dogs. Some of the most common causes of poisoning in cats are antifreeze and rodent baits. Cats may be particularly sensitive to environmental pollutants. When a cat has a sudden or prolonged serious illness without any obvious cause, it has possibly been exposed to a toxin.
Many human medicines should never be given to cats. For example, the painkiller paracetamol (or acetaminophen, sold as Tylenol and Panadol) is extremely toxic to cats: even very small doses need immediate treatment and can be fatal. Even aspirin, which is sometimes used to treat arthritis in cats, is much more toxic to them than to humans and must be administered cautiously. Similarly, application of minoxidil (Rogaine) to the skin of cats, either accidentally or by well-meaning guardians attempting to counter loss of fur, has sometimes been fatal. Essential oils can be toxic to cats and cases have been reported of serious illnesses caused by tea tree oil, including flea treatments and shampoos containing it.
Other common household substances that should be used with caution around cats include mothballs and other naphthalene products. Phenol-based products (e.g. Pine-Sol, Dettol/Lysol or hexachlorophene) are often used for cleaning and disinfecting near cats' feeding areas or litter boxes, but these can sometimes be fatal. Ethylene glycol, often used as an automotive antifreeze, is particularly appealing to cats, and as little as a teaspoonful can be fatal. Some human foods are toxic to cats; for example chocolate can cause theobromine poisoning, although (unlike dogs) few cats will eat chocolate. Large amounts of onions or garlic are also poisonous to cats. Many houseplants are also dangerous, such as Philodendron species and the leaves of the Easter lily (Lilium longiflorum), which can cause permanent and life-threatening kidney damage.




The domesticated cat and its closest wild ancestor are both diploid organisms that possess 38 chromosomes and roughly 20,000 genes. About 250 heritable genetic disorders have been identified in cats, many similar to human inborn errors. The high level of similarity among the metabolism of mammals allows many of these feline diseases to be diagnosed using genetic tests that were originally developed for use in humans, as well as the use of cats as animal models in the study of the human diseases.




Outdoor cats are active both day and night, although they tend to be slightly more active at night. The timing of cats' activity is quite flexible and varied, which means house cats may be more active in the morning and evening, as a response to greater human activity at these times. Although they spend the majority of their time in the vicinity of their home, housecats can range many hundreds of meters from this central point, and are known to establish territories that vary considerably in size, in one study ranging from 7 to 28 hectares (17–69 acres).
Cats conserve energy by sleeping more than most animals, especially as they grow older. The daily duration of sleep varies, usually between 12 and 16 hours, with 13 and 14 being the average. Some cats can sleep as much as 20 hours. The term "cat nap" for a short rest refers to the cat's tendency to fall asleep (lightly) for a brief period. While asleep, cats experience short periods of rapid eye movement sleep often accompanied by muscle twitches, which suggests they are dreaming.




Although wildcats are solitary, the social behavior of domestic cats is much more variable and ranges from widely dispersed individuals to feral cat colonies that form around a food source, based on groups of co-operating females. Within such groups, one cat is usually dominant over the others. Each cat in a colony holds a distinct territory, with sexually active males having the largest territories, which are about 10 times larger than those of female cats and may overlap with several females' territories. These territories are marked by urine spraying, by rubbing objects at head height with secretions from facial glands, and by defecation. Between these territories are neutral areas where cats watch and greet one another without territorial conflicts. Outside these neutral areas, territory holders usually chase away stranger cats, at first by staring, hissing, and growling, and if that does not work, by short but noisy and violent attacks. Despite some cats cohabiting in colonies, they do not have a social survival strategy, or a pack mentality, and always hunt alone.

However, some pet cats are poorly socialized. In particular, older cats may show aggressiveness towards newly arrived kittens, which may include biting and scratching; this type of behavior is known as feline asocial aggression.
Though cats and dogs are believed to be natural enemies, they can live together if correctly socialized.
Life in proximity to humans and other domestic animals has led to a symbiotic social adaptation in cats, and cats may express great affection toward humans or other animals. Ethologically, the human keeper of a cat may function as a sort of surrogate for the cat's mother, and adult housecats live their lives in a kind of extended kittenhood, a form of behavioral neoteny. The high-pitched sounds housecats make to solicit food may mimic the cries of a hungry human infant, making them particularly hard for humans to ignore.




Domestic cats use many vocalizations for communication, including purring, trilling, hissing, growling/snarling, grunting, and several different forms of meowing. (By contrast, feral cats are generally silent.) Their types of body language, including position of ears and tail, relaxation of the whole body, and kneading of the paws, are all indicators of mood. The tail and ears are particularly important social signal mechanisms in cats; for example, a raised tail acts as a friendly greeting, and flattened ears indicates hostility. Tail-raising also indicates the cat's position in the group's social hierarchy, with dominant individuals raising their tails less often than subordinate animals. Nose-to-nose touching is also a common greeting and may be followed by social grooming, which is solicited by one of the cats raising and tilting its head.
Purring may have developed as an evolutionary advantage as a signalling mechanism of reassurance between mother cats and nursing kittens. Post-nursing cats often purr as a sign of contentment: when being petted, becoming relaxed, or eating. The mechanism by which cats purr is elusive. The cat has no unique anatomical feature that is clearly responsible for the sound. It was, until recent times, believed that only the cats of the Felis genus could purr. However, felids of the Panthera genus (tiger, lion, jaguar, and leopard) also produce sounds similar to purring, but only when exhaling.




Cats are known for spending considerable amounts of time licking their coat to keep it clean. The cat's tongue has backwards-facing spines about 500 μm long, which are called papillae. These contain keratin which makes them rigid so the papillae act like a hairbrush. Some cats, particularly longhaired cats, occasionally regurgitate hairballs of fur that have collected in their stomachs from grooming. These clumps of fur are usually sausage-shaped and about 2–3 cm (0.8–1.2 in) long. Hairballs can be prevented with remedies that ease elimination of the hair through the gut, as well as regular grooming of the coat with a comb or stiff brush.



Among domestic cats, males are more likely to fight than females. Among feral cats, the most common reason for cat fighting is competition between two males to mate with a female. In such cases, most fights are won by the heavier male. Another common reason for fighting in domestic cats is the difficulty of establishing territories within a small home. Female cats also fight over territory or to defend their kittens. Neutering will decrease or eliminate this behavior in many cases, suggesting that the behavior is linked to sex hormones.

When cats become aggressive, they try to make themselves appear larger and more threatening by raising their fur, arching their backs, turning sideways and hissing or spitting. Often, the ears are pointed down and back to avoid damage to the inner ear and potentially listen for any changes behind them while focused forward. They may also vocalize loudly and bare their teeth in an effort to further intimidate their opponent. Fights usually consist of grappling and delivering powerful slaps to the face and body with the forepaws as well as bites. Cats also throw themselves to the ground in a defensive posture to rake their opponent's belly with their powerful hind legs.
Serious damage is rare, as the fights are usually short in duration, with the loser running away with little more than a few scratches to the face and ears. However, fights for mating rights are typically more severe and injuries may include deep puncture wounds and lacerations. Normally, serious injuries from fighting are limited to infections of scratches and bites, though these can occasionally kill cats if untreated. In addition, bites are probably the main route of transmission of feline immunodeficiency virus. Sexually active males are usually involved in many fights during their lives, and often have decidedly battered faces with obvious scars and cuts to their ears and nose.




Cats hunt small prey, primarily birds and rodents, and are often used as a form of pest control. Domestic cats are a major predator of wildlife in the United States, killing an estimated 1.4–3.7 billion birds and 6.9–20.7 billion mammals annually. The bulk of predation in the United States is done by 80 million feral and stray cats. Effective measures to reduce this population are elusive, meeting opposition from cat enthusiasts. In the case of free-ranging pets, equipping cats with bells and not letting them out at night will reduce wildlife predation.
Free-fed feral cats and house cats tend to consume many small meals in a single day, although the frequency and size of meals varies between individuals. Cats use two hunting strategies, either stalking prey actively, or waiting in ambush until an animal comes close enough to be captured. Although it is not certain, the strategy used may depend on the prey species in the area, with cats waiting in ambush outside burrows, but tending to actively stalk birds.
Perhaps the best known element of cats' hunting behavior, which is commonly misunderstood and often appalls cat owners because it looks like torture, is that cats often appear to "play" with prey by releasing it after capture. This behavior is due to an instinctive imperative to ensure that the prey is weak enough to be killed without endangering the cat. This behavior is referred to in the idiom "cat-and-mouse game" or simply "cat and mouse".
Another poorly understood element of cat hunting behavior is the presentation of prey to human guardians. Ethologist Paul Leyhausen proposed that cats adopt humans into their social group and share excess kill with others in the group according to the dominance hierarchy, in which humans are reacted to as if they are at, or near, the top. Anthropologist and zoologist Desmond Morris, in his 1986 book Catwatching, suggests, when cats bring home mice or birds, they are attempting to teach their human to hunt, or trying to help their human as if feeding "an elderly cat, or an inept kitten". Morris's hypothesis is inconsistent with the fact that male cats also bring home prey, despite males having no involvement with raising kittens.
Domestic cats select food based on its temperature, smell and texture; they dislike chilled foods and respond most strongly to moist foods rich in amino acids, which are similar to meat. Cats may reject novel flavors (a response termed neophobia) and learn quickly to avoid foods that have tasted unpleasant in the past. They may also avoid sugary foods and milk. Most adult cats are lactose intolerant; the sugars in milk are not easily digested and may cause soft stools or diarrhea. They can also develop odd eating habits. Some cats like to eat or chew on other things, most commonly wool, but also plastic, cables, paper, string, aluminum foil, or even coal. This condition, pica, can threaten their health, depending on the amount and toxicity of the items eaten.
Though cats usually prey on animals less than half their size, a feral cat in Australia has been photographed killing an adult pademelon of around the cat's weight at 4 kg (8.8 lb).
Since cats lack lips to create suction, they use a lapping method with the tongue to draw liquid upwards into their mouths. Lapping at a rate of four times a second, the cat touches the smooth tip of its tongue to the surface of the water, and quickly retracts it, drawing water upwards.




Domestic cats, especially young kittens, are known for their love of play. This behavior mimics hunting and is important in helping kittens learn to stalk, capture, and kill prey. Cats also engage in play fighting, with each other and with humans. This behavior may be a way for cats to practice the skills needed for real combat, and might also reduce any fear they associate with launching attacks on other animals.
Owing to the close similarity between play and hunting, cats prefer to play with objects that resemble prey, such as small furry toys that move rapidly, but rapidly lose interest (they become habituated) in a toy they have played with before. Cats also tend to play with toys more when they are hungry. String is often used as a toy, but if it is eaten, it can become caught at the base of the cat's tongue and then move into the intestines, a medical emergency which can cause serious illness, even death. Owing to the risks posed by cats eating string, it is sometimes replaced with a laser pointer's dot, which cats may chase. There are several important issues related to using a laser with a cat; first most, lasers can cause blindness in cats, even lasers which are sold as "eye safe" can actually be of much higher power of that proclaimed and can cause damages to the eyes. In addition, the cat thinks of the laser point as prey, but gets frustrated as he is unable to catch it.




Female cats are seasonally polyestrous, which means they may have many periods of heat over the course of a year, the season beginning in spring and ending in late autumn. Heat periods occur about every two weeks and last about 4 to 7 days. Multiple males will be attracted to a female in heat. The males will fight over her, and the victor wins the right to mate. At first, the female rejects the male, but eventually the female allows the male to mate. The female utters a loud yowl as the male pulls out of her because a male cat's penis has a band of about 120–150 backwards-pointing penile spines, which are about 1 mm long; upon withdrawal of the penis, the spines rake the walls of the female's vagina, which is a trigger for ovulation. This act also occurs to clear the vagina of other sperm in the context of a second (or more) mating, thus giving the later males a larger chance of conception.
After mating, the female washes her vulva thoroughly. If a male attempts to mate with her at this point, the female will attack him. After about 20 to 30 minutes, once the female is finished grooming, the cycle will repeat.
Because ovulation is not always triggered by a single mating, females may not be impregnated by the first male with which they mate. Furthermore, cats are superfecund; that is, a female may mate with more than one male when she is in heat, with the result that different kittens in a litter may have different fathers.

At 124 hours after conception, the morula forms. At 148 hours, early blastocysts form. At 10–12 days, implantation occurs.
The gestation period for cats is between 64 and 67 days, with an average of 66 days. The size of a litter usually is three to five kittens, with the first litter usually smaller than subsequent litters. Kittens are weaned between six and seven weeks old, and cats normally reach sexual maturity at 5–10 months (females) and to 5–7 months (males), although this can vary depending on breed. Females can have two to three litters per year, so may produce up to 150 kittens in their breeding span of around ten years.
Cats are ready to go to new homes at about 12 weeks of age, when they are ready to leave their mother. They can be surgically sterilized (spayed or castrated) as early as 7 weeks to limit unwanted reproduction. This surgery also prevents undesirable sex-related behavior, such as aggression, territory marking (spraying urine) in males and yowling (calling) in females. Traditionally, this surgery was performed at around six to nine months of age, but it is increasingly being performed prior to puberty, at about three to six months. In the US, about 80% of household cats are neutered.







Cats are a cosmopolitan species and are found across much of the world. Geneticist Stephen James O'Brien, of the National Cancer Institute in Frederick, Maryland, remarked on how successful cats have been in evolutionary terms: "Cats are one of evolution's most charismatic creatures. They can live on the highest mountains and in the hottest deserts." They are extremely adaptable and are now present on all continents except Antarctica, and on 118 of the 131 main groups of islands—even on isolated islands such as the Kerguelen Islands.
Feral cats can live in forests, grasslands, tundra, coastal areas, agricultural land, scrublands, urban areas, and wetlands. Their habitats even include small oceanic islands with no human inhabitants. Further, the close relatives of domestic cats, the African wildcat (Felis silvestris lybica) and the Arabian sand cat (Felis margarita) both inhabit desert environments, and domestic cats still show similar adaptations and behaviors. The cat's ability to thrive in almost any terrestrial habitat has led to its designation as one of the world's worst invasive species.
As domestic cats are little altered from wildcats, they can readily interbreed. This hybridization poses a danger to the genetic distinctiveness of some wildcat populations, particularly in Scotland and Hungary and possibly also the Iberian Peninsula.




Feral cats are domestic cats that were born in or have reverted to a wild state. They are unfamiliar with and wary of humans and roam freely in urban and rural areas. The numbers of feral cats is not known, but estimates of the US feral population range from 25 to 60 million. Feral cats may live alone, but most are found in large colonies, which occupy a specific territory and are usually associated with a source of food. Famous feral cat colonies are found in Rome around the Colosseum and Forum Romanum, with cats at some of these sites being fed and given medical attention by volunteers.
Public attitudes towards feral cats vary widely, ranging from seeing them as free-ranging pets, to regarding them as vermin. One common approach to reducing the feral cat population is termed 'trap-neuter-return', where the cats are trapped, neutered, immunized against rabies and the feline leukemia virus, and then released. Before releasing them back into their feral colonies, the attending veterinarian often nips the tip off one ear to mark it as neutered and inoculated, since these cats may be trapped again. Volunteers continue to feed and give care to these cats throughout their lives. Given this support, their lifespans are increased, and behavior and nuisance problems caused by competition for food are reduced.




To date, little scientific data is available to assess the impact of cat predation on prey populations. Even well-fed domestic cats may hunt and kill, mainly catching small mammals, but also birds, amphibians, reptiles, fish, and invertebrates. Hunting by domestic cats may be contributing to the decline in the numbers of birds in urban areas, although the importance of this effect remains controversial. In the wild, the introduction of feral cats during human settlement can threaten native species with extinction. In many cases, controlling or eliminating the populations of non-native cats can produce a rapid recovery in native animals. However, the ecological role of introduced cats can be more complicated. For example, cats can control the numbers of rats, which also prey on birds' eggs and young, so a cat population can protect an endangered bird species by suppressing mesopredators.
In isolated landmasses, such as Australasia, there are often no other native, medium-sized quadrupedal predators (including other feline species); this tends to exacerbate the impact of feral cats on small native animals. Native species such as the New Zealand kakapo and the Australian bettong, for example, tend to be more ecologically vulnerable and behaviorally "naive", when faced with predation by cats. Feral cats have had a major impact on these native species and have played a leading role in the endangerment and extinction of many animals.
Even in places with ancient and numerous cat populations, such as Western Europe, cats appear to be growing in number and independently of their environments' carrying capacity (such as the numbers of prey available). This may be explained, at least in part, by an abundance of food, from sources including feeding by pet owners and scavenging. For instance, research in Britain suggests that a high proportion of cats hunt only "recreationally". And in South Sweden, where research in 1982 found that the population density of cats was as high as 2,000 per square kilometre (5,200/sq mi).




The domestic cat is a significant predator of birds. UK assessments indicate they may be accountable for an estimated 64.8 million bird deaths each year. A 2012 study suggests feral cats may kill several billion birds each year in the United States. Certain species appear more susceptible than others; for example, 30% of house sparrow mortality is linked to the domestic cat. In the recovery of ringed robins (Erithacus rubecula) and dunnocks (Prunella modularis), 31% of deaths were a result of cat predation. In parts of North America, the presence of larger carnivores such as coyotes which prey on cats and other small predators reduces the effect of predation by cats and other small predators such as opossums and raccoons on bird numbers and variety. The proposal that cat populations will increase when the numbers of these top predators decline is called the mesopredator release hypothesis.
On islands, birds can contribute as much as 60% of a cat's diet. In nearly all cases, however, the cat cannot be identified as the sole cause for reducing the numbers of island birds, and in some instances, eradication of cats has caused a 'mesopredator release' effect; where the suppression of top carnivores creates an abundance of smaller predators that cause a severe decline in their shared prey. Domestic cats are, however, known to be a contributing factor to the decline of many species, a factor that has ultimately led, in some cases, to extinction. The South Island piopio, Chatham Islands rail, the Auckland Islands merganser, and the common diving petrel are a few from a long list, with the most extreme case being the flightless Stephens Island wren, which was driven to extinction only a few years after its discovery.
Some of the same factors that have promoted adaptive radiation of island avifauna over evolutionary time appear to promote vulnerability to non-native species in modern time. The susceptibility of many island birds is undoubtedly due to evolution in the absence of mainland predators, competitors, diseases, and parasites, in addition to lower reproductive rates and extended incubation periods. The loss of flight, or reduced flying ability is also characteristic of many island endemics. These biological aspects have increased vulnerability to extinction in the presence of introduced species, such as the domestic cat. Equally, behavioral traits exhibited by island species, such as "predatory naivety" and ground-nesting, have also contributed to their susceptibility.




Cats are common pets throughout the world, and their worldwide population exceeds 500 million. Although cat guardianship has commonly been associated with women, a 2007 Gallup poll reported that men and women in the United States of America were equally likely to own a cat.
As well as being kept as pets, cats are also used in the international fur and leather industries for making coats, hats, blankets and stuffed toys; and shoes, gloves and musical instruments respectively (about 24 cats are needed to make a cat fur coat). This use has now been outlawed in the United States, Australia, and the European Union. Cat pelts have been used for superstitious purposes as part of the practise of witchcraft, and are still made into blankets in Switzerland as folk remedies believed to help rheumatism. In the Western intellectual tradition, the idea of cats as everyday objects have served to illustrate problems of quantum mechanics in the Schrödinger's cat thought experiment.
A few attempts to build a cat census have been made over the years, both through associations or national and international organizations (such as the Canadian Federation of Humane Societies's one) and over the net, but such a task does not seem simple to achieve. General estimates for the global population of domestic cats range widely from anywhere between 200 million to 600 million.




Cats can be infected or infested with viruses, bacteria, fungus, protozoans, arthropods or worms that can transmit diseases to humans. In some cases, the cat exhibits no symptoms of the disease, however, the same disease can then become evident in a human. The likelihood that a person will become diseased depends on the age and immune status of the person. Humans who have cats living in their home or in close association are more likely to become infected, however, those who do not keep cats as pets might also acquire infections from cat feces and parasites exiting the cat's body. Some of the infections of most concern include salmonella, cat scratch disease and toxoplasmosis.




Traditionally, historians tended to think ancient Egypt was the site of cat domestication, owing to the clear depictions of house cats in Egyptian paintings about 3,600 years old. However, in 2004, a Neolithic grave excavated in Shillourokambos, Cyprus, contained the skeletons, laid close to one another, of both a human and a cat. The grave is estimated to be 9,500 years old, pushing back the earliest known feline–human association significantly. The cat specimen is large and closely resembles the African wildcat, rather than present-day domestic cats. This discovery, combined with genetic studies, suggests cats were probably domesticated in the Middle East, in the Fertile Crescent around the time of the development of agriculture, and then were brought to Cyprus and Egypt.
Direct evidence for the domestication of cats 5,300 years ago in Quanhucun, China has been published by archaeologists and paleontologists from the University of Washington and Chinese Academy of Sciences. The cats are believed to have been attracted to the village by rodents, which in turn were attracted by grain cultivated and stored by humans.
In ancient Egypt, cats were sacred animals, with the goddess Bastet often depicted in cat form, sometimes taking on the war-like aspect of a lioness. The Romans are often credited with introducing the domestic cat from Egypt to Europe; in Roman Aquitaine, a first- or second-century engraving of a young girl holding a cat is one of two earliest depictions of the Roman domesticated cat. However, cats possibly were already kept in Europe prior to the Roman Empire, as they may have been present in Britain in the late Iron Age. Domestic cats were spread throughout much of the rest of the world during the Age of Discovery, as they were carried on sailing ships to control shipboard rodents and as good-luck charms (see Ship's cat).
Several ancient religions believed cats are exalted souls, companions or guides for humans, that are all-knowing but mute so they cannot influence decisions made by humans. In Japan, the maneki neko cat is a symbol of good fortune.
Although no species are sacred in Islam, cats are revered by Muslims. Some Western writers have stated Muhammad had a favorite cat, Muezza. He is reported to have loved cats so much, "he would do without his cloak rather than disturb one that was sleeping on it". The story has no origin in early Muslim writers, and seems to confuse a story of a later Sufi saint, Ahmed ar-Rifa'i, centuries after Muhammad.
Freyja, the goddess of love, beauty, and fertility in Norse mythology, is depicted as riding a chariot drawn by cats.
Many cultures have negative superstitions about cats. An example would be the belief that a black cat "crossing one's path" leads to bad luck, or that cats are witches' familiars used to augment a witch's powers and skills. The killing of cats in Medieval Ypres, Belgium, is commemorated in the innocuous present-day Kattenstoet (cat parade).
According to a myth in many cultures, cats have multiple lives. In many countries, they are believed to have nine lives, but in Italy, Germany, Greece, Brazil and some Spanish-speaking regions, they are said to have seven lives, while in Turkish and Arabic traditions, the number of lives is six. The myth is attributed to the natural suppleness and swiftness cats exhibit to escape life-threatening situations. Also lending credence to this myth is the fact that falling cats often land on their feet, using an instinctive righting reflex to twist their bodies around. Nonetheless, cats can still be injured or killed by a high fall.




Cats by location
Cats in ancient Egypt
Cats in Australia
Cats in New Zealand
Cats in the United States







 The dictionary definition of kitty at Wiktionary
 Data related to Cat at Wikispecies
 Media related to Cat at Wikimedia Commons
 Animal Care at Wikibooks
 Quotations related to Cat at Wikiquote
 "Cat, Domestic, The". Encyclopedia Americana. 1920. 
High-Resolution Images of the Cat Brain
Biodiversity Heritage Library bibliography for Felis catus
Catpert. The Cat Expert – Cat articles
View the cat genome in EnsemblAnimals are multicellular, eukaryotic organisms of the kingdom Animalia (also called Metazoa). The animal kingdom emerged as a basal clade within Apoikozoa as a sister of the choanoflagellates. Sponges are the most basal clade of animals. Animals are motile, meaning they can move spontaneously and independently at some point in their lives. Their body plan eventually becomes fixed as they develop, although some undergo a process of metamorphosis later on in their lives. All animals are heterotrophs: they must ingest other organisms or their products for sustenance.
Most known animal phyla appeared in the fossil record as marine species during the Cambrian explosion, about 542 million years ago. Animals can be divided broadly into vertebrates and invertebrates. Vertebrates have a backbone or spine (vertebral column), and amount to less than five percent of all described animal species. They include fish, amphibians, reptiles, birds and mammals. The remaining animals are the invertebrates, which lack a backbone. These include molluscs (clams, oysters, octopuses, squid, snails); arthropods (millipedes, centipedes, insects, spiders, scorpions, crabs, lobsters, shrimp); annelids (earthworms, leeches), nematodes (filarial worms, hookworms), flatworms (tapeworms, liver flukes), cnidarians (jellyfish, sea anemones, corals), ctenophores (comb jellies), and sponges. The study of animals is called zoology.



The word "animal" comes from the Latin animalis, meaning having breath, having soul or living being. In everyday non-scientific usage the word excludes humans – that is, "animal" is often used to refer only to non-human members of the kingdom Animalia; often, only closer relatives of humans such as mammals and other vertebrates, are meant. The biological definition of the word refers to all members of the kingdom Animalia, encompassing creatures as diverse as sponges, jellyfish, insects, and humans.




Aristotle divided the living world between animals and plants, and this was followed by Carl Linnaeus, in the first hierarchical classification. In Linnaeus's original scheme, the animals were one of three kingdoms, divided into the classes of Vermes, Insecta, Pisces, Amphibia, Aves, and Mammalia. Since then the last four have all been subsumed into a single phylum, the Chordata, whereas the various other forms have been separated out.
In 1874, Ernst Haeckel divided the animal kingdom into two subkingdoms: Metazoa (multicellular animals) and Protozoa (single-celled animals). The protozoa were later moved to the kingdom Protista, leaving only the metazoa. Thus Metazoa is now considered a synonym of Animalia.



Animals have several characteristics that set them apart from other living things. Animals are eukaryotic and multicellular, which separates them from bacteria and most protists. They are heterotrophic, generally digesting food in an internal chamber, which separates them from plants and algae. They are also distinguished from plants, algae, and fungi by lacking rigid cell walls. All animals are motile, if only at certain life stages. In most animals, embryos pass through a blastula stage, which is a characteristic exclusive to animals.



With a few exceptions, most notably the sponges (Phylum Porifera) and Placozoa, animals have bodies differentiated into separate tissues. These include muscles, which are able to contract and control locomotion, and nerve tissues, which send and process signals. Typically, there is also an internal digestive chamber, with one or two openings. Animals with this sort of organization are called metazoans, or eumetazoans when the former is used for animals in general.
All animals have eukaryotic cells, surrounded by a characteristic extracellular matrix composed of collagen and elastic glycoproteins. This may be calcified to form structures like shells, bones, and spicules. During development, it forms a relatively flexible framework upon which cells can move about and be reorganized, making complex structures possible. In contrast, other multicellular organisms, like plants and fungi, have cells held in place by cell walls, and so develop by progressive growth. Also, unique to animal cells are the following intercellular junctions: tight junctions, gap junctions, and desmosomes.




Nearly all animals undergo some form of sexual reproduction. They produce haploid gametes by meiosis (see Origin and function of meiosis). The smaller, motile gametes are spermatozoa and the larger, non-motile gametes are ova. These fuse to form zygotes, which develop into new individuals (see Allogamy).
Many animals are also capable of asexual reproduction. This may take place through parthenogenesis, where fertile eggs are produced without mating, budding, or fragmentation.
A zygote initially develops into a hollow sphere, called a blastula, which undergoes rearrangement and differentiation. In sponges, blastula larvae swim to a new location and develop into a new sponge. In most other groups, the blastula undergoes more complicated rearrangement. It first invaginates to form a gastrula with a digestive chamber, and two separate germ layers—an external ectoderm and an internal endoderm. In most cases, a mesoderm also develops between them. These germ layers then differentiate to form tissues and organs.




During sexual reproduction, mating with a close relative (inbreeding) generally leads to inbreeding depression. For instance, inbreeding was found to increase juvenile mortality in 11 small animal species. Inbreeding depression is considered to be largely due to expression of deleterious recessive mutations. Mating with unrelated or distantly related members of the same species is generally thought to provide the advantage of masking deleterious recessive mutations in progeny. (see Heterosis). Animals have evolved numerous diverse mechanisms for avoiding close inbreeding and promoting outcrossing (see Inbreeding avoidance).
As indicated in the image of chimpanzees, they have adopted dispersal as a way to separate close relatives and prevent inbreeding. Their dispersal route is known as natal dispersal, whereby individuals move away from the area of birth.

In various species, such as the splendid fairywren, females benefit by mating with multiple males, thus producing more offspring of higher genetic quality. Females that are pair bonded to a male of poor genetic quality, as is the case in inbreeding, are more likely to engage in extra-pair copulations in order to improve their reproductive success and the survivability of their offspring.




All animals are heterotrophs, meaning that they feed directly or indirectly on other living things. They are often further subdivided into groups such as carnivores, herbivores, omnivores, and parasites.
Predation is a biological interaction where a predator (a heterotroph that is hunting) feeds on its prey (the organism that is attacked). Predators may or may not kill their prey prior to feeding on them, but the act of predation almost always results in the death of the prey. The other main category of consumption is detritivory, the consumption of dead organic matter. It can at times be difficult to separate the two feeding behaviours, for example, where parasitic species prey on a host organism and then lay their eggs on it for their offspring to feed on its decaying corpse. Selective pressures imposed on one another has led to an evolutionary arms race between prey and predator, resulting in various antipredator adaptations.
Most animals indirectly use the energy of sunlight by eating plants or plant-eating animals. Most plants use light to convert inorganic molecules in their environment into carbohydrates, fats, proteins and other biomolecules, characteristically containing reduced carbon in the form of carbon-hydrogen bonds. Starting with carbon dioxide (CO2) and water (H2O), photosynthesis converts the energy of sunlight into chemical energy in the form of simple sugars (e.g., glucose), with the release of molecular oxygen. These sugars are then used as the building blocks for plant growth, including the production of other biomolecules. When an animal eats plants (or eats other animals which have eaten plants), the reduced carbon compounds in the food become a source of energy and building materials for the animal. They are either used directly to help the animal grow, or broken down, releasing stored solar energy, and giving the animal the energy required for motion.
Animals living close to hydrothermal vents and cold seeps on the ocean floor are not dependent on the energy of sunlight. Instead chemosynthetic archaea and bacteria form the base of the food chain.




Animals are generally considered to have emerged within flagellated eukaryota. Their closest known living relatives are the choanoflagellates, collared flagellates that have a morphology similar to the choanocytes of certain sponges. Molecular studies place animals in a supergroup called the opisthokonts, which also include the choanoflagellates, fungi and a few small parasitic protists. The name comes from the posterior location of the flagellum in motile cells, such as most animal spermatozoa, whereas other eukaryotes tend to have anterior flagella.
The first fossils that might represent animals appear in the Trezona Formation at Trezona Bore, West Central Flinders, South Australia. These fossils are interpreted as being early sponges. They were found in 665-million-year-old rock.
The next oldest possible animal fossils are found towards the end of the Precambrian, around 610 million years ago, and are known as the Ediacaran or Vendian biota. These are difficult to relate to later fossils, however. Some may represent precursors of modern phyla, but they may be separate groups, and it is possible they are not really animals at all.
Aside from them, most known animal phyla make a more or less simultaneous appearance during the Cambrian period, about 542 million years ago. It is still disputed whether this event, called the Cambrian explosion, is due to a rapid divergence between different groups or due to a change in conditions that made fossilization possible.
Some palaeontologists suggest that animals appeared much earlier than the Cambrian explosion, possibly as early as 1 billion years ago. Trace fossils such as tracks and burrows found in the Tonian period indicate the presence of triploblastic worms, like metazoans, roughly as large (about 5 mm wide) and complex as earthworms. During the beginning of the Tonian period around 1 billion years ago, there was a decrease in Stromatolite diversity, which may indicate the appearance of grazing animals, since stromatolite diversity increased when grazing animals became extinct at the End Permian and End Ordovician extinction events, and decreased shortly after the grazer populations recovered. However the discovery that tracks very similar to these early trace fossils are produced today by the giant single-celled protist Gromia sphaerica casts doubt on their interpretation as evidence of early animal evolution.




Traditional morphological and modern molecular phylogenetic analysis have both recognized a major evolutionary transition from "non-bilaterian" animals, which are those lacking a bilaterally symmetric body plan (Porifera, Ctenophora, Cnidaria and Placozoa), to "bilaterian" animals (Bilateria) whose body plans display bilateral symmetry. The latter are further classified based on a major division between Deuterostomes and Protostomes. The relationships among non-bilaterian animals are disputed, but all bilaterian animals are thought to form a monophyletic group. Current understanding of the relationships among the major groups of animals is summarized by the following cladogram:



Several animal phyla are recognized for their lack of bilateral symmetry, and are thought to have diverged from other animals early in evolution. Among these, the sponges (Porifera) were long thought to have diverged first, representing the oldest animal phylum. They lack the complex organization found in most other phyla. Their cells are differentiated, but in most cases not organized into distinct tissues. Sponges typically feed by drawing in water through pores. However, a series of phylogenomic studies from 2008-2015 have found support for Ctenophora, or comb jellies, as the basal lineage of animals. This result has been controversial, since it would imply that sponges may not be so primitive, but may instead be secondarily simplified. Other researchers have argued that the placement of Ctenophora as the earliest-diverging animal phylum is a statistical anomaly caused by the high rate of evolution in ctenophore genomes.
Among the other phyla, the Ctenophora and the Cnidaria, which includes sea anemones, corals, and jellyfish, are radially symmetric and have digestive chambers with a single opening, which serves as both the mouth and the anus. Both have distinct tissues, but they are not organized into organs. There are only two main germ layers, the ectoderm and endoderm, with only scattered cells between them. As such, these animals are sometimes called diploblastic. The tiny placozoans are similar, but they do not have a permanent digestive chamber.
The Myxozoa, microscopic parasites that were originally considered Protozoa, are now believed to have evolved within Cnidaria.



The remaining animals form a monophyletic group called the Bilateria. For the most part, they are bilaterally symmetric, and often have a specialized head with feeding and sensory organs. The body is triploblastic, i.e. all three germ layers are well-developed, and tissues form distinct organs. The digestive chamber has two openings, a mouth and an anus, and there is also an internal body cavity called a coelom or pseudocoelom. There are exceptions to each of these characteristics, however—for instance adult echinoderms are radially symmetric, and certain parasitic worms have extremely simplified body structures.
Genetic studies have considerably changed our understanding of the relationships within the Bilateria. Most appear to belong to two major lineages: the deuterostomes and the protostomes, the latter of which includes the Ecdysozoa, and Lophotrochozoa. In addition, there are a few small groups of bilaterians with relatively similar structure whose relationships with other animals are not well-established. These include the Acoelomorpha, Rhombozoa, and Orthonectida.




Deuterostomes differ from protostomes in several ways. Animals from both groups possess a complete digestive tract. However, in protostomes, the first opening of the gut to appear in embryological development (the archenteron) develops into the mouth, with the anus forming secondarily. In deuterostomes the anus forms first, with the mouth developing secondarily. In most protostomes, cells simply fill in the interior of the gastrula to form the mesoderm, called schizocoelous development, but in deuterostomes, it forms through invagination of the endoderm, called enterocoelic pouching. Deuterostome embryos undergo radial cleavage during cell division, while protostomes undergo spiral cleavage.
All this suggests the deuterostomes and protostomes are separate, monophyletic lineages. The main phyla of deuterostomes are the Echinodermata and Chordata. The former are radially symmetric and exclusively marine, such as starfish, sea urchins, and sea cucumbers. The latter are dominated by the vertebrates, animals with backbones. These include fish, amphibians, reptiles, birds, and mammals.
In addition to these, the deuterostomes also include the Hemichordata, or acorn worms, which are thought to be closely related to Echinodermata forming a group known as Ambulacraria. Although they are not especially prominent today, the important fossil graptolites may belong to this group.




The Ecdysozoa are protostomes, named after the common trait of growth by moulting or ecdysis. The largest animal phylum belongs here, the Arthropoda, including insects, spiders, crabs, and their kin. All these organisms have a body divided into repeating segments, typically with paired appendages. Two smaller phyla, the Onychophora and Tardigrada, are close relatives of the arthropods and share these traits. The ecdysozoans also include the Nematoda or roundworms, perhaps the second largest animal phylum. Roundworms are typically microscopic, and occur in nearly every environment where there is water. A number are important parasites. Smaller phyla related to them are the Nematomorpha or horsehair worms, and the Kinorhyncha, Priapulida, and Loricifera. These groups have a reduced coelom, called a pseudocoelom.



The Lophotrochozoa, evolved within Protostomia, include two of the most successful animal phyla, the Mollusca and Annelida. The former, which is the second-largest animal phylum by number of described species, includes animals such as snails, clams, and squids, and the latter comprises the segmented worms, such as earthworms and leeches. These two groups have long been considered close relatives because of the common presence of trochophore larvae, but the annelids were considered closer to the arthropods because they are both segmented. Now, this is generally considered convergent evolution, owing to many morphological and genetic differences between the two phyla. The Lophotrochozoa also include the Nemertea or ribbon worms, the Sipuncula, and several phyla that have a ring of ciliated tentacles around the mouth, called a lophophore. These were traditionally grouped together as the lophophorates. but it now appears that the lophophorate group may be paraphyletic, with some closer to the nemerteans and some to the molluscs and annelids. They include the Brachiopoda or lamp shells, which are prominent in the fossil record, the Entoprocta, the Phoronida, and possibly the Bryozoa or moss animals.
The Platyzoa include the phylum Platyhelminthes, the flatworms. These were originally considered some of the most primitive Bilateria, but it now appears they developed from more complex ancestors. A number of parasites are included in this group, such as the flukes and tapeworms. Flatworms are acoelomates, lacking a body cavity, as are their closest relatives, the microscopic Gastrotricha. The other platyzoan phyla are mostly microscopic and pseudocoelomate. The most prominent are the Rotifera or rotifers, which are common in aqueous environments. They also include the Acanthocephala or spiny-headed worms, the Gnathostomulida, Micrognathozoa, and possibly the Cycliophora. These groups share the presence of complex jaws, from which they are called the Gnathifera.
The Chaetognatha or arrow worms have been traditionally classified as deuterostomes, though recent molecular studies have identified this group as a basal protostome lineage.



Animals can be divided into two broad groups: vertebrates (animals with a backbone) and invertebrates (animals without a backbone). Half of all described vertebrate species are fishes and three-quarters of all described invertebrate species are insects. The following table lists the number of described extant species for each major animal subgroup as estimated for the IUCN Red List of Threatened Species, 2014.3.

Over 95% of the described animal species in the world are invertebrates.




Because of the great diversity found in animals, it is more economical for scientists to study a small number of chosen species so that connections can be drawn from their work and conclusions extrapolated about how animals function in general. Because they are easy to keep and breed, the fruit fly Drosophila melanogaster and the nematode Caenorhabditis elegans have long been the most intensively studied metazoan model organisms, and were among the first life-forms to be genetically sequenced. This was facilitated by the severely reduced state of their genomes, but as many genes, introns, and linkages lost, these ecdysozoans can teach us little about the origins of animals in general. The extent of this type of evolution within the superphylum will be revealed by the crustacean, annelid, and molluscan genome projects currently in progress. Analysis of the starlet sea anemone genome has emphasized the importance of sponges, placozoans, and choanoflagellates, also being sequenced, in explaining the arrival of 1500 ancestral genes unique to the Eumetazoa.
An analysis of the homoscleromorph sponge Oscarella carmela also suggests that the last common ancestor of sponges and the eumetazoan animals was more complex than previously assumed.
Other model organisms belonging to the animal kingdom include the house mouse (Mus musculus) and zebrafish (Danio rerio).



Animal attacks
Animal coloration
Biological classification
Ethology
Fauna
List of animal names
Lists of animals
Lists of organisms by population










 Data related to Animalia at Wikispecies
Animal at the Encyclopedia of Life 
Tree of Life Project
Animal Diversity Web – University of Michigan's database of animals, showing taxonomic classification, images, and other information.
ARKive – multimedia database of worldwide endangered/protected species and common species of UK.
The Animal Kingdom
Getting a Leg Up on Land Scientific American Magazine (December 2005 Issue) – About the evolution of four-limbed animals from fish.Traffic on roads may consist of pedestrians, ridden or herded animals, vehicles, streetcars, buses and other conveyances, either singly or together, while using the public way for purposes of travel. Traffic laws are the laws which govern traffic and regulate vehicles, while rules of the road are both the laws and the informal rules that may have developed over time to facilitate the orderly and timely flow of traffic.
Organized traffic generally has well-established priorities, lanes, right-of-way, and traffic control at intersections.
Traffic is formally organized in many jurisdictions, with marked lanes, junctions, intersections, interchanges, traffic signals, or signs. Traffic is often classified by type: heavy motor vehicle (e.g., car, truck), other vehicle (e.g., moped, bicycle), and pedestrian. Different classes may share speed limits and easement, or may be segregated. Some jurisdictions may have very detailed and complex rules of the road while others rely more on drivers' common sense and willingness to cooperate.
Organization typically produces a better combination of travel safety and efficiency. Events which disrupt the flow and may cause traffic to degenerate into a disorganized mess include road construction, collisions, and debris in the roadway. On particularly busy freeways, a minor disruption may persist in a phenomenon known as traffic waves. A complete breakdown of organization may result in traffic congestion and gridlock. Simulations of organized traffic frequently involve queuing theory, stochastic processes and equations of mathematical physics applied to traffic flow.



The word traffic originally meant "trade" (as it still does) and comes from the Old Italian verb trafficare and noun traffico. The origin of the Italian words is unclear. Suggestions include Catalan trafegar "decant", an assumed Vulgar Latin verb transfricare 'rub across', an assumed Vulgar Latin combination of trans- and facere 'make or do', Arabic tafriq 'distribution', and Arabic taraffaqa, which can mean 'seek profit'.




Rules of the road and driving etiquette are the general practices and procedures that road users are required to follow. These rules usually apply to all road users, though they are of special importance to motorists and cyclists. These rules govern interactions between vehicles and with pedestrians. The basic traffic rules are defined by an international treaty under the authority of the United Nations, the 1968 Vienna Convention on Road Traffic. Not all countries are signatory to the convention and, even among signatories, local variations in practice may be found. There are also unwritten local rules of the road, which are generally understood by local drivers.
As a general rule, drivers are expected to avoid a collision with another vehicle and pedestrians, regardless of whether or not the applicable rules of the road allow them to be where they happen to be.
In addition to the rules applicable by default, traffic signs and traffic lights must be obeyed, and instructions may be given by a police officer, either routinely (on a busy crossing instead of traffic lights) or as road traffic control around a construction zone, accident, or other road disruption.
These rules should be distinguished from the mechanical procedures required to operate one's vehicle. See driving.




Traffic going in opposite directions should be separated in such a way that they do not block each other's way. The most basic rule is whether to use the left or right side of the road.




In many countries, the rules of the road are codified, setting out the legal requirements and punishments for breaking them.
In the United Kingdom, the rules are set out in the Highway Code, which includes obligations but also advice on how to drive sensibly and safely.
In the United States, traffic laws are regulated by the states and municipalities through their respective traffic codes. Most of these are based at least in part on the Uniform Vehicle Code, but there are variations from state to state. In states such as Florida, traffic law and criminal law are separate, therefore, unless someone flees a scene of an accident, commits vehicular homicide or manslaughter, they are only guilty of a minor traffic offense. However, states such as South Carolina have completely criminalized their traffic law, so, for example, one is guilty of a misdemeanor simply for travelling 5 miles over the speed limit.







Vehicles often come into conflict with other vehicles and pedestrians because their intended courses of travel intersect, and thus interfere with each other's routes. The general principle that establishes who has the right to go first is called "right of way", or "priority". It establishes who has the right to use the conflicting part of the road and who has to wait until the other does so.
Signs, signals, markings and other features are often used to make priority explicit. Some signs, such as the stop sign, are nearly universal. When there are no signs or markings, different rules are observed depending on the location. These default priority rules differ between countries, and may even vary within countries. Trends toward uniformity are exemplified at an international level by the Vienna Convention on Road Signs and Signals, which prescribes standardized traffic control devices (signs, signals, and markings) for establishing the right of way where necessary.
Crosswalks (or pedestrian crossings) are common in populated areas, and may indicate that pedestrians have priority over vehicular traffic. In most modern cities, the traffic signal is used to establish the right of way on the busy roads. Its primary purpose is to give each road a duration of time in which its traffic may use the intersection in an organized way. The intervals of time assigned for each road may be adjusted to take into account factors such as difference in volume of traffic, the needs of pedestrians, or other traffic signals. Pedestrian crossings may be located near other traffic control devices; if they are not also regulated in some way, vehicles must give priority to them when in use. Traffic on a public road usually has priority over other traffic such as traffic emerging from private access; rail crossings and drawbridges are typical exceptions.



Uncontrolled traffic comes in the absence of lane markings and traffic control signals. On roads without marked lanes, drivers tend to keep to the appropriate side if the road is wide enough. Drivers frequently overtake others. Obstructions are common.
Intersections have no signals or signage, and a particular road at a busy intersection may be dominant – that is, its traffic flows – until a break in traffic, at which time the dominance shifts to the other road where vehicles are queued. At the intersection of two perpendicular roads, a traffic jam may result if four vehicles face each other side-on.



Drivers will often want to cease to travel a straight line and turn onto another road or onto private property. The vehicle's directional signals (commonly known as "blinkers" or "indicators") are often used as a way to announce one's intention to turn, thus alerting other drivers. The actual usage of directional signals varies greatly amongst countries, although its purpose should be the same in all countries: to indicate a driver's intention to depart from the current (and natural) flow of traffic well before the departure is executed (typically 3 seconds as a guideline).
This will usually mean that turning traffic will have to stop in order to wait for a breach to turn, and this might cause inconvenience for drivers that follow them but do not want to turn. This is why dedicated lanes and protected traffic signals for turning are sometimes provided. On busier intersections where a protected lane would be ineffective or cannot be built, turning may be entirely prohibited, and drivers will be required to "drive around the block" in order to accomplish the turn. Many cities employ this tactic quite often; in San Francisco, due to its common practice, making three right turns is known colloquially as a "San Francisco left turn". Likewise, as many intersections in Taipei City are too busy to allow direct left turns, signs often direct drivers to drive around the block to turn.
Turning rules are by no means universal. For example, in New Zealand (a drive-on-the-left country) between 1977 and 2012, left turning traffic had to give way to opposing right-turning traffic wishing to take the same road (unless there were multiple lanes, but then one must take care in case a vehicle jumped lanes). New Zealand abolished this particular rule on 25 March 2012, except at roundabouts or when denoted by a Give Way or Stop sign. Although the rule caused initial driver confusion, and many intersections required or still require modification, the change is predicted to eventually prevent one death and 13 serious injuries annually.
On roads with multiple lanes, turning traffic is generally expected to move to the lane closest to the direction they wish to turn. For example, traffic intending to turn right will usually move to the rightmost lane before the intersection. Likewise, left-turning traffic will move to the leftmost lane. Exceptions to this rule may exist where for example the traffic authority decides that the two rightmost lanes will be for turning right, in which case drivers may take whichever of them to turn. In certain parts of the world traffic will adapt to informal patterns that rise naturally rather than by force of authority; for example, it is common for drivers to observe (and trust) the turn signals used by other drivers in order to make turns from other lanes. For example, if several vehicles on the right lane are all turning right, a vehicle may come from the next-to-right lane and turn right as well, doing so in parallel with the other right-turning vehicles.




In most of Continental Europe, the default rule is to give priority to the right, but this may be overridden by signs or road markings, and does not apply at T-shaped junctions in some of these countries, such as France. There, priority was initially given according to the social rank of each traveler, but early in the life of the automobile this rule was deemed impractical and replaced with the priorité à droite (priority to the right) rule, which still applies. At a traffic circle where priorité à droite is not overridden, traffic on what would otherwise be a roundabout gives way to traffic entering the circle. Most French roundabouts now have give-way signs for traffic entering the circle, but there remain some notable exceptions that operate on the old rule, such as the Place de l'Étoile around the Arc de Triomphe. Priority to the right where used in continental Europe may be overridden by an ascending hierarchy of markings, signs, signals, and authorized persons.
In the United Kingdom, priority is generally indicated by signs or markings, so that almost all junctions between public roads (except those governed by traffic signals) have a concept of a major road and minor road. The default give-way-to-the-right rule used in Continental Europe causes problems for many British and Irish drivers who are accustomed to having right of way by default unless otherwise indicated. A very small proportion of low-traffic junctions are unmarked - typically on housing estates or in rural areas. Here the rule is to "proceed with great care" i.e. slow the vehicle and check for traffic on the intersecting road.
Other countries use various methods similar to the above examples to establish the right of way at intersections. For example, in most of the United States, the default priority is to yield to traffic from the right, but this is usually overridden by traffic control devices or other rules, like the boulevard rule. This rule holds that traffic entering a major road from a smaller road or alley must yield to the traffic of the busier road, but signs are often still posted. The boulevard rule can be compared with the above concept of a major and minor road, or the priority roads that may be found in countries that are parties to the Vienna Convention on Road Signs and Signals.
Perpendicular intersections Also known as a "four-way" intersection, this intersection is the most common configuration for roads that cross each other, and the most basic type.
If traffic signals do not control a four-way intersection, signs or other features are typically used to control movements and make clear priorities. The most common arrangement is to indicate that one road has priority over the other, but there are complex cases where all traffic approaching an intersection must yield and may be required to stop.
In the United States, South Africa, and Canada, there are four-way intersections with a stop sign at every entrance, called four-way stops. A failed signal or a flashing red light is equivalent to a four-way stop, or an all-way stop. Special rules for four-way stops may include:
In the countries that use four-way stops, pedestrians always have priority at crosswalks – even at unmarked ones, which exist as the logical continuations of the sidewalks at every intersection with approximately right angles – unless signed or painted otherwise.
Whichever vehicle first stops at the stop line – or before the crosswalk, if there is no stop line – has priority.
If two vehicles stop at the same time, priority is given to the vehicle on the right.
If three vehicles stop at the same time, priority is given to the two vehicles going in opposite directions, if possible.
If four vehicles stop, drivers usually use gestures and other communication to establish right-of-way.
In Europe and other places, there are similar intersections. These may be marked by special signs (according to the Vienna Convention on Road Signs and Signals), a danger sign with a black X representing a crossroads. This sign informs drivers that the intersection is uncontrolled and that default rules apply. In Europe and in many areas of North America the default rules that apply at uncontrolled four-way intersections are almost identical:
Rules for pedestrians differ by country, in the United States and Canada pedestrians generally have priority at such an intersection.
All vehicles must give priority to any traffic approaching from their right,
Then, if the vehicle is turning right or continuing on the same road it may proceed.
Vehicles turning left must also give priority to traffic approaching from the opposite direction, unless that traffic is also turning left.
If the intersection is congested, vehicles must alternate directions and/or circulate priority to the right one vehicle at a time.




Pedestrians must often cross from one side of a road to the other, and in doing so may come into the way of vehicles traveling on the road. In many places pedestrians are entirely left to look after themselves, that is, they must observe the road and cross when they can see that no traffic will threaten them. Busier cities usually provide pedestrian crossings, which are strips of the road where pedestrians are expected to cross.
The actual appearance of pedestrian crossings varies greatly, but the two most common appearances are: (1) a series of parallel white stripes or (2) two long horizontal white lines. The former is usually preferred, as it stands out more conspicuously against the dark pavement.
Some pedestrian crossings also accompany a traffic signal which will make vehicles stop at regular intervals so the pedestrians can cross. Some countries have "intelligent" pedestrian signals, where the pedestrian must push a button in order to assert his intention to cross. The traffic signal will use that information to schedule itself, that is, when no pedestrians are present the signal will never pointlessly cause vehicle traffic to stop. In some countries, approaching traffic is monitored by radar or by electromagnetic sensors buried in the road surface, and the pedestrian crossing lights are set to red if a speed infringement is detected. This has the effect of enforcing the local speed limit without the necessity of issuing speeding citations, etc. See Speed Limits below.
Pedestrian crossings without traffic signals are also common. In this case, the traffic laws usually states that the pedestrian has the right of way when crossing, and that vehicles must stop when a pedestrian uses the crossing. Countries and driving cultures vary greatly as to the extent to which this is respected. In the state of Nevada the car has the right of way when the crosswalk signal specifically forbids pedestrian crossing.
Some jurisdictions forbid crossing or using the road anywhere other than at crossings, termed jaywalking. In other areas, pedestrians may have the right to cross where they choose, and have right of way over vehicular traffic while crossing.
In most areas, an intersection is considered to have a crosswalk, even if not painted, as long as the roads meet at approximate right angles. Examples of locations where this rule is not in effect are the United Kingdom and Croatia.
Pedestrian crossings may also be located away from intersections.




A level crossing is an at-grade intersection of a railway by a road. Because of safety issues, they are often equipped with closable gates, crossing bells and warning signs.




The higher the speed of a vehicle, the more difficult collision avoidance becomes and the greater the damage if a collision does occur. Therefore, many countries of the world limit the maximum speed allowed on their roads. Vehicles are not supposed to be driven at speeds which are higher than the posted maximum.
To enforce speed limits, two approaches are generally employed. In the United States, it is common for the police to patrol the streets and use special equipment (typically a radar unit) to measure the speed of vehicles, and pull over any vehicle found to be in violation of the speed limit. In Brazil, Colombia and some European countries, there are computerized speed-measuring devices spread throughout the city, which will automatically detect speeding drivers and take a photograph of the license plate (or number plate), which is later used for applying and mailing the ticket. Many jurisdictions in the U.S. use this technology as well.
A mechanism that was developed in Germany is the Grüne Welle, or green wave, which is an indicator that shows the optimal speed to travel for the synchronized green lights along that corridor. Driving faster or slower than the speed set by the behavior of the lights causes the driver to encounter many red lights. This discourages drivers from speeding or impeding the flow of traffic. See related traffic wave and Pedestrian Crossings, above.




Overtaking (or passing) refers to a maneuver by which one or more vehicles traveling in the same direction are passed by another vehicle. On two-lane roads, when there is a split line or a dashed line on the side of the overtaker, drivers may overtake when it is safe. On multi-lane roads in most jurisdictions, overtaking is permitted in the "slower" lanes, though many require a special circumstance. See "Lanes" below.
In the United Kingdom and Canada, notably on extra-urban roads, a solid white or yellow line closer to the driver is used to indicate that no overtaking is allowed in that lane. A double white or yellow line means that neither side may overtake.
In the United States, a solid white line means that lane changes are discouraged and a double white line means that the lane change is prohibited.




When a street is wide enough to accommodate several vehicles traveling side-by-side, it is usual for traffic to organize itself into lanes, that is, parallel corridors of traffic. Some roads have one lane for each direction of travel and others have multiple lanes for each direction. Most countries apply pavement markings to clearly indicate the limits of each lane and the direction of travel that it must be used for. In other countries lanes have no markings at all and drivers follow them mostly by intuition rather than visual stimulus.
On roads that have multiple lanes going in the same direction, drivers may usually shift amongst lanes as they please, but they must do so in a way that does not cause inconvenience to other drivers. Driving cultures vary greatly on the issue of "lane ownership": in some countries, drivers traveling in a lane will be very protective of their right to travel in it while in others drivers will routinely expect other drivers to shift back and forth.
Designation and overtaking
The usual designation for lanes on divided highways is the fastest lane is the one closest to the center of the road, and the slowest to the edge of the road. Drivers are usually expected to keep in the slowest lane unless overtaking, though with more traffic congestion all lanes are often used.
When driving on the left:
The lane designated for faster traffic is on the right.
The lane designated for slower traffic is on the left.
Most freeway exits are on the left.
Overtaking is permitted to the right, and sometimes to the left.
When driving on the right:
The lane designated for faster traffic is on the left.
The lane designated for slower traffic is on the right.
Most freeway exits are on the right.
Overtaking is permitted to the left, and sometimes to the right.
Countries party to the Vienna Convention on Road Traffic have uniform rules about overtaking and lane designation. The convention details (amongst other things) that "Every driver shall keep to the edge of the carriageway appropriate to the direction of traffic", and the "Drivers overtaking shall do so on the side opposite to that appropriate to the direction of traffic", notwithstanding the presence or absence of oncoming traffic. Allowed exceptions to these rules include turning or heavy traffic, traffic in lines, or situation in which signs or markings must dictate otherwise. These rules must be more strictly adhered to on roads with oncoming traffic, but still apply on multi-lane and divided highways. Many countries in Europe are party to the Vienna Conventions on traffic and roads. In Australia (which is not a contracting party), traveling in any lane other than the "slow" lane on a road with a speed limit at or above 80 km/h (50 mph) is an offence, unless signage is posted to the contrary or the driver is overtaking.
Many areas in North America do not have any laws about staying to the slowest lanes unless overtaking. In those areas, unlike many parts of Europe, traffic is allowed to overtake on any side, even in a slower lane. This practice is known as "passing on the right" in the United States and "overtaking on the inside" and "undertaking" in the United Kingdom. When referring to individual lanes on dual carriageways, one does not consider traffic travelling the opposite direction. The inside lane (in the British English sense, i.e. the lane beside the hard shoulder) refers to the lane used for normal travel, while the middle lane is used for overtaking cars on the inside lane. The outside lane (i.e. closest to oncoming traffic) is used for overtaking vehicles in the middle lane. The same principle lies with dual carriageways with more than three lanes.
U.S.-state-specific practices
In some US states (such as Louisiana, Massachusetts and New York), although there are laws requiring all traffic on a public way to use the right-most lane unless overtaking, this rule is often ignored and seldom enforced on multi-lane roadways. Some states, such as Colorado, use a combination of laws and signs restricting speeds or vehicles on certain lanes to emphasize overtaking only on the left lane, and to avoid a psychological condition commonly called road rage.
In California, cars may use any lane on multi-lane roadways. Drivers moving slower than the general flow of traffic are required to stay in the right-most lanes (by California Vehicle Code (CVC) 21654) to keep the way clear for faster vehicles and thus speed up traffic. However, faster drivers may legally pass in the slower lanes if conditions allow (by CVC 21754). But the CVC also requires trucks to stay in the right lane, or in the right two lanes if the roadway has four or more lanes going in their direction. The oldest freeways in California, and some freeway interchanges, often have ramps on the left, making signs like "TRUCKS OK ON LEFT LANE" or "TRUCKS MAY USE ALL LANES" necessary to override the default rule. Lane splitting, or riding motorcycles in the space between cars in traffic, is permitted as long as it is done in a safe and prudent manner.



Main articles: One-way traffic and Dual carriageway
In order to increase traffic capacity and safety, a route may have two or more separate roads for each direction of traffic. Alternatively, a given road might be declared one-way.



Main articles: Limited-access road and Controlled-access highway
In large cities, moving from one part of the city to another by means of ordinary streets and avenues can be time-consuming since traffic is often slowed by at-grade junctions, tight turns, narrow marked lanes and lack of a minimum speed limit. Therefore, it has become common practice for larger cities to build roads for faster through traffic. There are two different types of roads used to provide high-speed access across urban areas:
The controlled-access highway (freeway or motorway) is a divided multi-lane highway with fully controlled access and grade-separated intersections (no cross traffic). Some freeways are called expressways, super-highways, or turnpikes, depending on local usage. Access to freeways is fully controlled; entering and leaving the freeway is permitted only at grade-separated interchanges.
The limited-access road (often called expressway in areas where the name does not refer to a freeway or motorway) is a lower-grade type of road with some or many of the characteristics of a controlled-access highway: usually a broad multi-lane avenue, frequently divided, with some grade separation at intersections.
Motor vehicle drivers wishing to travel over great distances within the city will usually take the freeways or expressways in order to minimize travel time. When a crossing road is at the same grade as the freeway, a bridge (or, less often, an underpass) will be built for the crossing road. If the freeway is elevated, the crossing road will pass underneath it.
Minimum speed signs are sometimes posted (although increasingly rare) and usually indicate that any vehicle traveling slower than 40 mph (64 km/h) should indicate a slower speed of travel to other motor vehicles by engaging the vehicle's four-way flashing lights. Alternative slower-than-posted speeds may be in effect, based on the posted speed limit of the highway/freeway.
Systems of freeways and expressways are also built to connect distant and regional cities, notable systems include the Interstate highways, the Autobahnen and the Expressway Network of the People's Republic of China.



In more sophisticated systems such as large cities, this concept is further extended: some streets are marked as being one-way, and on those streets all traffic must flow in only one direction, but pedestrians on the sidewalks are generally not limited to one-way movement. A driver wishing to reach a destination he already passed must use other streets in order to return. Usage of one-way streets, despite the inconveniences it can bring to individual drivers, can greatly improve traffic flow since they usually allow traffic to move faster and tend to simplify intersections.




In some places traffic volume is consistently, extremely large, either during periods of time referred to as rush hour or perpetually. Exceptionally, traffic upstream of a vehicular collision or an obstruction, such as construction, may also be constrained, resulting in a traffic jam. Such dynamics in relation to traffic congestion is known as traffic flow. Traffic engineers sometimes gauge the quality of traffic flow in terms of level of service.
In measured traffic data, common spatiotemporal empirical features of traffic congestion have been found that are qualitatively the same for different highways in different countries. Some of these common features distinguish the wide moving jam and synchronized flow phases of congested traffic in Kerner’s three-phase traffic theory.




During business days in most major cities, traffic congestion reaches great intensity at predictable times of the day due to the large number of vehicles using the road at the same time. This phenomenon is called rush hour or peak hour, although the period of high traffic intensity often exceeds one hour.






Some cities adopt policies to reduce rush-hour traffic and pollution and encourage the use of public transportation. For example, in São Paulo, Manila and in Mexico City, each vehicle has a specific day of the week in which it is forbidden from traveling the roads during rush hour. The day for each vehicle is taken from the license plate number, and this rule is enforced by traffic police and also by hundreds of strategically positioned traffic cameras backed by computerized image-recognition systems that issue tickets to offending drivers.
In the United States and Canada, several expressways have a special lane (called an "HOV Lane" – High Occupancy Vehicle Lane) that can only be used by cars carrying two (some locations-three) or more people. Also, many major cities have instituted strict parking prohibitions during rush hour on major arterial streets leading to and from the central business district. During designated weekday hours, vehicles parked on these primary routes are subject to prompt ticketing and towing at owner expense. The purpose of these restrictions is to make available an additional traffic lane in order to maximize available traffic capacity. Additionally, several cities offer a public telephone service where citizens can arrange rides with others depending on where they live and work. The purpose of these policies is to reduce the number of vehicles on the roads and thus reduce rush-hour traffic intensity.
Metered freeways are also a solution for controlling rush hour traffic. In Phoenix, Arizona and Seattle, Washington, among other places, metered on-ramps have been implemented. During rush hour, traffic signals are used with green lights to allow one car per blink of the light to proceed on to the freeway.



In some areas, emergency responders are provided with specialized equipment, such as a Mobile Infrared Transmitter, which allows emergency response vehicles, particularly fire-fighting apparatus, to have high-priority travel by having the lights along their route change to green. The technology behind these methods has evolved, from panels at the fire department (which could trigger and control green lights for certain major corridors) to optical systems (which the individual fire apparatus can be equipped with to communicate directly with receivers on the signal head). In other areas, public transport buses have special equipment to get green lights.
During emergencies where evacuation of a heavily populated area is required, local authorities may institute contraflow lane reversal, in which all lanes of a road lead away from a danger zone regardless of their original flow. Aside from emergencies, contraflow may also be used to ease traffic congestion during rush hour or at the end of a sports event (where a large number of cars are leaving the venue at the same time). For example, the six lanes of the Lincoln Tunnel can be changed from three inbound and three outbound to a two/four configuration depending on traffic volume. The Brazilian highways Rodovia dos Imigrantes and Rodovia Anchieta connect São Paulo to the Atlantic coast. Almost all lanes of both highways are usually reversed during weekends to allow for heavy seaside traffic. The reversibility of the highways requires many additional highway ramps and complicated interchanges.



An intelligent transportation system (ITS) is a system of hardware, software, and operators that allow better monitoring and control of traffic in order to optimize traffic flow. As the number of vehicle lane miles traveled per year continues to increase dramatically, and as the number of vehicle lane miles constructed per year has not been keeping pace, this has led to ever-increasing traffic congestion. As a cost-effective solution toward optimizing traffic, ITS presents a number of technologies to reduce congestion by monitoring traffic flows through the use of sensors and live cameras or analysing cellular phone data travelling in cars (floating car data) and in turn rerouting traffic as needed through the use of variable message boards (VMS), highway advisory radio, on board or off board navigation devices and other systems through integration of traffic data with navigation systems. Additionally, the roadway network has been increasingly fitted with additional communications and control infrastructure to allow traffic operations personnel to monitor weather conditions, for dispatching maintenance crews to perform snow or ice removal, as well as intelligent systems such as automated bridge de-icing systems which help to prevent accidents.









May, Adolf. Traffic Flow Fundamentals. Prentice Hall, Englewood Cliffs, NJ, 1990.
2010 Highway Capacity Manual. Transportation Research Board, Washington, D.C. ISBN 0-309-06681-6,
Taylor, Nicholas. The Contram dynamic traffic assignment model TRL 2003
B. S. Kerner, The Physics of Traffic, Springer, Berlin, New York, 2004
B. S. Kerner, Introduction to Modern Traffic Flow Theory and Control: The Long Road to Three-Phase Traffic Theory, Springer, Berlin, New York, 2009
Traffic Monitoring: A Guidebook Federal Highway Administration
Vanderbilt, Tom. Traffic: Why We Drive the Way We Do (and What It Says About Us). Knopf, New York, 2008.



Road Transport in the European Union
Institute of Road Transport Engineers (IRTE)
The Greenroads Rating System
Interactive Map of Traffic for every major UK Road
SCATS Traffic Management Theory
Roads and Traffic Authority, NSW
SCATS Sydney Coordinated Adaptive Traffic SystemA hotel is an establishment that provides paid lodging on a short-term basis. Facilities provided may range from a modest-quality mattress in a small room to large suites with bigger, higher-quality beds, a dresser, a fridge and other kitchen facilities, upholstered chairs, a flatscreen television and en-suite bathrooms. Small, lower-priced hotels may offer only the most basic guest services and facilities. Larger, higher-priced hotels may provide additional guest facilities such as a swimming pool, business centre (with computers, printers and other office equipment), childcare, conference and event facilities, tennis or basketball courts, gymnasium, restaurants, day spa and social function services. Hotel rooms are usually numbered (or named in some smaller hotels and B&Bs) to allow guests to identify their room. Some boutique, high-end hotels have custom decorated rooms. Some hotels offer meals as part of a room and board arrangement. In the United Kingdom, a hotel is required by law to serve food and drinks to all guests within certain stated hours. In Japan, capsule hotels provide a tiny room suitable only for sleeping and shared bathroom facilities.
The precursor to the modern hotel was the inn of medieval Europe. For a period of about 200 years from the mid-17th century, coaching inns served as a place for lodging for coach travelers. Inns began to cater to richer clients in the mid-18th century. One of the first hotels in a modern sense was opened in Exeter in 1768. Hotels proliferated throughout Western Europe and North America in the early 19th century, and luxury hotels began to spring up in the later part of the 19th century.
Hotel operations vary in size, function, and cost. Most hotels and major hospitality companies have set industry standards to classify hotel types. An upscale full-service hotel facility offers luxury amenities, full service accommodations, an on-site restaurant, and the highest level of personalized service, such as a concierge, room service and clothes pressing staff. Full service hotels often contain upscale full-service facilities with a large number of full service accommodations, an on-site full service restaurant, and a variety of on-site amenities. Boutique hotels are smaller independent, non-branded hotels that often contain upscale facilities. Small to medium-sized hotel establishments offer a limited amount of on-site amenities. Economy hotels are small to medium-sized hotel establishments that offer basic accommodations with little to no services. Extended stay hotels are small to medium-sized hotels that offer longer-term full service accommodations compared to a traditional hotel.
Timeshare and destination clubs are a form of property ownership involving ownership of an individual unit of accommodation for seasonal usage. A motel is a small-sized low-rise lodging with direct access to individual rooms from the car park. Boutique hotels are typically hotels with a unique environment or intimate setting. A number of hotels have entered the public consciousness through popular culture, such as the Ritz Hotel in London. Some hotels are built specifically as a destination in itself, for example at casinos and holiday resorts.
Most hotel establishments are run by a General Manager who serves as the head executive (often referred to as the "Hotel Manager"), department heads who oversee various departments within a hotel (e.g., food service), middle managers, administrative staff, and line-level supervisors. The organizational chart and volume of job positions and hierarchy varies by hotel size, function and class, and is often determined by hotel ownership and managing companies.



The word hotel is derived from the French hôtel (coming from the same origin as hospital), which referred to a French version of a building seeing frequent visitors, and providing care, rather than a place offering accommodation. In contemporary French usage, hôtel now has the same meaning as the English term, and hôtel particulier is used for the old meaning, as well as "hôtel" in some place names such as Hôtel-Dieu (in Paris), which has been a hospital since the Middle Ages. The French spelling, with the circumflex, was also used in English, but is now rare. The circumflex replaces the 's' found in the earlier hostel spelling, which over time took on a new, but closely related meaning. Grammatically, hotels usually take the definite article – hence "The Astoria Hotel" or simply "The Astoria."




Facilities offering hospitality to travellers have been a feature of the earliest civilizations. In Greco-Roman culture hospitals for recuperation and rest were built at thermal baths. During the Middle Ages various religious orders at monasteries and abbeys would offer accommodation for travellers on the road.
The precursor to the modern hotel was the inn of medieval Europe, possibly dating back to the rule of Ancient Rome. These would provide for the needs of travelers, including food and lodging, stabling and fodder for the traveler's horse(s) and fresh horses for the mail coach. Famous London examples of inns include the George and the Tabard. A typical layout of an inn had an inner court with bedrooms on the two sides, with the kitchen and parlour at the front and the stables at the back.
For a period of about 200 years from the mid-17th century, coaching inns served as a place for lodging for coach travelers (in other words, a roadhouse). Coaching inns stabled teams of horses for stagecoaches and mail coaches and replaced tired teams with fresh teams. Traditionally they were seven miles apart but this depended very much on the terrain.

Some English towns had as many as ten such inns and rivalry between them was intense, not only for the income from the stagecoach operators but for the revenue for food and drink supplied to the wealthy passengers. By the end of the century, coaching inns were being run more professionally, with a regular timetable being followed and fixed menus for food.
Inns began to cater for richer clients in the mid-18th century, and consequently grew in grandeur and the level of service provided. One of the first hotels in a modern sense was opened in Exeter in 1768, although the idea only really caught on in the early 19th century. In 1812 Mivart's Hotel opened its doors in London, later changing its name to Claridge's.
Hotels proliferated throughout Western Europe and North America in the 19th century, and luxury hotels, including the Savoy Hotel in the United Kingdom and the Ritz chain of hotels in London and Paris and Tremont House and Astor House in the United States, began to spring up in the later part of the century, catering to an extremely wealthy clientele.



Hotels cater to travelers from many countries and languages, since no one country dominates the travel industry.




Hotel operations vary in size, function, and cost. Most hotels and major hospitality companies that operate hotels have set widely accepted industry standards to classify hotel types. General categories include the following:



An upscale full service hotel facility that offers luxury amenities, full service accommodations, on-site full service restaurant(s), and the highest level of personalized and professional service. Luxury hotels are normally classified with at least a Four Diamond or Five Diamond status or a Four or Five Star rating depending on the country and local classification standards. Examples may include: InterContinental, Waldorf Astoria, Four Seasons, Conrad, Fairmont, and The Ritz-Carlton.



Full service hotels often contain upscale full-service facilities with a large volume of full service accommodations, on-site full service restaurant(s), and a variety of on-site amenities such as swimming pools, a health club, children's activities, ballrooms, on-site conference facilities, and other amenities. Examples include: Holiday Inn, Sheraton, Westin, Hilton, Marriott, and Hyatt hotels.



Boutique hotels are smaller independent non-branded hotels that often contain upscale facilities of varying size in unique or intimate settings with full service accommodations. Boutique hotels are generally 100 rooms or less. Some historic inns and boutique hotels may be classified as luxury hotels. Examples include Hotel Indigo and Kimpton Hotels.



Small to medium-sized hotel establishments that offer a limited amount of on-site amenities that only cater and market to a specific demographic of travelers, such as the single business traveler. Most focused or select service hotels may still offer full service accommodations but may lack leisure amenities such as an on-site restaurant or a swimming pool. Examples include Crowne Plaza, Courtyard by Marriott and Hilton Garden Inn.



Small to medium-sized hotel establishments that offer a very limited amount of on-site amenities and often only offer basic accommodations with little to no services, these facilities normally only cater and market to a specific demographic of travelers, such as the budget-minded traveler seeking a "no frills" accommodation. Limited service hotels often lack an on-site restaurant but in return may offer a limited complimentary food and beverage amenity such as on-site continental breakfast service. Examples include Ibis Budget, Hampton Inn, Aloft, Holiday Inn Express, Fairfield Inn, Four Points by Sheraton, and Days Inn.



Extended stay hotels are small to medium-sized hotels that offer longer term full service accommodations compared to a traditional hotel. Extended stay hotels may offer non-traditional pricing methods such as a weekly rate that cater towards travelers in need of short-term accommodations for an extended period of time. Similar to limited and select service hotels, on-site amenities are normally limited and most extended stay hotels lack an on-site restaurant. Examples include Staybridge Suites, Candlewood Suites, Homewood Suites by Hilton, Home2 Suites by Hilton, Residence Inn by Marriott, Element, and Extended Stay Hotels.



Timeshare and Destination clubs are a form of property ownership also referred to as a vacation ownership involving the purchase and ownership of an individual unit of accommodation for seasonal usage during a specified period of time. Timeshare resorts often offer amenities similar that of a Full service hotel with on-site restaurant(s), swimming pools, recreation grounds, and other leisure-oriented amenities. Destination clubs on the other hand may offer more exclusive private accommodations such as private houses in a neighborhood-style setting. Examples of timeshare brands include Hilton Grand Vacations, Marriott Vacation Club International, Westgate Resorts, Disney Vacation Club, and Holiday Inn Club Vacations.



A motel, an abbreviation for "motor hotel", is a small-sized low-rise lodging establishment similar to a limited service, lower-cost hotel, but typically with direct access to individual rooms from the car park. Motels were built to serve road travellers, including travellers on road trip vacations and workers who drive for their job (travelling salespeople, truck drivers, etc.). Common during the 1950s and 1960s, motels were often located adjacent to a major highway, where they were built on inexpensive land at the edge of towns or along stretches of freeway.
New motel construction is rare in the 2000s as hotel chains have been building economy-priced, limited service franchised properties at freeway exits which compete for largely the same clientele, largely saturating the market by the 1990s. Motels are still useful in less populated areas for driving travelers, but the more populated an area becomes, the more hotels move in to meet the demand for accommodation. Many of the motels which remain in operation have joined national franchise chains, often rebranding themselves as hotels, inns or lodges.




Hotel management is a globally accepted professional career field and academic field of study. Degree programs such as hospitality management studies, a business degree, and/or certification programs formally prepare hotel managers for industry practice.
Most hotel establishments consist of a General Manager who serves as the head executive (often referred to as the "Hotel Manager"), department heads who oversee various departments within a hotel, middle managers, administrative staff, and line-level supervisors. The organizational chart and volume of job positions and hierarchy varies by hotel size, function, and is often determined by hotel ownership and managing companies.







Boutique hotels are typically hotels with a unique environment or intimate setting. Some hotels have gained their renown through tradition, by hosting significant events or persons, such as Schloss Cecilienhof in Potsdam, Germany, which derives its fame from the Potsdam Conference of the World War II allies Winston Churchill, Harry Truman and Joseph Stalin in 1945. The Taj Mahal Palace & Tower in Mumbai is one of India's most famous and historic hotels because of its association with the Indian independence movement. Some establishments have given name to a particular meal or beverage, as is the case with the Waldorf Astoria in New York City, United States where the Waldorf Salad was first created or the Hotel Sacher in Vienna, Austria, home of the Sachertorte. Others have achieved fame by association with dishes or cocktails created on their premises, such as the Hotel de Paris where the crêpe Suzette was invented or the Raffles Hotel in Singapore, where the Singapore Sling cocktail was devised.

A number of hotels have entered the public consciousness through popular culture, such as the Ritz Hotel in London, through its association with Irving Berlin's song, 'Puttin' on the Ritz'. The Algonquin Hotel in New York City is famed as the meeting place of the literary group, the Algonquin Round Table, and Hotel Chelsea, also in New York City, has been the subject of a number of songs and the scene of the stabbing of Nancy Spungen (allegedly by her boyfriend Sid Vicious).




Some hotels are built specifically as a destination in itself to create a captive trade, example at casinos, amusement parks and holiday resorts. Though of course hotels have always been built in popular destinations, the defining characteristic of a resort hotel is that it exists purely to serve another attraction, the two having the same owners.
On the Las Vegas Strip there is a tradition of one-upmanship with luxurious and extravagant hotels in a concentrated area. This trend now has extended to other resorts worldwide, but the concentration in Las Vegas is still the world's highest: nineteen of the world's twenty-five largest hotels by room count are on the Strip, with a total of over 67,000 rooms.
In Europe Center Parcs might be considered a chain of resort hotels, since the sites are largely man-made (though set in natural surroundings such as country parks) with captive trade, whereas holiday camps such as Butlins and Pontin's are probably not considered as resort hotels, since they are set at traditional holiday destinations which existed before the camps.




The Burj al-Arab hotel in Dubai, United Arab Emirates, built on an artificial island, is structured in the shape of a boat's sail.
The Library Hotel in New York City, is unique in that each of its ten floors is assigned one category from the Dewey Decimal System.
The Jailhotel Löwengraben in Lucerne, Switzerland is a converted prison now used as a hotel.
The Luxor, a hotel and casino on the Las Vegas Strip in Paradise, Nevada, United States is unusual due to its pyramidal structure.
The Liberty Hotel in Boston, used to be the Charles Street Jail.
Hotel Kakslauttanen in Finland, a collection of glass igloos in Lapland that allow you to watch the Northern Lights
Built in Scotland and completed in 1936, The former ocean liner RMS Queen Mary in Long Beach, California, United States uses its first-class staterooms as a hotel, after retiring in 1967 from Transatlantic service.
The Wigwam Motels used patented novelty architecture in which each motel room was a free-standing concrete wigwam or teepee.
Various Caboose Motel or Red Caboose Inn properties are built from decommissioned rail cars.
Throughout the world there are several hotels built from converted airliners.



The Null Stern Hotel in Teufen, Appenzellerland, Switzerland and the Concrete Mushrooms in Albania are former nuclear bunkers transformed into hotels.



The Cuevas Pedro Antonio de Alarcón (named after the author) in Guadix, Spain, as well as several hotels in Cappadocia, Turkey, are notable for being built into natural cave formations, some with rooms underground. The Desert Cave Hotel in Coober Pedy, South Australia is built into the remains of an opal mine.




Located on the coast but high above sea level, these hotels offer unobstructed panoramic views and a great sense of privacy without the feeling of total isolation. Some examples from around the globe are the Riosol Hotel in Gran Canaria, Caruso Belvedere Hotel in Amalfi Coast (Italy), Aman Resorts Amankila in Bali, Birkenhead House in Hermanus (South Africa), The Caves in Jamaica and Caesar Augustus in Capri.




Capsule hotels are a type of economical hotel first introduced in Japan, where people sleep in stacks of rectangular containers.



Some hotels fill daytime occupancy with day rooms, for example, Rodeway Inn and Suites near Port Everglades in Fort Lauderdale, Florida. Day rooms are booked in a block of hours typically between 8 am and 5 pm, before the typical night shift. These are similar to transit hotels in that they appeal to travelers, however, unlike transit hotels, they do not eliminate the need to go through Customs.



Garden hotels, famous for their gardens before they became hotels, include Gravetye Manor, the home of garden designer William Robinson, and Cliveden, designed by Charles Barry with a rose garden by Geoffrey Jellicoe.




The Ice Hotel in Jukkasjärvi, Sweden, was the first ice hotel in the world; first built in 1990, it is built each winter and melts every spring. Other ice hotels include the Igloo Village in Kakslauttanen, Finland, and the Hotel de Glace in Duschenay, Canada. They can also be included within larger ice complexes; for example, the Mammut Snow Hotel in Finland is located within the walls of the Kemi snow castle; and the Lainio Snow Hotel is part of a snow village near Ylläs, Finland.




A referral hotel is a hotel chain that offers branding to independently-operated hotels; the chain itself is founded by or owned by the member hotels as a group. Many former referral chains have been converted to franchises; the largest surviving member-owned chain is Best Western.




Frequently, expanding railway companies built grand hotels at their termini, such as the Midland Hotel, Manchester next to the former Manchester Central Station, and in London the ones above St Pancras railway station and Charing Cross railway station. London also has the Chiltern Court Hotel above Baker Street tube station, there are also Canada's grand railway hotels. They are or were mostly, but not exclusively, used by those traveling by rail.



The Maya Guesthouse in Nax Mont-Noble in the Swiss Alps, is the first hotel in Europe built entirely with straw bales. Due to the insulation values of the walls it needs no conventional heating or air conditioning system, although the Maya Guesthouse is built at an altitude of 1,300 metres (4,300 ft) in the Alps.




Transit hotels are short stay hotels typically used at international airports where passengers can stay while waiting to change airplanes. The hotels are typically on the airside and do not require a visa for a stay or re-admission through security checkpoints.



Some hotels are built with living trees as structural elements, for example the Treehotel near Piteå, Sweden, the Costa Rica Tree House in the Gandoca-Manzanillo Wildlife Refuge, Costa Rica; the Treetops Hotel in Aberdare National Park, Kenya; the Ariau Towers near Manaus, Brazil, on the Rio Negro in the Amazon; and Bayram's Tree Houses in Olympos, Turkey.




Some hotels have accommodation underwater, such as Utter Inn in Lake Mälaren, Sweden. Hydropolis, project in Dubai, would have had suites on the bottom of the Persian Gulf, and Jules' Undersea Lodge in Key Largo, Florida requires scuba diving to access its rooms.




A resort island is an island or an archipelago that contains resorts, hotels, overwater bungalows, restaurants, tourist attractions and its amenities. Maldives has the most overwater bungalows resorts and topped as having the best island resorts and they have become famous among the top celebrities and sportspersons around the world.







In 2006, Guinness World Records listed the First World Hotel in Genting Highlands, Malaysia, as the world's largest hotel with a total of 6,118 rooms. The Izmailovo Hotel in Moscow has the most rooms, with 7,500, followed by The Venetian and The Palazzo complex in Las Vegas (7,117 rooms) and MGM Grand Las Vegas complex (6,852 rooms).



According to the Guinness Book of World Records, the oldest hotel in operation is the Nisiyama Onsen Keiunkan in Yamanashi, Japan. The hotel, first opened in 707 A.D. has been operated by the same family for forty-six generations. The title was held until 2011 by the Hoshi Ryokan, in the Awazu Onsen area of Komatsu, Japan, which opened in the year 718, as the history of the Nisiyama Onsen Keiunkan was virtually unknown.



The Ritz-Carlton, Hong Kong claims to be the world's highest hotel. It is located on the top floors of the International Commerce Centre in Hong Kong, at 484 metres (1,588 ft) above ground level.



In October 2014, the Anbang Insurance Group, based in China, purchased the Waldorf Astoria New York in Manhattan for US$1.95 billion, making it the world's most expensive hotel ever sold.



A number of public figures have notably chosen to take up semi-permanent or permanent residence in hotels.
Fashion designer Coco Chanel lived in the Hôtel Ritz, Paris, on and off for more than 30 years.
Inventor Nikola Tesla lived the last ten years of his life at the New Yorker Hotel until he died in his room in 1943.
Larry Fine (of The Three Stooges) and his family lived in hotels, due to his extravagant spending habits and his wife's dislike for housekeeping. They first lived in the President Hotel in Atlantic City, New Jersey, where his daughter Phyllis was raised, then the Knickerbocker Hotel in Hollywood. Not until the late 1940s did Larry buy a home in the Los Feliz area of Los Angeles.
The Waldorf-Astoria Hotel and its affiliated Waldorf Towers has been the home of many famous persons over the years including former President Herbert Hoover who lived there from the end of his presidency in 1933 until his death in 1964. General Douglas MacArthur lived his last 14 years in the penthouse of the Waldorf Towers. And the composer Cole Porter also spent the last 25 years of his life in an apartment at the Waldorf Towers.
Millionaire Howard Hughes lived in hotels during the last ten years of his life (1966–76), primarily in Las Vegas, as well as Acapulco, Beverly Hills, Boston, Freeport, London, Managua, Nassau, Vancouver, and others.
Vladimir Nabokov and his wife Vera lived in the Montreux Palace Hotel in Montreux, Switzerland (1961-his death in 1977).
Actor Richard Harris lived at the Savoy Hotel while in London. Hotel archivist Susan Scott recounts an anecdote that, when he was being taken out of the building on a stretcher shortly before his death in 2002, he raised his hand and told the diners "it was the food."
Egyptian actor Ahmed Zaki lived his last 15 years in Ramses Hilton Hotel – Cairo.
British entrepreneur Jack Lyons lived in the Hotel Mirador Kempinski in Switzerland for several years until his death in 2008.
American actress Elaine Stritch lived in the Savoy Hotel in London for over a decade.



List of hotels
Lists of hotels
List of chained-brand hotels
List of defunct hotel chains
Casino hotel

List of casino hotels

Niche tourism markets











Hollywood (/ˈhɒliwʊd/ HOL-ee-wuud, informally Tinseltown /ˈtɪnsəlˌtaʊn/) is an ethnically diverse, densely populated, relatively low-income neighborhood in the central region of Los Angeles, California. It is notable as the home of the U.S. film industry, including several of its historic studios, and its name has come to be a shorthand reference for the industry and the people in it.
Hollywood was a small community in 1870 and was incorporated as a municipality in 1903. It was consolidated with the city of Los Angeles in 1910, and soon thereafter a prominent film industry emerged, eventually becoming the most recognizable film industry in the world.






In 1853, one adobe hut stood in Nopalera (Nopal field), named for the Mexican Nopal cactus indigenous to the area. By 1870, an agricultural community flourished. The area was known as the Cahuenga Valley, after the pass in the Santa Monica Mountains immediately to the north.
According to the diary of H. J. Whitley, known as the "Father of Hollywood", on his honeymoon in 1886 he stood at the top of the hill looking out over the valley. Along came a Chinese man in a wagon carrying wood. The man got out of the wagon and bowed. The Chinese man was asked what he was doing and replied, "I holly-wood", meaning 'hauling wood.' H. J. Whitley had an epiphany and decided to name his new town Hollywood. "Holly" would represent England and "wood" would represent his Scottish heritage. Whitley had already started over 100 towns across the western United States.
Whitley arranged to buy the 500-acre (200 ha) E.C. Hurd ranch and disclosed to him his plans for the land. They agreed on a price and Hurd agreed to sell at a later date. Before Whitley got off the ground with Hollywood, plans for the new town had spread to General Harrison Gray Otis, Hurd's wife, eastern adjacent ranch co-owner Daeida Wilcox, and others.

Daeida Wilcox may have learned of the name Hollywood from Ivar Weid, her neighbor in Holly Canyon (now Lake Hollywood) and a prominent investor and friend of Whitley's. She recommended the same name to her husband, Harvey. H. Wilcox. In August 1887, Wilcox filed with the Los Angeles County Recorder's office a deed and parcel map of property he had sold named "Hollywood, California." Wilcox wanted to be the first to record it on a deed. The early real-estate boom busted that same year, yet Hollywood began its slow growth.
By 1900, the region had a post office, newspaper, hotel, and two markets. Los Angeles, with a population of 102,479 lay 10 miles (16 km) east through the vineyards, barley fields, and citrus groves. A single-track streetcar line ran down the middle of Prospect Avenue from it, but service was infrequent and the trip took two hours. The old citrus fruit-packing house was converted into a livery stable, improving transportation for the inhabitants of Hollywood.

The Hollywood Hotel was opened in 1902 by H. J. Whitley, president of the Los Pacific Boulevard and Development Company. Having finally acquired the Hurd ranch and subdivided it, Whitley built the hotel to attract land buyers. Flanking the west side of Highland Avenue, the structure fronted on Prospect Avenue, which, still a dusty, unpaved road, was regularly graded and graveled. The hotel was to become internationally known and was the center of the civic and social life and home of the stars for many years.
Whitley's company developed and sold one of the early residential areas, the Ocean View Tract. Whitley did much to promote the area. He paid thousands of dollars for electric lighting, including bringing electricity and building a bank, as well as a road into the Cahuenga Pass. The lighting ran for several blocks down Prospect Avenue. Whitley's land was centered on Highland Avenue. His 1918 development, Whitley Heights, was named for him.



Hollywood was incorporated as a municipality on November 14, 1903, by a vote of 88 for and 77 against. On January 30, 1904, the voters in Hollywood decided, by a vote of 113 to 96, for the banishment of liquor in the city, except when it was being sold for medicinal purposes. Neither hotels nor restaurants were allowed to serve wine or liquor before or after meals.
In 1910, the city voted for merger with Los Angeles in order to secure an adequate water supply and to gain access to the L.A. sewer system. With annexation, the name of Prospect Avenue changed to Hollywood Boulevard and all the street numbers were also changed.




By 1912, major motion-picture companies had set up production near or in Los Angeles. In the early 1900s, most motion picture patents were held by Thomas Edison's Motion Picture Patents Company in New Jersey, and filmmakers were often sued to stop their productions. To escape this, filmmakers began moving out west, where Edison's patents could not be enforced. Also, the weather was ideal and there was quick access to various settings. Los Angeles became the capital of the film industry.

Director D. W. Griffith was the first to make a motion picture in Hollywood. His 17-minute short film In Old California (1910) was filmed for the Biograph Company. Although Hollywood banned movie theaters—of which it had none—before annexation that year, Los Angeles had no such restriction. The first film by a Hollywood studio, Nestor Motion Picture Company, was shot on October 26, 1911. The Whitley home was used as its set, and the unnamed movie was filmed in the middle of their groves at the corner of Whitley Avenue and Hollywood Boulevard.
The first studio in Hollywood, the Nestor Company, was established by the New Jersey–based Centaur Company in a roadhouse at 6121 Sunset Boulevard (the corner of Gower), in October 1911.
Four major film companies – Paramount, Warner Bros., RKO, and Columbia – had studios in Hollywood, as did several minor companies and rental studios. In the 1920s, Hollywood was the fifth-largest industry in the nation.
Hollywood became known as Tinseltown because of the glittering image of the movie industry. Hollywood has since become a major center for film study in the United States.




In 1923, the Hollywood sign was erected in the Hollywood Hills, reading "HOLLYWOODLAND," its purpose being to advertise a housing development. In 1949, the Hollywood Chamber of Commerce entered a contract with the City of Los Angeles to repair and rebuild the sign. The contract stipulated that "LAND" be removed to spell "HOLLYWOOD" and reflect the district, not the housing development.
During the early 1950s, the Hollywood Freeway was constructed through the northeast corner of Hollywood.
The Capitol Records Building on Vine Street, just north of Hollywood Boulevard, was built in 1956, and the Hollywood Walk of Fame was created in 1958 as a tribute to artists and other significant contributors to the entertainment industry. The official opening was on February 8, 1960.
The Hollywood Boulevard Commercial and Entertainment District was listed in the National Register of Historic Places in 1985.
In June 1999, the Hollywood extension of the Los Angeles County Metro Rail Red Line subway opened from Downtown Los Angeles to the San Fernando Valley, with stops along Hollywood Boulevard at Western Avenue (Hollywood/Western Metro station), Vine Street (Hollywood/Vine Metro station), and Highland Avenue (Hollywood/Highland Metro station).
The Dolby Theatre, which opened in 2001 as the Kodak Theatre at the Hollywood & Highland Center mall, is the home of the Oscars. The mall is located where the historic Hollywood Hotel once stood.



After years of serious decline in the 1980s, many Hollywood landmarks were threatened with demolition. Columbia Square, at the northeast corner of Sunset Boulevard and Gower Street, is part of the ongoing rebirth of Hollywood. The Art Deco-style studio complex completed in 1938, which was once the Hollywood headquarters for CBS, became home to a new generation of broadcasters when cable television networks MTV, Comedy Central, BET and Spike TV consolidated their offices here in 2014 as part of a $420-million office, residential and retail complex. Since 2000, Hollywood has been increasingly gentrified due to revitalization by private enterprise and public planners.



In 2002, some Hollywood voters began a campaign for the area to secede from Los Angeles and become a separate municipality. In June of that year, the Los Angeles County Board of Supervisors placed secession referendums for both Hollywood and the San Fernando Valley on the ballot. To pass, they required the approval of a majority of voters in the proposed new municipality as well as a majority of voters in all of Los Angeles. In the November election, both measures failed by wide margins in the citywide vote.



According to the Mapping L.A. project of the Los Angeles Times, Hollywood is flanked by Hollywood Hills to the north, Los Feliz to the northeast, East Hollywood to the east, Larchmont and Hancock Park to the south, Fairfax to the southwest, West Hollywood to the west and Hollywood Hills West to the northwest.
Street limits of the Hollywood neighborhood are: north, Hollywood Boulevard from La Brea Avenue to the east boundary of Wattles Garden Park and Franklin Avenue between Bonita and Western avenues; east, Western Avenue; south, Melrose Avenue, and west, La Brea Avenue or the West Hollywood city line.
In 1918, H. J. Whitley commissioned architect A. S. Barnes to design Whitley Heights as a Mediterranean-style village on the hills above Hollywood Boulevard, and it became the first celebrity community.
Other areas within Hollywood are Franklin Village, Little Armenia, Spaulding Square, Thai Town, and Yucca Corridor.



Relation of Hollywood to nearby communities:
The famous Hollywood Sign on Mount Lee is not actually in Hollywood but is instead to the north in the Hollywood Hills.



The 2000 U.S. census counted 77,818 residents in the 3.51-square-mile (9.1 km2) Hollywood neighborhood—an average of 22,193 people per square mile (8,569 per km2), the seventh-densest neighborhood in all of Los Angeles County. In 2008 the city estimated that the population had increased to 85,489. The median age for residents was 31, about the city's average.
Hollywood was said to be "highly diverse" when compared to the city at large. The ethnic breakdown in 2000 was: Latino or Hispanic, 42.2%, Non-Hispanic Whites, 41%; Asian, 7.1%; blacks, 5.2%, and others, 4.5%. Mexico (21.3%) and Guatemala (13%) were the most common places of birth for the 53.8% of the residents who were born abroad, a figure that was considered high for the city as a whole.
The median household income in 2008 dollars was $33,694, considered low for Los Angeles. The average household size of 2.1 people was also lower than the city norm. Renters occupied 92.4% of the housing units, and home- or apartment owners the rest.
The percentages of never-married men (55.1%), never-married women (39.8%) and widows (9.6%) were among the county's highest. There were 2,640 families headed by single parents, about average for Los Angeles.
In 2000, there were 2,828 military veterans, or 4.5%, a low rate for the city as a whole. These were the ten neighborhoods or cities in Los Angeles County with the highest population densities, according to the 2000 census, with the population per square mile:




KNX was the last radio station to broadcast from Hollywood before it left CBS Columbia Square for a studio in the Miracle Mile in 2005.
On January 22, 1947, the first commercial television station west of the Mississippi River, KTLA, began operating in Hollywood. In December of that year, The Public Prosecutor became the first network television series to be filmed in Hollywood.Television stations KTLA and KCET, both on Sunset Boulevard, are the last broadcasters (television or radio) with Hollywood addresses, but KCET has since sold its studios on Sunset and plans to move to another location. KNBC moved in 1962 from the former NBC Radio City Studios at the northeast corner of Sunset Boulevard and Vine Street to NBC Studios in Burbank. KTTV moved in 1996 from its former home at Metromedia Square on Sunset Boulevard to West Los Angeles, and KCOP left its home on La Brea Avenue to join KTTV on the Fox lot. KCBS-TV and KCAL-TV moved from their longtime home at CBS Columbia Square on Sunset Boulevard to a new facility at CBS Studio Center in Studio City.




As a neighborhood within the Los Angeles city limits, Hollywood does not have its own municipal government. There was an official, appointed by the Hollywood Chamber of Commerce, who served as an honorary "Mayor of Hollywood" for ceremonial purposes only. Johnny Grant held this position from 1980 until his death on January 9, 2008.



The Los Angeles Police Department is responsible for police services. The Hollywood police station is at 1358 N. Wilcox Ave.
Los Angeles Fire Department operates four fire stations – Station 27, 41, 52, and 82 – in the area.
The Los Angeles County Department of Health Services operates the Hollywood-Wilshire Health Center in Hollywood.



The United States Postal Service operates the Hollywood Post Office, the Hollywood Pavilion Post Office, and the Sunset Post Office.



Hollywood is included within the Hollywood United Neighborhood Council (HUNC) Hollywood Hills West Neighborhood Council and the Hollywood Studio District Neighborhood Council. Neighborhood Councils cast advisory votes on such issues as zoning, planning, and other community issues. The council members are voted in by stakeholders, generally defined as anyone living, working, owning property, or belonging to an organization within the boundaries of the council.



Hollywood residents aged 25 and older holding a four-year degree amounted to 28% of the population in 2000, about the same as in the county at large.



Public schools are operated by the Los Angeles Unified School District (LAUSD).
Schools in Hollywood include:



The Will and Ariel Durant Branch and the Frances Howard Goldwyn – Hollywood Regional Branch of the Los Angeles Public Library are in Hollywood.






The Academy Awards are held in late February/early March (since 2004) of each year, honoring the preceding year in film. Prior to 2004, they were held in late March/early April. Since 2002, the Oscars have been held at their new home at the Dolby (formerly Kodak) Theater at Hollywood Boulevard and Highland Avenue.
The annual Hollywood Christmas Parade: The 2006 parade on Nov 26 was the 75th edition of the Christmas Parade. The parade goes down Hollywood Boulevard and is broadcast in the LA area on KTLA, and around the United States on Tribune-owned stations and the WGN superstation.
The Hollywood Half Marathon takes place in April (since 2012) of each year, to raise funds and awareness for local youth homeless shelters. The event includes a Half Marathon, 10K, 5K, and Kids Fun Run along Hollywood Blvd.









Hollywood at DMOZRandomness is the lack of pattern or predictability in events. A random sequence of events, symbols or steps has no order and does not follow an intelligible pattern or combination. Individual random events are by definition unpredictable, but in many cases the frequency of different outcomes over a large number of events (or "trials") is predictable. For example, when throwing two dice, the outcome of any particular roll is unpredictable, but a sum of 7 will occur twice as often as 4. In this view, randomness is a measure of uncertainty of an outcome, rather than haphazardness, and applies to concepts of chance, probability, and information entropy.
The fields of mathematics, probability, and statistics use formal definitions of randomness. In statistics, a random variable is an assignment of a numerical value to each possible outcome of an event space. This association facilitates the identification and the calculation of probabilities of the events. Random variables can appear in random sequences. A random process is a sequence of random variables whose outcomes do not follow a deterministic pattern, but follow an evolution described by probability distributions. These and other constructs are extremely useful in probability theory and the various applications of randomness.
Randomness is most often used in statistics to signify well-defined statistical properties. Monte Carlo methods, which rely on random input (such as from random number generators or pseudorandom number generators), are important techniques in science, as, for instance, in computational science. By analogy, quasi-Monte Carlo methods use quasirandom number generators.
Random selection is a method of selecting items (often called units) from a population where the probability of choosing a specific item is the proportion of those items in the population. For example, with a bowl containing just 10 red marbles and 90 blue marbles, a random selection mechanism would choose a red marble with probability 1/10. Note that a random selection mechanism that selected 10 marbles from this bowl would not necessarily result in 1 red and 9 blue. In situations where a population consists of items that are distinguishable, a random selection mechanism requires equal probabilities for any item to be chosen. That is, if the selection process is such that each member of a population, of say research subjects, has the same probability of being chosen then we can say the selection process is random.




In ancient history, the concepts of chance and randomness were intertwined with that of fate. Many ancient peoples threw dice to determine fate, and this later evolved into games of chance. Most ancient cultures used various methods of divination to attempt to circumvent randomness and fate.
The Chinese were perhaps the earliest people to formalize odds and chance 3,000 years ago. The Greek philosophers discussed randomness at length, but only in non-quantitative forms. It was only in the 16th century that Italian mathematicians began to formalize the odds associated with various games of chance. The invention of the calculus had a positive impact on the formal study of randomness. In the 1888 edition of his book The Logic of Chance John Venn wrote a chapter on The conception of randomness that included his view of the randomness of the digits of the number pi by using them to construct a random walk in two dimensions.
The early part of the 20th century saw a rapid growth in the formal analysis of randomness, as various approaches to the mathematical foundations of probability were introduced. In the mid- to late-20th century, ideas of algorithmic information theory introduced new dimensions to the field via the concept of algorithmic randomness.
Although randomness had often been viewed as an obstacle and a nuisance for many centuries, in the 20th century computer scientists began to realize that the deliberate introduction of randomness into computations can be an effective tool for designing better algorithms. In some cases such randomized algorithms outperform the best deterministic methods.



Many scientific fields are concerned with randomness:



In the 19th century, scientists used the idea of random motions of molecules in the development of statistical mechanics to explain phenomena in thermodynamics and the properties of gases.
According to several standard interpretations of quantum mechanics, microscopic phenomena are objectively random. That is, in an experiment that controls all causally relevant parameters, some aspects of the outcome still vary randomly. For example, if you place a single unstable atom in a controlled environment, you cannot predict how long it will take for the atom to decay—only the probability of decay in a given time. Thus, quantum mechanics does not specify the outcome of individual experiments but only the probabilities. Hidden variable theories reject the view that nature contains irreducible randomness: such theories posit that in the processes that appear random, properties with a certain statistical distribution are at work behind the scenes, determining the outcome in each case.



The modern evolutionary synthesis ascribes the observed diversity of life to random genetic mutations followed by natural selection. The latter retains some random mutations in the gene pool due to the systematically improved chance for survival and reproduction that those mutated genes confer on individuals who possess them.
Several authors also claim that evolution and sometimes development require a specific form of randomness, namely the apparition of qualitatively new behaviors. Instead of the choice of one possibility among several pre-given ones, this randomness corresponds to the formation of new possibilities.
The characteristics of an organism arise to some extent deterministically (e.g., under the influence of genes and the environment) and to some extent randomly. For example, the density of freckles that appear on a person's skin is controlled by genes and exposure to light; whereas the exact location of individual freckles seems random.
As far as behavior is concerned, randomness is important if an animal is to behave in a way that is unpredictable to others. For instance, insects in flight tend to move about with random changes in direction, making it difficult for pursuing predators to predict their trajectories.



The mathematical theory of probability arose from attempts to formulate mathematical descriptions of chance events, originally in the context of gambling, but later in connection with physics. Statistics is used to infer the underlying probability distribution of a collection of empirical observations. For the purposes of simulation, it is necessary to have a large supply of random numbers or means to generate them on demand.
Algorithmic information theory studies, among other topics, what constitutes a random sequence. The central idea is that a string of bits is random if and only if it is shorter than any computer program that can produce that string (Kolmogorov randomness)—this means that random strings are those that cannot be compressed. Pioneers of this field include Andrey Kolmogorov and his student Per Martin-Löf, Ray Solomonoff, and Gregory Chaitin. For the notion of infinite sequence, one normally uses Per Martin-Löf's definition. That is, an infinite sequence is random if and only it withstands all recursively enumerable null sets. The other notions of random sequences include (but not limited to): recursive randomness and Schnorr randomness which are based on recursively computable martingales. It is shown in Yongge Wang  that these randomness notions are generally different
Randomness occurs in numbers such as log (2) and pi. The decimal digits of pi constitute an infinite sequence and "never repeat in a cyclical fashion." Numbers like pi are also considered likely to be normal, which means their digits are random in a certain statistical sense.

Pi certainly seems to behave this way. In the first six billion decimal places of pi, each of the digits from 0 through 9 shows up about six hundred million times. Yet such results, conceivably accidental, do not prove normality even in base 10, much less normality in other number bases.




In statistics, randomness is commonly used to create simple random samples. This lets surveys of completely random groups of people provide realistic data. Common methods of doing this include drawing names out of a hat or using a random digit chart. A random digit chart is simply a large table of random digits.



In information science, irrelevant or meaningless data is considered noise. Noise consists of a large number of transient disturbances with a statistically randomized time distribution.
In communication theory, randomness in a signal is called "noise" and is opposed to that component of its variation that is causally attributable to the source, the signal.
In terms of the development of random networks, for communication randomness rests on the two simple assumptions of Paul Erdős and Alfréd Rényi who said that there were a fixed number of nodes and this number remained fixed for the life of the network, and that all nodes were equal and linked randomly to each other.



The random walk hypothesis considers that asset prices in an organized market evolve at random, in the sense that the expected value of their change is zero but the actual value may turn out to be positive or negative. More generally, asset prices are influenced by a variety of unpredictable events in the general economic environment.



Random selection can be an official method to resolve tied elections in some jurisdictions. Its use in politics is very old, as office holders in Ancient Athens were chosen by lot, there being no voting.



Randomness can be seen as conflicting with the deterministic ideas of some religions, such as those where the universe is created by an omniscient deity who is aware of all past and future events. If the universe is regarded to have a purpose, then randomness can be seen as impossible. This is one of the rationales for religious opposition to evolution, which states that non-random selection is applied to the results of random genetic variation.
Hindu and Buddhist philosophies state that any event is the result of previous events, as reflected in the concept of karma, and as such there is no such thing as a random event or a first event.
In some religious contexts, procedures that are commonly perceived as randomizers are used for divination. Cleromancy uses the casting of bones or dice to reveal what is seen as the will of the gods.
Followers of Discordianism, who venerate Eris the Greco-Roman goddess of chaos, have a strong belief in randomness and unpredictability.




In most of its mathematical, political, social and religious uses, randomness is used for its innate "fairness" and lack of bias.
Politics: Athenian democracy was based on the concept of isonomia (equality of political rights) and used complex allotment machines to ensure that the positions on the ruling committees that ran Athens were fairly allocated. Allotment is now restricted to selecting jurors in Anglo-Saxon legal systems and in situations where "fairness" is approximated by randomization, such as selecting jurors and military draft lotteries.
Games: Random numbers were first investigated in the context of gambling, and many randomizing devices, such as dice, shuffling playing cards, and roulette wheels, were first developed for use in gambling. The ability to produce random numbers fairly is vital to electronic gambling, and, as such, the methods used to create them are usually regulated by government Gaming Control Boards. Random drawings are also used to determine lottery winners. Throughout history, randomness has been used for games of chance and to select out individuals for an unwanted task in a fair way (see drawing straws).
Sports: Some sports, including American Football, use coin tosses to randomly select starting conditions for games or seed tied teams for postseason play. The National Basketball Association uses a weighted lottery to order teams in its draft.
Mathematics: Random numbers are also used where their use is mathematically important, such as sampling for opinion polls and for statistical sampling in quality control systems. Computational solutions for some types of problems use random numbers extensively, such as in the Monte Carlo method and in genetic algorithms.
Medicine: Random allocation of a clinical intervention is used to reduce bias in controlled trials (e.g., randomized controlled trials).
Religion: Although not intended to be random, various forms of divination such as cleromancy see what appears to be a random event as a means for a divine being to communicate their will. (See also Free will and Determinism).




It is generally accepted that there exist three mechanisms responsible for (apparently) random behavior in systems:
Randomness coming from the environment (for example, Brownian motion, but also hardware random number generators)
Randomness coming from the initial conditions. This aspect is studied by chaos theory and is observed in systems whose behavior is very sensitive to small variations in initial conditions (such as pachinko machines and dice).
Randomness intrinsically generated by the system. This is also called pseudorandomness and is the kind used in pseudo-random number generators. There are many algorithms (based on arithmetics or cellular automaton) to generate pseudorandom numbers. The behavior of the system can be determined by knowing the seed state and the algorithm used. These methods are often quicker than getting "true" randomness from the environment.
The many applications of randomness have led to many different methods for generating random data. These methods may vary as to how unpredictable or statistically random they are, and how quickly they can generate random numbers.
Before the advent of computational random number generators, generating large amounts of sufficiently random numbers (important in statistics) required a lot of work. Results would sometimes be collected and distributed as random number tables.




There are many practical measures of randomness for a binary sequence. These include measures based on frequency, discrete transforms, and complexity, or a mixture of these. These include tests by Kak, Phillips, Yuen, Hopkins, Beth and Dai, Mund, and Marsaglia and Zaman.
Quantum Non-Locality has been used to certify the presence of genuine randomness in a given string of numbers.




Popular perceptions of randomness are frequently mistaken, based on fallacious reasoning or intuitions.




This argument is, "In a random selection of numbers, since all numbers eventually appear, those that have not come up yet are 'due', and thus more likely to come up soon." This logic is only correct if applied to a system where numbers that come up are removed from the system, such as when playing cards are drawn and not returned to the deck. In this case, once a jack is removed from the deck, the next draw is less likely to be a jack and more likely to be some other card. However, if the jack is returned to the deck, and the deck is thoroughly reshuffled, a jack is as likely to be drawn as any other card. The same applies in any other process where objects are selected independently, and none are removed after each event, such as the roll of a die, a coin toss, or most lottery number selection schemes. Truly random processes such as these do not have memory, making it impossible for past outcomes to affect future outcomes.




In a random sequence of numbers, a number may be said to be cursed because it has come up less often in the past, and so it is thought that it will occur less often in the future. A number may be assumed to be blessed because it has occurred more often than others in the past, and so it is thought likely to come up more often in the future. This logic is valid only if the randomisation is biased, for example with a loaded die. If the die is fair, then previous rolls give no indication of future events.
In nature, events rarely occur with perfectly equal frequency, so observing outcomes to determine which events are more probable makes sense. It is fallacious to apply this logic to systems designed to make all outcomes equally likely, such as shuffled cards, dice, and roulette wheels.



In the beginning of a scenario, one might calculate the probability of a certain event. The fact is, as soon as one gains more information about that situation, they may need to re-calculate the probability.

Say we are told that a woman has two children. If we ask whether either of them is a girl, and are told yes, what is the probability that the other child is also a girl? Considering this new child independently, one might expect the probability that the other child is female is ½ (50%). But by building a probability space (illustrating all possible outcomes), we see that the probability is actually only ⅓ (33%). This is because the possibility space illustrates 4 ways of having these two children: boy-boy, girl-boy, boy-girl, and girl-girl. But we were given more information. Once we are told that one of the children is a female, we use this new information to eliminate the boy-boy scenario. Thus the probability space reveals that there are still 3 ways to have two children where one is a female: boy-girl, girl-boy, girl-girl. Only ⅓ of these scenarios would have the other child also be a girl. Using a probability space, we are less likely to miss one of the possible scenarios, or to neglect the importance of new information. For further information, see Boy or girl paradox.
This technique provides insights in other situations such as the Monty Hall problem, a game show scenario in which a car is hidden behind one of three doors, and two goats are hidden as booby prizes behind the others. Once the contestant has chosen a door, the host opens one of the remaining doors to reveal a goat, eliminating that door as an option. With only two doors left (one with the car, the other with another goat), the player must decide to either keep their decision, or switch and select the other door. Intuitively, one might think the player is choosing between two doors with equal probability, and that the opportunity to choose another door makes no difference. But probability spaces reveal that the contestant has received new information, and can increase their chances of winning by changing to the other door.









Randomness by Deborah J. Bennett. Harvard University Press, 1998. ISBN 0-674-10745-4.
Random Measures, 4th ed. by Olav Kallenberg. Academic Press, New York, London; Akademie-Verlag, Berlin, 1986. MR0854102.
The Art of Computer Programming. Vol. 2: Seminumerical Algorithms, 3rd ed. by Donald E. Knuth. Reading, MA: Addison-Wesley, 1997. ISBN 0-201-89684-2.
Fooled by Randomness, 2nd ed. by Nassim Nicholas Taleb. Thomson Texere, 2004. ISBN 1-58799-190-X.
Exploring Randomness by Gregory Chaitin. Springer-Verlag London, 2001. ISBN 1-85233-417-7.
Random by Kenneth Chan includes a "Random Scale" for grading the level of randomness.
The Drunkard’s Walk: How Randomness Rules our Lives by Leonard Mlodinow. Pantheon Books, New York, 2008. ISBN 978-0-375-42404-5.



An 8-foot-tall (2.4 m) Probability Machine (named Sir Francis) comparing stock market returns to the randomness of the beans dropping through the quincunx pattern. on YouTube from Index Funds Advisors IFA.com
QuantumLab Quantum random number generator with single photons as interactive experiment.
Random.org generates random numbers using atmospheric noises (see also Random.org).
HotBits generates random numbers from radioactive decay.
QRBG Quantum Random Bit Generator
QRNG Fast Quantum Random Bit Generator
Chaitin: Randomness and Mathematical Proof
A Pseudorandom Number Sequence Test Program (Public Domain)
Dictionary of the History of Ideas: Chance
Philosophy: Free Will vs. Determinism
RAHM Nation Institute
History of randomness definitions, in Stephen Wolfram's A New Kind of Science
Computing a Glimpse of Randomness
Chance versus Randomness, from the Stanford Encyclopedia of Philosophy